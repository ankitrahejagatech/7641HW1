{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 45307 rows and 21 columns.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df_bank = pd.read_csv('/workspaces/7641HW1/data/BankMarketingData.csv')\n",
    "print(\"Data has\",len(df_bank),\"rows and\", len(df_bank.columns),\"columns.\")\n",
    "if df_bank.isnull().values.any():\n",
    "    print(\"Warning: Missing Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>housemaid</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.4y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.6y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>32</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.6y</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>wed</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.856</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>41</td>\n",
       "      <td>services</td>\n",
       "      <td>single</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>wed</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.856</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>59</td>\n",
       "      <td>housemaid</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.6y</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>wed</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.856</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>57</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.9y</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>wed</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.856</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>30</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>wed</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.856</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age           job  marital    education  default housing loan    contact  \\\n",
       "0     56     housemaid  married     basic.4y       no      no   no  telephone   \n",
       "1     57      services  married  high.school  unknown      no   no  telephone   \n",
       "2     37      services  married  high.school       no     yes   no  telephone   \n",
       "3     40        admin.  married     basic.6y       no      no   no  telephone   \n",
       "4     56      services  married  high.school       no      no  yes  telephone   \n",
       "..   ...           ...      ...          ...      ...     ...  ...        ...   \n",
       "995   32  entrepreneur  married     basic.6y       no     yes   no  telephone   \n",
       "996   41      services   single  high.school       no     yes  yes  telephone   \n",
       "997   59     housemaid  married     basic.6y       no     yes   no  telephone   \n",
       "998   57    technician  married     basic.9y       no     yes   no  telephone   \n",
       "999   30      services  married      unknown       no      no   no  telephone   \n",
       "\n",
       "    month day_of_week  ...  campaign  pdays  previous     poutcome  \\\n",
       "0     may         mon  ...         1    999         0  nonexistent   \n",
       "1     may         mon  ...         1    999         0  nonexistent   \n",
       "2     may         mon  ...         1    999         0  nonexistent   \n",
       "3     may         mon  ...         1    999         0  nonexistent   \n",
       "4     may         mon  ...         1    999         0  nonexistent   \n",
       "..    ...         ...  ...       ...    ...       ...          ...   \n",
       "995   may         wed  ...         1    999         0  nonexistent   \n",
       "996   may         wed  ...         1    999         0  nonexistent   \n",
       "997   may         wed  ...         1    999         0  nonexistent   \n",
       "998   may         wed  ...         1    999         0  nonexistent   \n",
       "999   may         wed  ...         3    999         0  nonexistent   \n",
       "\n",
       "    emp.var.rate  cons.price.idx  cons.conf.idx  euribor3m  nr.employed   y  \n",
       "0            1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "1            1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "2            1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "3            1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "4            1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "..           ...             ...            ...        ...          ...  ..  \n",
       "995          1.1          93.994          -36.4      4.856       5191.0  no  \n",
       "996          1.1          93.994          -36.4      4.856       5191.0  no  \n",
       "997          1.1          93.994          -36.4      4.856       5191.0  no  \n",
       "998          1.1          93.994          -36.4      4.856       5191.0  no  \n",
       "999          1.1          93.994          -36.4      4.856       5191.0  no  \n",
       "\n",
       "[1000 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_df = df_bank [:1000]\n",
    "subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35669/1444988200.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_bank['y'].replace(\"no\",0,inplace=True)\n",
      "/tmp/ipykernel_35669/1444988200.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_bank['y'].replace(\"yes\",1,inplace=True)\n",
      "/tmp/ipykernel_35669/1444988200.py:10: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_bank['y'].replace(\"yes\",1,inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>job_admin.</th>\n",
       "      <th>job_blue-collar</th>\n",
       "      <th>job_entrepreneur</th>\n",
       "      <th>job_housemaid</th>\n",
       "      <th>job_management</th>\n",
       "      <th>job_retired</th>\n",
       "      <th>job_self-employed</th>\n",
       "      <th>job_services</th>\n",
       "      <th>job_student</th>\n",
       "      <th>...</th>\n",
       "      <th>age</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>45307.0</td>\n",
       "      <td>45307</td>\n",
       "      <td>45307</td>\n",
       "      <td>45307</td>\n",
       "      <td>45307</td>\n",
       "      <td>45307</td>\n",
       "      <td>45307</td>\n",
       "      <td>45307</td>\n",
       "      <td>45307</td>\n",
       "      <td>45307</td>\n",
       "      <td>...</td>\n",
       "      <td>45307.000000</td>\n",
       "      <td>45307.000000</td>\n",
       "      <td>45307.000000</td>\n",
       "      <td>45307.000000</td>\n",
       "      <td>45307.000000</td>\n",
       "      <td>45307.000000</td>\n",
       "      <td>45307.000000</td>\n",
       "      <td>45307.000000</td>\n",
       "      <td>45307.000000</td>\n",
       "      <td>45307.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>40216.0</td>\n",
       "      <td>33873</td>\n",
       "      <td>35169</td>\n",
       "      <td>43703</td>\n",
       "      <td>44137</td>\n",
       "      <td>42059</td>\n",
       "      <td>43421</td>\n",
       "      <td>43727</td>\n",
       "      <td>40945</td>\n",
       "      <td>44350</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.284348</td>\n",
       "      <td>0.052491</td>\n",
       "      <td>0.028452</td>\n",
       "      <td>0.963252</td>\n",
       "      <td>0.024935</td>\n",
       "      <td>0.725451</td>\n",
       "      <td>0.535866</td>\n",
       "      <td>0.430867</td>\n",
       "      <td>0.677238</td>\n",
       "      <td>0.768943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128536</td>\n",
       "      <td>0.052636</td>\n",
       "      <td>0.050041</td>\n",
       "      <td>0.187558</td>\n",
       "      <td>0.071338</td>\n",
       "      <td>0.327131</td>\n",
       "      <td>0.225597</td>\n",
       "      <td>0.193519</td>\n",
       "      <td>0.393188</td>\n",
       "      <td>0.273651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.020740</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.340608</td>\n",
       "      <td>0.338912</td>\n",
       "      <td>0.160961</td>\n",
       "      <td>0.512287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.036600</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.603274</td>\n",
       "      <td>0.376569</td>\n",
       "      <td>0.957379</td>\n",
       "      <td>0.859735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.064864</td>\n",
       "      <td>0.036364</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.698753</td>\n",
       "      <td>0.602510</td>\n",
       "      <td>0.980957</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              y job_admin. job_blue-collar job_entrepreneur job_housemaid  \\\n",
       "count   45307.0      45307           45307            45307         45307   \n",
       "unique      2.0          2               2                2             2   \n",
       "top         0.0      False           False            False         False   \n",
       "freq    40216.0      33873           35169            43703         44137   \n",
       "mean        NaN        NaN             NaN              NaN           NaN   \n",
       "std         NaN        NaN             NaN              NaN           NaN   \n",
       "min         NaN        NaN             NaN              NaN           NaN   \n",
       "25%         NaN        NaN             NaN              NaN           NaN   \n",
       "50%         NaN        NaN             NaN              NaN           NaN   \n",
       "75%         NaN        NaN             NaN              NaN           NaN   \n",
       "max         NaN        NaN             NaN              NaN           NaN   \n",
       "\n",
       "       job_management job_retired job_self-employed job_services job_student  \\\n",
       "count           45307       45307             45307        45307       45307   \n",
       "unique              2           2                 2            2           2   \n",
       "top             False       False             False        False       False   \n",
       "freq            42059       43421             43727        40945       44350   \n",
       "mean              NaN         NaN               NaN          NaN         NaN   \n",
       "std               NaN         NaN               NaN          NaN         NaN   \n",
       "min               NaN         NaN               NaN          NaN         NaN   \n",
       "25%               NaN         NaN               NaN          NaN         NaN   \n",
       "50%               NaN         NaN               NaN          NaN         NaN   \n",
       "75%               NaN         NaN               NaN          NaN         NaN   \n",
       "max               NaN         NaN               NaN          NaN         NaN   \n",
       "\n",
       "        ...           age      duration      campaign         pdays  \\\n",
       "count   ...  45307.000000  45307.000000  45307.000000  45307.000000   \n",
       "unique  ...           NaN           NaN           NaN           NaN   \n",
       "top     ...           NaN           NaN           NaN           NaN   \n",
       "freq    ...           NaN           NaN           NaN           NaN   \n",
       "mean    ...      0.284348      0.052491      0.028452      0.963252   \n",
       "std     ...      0.128536      0.052636      0.050041      0.187558   \n",
       "min     ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%     ...      0.185185      0.020740      0.000000      1.000000   \n",
       "50%     ...      0.259259      0.036600      0.018182      1.000000   \n",
       "75%     ...      0.370370      0.064864      0.036364      1.000000   \n",
       "max     ...      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "            previous  emp.var.rate cons.price.idx cons.conf.idx     euribor3m  \\\n",
       "count   45307.000000  45307.000000   45307.000000  45307.000000  45307.000000   \n",
       "unique           NaN           NaN            NaN           NaN           NaN   \n",
       "top              NaN           NaN            NaN           NaN           NaN   \n",
       "freq             NaN           NaN            NaN           NaN           NaN   \n",
       "mean        0.024935      0.725451       0.535866      0.430867      0.677238   \n",
       "std         0.071338      0.327131       0.225597      0.193519      0.393188   \n",
       "min         0.000000      0.000000       0.000000      0.000000      0.000000   \n",
       "25%         0.000000      0.333333       0.340608      0.338912      0.160961   \n",
       "50%         0.000000      0.937500       0.603274      0.376569      0.957379   \n",
       "75%         0.000000      1.000000       0.698753      0.602510      0.980957   \n",
       "max         1.000000      1.000000       1.000000      1.000000      1.000000   \n",
       "\n",
       "         nr.employed  \n",
       "count   45307.000000  \n",
       "unique           NaN  \n",
       "top              NaN  \n",
       "freq             NaN  \n",
       "mean        0.768943  \n",
       "std         0.273651  \n",
       "min         0.000000  \n",
       "25%         0.512287  \n",
       "50%         0.859735  \n",
       "75%         1.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[11 rows x 64 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "col_1hot = ['job','marital','education','default','housing','loan','contact','month','day_of_week','poutcome']\n",
    "df_1hot = df_bank[col_1hot]\n",
    "df_1hot = pd.get_dummies(df_1hot).astype('category')\n",
    "df_others = df_bank.drop(col_1hot,axis=1)\n",
    "df_bank = pd.concat([df_others,df_1hot],axis=1)\n",
    "column_order = list(df_bank)\n",
    "column_order.insert(0, column_order.pop(column_order.index('y')))\n",
    "df_bank = df_bank.loc[:, column_order]\n",
    "df_bank['y'].replace(\"no\",0,inplace=True)\n",
    "df_bank['y'].replace(\"yes\",1,inplace=True)\n",
    "df_bank['y'] = df_bank['y'].astype('category')\n",
    "\n",
    "numericcols = ['age','duration','campaign','pdays','previous','emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m','nr.employed']\n",
    "df_num = df_bank[numericcols]\n",
    "df_stand =(df_num-df_num.min())/(df_num.max()-df_num.min())\n",
    "df_bank_categorical = df_bank.drop(numericcols,axis=1)\n",
    "df_bank = pd.concat([df_bank_categorical,df_stand],axis=1)\n",
    "df_bank.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_validate, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import itertools\n",
    "import timeit\n",
    "\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "def import_data():\n",
    "\n",
    "    #X1 = np.array(df_phish.values[:,1:-1],dtype='int64')\n",
    "    #Y1 = np.array(df_phish.values[:,0],dtype='int64')\n",
    "    X2 = np.array(df_bank.values[:,1:-1],dtype='int64')\n",
    "    Y2 = np.array(df_bank.values[:,0],dtype='int64')\n",
    "    return X2, Y2\n",
    "\n",
    "\n",
    "def plot_learning_curve(clf, X, y, title=\"Insert Title\"):\n",
    "    \n",
    "    n = len(y)\n",
    "    train_mean = []; train_std = [] #model performance score (f1)\n",
    "    cv_mean = []; cv_std = [] #model performance score (f1)\n",
    "    fit_mean = []; fit_std = [] #model fit/training time\n",
    "    pred_mean = []; pred_std = [] #model test/prediction times\n",
    "    train_sizes=(np.linspace(.05, 1.0, 20)*n).astype('int')  \n",
    "    \n",
    "    for i in train_sizes:\n",
    "        idx = np.random.randint(X.shape[0], size=i)\n",
    "        X_subset = X[idx,:]\n",
    "        y_subset = y[idx]\n",
    "        scores = cross_validate(clf, X_subset, y_subset, cv=10, scoring='f1', n_jobs=-1, return_train_score=True)\n",
    "        \n",
    "        train_mean.append(np.mean(scores['train_score'])); train_std.append(np.std(scores['train_score']))\n",
    "        cv_mean.append(np.mean(scores['test_score'])); cv_std.append(np.std(scores['test_score']))\n",
    "        fit_mean.append(np.mean(scores['fit_time'])); fit_std.append(np.std(scores['fit_time']))\n",
    "        pred_mean.append(np.mean(scores['score_time'])); pred_std.append(np.std(scores['score_time']))\n",
    "    \n",
    "    train_mean = np.array(train_mean); train_std = np.array(train_std)\n",
    "    cv_mean = np.array(cv_mean); cv_std = np.array(cv_std)\n",
    "    fit_mean = np.array(fit_mean); fit_std = np.array(fit_std)\n",
    "    pred_mean = np.array(pred_mean); pred_std = np.array(pred_std)\n",
    "    \n",
    "    plot_LC(train_sizes, train_mean, train_std, cv_mean, cv_std, title)\n",
    "    plot_times(train_sizes, fit_mean, fit_std, pred_mean, pred_std, title)\n",
    "    \n",
    "    return train_sizes, train_mean, fit_mean, pred_mean\n",
    "    \n",
    "\n",
    "def plot_LC(train_sizes, train_mean, train_std, cv_mean, cv_std, title):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Learning Curve: \"+ title)\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(\"Model F1 Score\")\n",
    "    plt.fill_between(train_sizes, train_mean - 2*train_std, train_mean + 2*train_std, alpha=0.1, color=\"b\")\n",
    "    plt.fill_between(train_sizes, cv_mean - 2*cv_std, cv_mean + 2*cv_std, alpha=0.1, color=\"r\")\n",
    "    plt.plot(train_sizes, train_mean, 'o-', color=\"b\", label=\"Training Score\")\n",
    "    plt.plot(train_sizes, cv_mean, 'o-', color=\"r\", label=\"Cross-Validation Score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_times(train_sizes, fit_mean, fit_std, pred_mean, pred_std, title):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Modeling Time: \"+ title)\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(\"Training Time (s)\")\n",
    "    plt.fill_between(train_sizes, fit_mean - 2*fit_std, fit_mean + 2*fit_std, alpha=0.1, color=\"b\")\n",
    "    plt.fill_between(train_sizes, pred_mean - 2*pred_std, pred_mean + 2*pred_std, alpha=0.1, color=\"r\")\n",
    "    plt.plot(train_sizes, fit_mean, 'o-', color=\"b\", label=\"Training Time (s)\")\n",
    "    plt.plot(train_sizes, pred_std, 'o-', color=\"r\", label=\"Prediction Time (s)\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(2), range(2)):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    \n",
    "    \n",
    "def final_classifier_evaluation(clf,X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end_time = timeit.default_timer()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    start_time = timeit.default_timer()    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    end_time = timeit.default_timer()\n",
    "    pred_time = end_time - start_time\n",
    "    \n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    recall = recall_score(y_test,y_pred)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "    print(\"Model Evaluation Metrics Using Untouched Test Dataset\")\n",
    "    print(\"*****************************************************\")\n",
    "    print(\"Model Training Time (s):   \"+\"{:.5f}\".format(training_time))\n",
    "    print(\"Model Prediction Time (s): \"+\"{:.5f}\\n\".format(pred_time))\n",
    "    print(\"F1 Score:  \"+\"{:.2f}\".format(f1))\n",
    "    print(\"Accuracy:  \"+\"{:.2f}\".format(accuracy)+\"     AUC:       \"+\"{:.2f}\".format(auc))\n",
    "    print(\"Precision: \"+\"{:.2f}\".format(precision)+\"     Recall:    \"+\"{:.2f}\".format(recall))\n",
    "    print(\"*****************************************************\")\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cm, classes=[\"0\",\"1\"], title='Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def hyperNN(X_train, y_train, X_test, y_test, title):\n",
    "\n",
    "    f1_test = []\n",
    "    f1_train = []\n",
    "    hlist = np.linspace(1,90,30).astype('int')\n",
    "    for i in hlist:         \n",
    "            clf = MLPClassifier(hidden_layer_sizes=(i,), solver='adam', activation='logistic', \n",
    "                                learning_rate_init=0.1, random_state=100,verbose=True)\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred_test = clf.predict(X_test)\n",
    "            y_pred_train = clf.predict(X_train)\n",
    "            f1_test.append(f1_score(y_test, y_pred_test))\n",
    "            f1_train.append(f1_score(y_train, y_pred_train))\n",
    "      \n",
    "    plt.plot(hlist, f1_test, 'o-', color='r', label='Test F1 Score')\n",
    "    plt.plot(hlist, f1_train, 'o-', color = 'b', label='Train F1 Score')\n",
    "    plt.ylabel('Model F1 Score')\n",
    "    plt.xlabel('No. Hidden Units')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def NNGridSearchCV(X_train, y_train):\n",
    "    #parameters to search:\n",
    "    #number of hidden units\n",
    "    #learning_rate\n",
    "    h_units = [5, 10, 20, 30, 40, 50, 75, 100]\n",
    "    learning_rates = [0.01, 0.05, .1]\n",
    "    param_grid = {'hidden_layer_sizes': h_units, 'learning_rate_init': learning_rates}\n",
    "\n",
    "    net = GridSearchCV(estimator = MLPClassifier(solver='adam',activation='logistic',random_state=100),\n",
    "                       param_grid=param_grid, cv=3,verbose=True)\n",
    "    net.fit(X_train, y_train)\n",
    "    print(\"Per Hyperparameter tuning, best parameters are:\")\n",
    "    print(net.best_params_)\n",
    "    return net.best_params_['hidden_layer_sizes'], net.best_params_['learning_rate_init']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = np.array(df_bank.values[:,1:-1],dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.36242032\n",
      "Iteration 2, loss = 0.35055736\n",
      "Iteration 3, loss = 0.29295276\n",
      "Iteration 4, loss = 0.28753920\n",
      "Iteration 5, loss = 0.28706678\n",
      "Iteration 6, loss = 0.28718639\n",
      "Iteration 7, loss = 0.28786214\n",
      "Iteration 8, loss = 0.28670038\n",
      "Iteration 9, loss = 0.28634482\n",
      "Iteration 10, loss = 0.28695031\n",
      "Iteration 11, loss = 0.28688060\n",
      "Iteration 12, loss = 0.28624679\n",
      "Iteration 13, loss = 0.28616109\n",
      "Iteration 14, loss = 0.28642824\n",
      "Iteration 15, loss = 0.28601429\n",
      "Iteration 16, loss = 0.28581336\n",
      "Iteration 17, loss = 0.28583175\n",
      "Iteration 18, loss = 0.28634292\n",
      "Iteration 19, loss = 0.28598237\n",
      "Iteration 20, loss = 0.28646181\n",
      "Iteration 21, loss = 0.28649642\n",
      "Iteration 22, loss = 0.28583607\n",
      "Iteration 23, loss = 0.28603888\n",
      "Iteration 24, loss = 0.28686153\n",
      "Iteration 25, loss = 0.28682440\n",
      "Iteration 26, loss = 0.28616814\n",
      "Iteration 27, loss = 0.28621075\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.36579039\n",
      "Iteration 2, loss = 0.33975992\n",
      "Iteration 3, loss = 0.28953824\n",
      "Iteration 4, loss = 0.28780069\n",
      "Iteration 5, loss = 0.28638479\n",
      "Iteration 6, loss = 0.28657429\n",
      "Iteration 7, loss = 0.28608232\n",
      "Iteration 8, loss = 0.28703204\n",
      "Iteration 9, loss = 0.28655748\n",
      "Iteration 10, loss = 0.28632734\n",
      "Iteration 11, loss = 0.28585139\n",
      "Iteration 12, loss = 0.28560856\n",
      "Iteration 13, loss = 0.28569892\n",
      "Iteration 14, loss = 0.28580485\n",
      "Iteration 15, loss = 0.28402241\n",
      "Iteration 16, loss = 0.28399161\n",
      "Iteration 17, loss = 0.28279573\n",
      "Iteration 18, loss = 0.28271847\n",
      "Iteration 19, loss = 0.28246848\n",
      "Iteration 20, loss = 0.28306714\n",
      "Iteration 21, loss = 0.28306474\n",
      "Iteration 22, loss = 0.28220629\n",
      "Iteration 23, loss = 0.28262252\n",
      "Iteration 24, loss = 0.28133022\n",
      "Iteration 25, loss = 0.28103624\n",
      "Iteration 26, loss = 0.28120061\n",
      "Iteration 27, loss = 0.28248923\n",
      "Iteration 28, loss = 0.28120582\n",
      "Iteration 29, loss = 0.28127397\n",
      "Iteration 30, loss = 0.28057594\n",
      "Iteration 31, loss = 0.28097987\n",
      "Iteration 32, loss = 0.28070621\n",
      "Iteration 33, loss = 0.28156047\n",
      "Iteration 34, loss = 0.28127830\n",
      "Iteration 35, loss = 0.28061182\n",
      "Iteration 36, loss = 0.28198051\n",
      "Iteration 37, loss = 0.28087900\n",
      "Iteration 38, loss = 0.28009522\n",
      "Iteration 39, loss = 0.28013785\n",
      "Iteration 40, loss = 0.28147248\n",
      "Iteration 41, loss = 0.28026812\n",
      "Iteration 42, loss = 0.28002705\n",
      "Iteration 43, loss = 0.28106770\n",
      "Iteration 44, loss = 0.28096824\n",
      "Iteration 45, loss = 0.28041926\n",
      "Iteration 46, loss = 0.28039813\n",
      "Iteration 47, loss = 0.28052283\n",
      "Iteration 48, loss = 0.28031566\n",
      "Iteration 49, loss = 0.28079765\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30159748\n",
      "Iteration 2, loss = 0.28278286\n",
      "Iteration 3, loss = 0.28269387\n",
      "Iteration 4, loss = 0.28145876\n",
      "Iteration 5, loss = 0.28135071\n",
      "Iteration 6, loss = 0.27950767\n",
      "Iteration 7, loss = 0.27983854\n",
      "Iteration 8, loss = 0.27954916\n",
      "Iteration 9, loss = 0.27936778\n",
      "Iteration 10, loss = 0.27947787\n",
      "Iteration 11, loss = 0.27850896\n",
      "Iteration 12, loss = 0.27909976\n",
      "Iteration 13, loss = 0.27857231\n",
      "Iteration 14, loss = 0.27804791\n",
      "Iteration 15, loss = 0.27842491\n",
      "Iteration 16, loss = 0.27787014\n",
      "Iteration 17, loss = 0.27838771\n",
      "Iteration 18, loss = 0.27799538\n",
      "Iteration 19, loss = 0.27894634\n",
      "Iteration 20, loss = 0.27738510\n",
      "Iteration 21, loss = 0.27854991\n",
      "Iteration 22, loss = 0.27727239\n",
      "Iteration 23, loss = 0.27806227\n",
      "Iteration 24, loss = 0.27932989\n",
      "Iteration 25, loss = 0.27734912\n",
      "Iteration 26, loss = 0.27758214\n",
      "Iteration 27, loss = 0.27831414\n",
      "Iteration 28, loss = 0.27751898\n",
      "Iteration 29, loss = 0.28003670\n",
      "Iteration 30, loss = 0.27835984\n",
      "Iteration 31, loss = 0.27773341\n",
      "Iteration 32, loss = 0.27851515\n",
      "Iteration 33, loss = 0.27819974\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.32019102\n",
      "Iteration 2, loss = 0.28295701\n",
      "Iteration 3, loss = 0.28167963\n",
      "Iteration 4, loss = 0.28112302\n",
      "Iteration 5, loss = 0.28119479\n",
      "Iteration 6, loss = 0.28136017\n",
      "Iteration 7, loss = 0.28027132\n",
      "Iteration 8, loss = 0.27938348\n",
      "Iteration 9, loss = 0.28017397\n",
      "Iteration 10, loss = 0.27923341\n",
      "Iteration 11, loss = 0.27951050\n",
      "Iteration 12, loss = 0.27937543\n",
      "Iteration 13, loss = 0.27958036\n",
      "Iteration 14, loss = 0.27865384\n",
      "Iteration 15, loss = 0.28002527\n",
      "Iteration 16, loss = 0.28019344\n",
      "Iteration 17, loss = 0.27954684\n",
      "Iteration 18, loss = 0.27874727\n",
      "Iteration 19, loss = 0.28028767\n",
      "Iteration 20, loss = 0.27966811\n",
      "Iteration 21, loss = 0.27881263\n",
      "Iteration 22, loss = 0.27939961\n",
      "Iteration 23, loss = 0.27969506\n",
      "Iteration 24, loss = 0.27956809\n",
      "Iteration 25, loss = 0.27824848\n",
      "Iteration 26, loss = 0.27805875\n",
      "Iteration 27, loss = 0.27841318\n",
      "Iteration 28, loss = 0.27762193\n",
      "Iteration 29, loss = 0.27989871\n",
      "Iteration 30, loss = 0.27965133\n",
      "Iteration 31, loss = 0.27823362\n",
      "Iteration 32, loss = 0.27896315\n",
      "Iteration 33, loss = 0.27768363\n",
      "Iteration 34, loss = 0.27873185\n",
      "Iteration 35, loss = 0.27815404\n",
      "Iteration 36, loss = 0.27792306\n",
      "Iteration 37, loss = 0.27842700\n",
      "Iteration 38, loss = 0.27781147\n",
      "Iteration 39, loss = 0.27648498\n",
      "Iteration 40, loss = 0.27768789\n",
      "Iteration 41, loss = 0.27957568\n",
      "Iteration 42, loss = 0.27759068\n",
      "Iteration 43, loss = 0.27772292\n",
      "Iteration 44, loss = 0.27790177\n",
      "Iteration 45, loss = 0.27734967\n",
      "Iteration 46, loss = 0.27767808\n",
      "Iteration 47, loss = 0.27848746\n",
      "Iteration 48, loss = 0.27747991\n",
      "Iteration 49, loss = 0.27798811\n",
      "Iteration 50, loss = 0.27731665\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30107881\n",
      "Iteration 2, loss = 0.28499545\n",
      "Iteration 3, loss = 0.28161448\n",
      "Iteration 4, loss = 0.27985644\n",
      "Iteration 5, loss = 0.27978867\n",
      "Iteration 6, loss = 0.27938783\n",
      "Iteration 7, loss = 0.27878394\n",
      "Iteration 8, loss = 0.27916938\n",
      "Iteration 9, loss = 0.27831705\n",
      "Iteration 10, loss = 0.27932349\n",
      "Iteration 11, loss = 0.27763370\n",
      "Iteration 12, loss = 0.27757172\n",
      "Iteration 13, loss = 0.27825033\n",
      "Iteration 14, loss = 0.27718558\n",
      "Iteration 15, loss = 0.27670519\n",
      "Iteration 16, loss = 0.27726007\n",
      "Iteration 17, loss = 0.27637703\n",
      "Iteration 18, loss = 0.27626946\n",
      "Iteration 19, loss = 0.27708285\n",
      "Iteration 20, loss = 0.27821742\n",
      "Iteration 21, loss = 0.27632613\n",
      "Iteration 22, loss = 0.27622746\n",
      "Iteration 23, loss = 0.27657600\n",
      "Iteration 24, loss = 0.27778634\n",
      "Iteration 25, loss = 0.27604420\n",
      "Iteration 26, loss = 0.27690699\n",
      "Iteration 27, loss = 0.27591894\n",
      "Iteration 28, loss = 0.27654432\n",
      "Iteration 29, loss = 0.27682058\n",
      "Iteration 30, loss = 0.27616459\n",
      "Iteration 31, loss = 0.27501491\n",
      "Iteration 32, loss = 0.27492785\n",
      "Iteration 33, loss = 0.27560357\n",
      "Iteration 34, loss = 0.27551012\n",
      "Iteration 35, loss = 0.27495086\n",
      "Iteration 36, loss = 0.27545131\n",
      "Iteration 37, loss = 0.27586137\n",
      "Iteration 38, loss = 0.27613651\n",
      "Iteration 39, loss = 0.27642120\n",
      "Iteration 40, loss = 0.27553383\n",
      "Iteration 41, loss = 0.27600130\n",
      "Iteration 42, loss = 0.27468862\n",
      "Iteration 43, loss = 0.27608981\n",
      "Iteration 44, loss = 0.27476129\n",
      "Iteration 45, loss = 0.27546863\n",
      "Iteration 46, loss = 0.27667857\n",
      "Iteration 47, loss = 0.27550145\n",
      "Iteration 48, loss = 0.27530356\n",
      "Iteration 49, loss = 0.27544319\n",
      "Iteration 50, loss = 0.27431492\n",
      "Iteration 51, loss = 0.27561427\n",
      "Iteration 52, loss = 0.27357551\n",
      "Iteration 53, loss = 0.27488549\n",
      "Iteration 54, loss = 0.27470572\n",
      "Iteration 55, loss = 0.27477399\n",
      "Iteration 56, loss = 0.27484780\n",
      "Iteration 57, loss = 0.27419511\n",
      "Iteration 58, loss = 0.27537410\n",
      "Iteration 59, loss = 0.27449412\n",
      "Iteration 60, loss = 0.27465165\n",
      "Iteration 61, loss = 0.27597446\n",
      "Iteration 62, loss = 0.27399950\n",
      "Iteration 63, loss = 0.27371695\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29360219\n",
      "Iteration 2, loss = 0.28400753\n",
      "Iteration 3, loss = 0.28103584\n",
      "Iteration 4, loss = 0.28117390\n",
      "Iteration 5, loss = 0.27977665\n",
      "Iteration 6, loss = 0.27922390\n",
      "Iteration 7, loss = 0.27936218\n",
      "Iteration 8, loss = 0.27718173\n",
      "Iteration 9, loss = 0.27856740\n",
      "Iteration 10, loss = 0.27733085\n",
      "Iteration 11, loss = 0.27813980\n",
      "Iteration 12, loss = 0.27893869\n",
      "Iteration 13, loss = 0.27785770\n",
      "Iteration 14, loss = 0.27774723\n",
      "Iteration 15, loss = 0.27796148\n",
      "Iteration 16, loss = 0.27776306\n",
      "Iteration 17, loss = 0.27779851\n",
      "Iteration 18, loss = 0.27827589\n",
      "Iteration 19, loss = 0.27681091\n",
      "Iteration 20, loss = 0.27724984\n",
      "Iteration 21, loss = 0.27677510\n",
      "Iteration 22, loss = 0.27634041\n",
      "Iteration 23, loss = 0.27832945\n",
      "Iteration 24, loss = 0.27960205\n",
      "Iteration 25, loss = 0.27698919\n",
      "Iteration 26, loss = 0.27711869\n",
      "Iteration 27, loss = 0.27665755\n",
      "Iteration 28, loss = 0.27759296\n",
      "Iteration 29, loss = 0.27697285\n",
      "Iteration 30, loss = 0.27686123\n",
      "Iteration 31, loss = 0.27643233\n",
      "Iteration 32, loss = 0.27652078\n",
      "Iteration 33, loss = 0.27706308\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29750288\n",
      "Iteration 2, loss = 0.28083023\n",
      "Iteration 3, loss = 0.28365256\n",
      "Iteration 4, loss = 0.28168082\n",
      "Iteration 5, loss = 0.27997493\n",
      "Iteration 6, loss = 0.27935423\n",
      "Iteration 7, loss = 0.27986136\n",
      "Iteration 8, loss = 0.27996182\n",
      "Iteration 9, loss = 0.27859296\n",
      "Iteration 10, loss = 0.27975974\n",
      "Iteration 11, loss = 0.28114575\n",
      "Iteration 12, loss = 0.27805932\n",
      "Iteration 13, loss = 0.27859047\n",
      "Iteration 14, loss = 0.27840381\n",
      "Iteration 15, loss = 0.27787536\n",
      "Iteration 16, loss = 0.27786745\n",
      "Iteration 17, loss = 0.27694877\n",
      "Iteration 18, loss = 0.27744899\n",
      "Iteration 19, loss = 0.27725687\n",
      "Iteration 20, loss = 0.27678447\n",
      "Iteration 21, loss = 0.27705908\n",
      "Iteration 22, loss = 0.27699619\n",
      "Iteration 23, loss = 0.27757800\n",
      "Iteration 24, loss = 0.27649640\n",
      "Iteration 25, loss = 0.27688993\n",
      "Iteration 26, loss = 0.27560890\n",
      "Iteration 27, loss = 0.27772859\n",
      "Iteration 28, loss = 0.27653253\n",
      "Iteration 29, loss = 0.27790788\n",
      "Iteration 30, loss = 0.27535975\n",
      "Iteration 31, loss = 0.27568364\n",
      "Iteration 32, loss = 0.27547992\n",
      "Iteration 33, loss = 0.27616191\n",
      "Iteration 34, loss = 0.27551047\n",
      "Iteration 35, loss = 0.27522110\n",
      "Iteration 36, loss = 0.27522185\n",
      "Iteration 37, loss = 0.27510105\n",
      "Iteration 38, loss = 0.27463410\n",
      "Iteration 39, loss = 0.27426718\n",
      "Iteration 40, loss = 0.27555613\n",
      "Iteration 41, loss = 0.27543157\n",
      "Iteration 42, loss = 0.27387168\n",
      "Iteration 43, loss = 0.27445593\n",
      "Iteration 44, loss = 0.27543004\n",
      "Iteration 45, loss = 0.27500345\n",
      "Iteration 46, loss = 0.27541021\n",
      "Iteration 47, loss = 0.27361715\n",
      "Iteration 48, loss = 0.27474849\n",
      "Iteration 49, loss = 0.27464649\n",
      "Iteration 50, loss = 0.27475200\n",
      "Iteration 51, loss = 0.27360142\n",
      "Iteration 52, loss = 0.27382522\n",
      "Iteration 53, loss = 0.27513531\n",
      "Iteration 54, loss = 0.27521551\n",
      "Iteration 55, loss = 0.27397394\n",
      "Iteration 56, loss = 0.27338938\n",
      "Iteration 57, loss = 0.27324280\n",
      "Iteration 58, loss = 0.27567492\n",
      "Iteration 59, loss = 0.27363634\n",
      "Iteration 60, loss = 0.27461422\n",
      "Iteration 61, loss = 0.27508395\n",
      "Iteration 62, loss = 0.27372005\n",
      "Iteration 63, loss = 0.27414717\n",
      "Iteration 64, loss = 0.27520566\n",
      "Iteration 65, loss = 0.27467587\n",
      "Iteration 66, loss = 0.27398504\n",
      "Iteration 67, loss = 0.27431081\n",
      "Iteration 68, loss = 0.27380393\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29683640\n",
      "Iteration 2, loss = 0.28408001\n",
      "Iteration 3, loss = 0.28083199\n",
      "Iteration 4, loss = 0.28113717\n",
      "Iteration 5, loss = 0.27958952\n",
      "Iteration 6, loss = 0.28029286\n",
      "Iteration 7, loss = 0.27934351\n",
      "Iteration 8, loss = 0.27844307\n",
      "Iteration 9, loss = 0.27877027\n",
      "Iteration 10, loss = 0.27784613\n",
      "Iteration 11, loss = 0.27736571\n",
      "Iteration 12, loss = 0.27837913\n",
      "Iteration 13, loss = 0.27720037\n",
      "Iteration 14, loss = 0.27711955\n",
      "Iteration 15, loss = 0.27776565\n",
      "Iteration 16, loss = 0.28007512\n",
      "Iteration 17, loss = 0.27898284\n",
      "Iteration 18, loss = 0.27773654\n",
      "Iteration 19, loss = 0.27648447\n",
      "Iteration 20, loss = 0.27834014\n",
      "Iteration 21, loss = 0.27654060\n",
      "Iteration 22, loss = 0.27720670\n",
      "Iteration 23, loss = 0.27651405\n",
      "Iteration 24, loss = 0.27654473\n",
      "Iteration 25, loss = 0.27696095\n",
      "Iteration 26, loss = 0.27705966\n",
      "Iteration 27, loss = 0.27689953\n",
      "Iteration 28, loss = 0.27802552\n",
      "Iteration 29, loss = 0.27665223\n",
      "Iteration 30, loss = 0.27542012\n",
      "Iteration 31, loss = 0.27659820\n",
      "Iteration 32, loss = 0.27589119\n",
      "Iteration 33, loss = 0.27628772\n",
      "Iteration 34, loss = 0.27481926\n",
      "Iteration 35, loss = 0.27754846\n",
      "Iteration 36, loss = 0.27685185\n",
      "Iteration 37, loss = 0.27560230\n",
      "Iteration 38, loss = 0.27556740\n",
      "Iteration 39, loss = 0.27491761\n",
      "Iteration 40, loss = 0.27557196\n",
      "Iteration 41, loss = 0.27683695\n",
      "Iteration 42, loss = 0.27664356\n",
      "Iteration 43, loss = 0.27577684\n",
      "Iteration 44, loss = 0.27620803\n",
      "Iteration 45, loss = 0.27471392\n",
      "Iteration 46, loss = 0.27485711\n",
      "Iteration 47, loss = 0.27553219\n",
      "Iteration 48, loss = 0.27612199\n",
      "Iteration 49, loss = 0.27560716\n",
      "Iteration 50, loss = 0.27628038\n",
      "Iteration 51, loss = 0.27580534\n",
      "Iteration 52, loss = 0.27598353\n",
      "Iteration 53, loss = 0.27618242\n",
      "Iteration 54, loss = 0.27563860\n",
      "Iteration 55, loss = 0.27544068\n",
      "Iteration 56, loss = 0.27476479\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30761865\n",
      "Iteration 2, loss = 0.28208439\n",
      "Iteration 3, loss = 0.28186497\n",
      "Iteration 4, loss = 0.27996872\n",
      "Iteration 5, loss = 0.27936871\n",
      "Iteration 6, loss = 0.28021457\n",
      "Iteration 7, loss = 0.27990336\n",
      "Iteration 8, loss = 0.28006475\n",
      "Iteration 9, loss = 0.27969405\n",
      "Iteration 10, loss = 0.27935261\n",
      "Iteration 11, loss = 0.27900801\n",
      "Iteration 12, loss = 0.27898219\n",
      "Iteration 13, loss = 0.28048959\n",
      "Iteration 14, loss = 0.27887168\n",
      "Iteration 15, loss = 0.27804458\n",
      "Iteration 16, loss = 0.27750050\n",
      "Iteration 17, loss = 0.27804307\n",
      "Iteration 18, loss = 0.27712549\n",
      "Iteration 19, loss = 0.27781623\n",
      "Iteration 20, loss = 0.27692082\n",
      "Iteration 21, loss = 0.27622708\n",
      "Iteration 22, loss = 0.27772652\n",
      "Iteration 23, loss = 0.27746937\n",
      "Iteration 24, loss = 0.27808557\n",
      "Iteration 25, loss = 0.27841490\n",
      "Iteration 26, loss = 0.27671872\n",
      "Iteration 27, loss = 0.27881742\n",
      "Iteration 28, loss = 0.27746800\n",
      "Iteration 29, loss = 0.27699021\n",
      "Iteration 30, loss = 0.27641823\n",
      "Iteration 31, loss = 0.27643109\n",
      "Iteration 32, loss = 0.27732884\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29858941\n",
      "Iteration 2, loss = 0.28262895\n",
      "Iteration 3, loss = 0.28274841\n",
      "Iteration 4, loss = 0.28191993\n",
      "Iteration 5, loss = 0.28056297\n",
      "Iteration 6, loss = 0.28091502\n",
      "Iteration 7, loss = 0.28022445\n",
      "Iteration 8, loss = 0.28012751\n",
      "Iteration 9, loss = 0.27888086\n",
      "Iteration 10, loss = 0.27972814\n",
      "Iteration 11, loss = 0.27887081\n",
      "Iteration 12, loss = 0.27946164\n",
      "Iteration 13, loss = 0.27912387\n",
      "Iteration 14, loss = 0.27894663\n",
      "Iteration 15, loss = 0.27790798\n",
      "Iteration 16, loss = 0.27777543\n",
      "Iteration 17, loss = 0.27837742\n",
      "Iteration 18, loss = 0.27831401\n",
      "Iteration 19, loss = 0.27793766\n",
      "Iteration 20, loss = 0.27723111\n",
      "Iteration 21, loss = 0.27577642\n",
      "Iteration 22, loss = 0.27761099\n",
      "Iteration 23, loss = 0.27952872\n",
      "Iteration 24, loss = 0.27929914\n",
      "Iteration 25, loss = 0.27553765\n",
      "Iteration 26, loss = 0.27628878\n",
      "Iteration 27, loss = 0.27609930\n",
      "Iteration 28, loss = 0.27568740\n",
      "Iteration 29, loss = 0.27597062\n",
      "Iteration 30, loss = 0.27606790\n",
      "Iteration 31, loss = 0.27493756\n",
      "Iteration 32, loss = 0.27696325\n",
      "Iteration 33, loss = 0.27531350\n",
      "Iteration 34, loss = 0.27543604\n",
      "Iteration 35, loss = 0.27576939\n",
      "Iteration 36, loss = 0.27597261\n",
      "Iteration 37, loss = 0.27516839\n",
      "Iteration 38, loss = 0.27487791\n",
      "Iteration 39, loss = 0.27625511\n",
      "Iteration 40, loss = 0.27550874\n",
      "Iteration 41, loss = 0.27635986\n",
      "Iteration 42, loss = 0.27725209\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30159254\n",
      "Iteration 2, loss = 0.28338781\n",
      "Iteration 3, loss = 0.28266419\n",
      "Iteration 4, loss = 0.28281914\n",
      "Iteration 5, loss = 0.28126928\n",
      "Iteration 6, loss = 0.27997393\n",
      "Iteration 7, loss = 0.27998780\n",
      "Iteration 8, loss = 0.28064547\n",
      "Iteration 9, loss = 0.28046110\n",
      "Iteration 10, loss = 0.27935124\n",
      "Iteration 11, loss = 0.27925145\n",
      "Iteration 12, loss = 0.27876811\n",
      "Iteration 13, loss = 0.27784499\n",
      "Iteration 14, loss = 0.27867580\n",
      "Iteration 15, loss = 0.27829519\n",
      "Iteration 16, loss = 0.27697298\n",
      "Iteration 17, loss = 0.27757957\n",
      "Iteration 18, loss = 0.27650042\n",
      "Iteration 19, loss = 0.27885368\n",
      "Iteration 20, loss = 0.27626348\n",
      "Iteration 21, loss = 0.27650054\n",
      "Iteration 22, loss = 0.27660178\n",
      "Iteration 23, loss = 0.27729831\n",
      "Iteration 24, loss = 0.27598926\n",
      "Iteration 25, loss = 0.27604262\n",
      "Iteration 26, loss = 0.27658031\n",
      "Iteration 27, loss = 0.27441564\n",
      "Iteration 28, loss = 0.27559614\n",
      "Iteration 29, loss = 0.27609110\n",
      "Iteration 30, loss = 0.27530016\n",
      "Iteration 31, loss = 0.27483105\n",
      "Iteration 32, loss = 0.27494496\n",
      "Iteration 33, loss = 0.27577671\n",
      "Iteration 34, loss = 0.27465292\n",
      "Iteration 35, loss = 0.27514496\n",
      "Iteration 36, loss = 0.27554418\n",
      "Iteration 37, loss = 0.27535506\n",
      "Iteration 38, loss = 0.27414501\n",
      "Iteration 39, loss = 0.27476612\n",
      "Iteration 40, loss = 0.27589076\n",
      "Iteration 41, loss = 0.27394444\n",
      "Iteration 42, loss = 0.27564174\n",
      "Iteration 43, loss = 0.27551985\n",
      "Iteration 44, loss = 0.27534596\n",
      "Iteration 45, loss = 0.27444068\n",
      "Iteration 46, loss = 0.27266992\n",
      "Iteration 47, loss = 0.27568090\n",
      "Iteration 48, loss = 0.27605884\n",
      "Iteration 49, loss = 0.27411314\n",
      "Iteration 50, loss = 0.27459614\n",
      "Iteration 51, loss = 0.27442333\n",
      "Iteration 52, loss = 0.27327177\n",
      "Iteration 53, loss = 0.27389841\n",
      "Iteration 54, loss = 0.27276410\n",
      "Iteration 55, loss = 0.27539405\n",
      "Iteration 56, loss = 0.27373729\n",
      "Iteration 57, loss = 0.27408538\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29687575\n",
      "Iteration 2, loss = 0.28366769\n",
      "Iteration 3, loss = 0.28141636\n",
      "Iteration 4, loss = 0.28118842\n",
      "Iteration 5, loss = 0.28207959\n",
      "Iteration 6, loss = 0.28149582\n",
      "Iteration 7, loss = 0.28019299\n",
      "Iteration 8, loss = 0.27889496\n",
      "Iteration 9, loss = 0.27984772\n",
      "Iteration 10, loss = 0.27929757\n",
      "Iteration 11, loss = 0.27882562\n",
      "Iteration 12, loss = 0.27868071\n",
      "Iteration 13, loss = 0.27847741\n",
      "Iteration 14, loss = 0.27939823\n",
      "Iteration 15, loss = 0.27760333\n",
      "Iteration 16, loss = 0.27674705\n",
      "Iteration 17, loss = 0.27665070\n",
      "Iteration 18, loss = 0.27731637\n",
      "Iteration 19, loss = 0.27654059\n",
      "Iteration 20, loss = 0.27674275\n",
      "Iteration 21, loss = 0.27619426\n",
      "Iteration 22, loss = 0.27637556\n",
      "Iteration 23, loss = 0.27751038\n",
      "Iteration 24, loss = 0.27672257\n",
      "Iteration 25, loss = 0.27587175\n",
      "Iteration 26, loss = 0.27555280\n",
      "Iteration 27, loss = 0.27644241\n",
      "Iteration 28, loss = 0.27656502\n",
      "Iteration 29, loss = 0.27684994\n",
      "Iteration 30, loss = 0.27556735\n",
      "Iteration 31, loss = 0.27607303\n",
      "Iteration 32, loss = 0.27529951\n",
      "Iteration 33, loss = 0.27508030\n",
      "Iteration 34, loss = 0.27494056\n",
      "Iteration 35, loss = 0.27473154\n",
      "Iteration 36, loss = 0.27543420\n",
      "Iteration 37, loss = 0.27431760\n",
      "Iteration 38, loss = 0.27492037\n",
      "Iteration 39, loss = 0.27349413\n",
      "Iteration 40, loss = 0.27356676\n",
      "Iteration 41, loss = 0.27428101\n",
      "Iteration 42, loss = 0.27417436\n",
      "Iteration 43, loss = 0.27440558\n",
      "Iteration 44, loss = 0.27258040\n",
      "Iteration 45, loss = 0.27336517\n",
      "Iteration 46, loss = 0.27420048\n",
      "Iteration 47, loss = 0.27458476\n",
      "Iteration 48, loss = 0.27427145\n",
      "Iteration 49, loss = 0.27414722\n",
      "Iteration 50, loss = 0.27378479\n",
      "Iteration 51, loss = 0.27412147\n",
      "Iteration 52, loss = 0.27399739\n",
      "Iteration 53, loss = 0.27453143\n",
      "Iteration 54, loss = 0.27365025\n",
      "Iteration 55, loss = 0.27439412\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29855967\n",
      "Iteration 2, loss = 0.28385296\n",
      "Iteration 3, loss = 0.28234067\n",
      "Iteration 4, loss = 0.28066018\n",
      "Iteration 5, loss = 0.28000438\n",
      "Iteration 6, loss = 0.28184487\n",
      "Iteration 7, loss = 0.27983026\n",
      "Iteration 8, loss = 0.27917831\n",
      "Iteration 9, loss = 0.27889274\n",
      "Iteration 10, loss = 0.27959805\n",
      "Iteration 11, loss = 0.27906559\n",
      "Iteration 12, loss = 0.27903707\n",
      "Iteration 13, loss = 0.28002083\n",
      "Iteration 14, loss = 0.27819204\n",
      "Iteration 15, loss = 0.27879747\n",
      "Iteration 16, loss = 0.27877267\n",
      "Iteration 17, loss = 0.27727732\n",
      "Iteration 18, loss = 0.27845063\n",
      "Iteration 19, loss = 0.27886883\n",
      "Iteration 20, loss = 0.28043148\n",
      "Iteration 21, loss = 0.27872205\n",
      "Iteration 22, loss = 0.27765105\n",
      "Iteration 23, loss = 0.27682476\n",
      "Iteration 24, loss = 0.27768320\n",
      "Iteration 25, loss = 0.27749769\n",
      "Iteration 26, loss = 0.27803396\n",
      "Iteration 27, loss = 0.27879642\n",
      "Iteration 28, loss = 0.27818065\n",
      "Iteration 29, loss = 0.27778638\n",
      "Iteration 30, loss = 0.27755440\n",
      "Iteration 31, loss = 0.27708327\n",
      "Iteration 32, loss = 0.27566370\n",
      "Iteration 33, loss = 0.27866618\n",
      "Iteration 34, loss = 0.27790368\n",
      "Iteration 35, loss = 0.27826328\n",
      "Iteration 36, loss = 0.27771635\n",
      "Iteration 37, loss = 0.27694597\n",
      "Iteration 38, loss = 0.27729197\n",
      "Iteration 39, loss = 0.27754152\n",
      "Iteration 40, loss = 0.27815682\n",
      "Iteration 41, loss = 0.27722002\n",
      "Iteration 42, loss = 0.27655345\n",
      "Iteration 43, loss = 0.27723586\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29705952\n",
      "Iteration 2, loss = 0.28458561\n",
      "Iteration 3, loss = 0.28251259\n",
      "Iteration 4, loss = 0.28128716\n",
      "Iteration 5, loss = 0.28187398\n",
      "Iteration 6, loss = 0.28100749\n",
      "Iteration 7, loss = 0.28017250\n",
      "Iteration 8, loss = 0.27967963\n",
      "Iteration 9, loss = 0.27901975\n",
      "Iteration 10, loss = 0.27901738\n",
      "Iteration 11, loss = 0.27824680\n",
      "Iteration 12, loss = 0.27912044\n",
      "Iteration 13, loss = 0.28039967\n",
      "Iteration 14, loss = 0.27942793\n",
      "Iteration 15, loss = 0.27872719\n",
      "Iteration 16, loss = 0.27887937\n",
      "Iteration 17, loss = 0.27720166\n",
      "Iteration 18, loss = 0.27817523\n",
      "Iteration 19, loss = 0.27750365\n",
      "Iteration 20, loss = 0.27586107\n",
      "Iteration 21, loss = 0.27670697\n",
      "Iteration 22, loss = 0.27685517\n",
      "Iteration 23, loss = 0.27851904\n",
      "Iteration 24, loss = 0.27701484\n",
      "Iteration 25, loss = 0.27594666\n",
      "Iteration 26, loss = 0.27540553\n",
      "Iteration 27, loss = 0.27743161\n",
      "Iteration 28, loss = 0.27606342\n",
      "Iteration 29, loss = 0.27603292\n",
      "Iteration 30, loss = 0.27436262\n",
      "Iteration 31, loss = 0.27452966\n",
      "Iteration 32, loss = 0.27593793\n",
      "Iteration 33, loss = 0.27446953\n",
      "Iteration 34, loss = 0.27536636\n",
      "Iteration 35, loss = 0.27498945\n",
      "Iteration 36, loss = 0.27664275\n",
      "Iteration 37, loss = 0.27533593\n",
      "Iteration 38, loss = 0.27515866\n",
      "Iteration 39, loss = 0.27501285\n",
      "Iteration 40, loss = 0.27412568\n",
      "Iteration 41, loss = 0.27623871\n",
      "Iteration 42, loss = 0.27444341\n",
      "Iteration 43, loss = 0.27598966\n",
      "Iteration 44, loss = 0.27445115\n",
      "Iteration 45, loss = 0.27458094\n",
      "Iteration 46, loss = 0.27374709\n",
      "Iteration 47, loss = 0.27396721\n",
      "Iteration 48, loss = 0.27451100\n",
      "Iteration 49, loss = 0.27695540\n",
      "Iteration 50, loss = 0.27366195\n",
      "Iteration 51, loss = 0.27464051\n",
      "Iteration 52, loss = 0.27542312\n",
      "Iteration 53, loss = 0.27536569\n",
      "Iteration 54, loss = 0.27401960\n",
      "Iteration 55, loss = 0.27359344\n",
      "Iteration 56, loss = 0.27583584\n",
      "Iteration 57, loss = 0.27288537\n",
      "Iteration 58, loss = 0.27464862\n",
      "Iteration 59, loss = 0.27465801\n",
      "Iteration 60, loss = 0.27296273\n",
      "Iteration 61, loss = 0.27261722\n",
      "Iteration 62, loss = 0.27332591\n",
      "Iteration 63, loss = 0.27502060\n",
      "Iteration 64, loss = 0.27455031\n",
      "Iteration 65, loss = 0.27446850\n",
      "Iteration 66, loss = 0.27349465\n",
      "Iteration 67, loss = 0.27430964\n",
      "Iteration 68, loss = 0.27434322\n",
      "Iteration 69, loss = 0.27370500\n",
      "Iteration 70, loss = 0.27240302\n",
      "Iteration 71, loss = 0.27457571\n",
      "Iteration 72, loss = 0.27386951\n",
      "Iteration 73, loss = 0.27399692\n",
      "Iteration 74, loss = 0.27349325\n",
      "Iteration 75, loss = 0.27296787\n",
      "Iteration 76, loss = 0.27426627\n",
      "Iteration 77, loss = 0.27348407\n",
      "Iteration 78, loss = 0.27278940\n",
      "Iteration 79, loss = 0.27297783\n",
      "Iteration 80, loss = 0.27378334\n",
      "Iteration 81, loss = 0.27489336\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30105621\n",
      "Iteration 2, loss = 0.28409021\n",
      "Iteration 3, loss = 0.28196980\n",
      "Iteration 4, loss = 0.28109438\n",
      "Iteration 5, loss = 0.28014701\n",
      "Iteration 6, loss = 0.27934411\n",
      "Iteration 7, loss = 0.28008396\n",
      "Iteration 8, loss = 0.28002990\n",
      "Iteration 9, loss = 0.27838280\n",
      "Iteration 10, loss = 0.28039073\n",
      "Iteration 11, loss = 0.27800912\n",
      "Iteration 12, loss = 0.28041367\n",
      "Iteration 13, loss = 0.27872783\n",
      "Iteration 14, loss = 0.28017094\n",
      "Iteration 15, loss = 0.27920684\n",
      "Iteration 16, loss = 0.27818559\n",
      "Iteration 17, loss = 0.27962971\n",
      "Iteration 18, loss = 0.27667001\n",
      "Iteration 19, loss = 0.27752538\n",
      "Iteration 20, loss = 0.27823944\n",
      "Iteration 21, loss = 0.27925171\n",
      "Iteration 22, loss = 0.27736715\n",
      "Iteration 23, loss = 0.27660471\n",
      "Iteration 24, loss = 0.27880409\n",
      "Iteration 25, loss = 0.27675048\n",
      "Iteration 26, loss = 0.27591671\n",
      "Iteration 27, loss = 0.27611211\n",
      "Iteration 28, loss = 0.27748941\n",
      "Iteration 29, loss = 0.27604263\n",
      "Iteration 30, loss = 0.27478852\n",
      "Iteration 31, loss = 0.27497511\n",
      "Iteration 32, loss = 0.27602832\n",
      "Iteration 33, loss = 0.27651835\n",
      "Iteration 34, loss = 0.27495414\n",
      "Iteration 35, loss = 0.27496482\n",
      "Iteration 36, loss = 0.27437251\n",
      "Iteration 37, loss = 0.27612181\n",
      "Iteration 38, loss = 0.27276586\n",
      "Iteration 39, loss = 0.27357372\n",
      "Iteration 40, loss = 0.27584858\n",
      "Iteration 41, loss = 0.27504921\n",
      "Iteration 42, loss = 0.27373825\n",
      "Iteration 43, loss = 0.27360370\n",
      "Iteration 44, loss = 0.27456927\n",
      "Iteration 45, loss = 0.27283382\n",
      "Iteration 46, loss = 0.27558934\n",
      "Iteration 47, loss = 0.27472726\n",
      "Iteration 48, loss = 0.27334314\n",
      "Iteration 49, loss = 0.27266433\n",
      "Iteration 50, loss = 0.27180489\n",
      "Iteration 51, loss = 0.27313275\n",
      "Iteration 52, loss = 0.27366010\n",
      "Iteration 53, loss = 0.27358824\n",
      "Iteration 54, loss = 0.27420499\n",
      "Iteration 55, loss = 0.27380828\n",
      "Iteration 56, loss = 0.27256685\n",
      "Iteration 57, loss = 0.27140702\n",
      "Iteration 58, loss = 0.27217793\n",
      "Iteration 59, loss = 0.27208288\n",
      "Iteration 60, loss = 0.27261748\n",
      "Iteration 61, loss = 0.27258270\n",
      "Iteration 62, loss = 0.27414347\n",
      "Iteration 63, loss = 0.27281429\n",
      "Iteration 64, loss = 0.27309208\n",
      "Iteration 65, loss = 0.27242656\n",
      "Iteration 66, loss = 0.27368206\n",
      "Iteration 67, loss = 0.27210965\n",
      "Iteration 68, loss = 0.27286081\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29945960\n",
      "Iteration 2, loss = 0.28592819\n",
      "Iteration 3, loss = 0.28300596\n",
      "Iteration 4, loss = 0.28148876\n",
      "Iteration 5, loss = 0.27982564\n",
      "Iteration 6, loss = 0.28110274\n",
      "Iteration 7, loss = 0.27912951\n",
      "Iteration 8, loss = 0.27977732\n",
      "Iteration 9, loss = 0.27986989\n",
      "Iteration 10, loss = 0.27948855\n",
      "Iteration 11, loss = 0.28037101\n",
      "Iteration 12, loss = 0.27889526\n",
      "Iteration 13, loss = 0.27829508\n",
      "Iteration 14, loss = 0.27708389\n",
      "Iteration 15, loss = 0.27863085\n",
      "Iteration 16, loss = 0.27792128\n",
      "Iteration 17, loss = 0.27990198\n",
      "Iteration 18, loss = 0.27713478\n",
      "Iteration 19, loss = 0.27675129\n",
      "Iteration 20, loss = 0.27593765\n",
      "Iteration 21, loss = 0.27581010\n",
      "Iteration 22, loss = 0.27603825\n",
      "Iteration 23, loss = 0.27779022\n",
      "Iteration 24, loss = 0.27650686\n",
      "Iteration 25, loss = 0.27554450\n",
      "Iteration 26, loss = 0.27475750\n",
      "Iteration 27, loss = 0.27467517\n",
      "Iteration 28, loss = 0.27532529\n",
      "Iteration 29, loss = 0.27679478\n",
      "Iteration 30, loss = 0.27533985\n",
      "Iteration 31, loss = 0.27507423\n",
      "Iteration 32, loss = 0.27574009\n",
      "Iteration 33, loss = 0.27594308\n",
      "Iteration 34, loss = 0.27545979\n",
      "Iteration 35, loss = 0.27582980\n",
      "Iteration 36, loss = 0.27423103\n",
      "Iteration 37, loss = 0.27568577\n",
      "Iteration 38, loss = 0.27297148\n",
      "Iteration 39, loss = 0.27393202\n",
      "Iteration 40, loss = 0.27520211\n",
      "Iteration 41, loss = 0.27387691\n",
      "Iteration 42, loss = 0.27526301\n",
      "Iteration 43, loss = 0.27637304\n",
      "Iteration 44, loss = 0.27428104\n",
      "Iteration 45, loss = 0.27368338\n",
      "Iteration 46, loss = 0.27352354\n",
      "Iteration 47, loss = 0.27562690\n",
      "Iteration 48, loss = 0.27712959\n",
      "Iteration 49, loss = 0.27513946\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29962662\n",
      "Iteration 2, loss = 0.28341308\n",
      "Iteration 3, loss = 0.28276487\n",
      "Iteration 4, loss = 0.28202151\n",
      "Iteration 5, loss = 0.28070368\n",
      "Iteration 6, loss = 0.28155716\n",
      "Iteration 7, loss = 0.28013163\n",
      "Iteration 8, loss = 0.28083367\n",
      "Iteration 9, loss = 0.27938280\n",
      "Iteration 10, loss = 0.27919648\n",
      "Iteration 11, loss = 0.27968357\n",
      "Iteration 12, loss = 0.27926992\n",
      "Iteration 13, loss = 0.27907029\n",
      "Iteration 14, loss = 0.27869356\n",
      "Iteration 15, loss = 0.27754919\n",
      "Iteration 16, loss = 0.27863222\n",
      "Iteration 17, loss = 0.27736500\n",
      "Iteration 18, loss = 0.27852359\n",
      "Iteration 19, loss = 0.27656816\n",
      "Iteration 20, loss = 0.27656892\n",
      "Iteration 21, loss = 0.27728075\n",
      "Iteration 22, loss = 0.27681301\n",
      "Iteration 23, loss = 0.27605732\n",
      "Iteration 24, loss = 0.27622379\n",
      "Iteration 25, loss = 0.27576430\n",
      "Iteration 26, loss = 0.27720675\n",
      "Iteration 27, loss = 0.27622391\n",
      "Iteration 28, loss = 0.27756673\n",
      "Iteration 29, loss = 0.27457856\n",
      "Iteration 30, loss = 0.27506890\n",
      "Iteration 31, loss = 0.27635191\n",
      "Iteration 32, loss = 0.27437094\n",
      "Iteration 33, loss = 0.27366774\n",
      "Iteration 34, loss = 0.27341347\n",
      "Iteration 35, loss = 0.27431566\n",
      "Iteration 36, loss = 0.27386914\n",
      "Iteration 37, loss = 0.27361912\n",
      "Iteration 38, loss = 0.27495207\n",
      "Iteration 39, loss = 0.27542062\n",
      "Iteration 40, loss = 0.27333143\n",
      "Iteration 41, loss = 0.27234011\n",
      "Iteration 42, loss = 0.27583031\n",
      "Iteration 43, loss = 0.27419508\n",
      "Iteration 44, loss = 0.27255837\n",
      "Iteration 45, loss = 0.27443956\n",
      "Iteration 46, loss = 0.27326873\n",
      "Iteration 47, loss = 0.27541622\n",
      "Iteration 48, loss = 0.27237457\n",
      "Iteration 49, loss = 0.27252244\n",
      "Iteration 50, loss = 0.27281629\n",
      "Iteration 51, loss = 0.27347947\n",
      "Iteration 52, loss = 0.27353289\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30262461\n",
      "Iteration 2, loss = 0.28409836\n",
      "Iteration 3, loss = 0.28372028\n",
      "Iteration 4, loss = 0.28373190\n",
      "Iteration 5, loss = 0.28135342\n",
      "Iteration 6, loss = 0.27996806\n",
      "Iteration 7, loss = 0.27963079\n",
      "Iteration 8, loss = 0.27820320\n",
      "Iteration 9, loss = 0.27828468\n",
      "Iteration 10, loss = 0.27760226\n",
      "Iteration 11, loss = 0.27867771\n",
      "Iteration 12, loss = 0.27829716\n",
      "Iteration 13, loss = 0.27895227\n",
      "Iteration 14, loss = 0.27901531\n",
      "Iteration 15, loss = 0.27892662\n",
      "Iteration 16, loss = 0.27787276\n",
      "Iteration 17, loss = 0.27594857\n",
      "Iteration 18, loss = 0.27954049\n",
      "Iteration 19, loss = 0.27655181\n",
      "Iteration 20, loss = 0.27782970\n",
      "Iteration 21, loss = 0.27647029\n",
      "Iteration 22, loss = 0.27659882\n",
      "Iteration 23, loss = 0.27457428\n",
      "Iteration 24, loss = 0.27494260\n",
      "Iteration 25, loss = 0.27561952\n",
      "Iteration 26, loss = 0.27484139\n",
      "Iteration 27, loss = 0.27447541\n",
      "Iteration 28, loss = 0.27515510\n",
      "Iteration 29, loss = 0.27476772\n",
      "Iteration 30, loss = 0.27372819\n",
      "Iteration 31, loss = 0.27357303\n",
      "Iteration 32, loss = 0.27438767\n",
      "Iteration 33, loss = 0.27524648\n",
      "Iteration 34, loss = 0.27291410\n",
      "Iteration 35, loss = 0.27460540\n",
      "Iteration 36, loss = 0.27296208\n",
      "Iteration 37, loss = 0.27408583\n",
      "Iteration 38, loss = 0.27368974\n",
      "Iteration 39, loss = 0.27180571\n",
      "Iteration 40, loss = 0.27471354\n",
      "Iteration 41, loss = 0.27217795\n",
      "Iteration 42, loss = 0.27367932\n",
      "Iteration 43, loss = 0.27418246\n",
      "Iteration 44, loss = 0.27279301\n",
      "Iteration 45, loss = 0.27170765\n",
      "Iteration 46, loss = 0.27135247\n",
      "Iteration 47, loss = 0.27234560\n",
      "Iteration 48, loss = 0.27289373\n",
      "Iteration 49, loss = 0.27346221\n",
      "Iteration 50, loss = 0.27237533\n",
      "Iteration 51, loss = 0.27282600\n",
      "Iteration 52, loss = 0.27298172\n",
      "Iteration 53, loss = 0.27195350\n",
      "Iteration 54, loss = 0.27265500\n",
      "Iteration 55, loss = 0.27145996\n",
      "Iteration 56, loss = 0.27188140\n",
      "Iteration 57, loss = 0.27293706\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29944960\n",
      "Iteration 2, loss = 0.28271725\n",
      "Iteration 3, loss = 0.28201044\n",
      "Iteration 4, loss = 0.28156565\n",
      "Iteration 5, loss = 0.28166687\n",
      "Iteration 6, loss = 0.28136560\n",
      "Iteration 7, loss = 0.28154392\n",
      "Iteration 8, loss = 0.28046712\n",
      "Iteration 9, loss = 0.28066104\n",
      "Iteration 10, loss = 0.27923538\n",
      "Iteration 11, loss = 0.27939808\n",
      "Iteration 12, loss = 0.27896807\n",
      "Iteration 13, loss = 0.27753514\n",
      "Iteration 14, loss = 0.27903109\n",
      "Iteration 15, loss = 0.27848019\n",
      "Iteration 16, loss = 0.27849487\n",
      "Iteration 17, loss = 0.27801732\n",
      "Iteration 18, loss = 0.27712878\n",
      "Iteration 19, loss = 0.27667825\n",
      "Iteration 20, loss = 0.27548157\n",
      "Iteration 21, loss = 0.27735440\n",
      "Iteration 22, loss = 0.27539823\n",
      "Iteration 23, loss = 0.27510635\n",
      "Iteration 24, loss = 0.27684009\n",
      "Iteration 25, loss = 0.27607718\n",
      "Iteration 26, loss = 0.27639280\n",
      "Iteration 27, loss = 0.27517319\n",
      "Iteration 28, loss = 0.27471820\n",
      "Iteration 29, loss = 0.27686894\n",
      "Iteration 30, loss = 0.27572531\n",
      "Iteration 31, loss = 0.27589942\n",
      "Iteration 32, loss = 0.27461997\n",
      "Iteration 33, loss = 0.27600801\n",
      "Iteration 34, loss = 0.27566565\n",
      "Iteration 35, loss = 0.27429399\n",
      "Iteration 36, loss = 0.27564460\n",
      "Iteration 37, loss = 0.27517823\n",
      "Iteration 38, loss = 0.27614101\n",
      "Iteration 39, loss = 0.27452596\n",
      "Iteration 40, loss = 0.27585533\n",
      "Iteration 41, loss = 0.27424298\n",
      "Iteration 42, loss = 0.27614877\n",
      "Iteration 43, loss = 0.27351577\n",
      "Iteration 44, loss = 0.27492995\n",
      "Iteration 45, loss = 0.27512840\n",
      "Iteration 46, loss = 0.27413728\n",
      "Iteration 47, loss = 0.27393864\n",
      "Iteration 48, loss = 0.27456457\n",
      "Iteration 49, loss = 0.27494130\n",
      "Iteration 50, loss = 0.27598186\n",
      "Iteration 51, loss = 0.27367239\n",
      "Iteration 52, loss = 0.27455224\n",
      "Iteration 53, loss = 0.27372568\n",
      "Iteration 54, loss = 0.27418627\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30352215\n",
      "Iteration 2, loss = 0.28283125\n",
      "Iteration 3, loss = 0.28280349\n",
      "Iteration 4, loss = 0.28153732\n",
      "Iteration 5, loss = 0.28043682\n",
      "Iteration 6, loss = 0.27928511\n",
      "Iteration 7, loss = 0.27961124\n",
      "Iteration 8, loss = 0.28025843\n",
      "Iteration 9, loss = 0.28073906\n",
      "Iteration 10, loss = 0.27987039\n",
      "Iteration 11, loss = 0.27773017\n",
      "Iteration 12, loss = 0.27941131\n",
      "Iteration 13, loss = 0.27769918\n",
      "Iteration 14, loss = 0.27786022\n",
      "Iteration 15, loss = 0.27736360\n",
      "Iteration 16, loss = 0.27754369\n",
      "Iteration 17, loss = 0.27788921\n",
      "Iteration 18, loss = 0.27635938\n",
      "Iteration 19, loss = 0.27635842\n",
      "Iteration 20, loss = 0.27628134\n",
      "Iteration 21, loss = 0.27694241\n",
      "Iteration 22, loss = 0.27655586\n",
      "Iteration 23, loss = 0.27547399\n",
      "Iteration 24, loss = 0.27553624\n",
      "Iteration 25, loss = 0.27674258\n",
      "Iteration 26, loss = 0.27814166\n",
      "Iteration 27, loss = 0.27511285\n",
      "Iteration 28, loss = 0.27462214\n",
      "Iteration 29, loss = 0.27619878\n",
      "Iteration 30, loss = 0.27451950\n",
      "Iteration 31, loss = 0.27482316\n",
      "Iteration 32, loss = 0.27368649\n",
      "Iteration 33, loss = 0.27396775\n",
      "Iteration 34, loss = 0.27564434\n",
      "Iteration 35, loss = 0.27319479\n",
      "Iteration 36, loss = 0.27549735\n",
      "Iteration 37, loss = 0.27398093\n",
      "Iteration 38, loss = 0.27649916\n",
      "Iteration 39, loss = 0.27536007\n",
      "Iteration 40, loss = 0.27366816\n",
      "Iteration 41, loss = 0.27292980\n",
      "Iteration 42, loss = 0.27587244\n",
      "Iteration 43, loss = 0.27298341\n",
      "Iteration 44, loss = 0.27331269\n",
      "Iteration 45, loss = 0.27420897\n",
      "Iteration 46, loss = 0.27433132\n",
      "Iteration 47, loss = 0.27404524\n",
      "Iteration 48, loss = 0.27323361\n",
      "Iteration 49, loss = 0.27450838\n",
      "Iteration 50, loss = 0.27198020\n",
      "Iteration 51, loss = 0.27194340\n",
      "Iteration 52, loss = 0.27301590\n",
      "Iteration 53, loss = 0.27290951\n",
      "Iteration 54, loss = 0.27530581\n",
      "Iteration 55, loss = 0.27374380\n",
      "Iteration 56, loss = 0.27321997\n",
      "Iteration 57, loss = 0.27396486\n",
      "Iteration 58, loss = 0.27312129\n",
      "Iteration 59, loss = 0.27366135\n",
      "Iteration 60, loss = 0.27406641\n",
      "Iteration 61, loss = 0.27185733\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30318687\n",
      "Iteration 2, loss = 0.28461043\n",
      "Iteration 3, loss = 0.28276029\n",
      "Iteration 4, loss = 0.28220756\n",
      "Iteration 5, loss = 0.28123706\n",
      "Iteration 6, loss = 0.28116100\n",
      "Iteration 7, loss = 0.28117520\n",
      "Iteration 8, loss = 0.27973009\n",
      "Iteration 9, loss = 0.27738651\n",
      "Iteration 10, loss = 0.27803865\n",
      "Iteration 11, loss = 0.27751202\n",
      "Iteration 12, loss = 0.27854116\n",
      "Iteration 13, loss = 0.27861088\n",
      "Iteration 14, loss = 0.27487619\n",
      "Iteration 15, loss = 0.27667506\n",
      "Iteration 16, loss = 0.27521917\n",
      "Iteration 17, loss = 0.27715511\n",
      "Iteration 18, loss = 0.27655626\n",
      "Iteration 19, loss = 0.27689955\n",
      "Iteration 20, loss = 0.27480379\n",
      "Iteration 21, loss = 0.27547113\n",
      "Iteration 22, loss = 0.27566968\n",
      "Iteration 23, loss = 0.27553449\n",
      "Iteration 24, loss = 0.27572978\n",
      "Iteration 25, loss = 0.27454507\n",
      "Iteration 26, loss = 0.27551441\n",
      "Iteration 27, loss = 0.27508146\n",
      "Iteration 28, loss = 0.27460124\n",
      "Iteration 29, loss = 0.27512651\n",
      "Iteration 30, loss = 0.27367250\n",
      "Iteration 31, loss = 0.27437863\n",
      "Iteration 32, loss = 0.27348974\n",
      "Iteration 33, loss = 0.27458599\n",
      "Iteration 34, loss = 0.27570925\n",
      "Iteration 35, loss = 0.27333880\n",
      "Iteration 36, loss = 0.27540616\n",
      "Iteration 37, loss = 0.27527040\n",
      "Iteration 38, loss = 0.27355515\n",
      "Iteration 39, loss = 0.27436532\n",
      "Iteration 40, loss = 0.27370196\n",
      "Iteration 41, loss = 0.27307954\n",
      "Iteration 42, loss = 0.27397112\n",
      "Iteration 43, loss = 0.27179899\n",
      "Iteration 44, loss = 0.27243303\n",
      "Iteration 45, loss = 0.27255549\n",
      "Iteration 46, loss = 0.27343750\n",
      "Iteration 47, loss = 0.27273169\n",
      "Iteration 48, loss = 0.27256020\n",
      "Iteration 49, loss = 0.27215682\n",
      "Iteration 50, loss = 0.27472964\n",
      "Iteration 51, loss = 0.27426369\n",
      "Iteration 52, loss = 0.27151930\n",
      "Iteration 53, loss = 0.27213259\n",
      "Iteration 54, loss = 0.27506618\n",
      "Iteration 55, loss = 0.27261423\n",
      "Iteration 56, loss = 0.27219925\n",
      "Iteration 57, loss = 0.27235482\n",
      "Iteration 58, loss = 0.27117274\n",
      "Iteration 59, loss = 0.27194506\n",
      "Iteration 60, loss = 0.27361302\n",
      "Iteration 61, loss = 0.27439739\n",
      "Iteration 62, loss = 0.27203417\n",
      "Iteration 63, loss = 0.27160785\n",
      "Iteration 64, loss = 0.27310376\n",
      "Iteration 65, loss = 0.27291733\n",
      "Iteration 66, loss = 0.27322911\n",
      "Iteration 67, loss = 0.27398699\n",
      "Iteration 68, loss = 0.27263761\n",
      "Iteration 69, loss = 0.27188445\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30929800\n",
      "Iteration 2, loss = 0.28399937\n",
      "Iteration 3, loss = 0.28124769\n",
      "Iteration 4, loss = 0.28120851\n",
      "Iteration 5, loss = 0.28037382\n",
      "Iteration 6, loss = 0.28178362\n",
      "Iteration 7, loss = 0.27955550\n",
      "Iteration 8, loss = 0.27861888\n",
      "Iteration 9, loss = 0.27908387\n",
      "Iteration 10, loss = 0.27946442\n",
      "Iteration 11, loss = 0.28061801\n",
      "Iteration 12, loss = 0.27929218\n",
      "Iteration 13, loss = 0.28099513\n",
      "Iteration 14, loss = 0.27821464\n",
      "Iteration 15, loss = 0.27652832\n",
      "Iteration 16, loss = 0.27665346\n",
      "Iteration 17, loss = 0.27887719\n",
      "Iteration 18, loss = 0.27709622\n",
      "Iteration 19, loss = 0.27788488\n",
      "Iteration 20, loss = 0.27783870\n",
      "Iteration 21, loss = 0.27833104\n",
      "Iteration 22, loss = 0.27916508\n",
      "Iteration 23, loss = 0.27775181\n",
      "Iteration 24, loss = 0.27740145\n",
      "Iteration 25, loss = 0.27571783\n",
      "Iteration 26, loss = 0.27562021\n",
      "Iteration 27, loss = 0.27593841\n",
      "Iteration 28, loss = 0.27615619\n",
      "Iteration 29, loss = 0.27615807\n",
      "Iteration 30, loss = 0.27634193\n",
      "Iteration 31, loss = 0.27420062\n",
      "Iteration 32, loss = 0.27411054\n",
      "Iteration 33, loss = 0.27353543\n",
      "Iteration 34, loss = 0.27523906\n",
      "Iteration 35, loss = 0.27591049\n",
      "Iteration 36, loss = 0.27459942\n",
      "Iteration 37, loss = 0.27509611\n",
      "Iteration 38, loss = 0.27432191\n",
      "Iteration 39, loss = 0.27373580\n",
      "Iteration 40, loss = 0.27332498\n",
      "Iteration 41, loss = 0.27435272\n",
      "Iteration 42, loss = 0.27560281\n",
      "Iteration 43, loss = 0.27448245\n",
      "Iteration 44, loss = 0.27328194\n",
      "Iteration 45, loss = 0.27307073\n",
      "Iteration 46, loss = 0.27474021\n",
      "Iteration 47, loss = 0.27415805\n",
      "Iteration 48, loss = 0.27316862\n",
      "Iteration 49, loss = 0.27411045\n",
      "Iteration 50, loss = 0.27351089\n",
      "Iteration 51, loss = 0.27301047\n",
      "Iteration 52, loss = 0.27307805\n",
      "Iteration 53, loss = 0.27350124\n",
      "Iteration 54, loss = 0.27215144\n",
      "Iteration 55, loss = 0.27278781\n",
      "Iteration 56, loss = 0.27367647\n",
      "Iteration 57, loss = 0.27299649\n",
      "Iteration 58, loss = 0.27265684\n",
      "Iteration 59, loss = 0.27212021\n",
      "Iteration 60, loss = 0.27341625\n",
      "Iteration 61, loss = 0.27304350\n",
      "Iteration 62, loss = 0.27174555\n",
      "Iteration 63, loss = 0.27300432\n",
      "Iteration 64, loss = 0.27309239\n",
      "Iteration 65, loss = 0.27352474\n",
      "Iteration 66, loss = 0.27205703\n",
      "Iteration 67, loss = 0.27184803\n",
      "Iteration 68, loss = 0.27227524\n",
      "Iteration 69, loss = 0.27152968\n",
      "Iteration 70, loss = 0.27472883\n",
      "Iteration 71, loss = 0.27085518\n",
      "Iteration 72, loss = 0.27113394\n",
      "Iteration 73, loss = 0.27254506\n",
      "Iteration 74, loss = 0.27362612\n",
      "Iteration 75, loss = 0.27336989\n",
      "Iteration 76, loss = 0.27194232\n",
      "Iteration 77, loss = 0.27155522\n",
      "Iteration 78, loss = 0.27107487\n",
      "Iteration 79, loss = 0.27224506\n",
      "Iteration 80, loss = 0.27277173\n",
      "Iteration 81, loss = 0.27262854\n",
      "Iteration 82, loss = 0.27177119\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30551837\n",
      "Iteration 2, loss = 0.28245347\n",
      "Iteration 3, loss = 0.28184658\n",
      "Iteration 4, loss = 0.28182915\n",
      "Iteration 5, loss = 0.28123602\n",
      "Iteration 6, loss = 0.27953284\n",
      "Iteration 7, loss = 0.27975504\n",
      "Iteration 8, loss = 0.27989460\n",
      "Iteration 9, loss = 0.27771238\n",
      "Iteration 10, loss = 0.28093698\n",
      "Iteration 11, loss = 0.27948520\n",
      "Iteration 12, loss = 0.28307938\n",
      "Iteration 13, loss = 0.27968263\n",
      "Iteration 14, loss = 0.27950185\n",
      "Iteration 15, loss = 0.27847457\n",
      "Iteration 16, loss = 0.27878331\n",
      "Iteration 17, loss = 0.27988298\n",
      "Iteration 18, loss = 0.27848976\n",
      "Iteration 19, loss = 0.27838113\n",
      "Iteration 20, loss = 0.27610088\n",
      "Iteration 21, loss = 0.27738314\n",
      "Iteration 22, loss = 0.27930370\n",
      "Iteration 23, loss = 0.27787025\n",
      "Iteration 24, loss = 0.27788942\n",
      "Iteration 25, loss = 0.27634494\n",
      "Iteration 26, loss = 0.27679336\n",
      "Iteration 27, loss = 0.27487135\n",
      "Iteration 28, loss = 0.27576329\n",
      "Iteration 29, loss = 0.27423410\n",
      "Iteration 30, loss = 0.27569377\n",
      "Iteration 31, loss = 0.27444252\n",
      "Iteration 32, loss = 0.27451856\n",
      "Iteration 33, loss = 0.27617017\n",
      "Iteration 34, loss = 0.27530443\n",
      "Iteration 35, loss = 0.27435338\n",
      "Iteration 36, loss = 0.27553224\n",
      "Iteration 37, loss = 0.27417828\n",
      "Iteration 38, loss = 0.27567021\n",
      "Iteration 39, loss = 0.27692113\n",
      "Iteration 40, loss = 0.27386353\n",
      "Iteration 41, loss = 0.27411177\n",
      "Iteration 42, loss = 0.27444390\n",
      "Iteration 43, loss = 0.27279038\n",
      "Iteration 44, loss = 0.27337658\n",
      "Iteration 45, loss = 0.27394859\n",
      "Iteration 46, loss = 0.27334626\n",
      "Iteration 47, loss = 0.27373927\n",
      "Iteration 48, loss = 0.27223957\n",
      "Iteration 49, loss = 0.27307514\n",
      "Iteration 50, loss = 0.27277005\n",
      "Iteration 51, loss = 0.27345927\n",
      "Iteration 52, loss = 0.27181568\n",
      "Iteration 53, loss = 0.27274335\n",
      "Iteration 54, loss = 0.27220355\n",
      "Iteration 55, loss = 0.27284172\n",
      "Iteration 56, loss = 0.27247061\n",
      "Iteration 57, loss = 0.27184580\n",
      "Iteration 58, loss = 0.27271315\n",
      "Iteration 59, loss = 0.27287247\n",
      "Iteration 60, loss = 0.27120322\n",
      "Iteration 61, loss = 0.27126697\n",
      "Iteration 62, loss = 0.27266935\n",
      "Iteration 63, loss = 0.27149154\n",
      "Iteration 64, loss = 0.27278563\n",
      "Iteration 65, loss = 0.27173610\n",
      "Iteration 66, loss = 0.27304139\n",
      "Iteration 67, loss = 0.27040250\n",
      "Iteration 68, loss = 0.27330697\n",
      "Iteration 69, loss = 0.27139568\n",
      "Iteration 70, loss = 0.27139675\n",
      "Iteration 71, loss = 0.27136476\n",
      "Iteration 72, loss = 0.27269686\n",
      "Iteration 73, loss = 0.27214816\n",
      "Iteration 74, loss = 0.27146956\n",
      "Iteration 75, loss = 0.27067221\n",
      "Iteration 76, loss = 0.27122183\n",
      "Iteration 77, loss = 0.27215330\n",
      "Iteration 78, loss = 0.27223369\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30319742\n",
      "Iteration 2, loss = 0.28439586\n",
      "Iteration 3, loss = 0.28345175\n",
      "Iteration 4, loss = 0.28186352\n",
      "Iteration 5, loss = 0.28065721\n",
      "Iteration 6, loss = 0.28108821\n",
      "Iteration 7, loss = 0.27978344\n",
      "Iteration 8, loss = 0.28029340\n",
      "Iteration 9, loss = 0.27919377\n",
      "Iteration 10, loss = 0.27729231\n",
      "Iteration 11, loss = 0.27765756\n",
      "Iteration 12, loss = 0.27836957\n",
      "Iteration 13, loss = 0.27772104\n",
      "Iteration 14, loss = 0.27778460\n",
      "Iteration 15, loss = 0.27739018\n",
      "Iteration 16, loss = 0.27613899\n",
      "Iteration 17, loss = 0.27781827\n",
      "Iteration 18, loss = 0.27540903\n",
      "Iteration 19, loss = 0.27491310\n",
      "Iteration 20, loss = 0.27560425\n",
      "Iteration 21, loss = 0.27515505\n",
      "Iteration 22, loss = 0.27470088\n",
      "Iteration 23, loss = 0.27416739\n",
      "Iteration 24, loss = 0.27368345\n",
      "Iteration 25, loss = 0.27325879\n",
      "Iteration 26, loss = 0.27308139\n",
      "Iteration 27, loss = 0.27494485\n",
      "Iteration 28, loss = 0.27501802\n",
      "Iteration 29, loss = 0.27396545\n",
      "Iteration 30, loss = 0.27404375\n",
      "Iteration 31, loss = 0.27308683\n",
      "Iteration 32, loss = 0.27285227\n",
      "Iteration 33, loss = 0.27269875\n",
      "Iteration 34, loss = 0.27234183\n",
      "Iteration 35, loss = 0.27232033\n",
      "Iteration 36, loss = 0.27181970\n",
      "Iteration 37, loss = 0.27271387\n",
      "Iteration 38, loss = 0.27132550\n",
      "Iteration 39, loss = 0.27151379\n",
      "Iteration 40, loss = 0.27270157\n",
      "Iteration 41, loss = 0.27166200\n",
      "Iteration 42, loss = 0.27244321\n",
      "Iteration 43, loss = 0.27362966\n",
      "Iteration 44, loss = 0.27181933\n",
      "Iteration 45, loss = 0.27108713\n",
      "Iteration 46, loss = 0.27209684\n",
      "Iteration 47, loss = 0.27247742\n",
      "Iteration 48, loss = 0.27263901\n",
      "Iteration 49, loss = 0.27253642\n",
      "Iteration 50, loss = 0.27198087\n",
      "Iteration 51, loss = 0.27207265\n",
      "Iteration 52, loss = 0.27295313\n",
      "Iteration 53, loss = 0.27248885\n",
      "Iteration 54, loss = 0.27241294\n",
      "Iteration 55, loss = 0.27169270\n",
      "Iteration 56, loss = 0.27113061\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30416277\n",
      "Iteration 2, loss = 0.28461851\n",
      "Iteration 3, loss = 0.28266164\n",
      "Iteration 4, loss = 0.28167872\n",
      "Iteration 5, loss = 0.28150297\n",
      "Iteration 6, loss = 0.28002793\n",
      "Iteration 7, loss = 0.28075430\n",
      "Iteration 8, loss = 0.27977696\n",
      "Iteration 9, loss = 0.27951743\n",
      "Iteration 10, loss = 0.27957293\n",
      "Iteration 11, loss = 0.27900037\n",
      "Iteration 12, loss = 0.27820667\n",
      "Iteration 13, loss = 0.27911953\n",
      "Iteration 14, loss = 0.27848042\n",
      "Iteration 15, loss = 0.28031329\n",
      "Iteration 16, loss = 0.28260034\n",
      "Iteration 17, loss = 0.28052568\n",
      "Iteration 18, loss = 0.28020326\n",
      "Iteration 19, loss = 0.27890940\n",
      "Iteration 20, loss = 0.27746972\n",
      "Iteration 21, loss = 0.27870272\n",
      "Iteration 22, loss = 0.27872812\n",
      "Iteration 23, loss = 0.27728134\n",
      "Iteration 24, loss = 0.27808040\n",
      "Iteration 25, loss = 0.27794233\n",
      "Iteration 26, loss = 0.27718764\n",
      "Iteration 27, loss = 0.27786915\n",
      "Iteration 28, loss = 0.27582321\n",
      "Iteration 29, loss = 0.27697515\n",
      "Iteration 30, loss = 0.27643230\n",
      "Iteration 31, loss = 0.27734850\n",
      "Iteration 32, loss = 0.27572376\n",
      "Iteration 33, loss = 0.27671270\n",
      "Iteration 34, loss = 0.27625881\n",
      "Iteration 35, loss = 0.27667039\n",
      "Iteration 36, loss = 0.27575223\n",
      "Iteration 37, loss = 0.27694753\n",
      "Iteration 38, loss = 0.27551388\n",
      "Iteration 39, loss = 0.27629697\n",
      "Iteration 40, loss = 0.27568409\n",
      "Iteration 41, loss = 0.27611160\n",
      "Iteration 42, loss = 0.27688031\n",
      "Iteration 43, loss = 0.27592850\n",
      "Iteration 44, loss = 0.27549203\n",
      "Iteration 45, loss = 0.27409257\n",
      "Iteration 46, loss = 0.27583960\n",
      "Iteration 47, loss = 0.27423671\n",
      "Iteration 48, loss = 0.27427241\n",
      "Iteration 49, loss = 0.27615943\n",
      "Iteration 50, loss = 0.27502943\n",
      "Iteration 51, loss = 0.27542831\n",
      "Iteration 52, loss = 0.27671672\n",
      "Iteration 53, loss = 0.27531074\n",
      "Iteration 54, loss = 0.27592517\n",
      "Iteration 55, loss = 0.27619300\n",
      "Iteration 56, loss = 0.27480973\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30280155\n",
      "Iteration 2, loss = 0.28418214\n",
      "Iteration 3, loss = 0.28176070\n",
      "Iteration 4, loss = 0.28054264\n",
      "Iteration 5, loss = 0.28164718\n",
      "Iteration 6, loss = 0.28033420\n",
      "Iteration 7, loss = 0.27993919\n",
      "Iteration 8, loss = 0.28009937\n",
      "Iteration 9, loss = 0.28093311\n",
      "Iteration 10, loss = 0.27961272\n",
      "Iteration 11, loss = 0.27970759\n",
      "Iteration 12, loss = 0.27871128\n",
      "Iteration 13, loss = 0.27780232\n",
      "Iteration 14, loss = 0.27895779\n",
      "Iteration 15, loss = 0.27715647\n",
      "Iteration 16, loss = 0.27672673\n",
      "Iteration 17, loss = 0.27804020\n",
      "Iteration 18, loss = 0.27793997\n",
      "Iteration 19, loss = 0.27697818\n",
      "Iteration 20, loss = 0.27696757\n",
      "Iteration 21, loss = 0.27758835\n",
      "Iteration 22, loss = 0.27640980\n",
      "Iteration 23, loss = 0.27730275\n",
      "Iteration 24, loss = 0.27754469\n",
      "Iteration 25, loss = 0.27570938\n",
      "Iteration 26, loss = 0.27737093\n",
      "Iteration 27, loss = 0.28006891\n",
      "Iteration 28, loss = 0.27700028\n",
      "Iteration 29, loss = 0.27664550\n",
      "Iteration 30, loss = 0.27669926\n",
      "Iteration 31, loss = 0.27658486\n",
      "Iteration 32, loss = 0.27574560\n",
      "Iteration 33, loss = 0.27629221\n",
      "Iteration 34, loss = 0.27650962\n",
      "Iteration 35, loss = 0.27699359\n",
      "Iteration 36, loss = 0.27445639\n",
      "Iteration 37, loss = 0.27433398\n",
      "Iteration 38, loss = 0.27569552\n",
      "Iteration 39, loss = 0.27443579\n",
      "Iteration 40, loss = 0.27620859\n",
      "Iteration 41, loss = 0.27619454\n",
      "Iteration 42, loss = 0.27483873\n",
      "Iteration 43, loss = 0.27334561\n",
      "Iteration 44, loss = 0.27487052\n",
      "Iteration 45, loss = 0.27452073\n",
      "Iteration 46, loss = 0.27540468\n",
      "Iteration 47, loss = 0.27507263\n",
      "Iteration 48, loss = 0.27286105\n",
      "Iteration 49, loss = 0.27321795\n",
      "Iteration 50, loss = 0.27270119\n",
      "Iteration 51, loss = 0.27438923\n",
      "Iteration 52, loss = 0.27391303\n",
      "Iteration 53, loss = 0.27380646\n",
      "Iteration 54, loss = 0.27383792\n",
      "Iteration 55, loss = 0.27312671\n",
      "Iteration 56, loss = 0.27425433\n",
      "Iteration 57, loss = 0.27351774\n",
      "Iteration 58, loss = 0.27476414\n",
      "Iteration 59, loss = 0.27379555\n",
      "Iteration 60, loss = 0.27437092\n",
      "Iteration 61, loss = 0.27261934\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30530530\n",
      "Iteration 2, loss = 0.28340721\n",
      "Iteration 3, loss = 0.28103389\n",
      "Iteration 4, loss = 0.28016732\n",
      "Iteration 5, loss = 0.28147864\n",
      "Iteration 6, loss = 0.28092608\n",
      "Iteration 7, loss = 0.28063157\n",
      "Iteration 8, loss = 0.28116706\n",
      "Iteration 9, loss = 0.27964289\n",
      "Iteration 10, loss = 0.27910839\n",
      "Iteration 11, loss = 0.27838956\n",
      "Iteration 12, loss = 0.27971862\n",
      "Iteration 13, loss = 0.27803649\n",
      "Iteration 14, loss = 0.27968416\n",
      "Iteration 15, loss = 0.27797573\n",
      "Iteration 16, loss = 0.27797371\n",
      "Iteration 17, loss = 0.27846704\n",
      "Iteration 18, loss = 0.27863817\n",
      "Iteration 19, loss = 0.27718929\n",
      "Iteration 20, loss = 0.27611122\n",
      "Iteration 21, loss = 0.27725808\n",
      "Iteration 22, loss = 0.28097369\n",
      "Iteration 23, loss = 0.27734493\n",
      "Iteration 24, loss = 0.27773178\n",
      "Iteration 25, loss = 0.27737507\n",
      "Iteration 26, loss = 0.27610443\n",
      "Iteration 27, loss = 0.27670255\n",
      "Iteration 28, loss = 0.27702956\n",
      "Iteration 29, loss = 0.27785393\n",
      "Iteration 30, loss = 0.27552208\n",
      "Iteration 31, loss = 0.27816036\n",
      "Iteration 32, loss = 0.27714158\n",
      "Iteration 33, loss = 0.27590474\n",
      "Iteration 34, loss = 0.27511006\n",
      "Iteration 35, loss = 0.27385654\n",
      "Iteration 36, loss = 0.27497619\n",
      "Iteration 37, loss = 0.27489214\n",
      "Iteration 38, loss = 0.27561061\n",
      "Iteration 39, loss = 0.27479106\n",
      "Iteration 40, loss = 0.27720264\n",
      "Iteration 41, loss = 0.27538286\n",
      "Iteration 42, loss = 0.27447590\n",
      "Iteration 43, loss = 0.27486006\n",
      "Iteration 44, loss = 0.27447675\n",
      "Iteration 45, loss = 0.27367529\n",
      "Iteration 46, loss = 0.27312806\n",
      "Iteration 47, loss = 0.27382325\n",
      "Iteration 48, loss = 0.27283316\n",
      "Iteration 49, loss = 0.27438020\n",
      "Iteration 50, loss = 0.27463368\n",
      "Iteration 51, loss = 0.27535011\n",
      "Iteration 52, loss = 0.27380134\n",
      "Iteration 53, loss = 0.27464284\n",
      "Iteration 54, loss = 0.27392003\n",
      "Iteration 55, loss = 0.27406482\n",
      "Iteration 56, loss = 0.27404430\n",
      "Iteration 57, loss = 0.27340223\n",
      "Iteration 58, loss = 0.27236925\n",
      "Iteration 59, loss = 0.27390305\n",
      "Iteration 60, loss = 0.27348998\n",
      "Iteration 61, loss = 0.27379153\n",
      "Iteration 62, loss = 0.27377884\n",
      "Iteration 63, loss = 0.27289778\n",
      "Iteration 64, loss = 0.27238896\n",
      "Iteration 65, loss = 0.27210541\n",
      "Iteration 66, loss = 0.27201402\n",
      "Iteration 67, loss = 0.27341163\n",
      "Iteration 68, loss = 0.27290801\n",
      "Iteration 69, loss = 0.27267180\n",
      "Iteration 70, loss = 0.27158030\n",
      "Iteration 71, loss = 0.27264151\n",
      "Iteration 72, loss = 0.27448579\n",
      "Iteration 73, loss = 0.27242578\n",
      "Iteration 74, loss = 0.27138347\n",
      "Iteration 75, loss = 0.27224491\n",
      "Iteration 76, loss = 0.27210640\n",
      "Iteration 77, loss = 0.27278730\n",
      "Iteration 78, loss = 0.27228650\n",
      "Iteration 79, loss = 0.27354576\n",
      "Iteration 80, loss = 0.27083961\n",
      "Iteration 81, loss = 0.27166421\n",
      "Iteration 82, loss = 0.27253874\n",
      "Iteration 83, loss = 0.27191414\n",
      "Iteration 84, loss = 0.27347839\n",
      "Iteration 85, loss = 0.27169333\n",
      "Iteration 86, loss = 0.27193257\n",
      "Iteration 87, loss = 0.27203906\n",
      "Iteration 88, loss = 0.27244722\n",
      "Iteration 89, loss = 0.27252634\n",
      "Iteration 90, loss = 0.27218625\n",
      "Iteration 91, loss = 0.27234756\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30761692\n",
      "Iteration 2, loss = 0.28434626\n",
      "Iteration 3, loss = 0.28142802\n",
      "Iteration 4, loss = 0.28104519\n",
      "Iteration 5, loss = 0.28006155\n",
      "Iteration 6, loss = 0.28112247\n",
      "Iteration 7, loss = 0.28159118\n",
      "Iteration 8, loss = 0.27976157\n",
      "Iteration 9, loss = 0.28040260\n",
      "Iteration 10, loss = 0.28188963\n",
      "Iteration 11, loss = 0.27909395\n",
      "Iteration 12, loss = 0.27727480\n",
      "Iteration 13, loss = 0.27831413\n",
      "Iteration 14, loss = 0.27674757\n",
      "Iteration 15, loss = 0.27799242\n",
      "Iteration 16, loss = 0.27800146\n",
      "Iteration 17, loss = 0.27860491\n",
      "Iteration 18, loss = 0.27814548\n",
      "Iteration 19, loss = 0.27791732\n",
      "Iteration 20, loss = 0.27754984\n",
      "Iteration 21, loss = 0.27707217\n",
      "Iteration 22, loss = 0.27791414\n",
      "Iteration 23, loss = 0.27773765\n",
      "Iteration 24, loss = 0.27561156\n",
      "Iteration 25, loss = 0.27683819\n",
      "Iteration 26, loss = 0.27527961\n",
      "Iteration 27, loss = 0.27571540\n",
      "Iteration 28, loss = 0.27518153\n",
      "Iteration 29, loss = 0.27538953\n",
      "Iteration 30, loss = 0.27560010\n",
      "Iteration 31, loss = 0.27881125\n",
      "Iteration 32, loss = 0.27715447\n",
      "Iteration 33, loss = 0.27516344\n",
      "Iteration 34, loss = 0.27626462\n",
      "Iteration 35, loss = 0.27493188\n",
      "Iteration 36, loss = 0.27572233\n",
      "Iteration 37, loss = 0.27345172\n",
      "Iteration 38, loss = 0.27462744\n",
      "Iteration 39, loss = 0.27473606\n",
      "Iteration 40, loss = 0.27552479\n",
      "Iteration 41, loss = 0.27439101\n",
      "Iteration 42, loss = 0.27408896\n",
      "Iteration 43, loss = 0.27411471\n",
      "Iteration 44, loss = 0.27409464\n",
      "Iteration 45, loss = 0.27622955\n",
      "Iteration 46, loss = 0.27711035\n",
      "Iteration 47, loss = 0.27448399\n",
      "Iteration 48, loss = 0.27543789\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30448057\n",
      "Iteration 2, loss = 0.28481694\n",
      "Iteration 3, loss = 0.28152570\n",
      "Iteration 4, loss = 0.28085391\n",
      "Iteration 5, loss = 0.28036388\n",
      "Iteration 6, loss = 0.28049939\n",
      "Iteration 7, loss = 0.27875631\n",
      "Iteration 8, loss = 0.28005954\n",
      "Iteration 9, loss = 0.27893830\n",
      "Iteration 10, loss = 0.27912380\n",
      "Iteration 11, loss = 0.28049001\n",
      "Iteration 12, loss = 0.27960884\n",
      "Iteration 13, loss = 0.27823047\n",
      "Iteration 14, loss = 0.27988560\n",
      "Iteration 15, loss = 0.27780532\n",
      "Iteration 16, loss = 0.27764990\n",
      "Iteration 17, loss = 0.27886512\n",
      "Iteration 18, loss = 0.27696569\n",
      "Iteration 19, loss = 0.27840744\n",
      "Iteration 20, loss = 0.27639678\n",
      "Iteration 21, loss = 0.27780156\n",
      "Iteration 22, loss = 0.27702147\n",
      "Iteration 23, loss = 0.27809284\n",
      "Iteration 24, loss = 0.27564128\n",
      "Iteration 25, loss = 0.27852711\n",
      "Iteration 26, loss = 0.27635087\n",
      "Iteration 27, loss = 0.27738199\n",
      "Iteration 28, loss = 0.27712281\n",
      "Iteration 29, loss = 0.27756510\n",
      "Iteration 30, loss = 0.27502549\n",
      "Iteration 31, loss = 0.27582175\n",
      "Iteration 32, loss = 0.27409986\n",
      "Iteration 33, loss = 0.27497648\n",
      "Iteration 34, loss = 0.27455942\n",
      "Iteration 35, loss = 0.27556098\n",
      "Iteration 36, loss = 0.27533195\n",
      "Iteration 37, loss = 0.27391425\n",
      "Iteration 38, loss = 0.27520773\n",
      "Iteration 39, loss = 0.27545261\n",
      "Iteration 40, loss = 0.27472012\n",
      "Iteration 41, loss = 0.27603170\n",
      "Iteration 42, loss = 0.27402513\n",
      "Iteration 43, loss = 0.27355838\n",
      "Iteration 44, loss = 0.27441214\n",
      "Iteration 45, loss = 0.27378709\n",
      "Iteration 46, loss = 0.27385481\n",
      "Iteration 47, loss = 0.27505438\n",
      "Iteration 48, loss = 0.27545039\n",
      "Iteration 49, loss = 0.27487046\n",
      "Iteration 50, loss = 0.27511491\n",
      "Iteration 51, loss = 0.27348535\n",
      "Iteration 52, loss = 0.27384678\n",
      "Iteration 53, loss = 0.27451421\n",
      "Iteration 54, loss = 0.27330675\n",
      "Iteration 55, loss = 0.27294806\n",
      "Iteration 56, loss = 0.27436403\n",
      "Iteration 57, loss = 0.27438093\n",
      "Iteration 58, loss = 0.27385200\n",
      "Iteration 59, loss = 0.27378127\n",
      "Iteration 60, loss = 0.27392012\n",
      "Iteration 61, loss = 0.27323184\n",
      "Iteration 62, loss = 0.27467256\n",
      "Iteration 63, loss = 0.27325461\n",
      "Iteration 64, loss = 0.27268752\n",
      "Iteration 65, loss = 0.27349956\n",
      "Iteration 66, loss = 0.27268542\n",
      "Iteration 67, loss = 0.27330215\n",
      "Iteration 68, loss = 0.27307441\n",
      "Iteration 69, loss = 0.27261946\n",
      "Iteration 70, loss = 0.27640234\n",
      "Iteration 71, loss = 0.27716326\n",
      "Iteration 72, loss = 0.27216087\n",
      "Iteration 73, loss = 0.27154250\n",
      "Iteration 74, loss = 0.27395570\n",
      "Iteration 75, loss = 0.27134760\n",
      "Iteration 76, loss = 0.27369576\n",
      "Iteration 77, loss = 0.27168370\n",
      "Iteration 78, loss = 0.27365639\n",
      "Iteration 79, loss = 0.27404079\n",
      "Iteration 80, loss = 0.27313755\n",
      "Iteration 81, loss = 0.27424541\n",
      "Iteration 82, loss = 0.27241332\n",
      "Iteration 83, loss = 0.27241257\n",
      "Iteration 84, loss = 0.27337699\n",
      "Iteration 85, loss = 0.27308632\n",
      "Iteration 86, loss = 0.27062982\n",
      "Iteration 87, loss = 0.27327867\n",
      "Iteration 88, loss = 0.27330250\n",
      "Iteration 89, loss = 0.27308131\n",
      "Iteration 90, loss = 0.27363146\n",
      "Iteration 91, loss = 0.27254306\n",
      "Iteration 92, loss = 0.27254239\n",
      "Iteration 93, loss = 0.27135203\n",
      "Iteration 94, loss = 0.27205559\n",
      "Iteration 95, loss = 0.27135720\n",
      "Iteration 96, loss = 0.27178329\n",
      "Iteration 97, loss = 0.27241244\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.31173467\n",
      "Iteration 2, loss = 0.28239935\n",
      "Iteration 3, loss = 0.28174030\n",
      "Iteration 4, loss = 0.28144943\n",
      "Iteration 5, loss = 0.28166478\n",
      "Iteration 6, loss = 0.28044425\n",
      "Iteration 7, loss = 0.28029388\n",
      "Iteration 8, loss = 0.28030715\n",
      "Iteration 9, loss = 0.28196893\n",
      "Iteration 10, loss = 0.27960619\n",
      "Iteration 11, loss = 0.27906077\n",
      "Iteration 12, loss = 0.28002410\n",
      "Iteration 13, loss = 0.28106122\n",
      "Iteration 14, loss = 0.27956251\n",
      "Iteration 15, loss = 0.27937615\n",
      "Iteration 16, loss = 0.28028864\n",
      "Iteration 17, loss = 0.27872989\n",
      "Iteration 18, loss = 0.27881665\n",
      "Iteration 19, loss = 0.27889327\n",
      "Iteration 20, loss = 0.27796374\n",
      "Iteration 21, loss = 0.27687114\n",
      "Iteration 22, loss = 0.27741293\n",
      "Iteration 23, loss = 0.27602311\n",
      "Iteration 24, loss = 0.27856176\n",
      "Iteration 25, loss = 0.27667542\n",
      "Iteration 26, loss = 0.27637670\n",
      "Iteration 27, loss = 0.27659357\n",
      "Iteration 28, loss = 0.27657162\n",
      "Iteration 29, loss = 0.27460372\n",
      "Iteration 30, loss = 0.27467630\n",
      "Iteration 31, loss = 0.27555478\n",
      "Iteration 32, loss = 0.27470066\n",
      "Iteration 33, loss = 0.27416097\n",
      "Iteration 34, loss = 0.27501490\n",
      "Iteration 35, loss = 0.27398176\n",
      "Iteration 36, loss = 0.27485612\n",
      "Iteration 37, loss = 0.27466671\n",
      "Iteration 38, loss = 0.27419025\n",
      "Iteration 39, loss = 0.27494223\n",
      "Iteration 40, loss = 0.27273605\n",
      "Iteration 41, loss = 0.27251002\n",
      "Iteration 42, loss = 0.27435815\n",
      "Iteration 43, loss = 0.27274396\n",
      "Iteration 44, loss = 0.27396008\n",
      "Iteration 45, loss = 0.27230437\n",
      "Iteration 46, loss = 0.27315150\n",
      "Iteration 47, loss = 0.27243246\n",
      "Iteration 48, loss = 0.27339884\n",
      "Iteration 49, loss = 0.27244770\n",
      "Iteration 50, loss = 0.27141283\n",
      "Iteration 51, loss = 0.27192141\n",
      "Iteration 52, loss = 0.27059993\n",
      "Iteration 53, loss = 0.27035200\n",
      "Iteration 54, loss = 0.27159920\n",
      "Iteration 55, loss = 0.27299051\n",
      "Iteration 56, loss = 0.27117863\n",
      "Iteration 57, loss = 0.27266310\n",
      "Iteration 58, loss = 0.27315123\n",
      "Iteration 59, loss = 0.26983447\n",
      "Iteration 60, loss = 0.27123495\n",
      "Iteration 61, loss = 0.27269365\n",
      "Iteration 62, loss = 0.27066694\n",
      "Iteration 63, loss = 0.27095907\n",
      "Iteration 64, loss = 0.27082952\n",
      "Iteration 65, loss = 0.27013165\n",
      "Iteration 66, loss = 0.27092744\n",
      "Iteration 67, loss = 0.27207909\n",
      "Iteration 68, loss = 0.27135549\n",
      "Iteration 69, loss = 0.27015504\n",
      "Iteration 70, loss = 0.27063393\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per Hyperparameter tuning, best parameters are:\n",
      "{'hidden_layer_sizes': 5, 'learning_rate_init': 0.01}\n",
      "Iteration 1, loss = 0.41840333\n",
      "Iteration 2, loss = 0.34394201\n",
      "Iteration 1, loss = 0.41814265\n",
      "Iteration 3, loss = 0.32920225\n",
      "Iteration 2, loss = 0.34368830\n",
      "Iteration 4, loss = 0.32846976\n",
      "Iteration 3, loss = 0.32978133\n",
      "Iteration 5, loss = 0.32592796\n",
      "Iteration 6, loss = 0.32099280\n",
      "Iteration 4, loss = 0.32841086\n",
      "Iteration 7, loss = 0.31657918\n",
      "Iteration 5, loss = 0.32530502\n",
      "Iteration 8, loss = 0.31207478\n",
      "Iteration 6, loss = 0.32044546\n",
      "Iteration 9, loss = 0.30635887\n",
      "Iteration 7, loss = 0.31625890\n",
      "Iteration 10, loss = 0.30179150\n",
      "Iteration 8, loss = 0.31155263\n",
      "Iteration 11, loss = 0.29630361\n",
      "Iteration 9, loss = 0.30578443\n",
      "Iteration 12, loss = 0.29196475\n",
      "Iteration 10, loss = 0.30088891\n",
      "Iteration 13, loss = 0.28786884\n",
      "Iteration 11, loss = 0.29527240\n",
      "Iteration 14, loss = 0.28426659\n",
      "Iteration 12, loss = 0.29079363\n",
      "Iteration 15, loss = 0.28108950\n",
      "Iteration 13, loss = 0.28704756\n",
      "Iteration 16, loss = 0.27849431\n",
      "Iteration 17, loss = 0.27610011\n",
      "Iteration 14, loss = 0.28308311\n",
      "Iteration 18, loss = 0.27361900\n",
      "Iteration 15, loss = 0.28010679\n",
      "Iteration 19, loss = 0.27148429\n",
      "Iteration 16, loss = 0.27783621\n",
      "Iteration 20, loss = 0.27009664\n",
      "Iteration 17, loss = 0.27542997\n",
      "Iteration 18, loss = 0.27308412\n",
      "Iteration 21, loss = 0.26735133\n",
      "Iteration 19, loss = 0.27104253\n",
      "Iteration 22, loss = 0.26606089\n",
      "Iteration 20, loss = 0.26960548Iteration 23, loss = 0.26523313\n",
      "\n",
      "Iteration 24, loss = 0.26390149\n",
      "Iteration 21, loss = 0.26735450\n",
      "Iteration 25, loss = 0.26271446\n",
      "Iteration 22, loss = 0.26608940\n",
      "Iteration 26, loss = 0.26240523\n",
      "Iteration 23, loss = 0.26549035\n",
      "Iteration 27, loss = 0.26110982\n",
      "Iteration 24, loss = 0.26359403\n",
      "Iteration 28, loss = 0.26069284\n",
      "Iteration 25, loss = 0.26340682\n",
      "Iteration 29, loss = 0.25936657\n",
      "Iteration 26, loss = 0.26226410\n",
      "Iteration 30, loss = 0.25866395\n",
      "Iteration 27, loss = 0.26116322\n",
      "Iteration 31, loss = 0.25826850\n",
      "Iteration 28, loss = 0.26030664\n",
      "Iteration 32, loss = 0.25775467\n",
      "Iteration 33, loss = 0.25690388\n",
      "Iteration 29, loss = 0.25933564\n",
      "Iteration 34, loss = 0.25649519\n",
      "Iteration 30, loss = 0.25893877\n",
      "Iteration 35, loss = 0.25624276\n",
      "Iteration 31, loss = 0.25821710\n",
      "Iteration 36, loss = 0.25630592\n",
      "Iteration 32, loss = 0.25784280\n",
      "Iteration 37, loss = 0.25619881\n",
      "Iteration 33, loss = 0.25697946\n",
      "Iteration 38, loss = 0.25603727\n",
      "Iteration 34, loss = 0.25660168\n",
      "Iteration 39, loss = 0.25474416\n",
      "Iteration 35, loss = 0.25614776\n",
      "Iteration 40, loss = 0.25436783\n",
      "Iteration 36, loss = 0.25627671\n",
      "Iteration 41, loss = 0.25369336\n",
      "Iteration 42, loss = 0.25362634\n",
      "Iteration 37, loss = 0.25610730\n",
      "Iteration 43, loss = 0.25284352\n",
      "Iteration 38, loss = 0.25609880\n",
      "Iteration 44, loss = 0.25235716\n",
      "Iteration 39, loss = 0.25491971\n",
      "Iteration 40, loss = 0.25434628\n",
      "Iteration 45, loss = 0.25246096\n",
      "Iteration 41, loss = 0.25403562\n",
      "Iteration 46, loss = 0.25225504\n",
      "Iteration 47, loss = 0.25198744\n",
      "Iteration 42, loss = 0.25387928\n",
      "Iteration 48, loss = 0.25085786\n",
      "Iteration 43, loss = 0.25316003\n",
      "Iteration 49, loss = 0.25087448\n",
      "Iteration 44, loss = 0.25259476\n",
      "Iteration 50, loss = 0.25031075\n",
      "Iteration 45, loss = 0.25264625\n",
      "Iteration 51, loss = 0.25079537\n",
      "Iteration 46, loss = 0.25232755\n",
      "Iteration 52, loss = 0.24986522\n",
      "Iteration 53, loss = 0.24998998\n",
      "Iteration 47, loss = 0.25206667\n",
      "Iteration 54, loss = 0.24968865\n",
      "Iteration 48, loss = 0.25127180\n",
      "Iteration 55, loss = 0.24889472\n",
      "Iteration 49, loss = 0.25099086\n",
      "Iteration 56, loss = 0.24872970\n",
      "Iteration 50, loss = 0.25068333\n",
      "Iteration 57, loss = 0.24823600\n",
      "Iteration 51, loss = 0.25087443\n",
      "Iteration 58, loss = 0.24804125\n",
      "Iteration 52, loss = 0.24995442\n",
      "Iteration 59, loss = 0.24844644\n",
      "Iteration 60, loss = 0.24760016Iteration 53, loss = 0.24998926\n",
      "\n",
      "Iteration 61, loss = 0.24805188\n",
      "Iteration 54, loss = 0.24972454\n",
      "Iteration 62, loss = 0.24938194\n",
      "Iteration 55, loss = 0.24881925\n",
      "Iteration 63, loss = 0.24604966\n",
      "Iteration 56, loss = 0.24843351\n",
      "Iteration 64, loss = 0.24682438\n",
      "Iteration 57, loss = 0.24810536\n",
      "Iteration 65, loss = 0.24599415\n",
      "Iteration 58, loss = 0.24764592\n",
      "Iteration 66, loss = 0.24562431\n",
      "Iteration 59, loss = 0.24804048\n",
      "Iteration 67, loss = 0.24539584\n",
      "Iteration 60, loss = 0.24660137\n",
      "Iteration 68, loss = 0.24491999\n",
      "Iteration 61, loss = 0.24808302\n",
      "Iteration 69, loss = 0.24543504\n",
      "Iteration 62, loss = 0.24755841\n",
      "Iteration 70, loss = 0.24471977\n",
      "Iteration 63, loss = 0.24511600\n",
      "Iteration 71, loss = 0.24420236\n",
      "Iteration 64, loss = 0.24521839\n",
      "Iteration 72, loss = 0.24404335\n",
      "Iteration 65, loss = 0.24451443\n",
      "Iteration 73, loss = 0.24329991\n",
      "Iteration 66, loss = 0.24391151\n",
      "Iteration 74, loss = 0.24293553\n",
      "Iteration 67, loss = 0.24344843\n",
      "Iteration 75, loss = 0.24277216\n",
      "Iteration 68, loss = 0.24309859\n",
      "Iteration 76, loss = 0.24289624\n",
      "Iteration 69, loss = 0.24338502\n",
      "Iteration 77, loss = 0.24167714\n",
      "Iteration 70, loss = 0.24270524\n",
      "Iteration 78, loss = 0.24176735\n",
      "Iteration 71, loss = 0.24201980Iteration 79, loss = 0.24142138\n",
      "\n",
      "Iteration 80, loss = 0.24081346\n",
      "Iteration 72, loss = 0.24184929\n",
      "Iteration 81, loss = 0.24087655\n",
      "Iteration 73, loss = 0.24063853\n",
      "Iteration 82, loss = 0.24022560\n",
      "Iteration 74, loss = 0.24029750\n",
      "Iteration 83, loss = 0.24029793\n",
      "Iteration 75, loss = 0.23966739\n",
      "Iteration 84, loss = 0.24002840\n",
      "Iteration 76, loss = 0.24013422\n",
      "Iteration 85, loss = 0.23951949\n",
      "Iteration 77, loss = 0.23872200\n",
      "Iteration 86, loss = 0.24004762\n",
      "Iteration 78, loss = 0.23871886\n",
      "Iteration 87, loss = 0.23865217\n",
      "Iteration 88, loss = 0.23846223\n",
      "Iteration 79, loss = 0.23802876\n",
      "Iteration 89, loss = 0.23789654\n",
      "Iteration 80, loss = 0.23742964\n",
      "Iteration 90, loss = 0.23760352\n",
      "Iteration 81, loss = 0.23687487\n",
      "Iteration 91, loss = 0.23731155\n",
      "Iteration 82, loss = 0.23648958\n",
      "Iteration 92, loss = 0.23811915\n",
      "Iteration 83, loss = 0.23614429\n",
      "Iteration 93, loss = 0.23719512\n",
      "Iteration 84, loss = 0.23587463\n",
      "Iteration 94, loss = 0.23695838\n",
      "Iteration 85, loss = 0.23506407\n",
      "Iteration 86, loss = 0.23589135\n",
      "Iteration 95, loss = 0.23575026\n",
      "Iteration 87, loss = 0.23412225\n",
      "Iteration 96, loss = 0.23520119\n",
      "Iteration 88, loss = 0.23403860\n",
      "Iteration 97, loss = 0.23508579\n",
      "Iteration 89, loss = 0.23348753\n",
      "Iteration 98, loss = 0.23475471\n",
      "Iteration 90, loss = 0.23298131\n",
      "Iteration 99, loss = 0.23421006\n",
      "Iteration 91, loss = 0.23229985\n",
      "Iteration 100, loss = 0.23358113\n",
      "Iteration 92, loss = 0.23268958\n",
      "Iteration 101, loss = 0.23336823\n",
      "Iteration 93, loss = 0.23182229\n",
      "Iteration 102, loss = 0.23252798\n",
      "Iteration 94, loss = 0.23164684\n",
      "Iteration 103, loss = 0.23246692Iteration 95, loss = 0.23061020\n",
      "\n",
      "Iteration 96, loss = 0.23018254\n",
      "Iteration 104, loss = 0.23172934\n",
      "Iteration 97, loss = 0.22966847\n",
      "Iteration 105, loss = 0.23177284\n",
      "Iteration 98, loss = 0.22930743\n",
      "Iteration 106, loss = 0.23083434\n",
      "Iteration 99, loss = 0.22868779\n",
      "Iteration 107, loss = 0.23181253\n",
      "Iteration 100, loss = 0.22840448\n",
      "Iteration 108, loss = 0.23054643\n",
      "Iteration 101, loss = 0.22821509\n",
      "Iteration 109, loss = 0.23071907\n",
      "Iteration 102, loss = 0.22716247\n",
      "Iteration 110, loss = 0.23011703\n",
      "Iteration 103, loss = 0.22750269\n",
      "Iteration 111, loss = 0.22898854\n",
      "Iteration 104, loss = 0.22650239\n",
      "Iteration 112, loss = 0.22846188\n",
      "Iteration 105, loss = 0.22636565\n",
      "Iteration 113, loss = 0.22821254\n",
      "Iteration 106, loss = 0.22549137\n",
      "Iteration 107, loss = 0.22651233Iteration 114, loss = 0.22740650\n",
      "\n",
      "Iteration 108, loss = 0.22539655\n",
      "Iteration 115, loss = 0.22874641\n",
      "Iteration 109, loss = 0.22525001\n",
      "Iteration 116, loss = 0.22705725\n",
      "Iteration 110, loss = 0.22443188\n",
      "Iteration 117, loss = 0.22772935\n",
      "Iteration 111, loss = 0.22346721\n",
      "Iteration 118, loss = 0.22653914\n",
      "Iteration 112, loss = 0.22308247\n",
      "Iteration 119, loss = 0.22597880\n",
      "Iteration 113, loss = 0.22306943\n",
      "Iteration 120, loss = 0.22491478\n",
      "Iteration 114, loss = 0.22230790\n",
      "Iteration 115, loss = 0.22299397Iteration 121, loss = 0.22487827\n",
      "\n",
      "Iteration 116, loss = 0.22197842\n",
      "Iteration 122, loss = 0.22360656\n",
      "Iteration 117, loss = 0.22166587\n",
      "Iteration 123, loss = 0.22315002\n",
      "Iteration 118, loss = 0.22122194\n",
      "Iteration 124, loss = 0.22267976\n",
      "Iteration 119, loss = 0.22070119\n",
      "Iteration 125, loss = 0.22229309\n",
      "Iteration 120, loss = 0.21968598\n",
      "Iteration 126, loss = 0.22192604\n",
      "Iteration 121, loss = 0.22016415\n",
      "Iteration 127, loss = 0.22130448\n",
      "Iteration 122, loss = 0.21874803\n",
      "Iteration 128, loss = 0.22149423\n",
      "Iteration 123, loss = 0.21878118\n",
      "Iteration 129, loss = 0.22133767\n",
      "Iteration 124, loss = 0.21826407\n",
      "Iteration 130, loss = 0.21998296\n",
      "Iteration 125, loss = 0.21747196\n",
      "Iteration 131, loss = 0.21956955\n",
      "Iteration 126, loss = 0.21743502\n",
      "Iteration 132, loss = 0.21923447\n",
      "Iteration 127, loss = 0.21702390\n",
      "Iteration 133, loss = 0.21890861Iteration 128, loss = 0.21669224\n",
      "\n",
      "Iteration 129, loss = 0.21668361\n",
      "Iteration 134, loss = 0.21870586\n",
      "Iteration 130, loss = 0.21568622\n",
      "Iteration 135, loss = 0.21831154\n",
      "Iteration 131, loss = 0.21533651\n",
      "Iteration 136, loss = 0.21737133\n",
      "Iteration 132, loss = 0.21491806\n",
      "Iteration 137, loss = 0.21668599\n",
      "Iteration 133, loss = 0.21502786\n",
      "Iteration 138, loss = 0.21670128\n",
      "Iteration 134, loss = 0.21411427\n",
      "Iteration 139, loss = 0.21654052\n",
      "Iteration 135, loss = 0.21394543\n",
      "Iteration 140, loss = 0.21589254\n",
      "Iteration 136, loss = 0.21326250\n",
      "Iteration 141, loss = 0.21489992\n",
      "Iteration 137, loss = 0.21319387\n",
      "Iteration 138, loss = 0.21310392\n",
      "Iteration 142, loss = 0.21452944\n",
      "Iteration 139, loss = 0.21269954\n",
      "Iteration 143, loss = 0.21447187\n",
      "Iteration 140, loss = 0.21231973\n",
      "Iteration 144, loss = 0.21359493\n",
      "Iteration 141, loss = 0.21163912\n",
      "Iteration 145, loss = 0.21369115\n",
      "Iteration 142, loss = 0.21135772\n",
      "Iteration 146, loss = 0.21238962\n",
      "Iteration 143, loss = 0.21144352\n",
      "Iteration 147, loss = 0.21271858\n",
      "Iteration 144, loss = 0.21150086\n",
      "Iteration 148, loss = 0.21104896\n",
      "Iteration 145, loss = 0.21077862\n",
      "Iteration 149, loss = 0.21214307\n",
      "Iteration 146, loss = 0.21004649\n",
      "Iteration 150, loss = 0.21127710\n",
      "Iteration 147, loss = 0.20976720\n",
      "Iteration 151, loss = 0.21044227\n",
      "Iteration 152, loss = 0.20996464\n",
      "Iteration 148, loss = 0.20899109\n",
      "Iteration 153, loss = 0.21002647\n",
      "Iteration 149, loss = 0.20908393\n",
      "Iteration 154, loss = 0.20889574Iteration 150, loss = 0.20882007\n",
      "\n",
      "Iteration 151, loss = 0.20827576\n",
      "Iteration 155, loss = 0.20849554\n",
      "Iteration 152, loss = 0.20789179\n",
      "Iteration 156, loss = 0.20796366\n",
      "Iteration 153, loss = 0.20815073\n",
      "Iteration 157, loss = 0.20748598\n",
      "Iteration 154, loss = 0.20724273\n",
      "Iteration 158, loss = 0.20710873\n",
      "Iteration 155, loss = 0.20708496\n",
      "Iteration 159, loss = 0.20677153\n",
      "Iteration 156, loss = 0.20668393\n",
      "Iteration 160, loss = 0.20641622\n",
      "Iteration 157, loss = 0.20631149\n",
      "Iteration 161, loss = 0.20584028\n",
      "Iteration 158, loss = 0.20577792\n",
      "Iteration 162, loss = 0.20574657\n",
      "Iteration 159, loss = 0.20590692\n",
      "Iteration 160, loss = 0.20519938Iteration 163, loss = 0.20572581\n",
      "\n",
      "Iteration 161, loss = 0.20495202\n",
      "Iteration 164, loss = 0.20507191\n",
      "Iteration 162, loss = 0.20479706\n",
      "Iteration 165, loss = 0.20408907\n",
      "Iteration 163, loss = 0.20454834\n",
      "Iteration 166, loss = 0.20379459\n",
      "Iteration 164, loss = 0.20412720\n",
      "Iteration 167, loss = 0.20323364\n",
      "Iteration 165, loss = 0.20366156\n",
      "Iteration 168, loss = 0.20289023\n",
      "Iteration 166, loss = 0.20342972\n",
      "Iteration 167, loss = 0.20289123Iteration 169, loss = 0.20259717\n",
      "\n",
      "Iteration 170, loss = 0.20257122\n",
      "Iteration 168, loss = 0.20273792\n",
      "Iteration 169, loss = 0.20232917\n",
      "Iteration 171, loss = 0.20174080\n",
      "Iteration 170, loss = 0.20229266\n",
      "Iteration 172, loss = 0.20166393\n",
      "Iteration 171, loss = 0.20197683\n",
      "Iteration 173, loss = 0.20127765\n",
      "Iteration 172, loss = 0.20166821\n",
      "Iteration 174, loss = 0.20114215\n",
      "Iteration 173, loss = 0.20101395\n",
      "Iteration 175, loss = 0.20114017\n",
      "Iteration 174, loss = 0.20136453\n",
      "Iteration 176, loss = 0.20010600\n",
      "Iteration 175, loss = 0.20127669\n",
      "Iteration 177, loss = 0.20036430\n",
      "Iteration 176, loss = 0.20049753\n",
      "Iteration 178, loss = 0.19924609\n",
      "Iteration 177, loss = 0.20038380\n",
      "Iteration 179, loss = 0.19887860\n",
      "Iteration 178, loss = 0.19982665\n",
      "Iteration 180, loss = 0.19941250\n",
      "Iteration 179, loss = 0.19901013\n",
      "Iteration 181, loss = 0.19790142\n",
      "Iteration 180, loss = 0.20000737\n",
      "Iteration 182, loss = 0.19853696\n",
      "Iteration 181, loss = 0.19840959\n",
      "Iteration 183, loss = 0.19747387\n",
      "Iteration 182, loss = 0.19893645\n",
      "Iteration 184, loss = 0.19682697\n",
      "Iteration 183, loss = 0.19830821\n",
      "Iteration 185, loss = 0.19699872\n",
      "Iteration 184, loss = 0.19737357\n",
      "Iteration 186, loss = 0.19609469\n",
      "Iteration 185, loss = 0.19770845\n",
      "Iteration 187, loss = 0.19595746\n",
      "Iteration 186, loss = 0.19712686\n",
      "Iteration 187, loss = 0.19709000\n",
      "Iteration 188, loss = 0.19712201\n",
      "Iteration 189, loss = 0.19805264\n",
      "Iteration 188, loss = 0.19587165\n",
      "Iteration 190, loss = 0.19599148\n",
      "Iteration 191, loss = 0.19663090\n",
      "Iteration 189, loss = 0.19626588\n",
      "Iteration 192, loss = 0.19557842\n",
      "Iteration 193, loss = 0.19517086\n",
      "Iteration 190, loss = 0.19467179\n",
      "Iteration 194, loss = 0.19473959\n",
      "Iteration 191, loss = 0.19609500\n",
      "Iteration 195, loss = 0.19420925\n",
      "Iteration 196, loss = 0.19400772\n",
      "Iteration 192, loss = 0.19449376\n",
      "Iteration 197, loss = 0.19382441\n",
      "Iteration 198, loss = 0.19409314\n",
      "Iteration 199, loss = 0.19370920\n",
      "Iteration 200, loss = 0.19257389\n",
      "Iteration 193, loss = 0.19441590\n",
      "Iteration 194, loss = 0.19424440\n",
      "Iteration 195, loss = 0.19361376\n",
      "Iteration 196, loss = 0.19302629\n",
      "Iteration 197, loss = 0.19258862\n",
      "Iteration 1, loss = 0.41827830\n",
      "Iteration 198, loss = 0.19272040\n",
      "Iteration 2, loss = 0.34563356\n",
      "Iteration 3, loss = 0.33001504\n",
      "Iteration 4, loss = 0.32892850\n",
      "Iteration 199, loss = 0.19233703\n",
      "Iteration 5, loss = 0.32675608\n",
      "Iteration 6, loss = 0.32278866\n",
      "Iteration 7, loss = 0.31785349\n",
      "Iteration 200, loss = 0.19205790\n",
      "Iteration 8, loss = 0.31299727\n",
      "Iteration 9, loss = 0.30798367\n",
      "Iteration 10, loss = 0.30223987\n",
      "Iteration 11, loss = 0.29769387\n",
      "Iteration 12, loss = 0.29327288\n",
      "Iteration 13, loss = 0.28943083\n",
      "Iteration 14, loss = 0.28597661\n",
      "Iteration 15, loss = 0.28300908\n",
      "Iteration 16, loss = 0.28062889\n",
      "Iteration 17, loss = 0.27825742\n",
      "Iteration 18, loss = 0.27559827\n",
      "Iteration 19, loss = 0.27383048\n",
      "Iteration 1, loss = 0.41825506\n",
      "Iteration 20, loss = 0.27230500\n",
      "Iteration 2, loss = 0.34577226\n",
      "Iteration 21, loss = 0.27020293Iteration 3, loss = 0.32929820\n",
      "\n",
      "Iteration 4, loss = 0.32812493\n",
      "Iteration 22, loss = 0.26993283\n",
      "Iteration 23, loss = 0.26850025\n",
      "Iteration 5, loss = 0.32557608\n",
      "Iteration 24, loss = 0.26657634\n",
      "Iteration 6, loss = 0.32099324\n",
      "Iteration 25, loss = 0.26618768Iteration 7, loss = 0.31507415\n",
      "\n",
      "Iteration 8, loss = 0.30963549\n",
      "Iteration 26, loss = 0.26466887\n",
      "Iteration 27, loss = 0.26425836\n",
      "Iteration 28, loss = 0.26328426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29, loss = 0.26250488\n",
      "Iteration 9, loss = 0.30404426\n",
      "Iteration 30, loss = 0.26159534\n",
      "Iteration 31, loss = 0.26365265\n",
      "Iteration 32, loss = 0.26091074\n",
      "Iteration 10, loss = 0.29817044\n",
      "Iteration 33, loss = 0.26129152\n",
      "Iteration 34, loss = 0.26247201\n",
      "Iteration 11, loss = 0.29315749\n",
      "Iteration 35, loss = 0.26019474\n",
      "Iteration 36, loss = 0.25887561\n",
      "Iteration 37, loss = 0.25843247\n",
      "Iteration 12, loss = 0.28869422\n",
      "Iteration 38, loss = 0.25811222\n",
      "Iteration 39, loss = 0.25838528\n",
      "Iteration 13, loss = 0.28451726\n",
      "Iteration 40, loss = 0.25769111\n",
      "Iteration 41, loss = 0.25735762\n",
      "Iteration 14, loss = 0.28092402\n",
      "Iteration 42, loss = 0.25684461\n",
      "Iteration 43, loss = 0.25644676\n",
      "Iteration 15, loss = 0.27769594\n",
      "Iteration 44, loss = 0.25617867\n",
      "Iteration 16, loss = 0.27508465Iteration 45, loss = 0.25597264\n",
      "\n",
      "Iteration 46, loss = 0.25494572\n",
      "Iteration 47, loss = 0.25527522\n",
      "Iteration 48, loss = 0.25450299\n",
      "Iteration 17, loss = 0.27241787\n",
      "Iteration 49, loss = 0.25396214\n",
      "Iteration 18, loss = 0.26973840\n",
      "Iteration 50, loss = 0.25342099Iteration 19, loss = 0.26799013\n",
      "\n",
      "Iteration 20, loss = 0.26618946Iteration 51, loss = 0.25347005\n",
      "\n",
      "Iteration 21, loss = 0.26413654\n",
      "Iteration 22, loss = 0.26331377\n",
      "Iteration 23, loss = 0.26197526Iteration 52, loss = 0.25308728\n",
      "\n",
      "Iteration 24, loss = 0.26013997\n",
      "Iteration 25, loss = 0.25961695\n",
      "Iteration 53, loss = 0.25222499\n",
      "Iteration 26, loss = 0.25799435\n",
      "Iteration 54, loss = 0.25203119\n",
      "Iteration 27, loss = 0.25709254\n",
      "Iteration 28, loss = 0.25630533\n",
      "Iteration 55, loss = 0.25140554\n",
      "Iteration 29, loss = 0.25551827\n",
      "Iteration 56, loss = 0.25120175Iteration 30, loss = 0.25493461\n",
      "\n",
      "Iteration 31, loss = 0.25481178\n",
      "Iteration 57, loss = 0.25024820\n",
      "Iteration 32, loss = 0.25386771\n",
      "Iteration 58, loss = 0.24985464\n",
      "Iteration 33, loss = 0.25381366\n",
      "Iteration 59, loss = 0.25000407\n",
      "Iteration 34, loss = 0.25484436\n",
      "Iteration 60, loss = 0.24960688\n",
      "Iteration 35, loss = 0.25245080\n",
      "Iteration 61, loss = 0.24875684\n",
      "Iteration 62, loss = 0.24858358\n",
      "Iteration 36, loss = 0.25183222\n",
      "Iteration 63, loss = 0.24774585\n",
      "Iteration 37, loss = 0.25114532\n",
      "Iteration 64, loss = 0.24730008\n",
      "Iteration 38, loss = 0.25087698\n",
      "Iteration 65, loss = 0.24684977\n",
      "Iteration 39, loss = 0.25044105\n",
      "Iteration 66, loss = 0.24700068\n",
      "Iteration 40, loss = 0.25140919\n",
      "Iteration 67, loss = 0.24576777\n",
      "Iteration 41, loss = 0.25004090\n",
      "Iteration 68, loss = 0.24499306\n",
      "Iteration 42, loss = 0.25052302\n",
      "Iteration 69, loss = 0.24428817\n",
      "Iteration 43, loss = 0.24924049\n",
      "Iteration 70, loss = 0.24365199\n",
      "Iteration 44, loss = 0.24881419\n",
      "Iteration 71, loss = 0.24322590\n",
      "Iteration 45, loss = 0.24856465\n",
      "Iteration 72, loss = 0.24263818\n",
      "Iteration 46, loss = 0.24803009\n",
      "Iteration 73, loss = 0.24201961\n",
      "Iteration 47, loss = 0.24784299\n",
      "Iteration 74, loss = 0.24157140\n",
      "Iteration 48, loss = 0.24758547\n",
      "Iteration 75, loss = 0.24099309\n",
      "Iteration 49, loss = 0.24714763\n",
      "Iteration 76, loss = 0.24038577\n",
      "Iteration 50, loss = 0.24687664\n",
      "Iteration 77, loss = 0.24004608\n",
      "Iteration 51, loss = 0.24715806\n",
      "Iteration 78, loss = 0.24034285\n",
      "Iteration 52, loss = 0.24674060\n",
      "Iteration 79, loss = 0.23931623\n",
      "Iteration 53, loss = 0.24586022\n",
      "Iteration 80, loss = 0.23839755\n",
      "Iteration 54, loss = 0.24577540\n",
      "Iteration 81, loss = 0.23771921\n",
      "Iteration 55, loss = 0.24545377\n",
      "Iteration 82, loss = 0.23719102\n",
      "Iteration 56, loss = 0.24517226\n",
      "Iteration 83, loss = 0.23694439\n",
      "Iteration 57, loss = 0.24456467\n",
      "Iteration 84, loss = 0.23635428\n",
      "Iteration 58, loss = 0.24437270\n",
      "Iteration 85, loss = 0.23650286\n",
      "Iteration 59, loss = 0.24408759\n",
      "Iteration 86, loss = 0.23516818\n",
      "Iteration 87, loss = 0.23467098\n",
      "Iteration 60, loss = 0.24408537\n",
      "Iteration 88, loss = 0.23393430\n",
      "Iteration 61, loss = 0.24369746\n",
      "Iteration 89, loss = 0.23358984\n",
      "Iteration 62, loss = 0.24319416\n",
      "Iteration 90, loss = 0.23298876\n",
      "Iteration 63, loss = 0.24272111\n",
      "Iteration 91, loss = 0.23273000\n",
      "Iteration 64, loss = 0.24241442\n",
      "Iteration 92, loss = 0.23200632\n",
      "Iteration 65, loss = 0.24234230\n",
      "Iteration 93, loss = 0.23127978\n",
      "Iteration 66, loss = 0.24224891\n",
      "Iteration 94, loss = 0.23075533\n",
      "Iteration 67, loss = 0.24152674\n",
      "Iteration 95, loss = 0.23088623\n",
      "Iteration 68, loss = 0.24093415\n",
      "Iteration 96, loss = 0.22988130\n",
      "Iteration 69, loss = 0.24043212\n",
      "Iteration 97, loss = 0.22962897\n",
      "Iteration 70, loss = 0.24020135\n",
      "Iteration 98, loss = 0.22832356\n",
      "Iteration 71, loss = 0.23982828\n",
      "Iteration 99, loss = 0.22924893\n",
      "Iteration 72, loss = 0.23944439\n",
      "Iteration 100, loss = 0.22784086\n",
      "Iteration 73, loss = 0.23882528\n",
      "Iteration 101, loss = 0.22905847\n",
      "Iteration 74, loss = 0.23855485\n",
      "Iteration 102, loss = 0.22685954\n",
      "Iteration 75, loss = 0.23825755\n",
      "Iteration 103, loss = 0.22664045\n",
      "Iteration 76, loss = 0.23787009\n",
      "Iteration 104, loss = 0.22603131\n",
      "Iteration 77, loss = 0.23740496\n",
      "Iteration 105, loss = 0.22531238\n",
      "Iteration 78, loss = 0.23814133\n",
      "Iteration 79, loss = 0.23709905\n",
      "Iteration 106, loss = 0.22555089\n",
      "Iteration 80, loss = 0.23629098\n",
      "Iteration 107, loss = 0.22461416\n",
      "Iteration 81, loss = 0.23583127\n",
      "Iteration 108, loss = 0.22371568\n",
      "Iteration 82, loss = 0.23574461\n",
      "Iteration 109, loss = 0.22337644\n",
      "Iteration 110, loss = 0.22276739\n",
      "Iteration 83, loss = 0.23588370\n",
      "Iteration 111, loss = 0.22223469\n",
      "Iteration 84, loss = 0.23514916\n",
      "Iteration 112, loss = 0.22195298\n",
      "Iteration 85, loss = 0.23518083\n",
      "Iteration 113, loss = 0.22148410\n",
      "Iteration 86, loss = 0.23434491\n",
      "Iteration 114, loss = 0.22091643\n",
      "Iteration 87, loss = 0.23390901\n",
      "Iteration 115, loss = 0.22039822\n",
      "Iteration 88, loss = 0.23341167\n",
      "Iteration 116, loss = 0.22022747\n",
      "Iteration 89, loss = 0.23325532\n",
      "Iteration 117, loss = 0.21972042\n",
      "Iteration 90, loss = 0.23287779\n",
      "Iteration 118, loss = 0.21964822\n",
      "Iteration 91, loss = 0.23248287\n",
      "Iteration 119, loss = 0.21867430\n",
      "Iteration 92, loss = 0.23208029\n",
      "Iteration 120, loss = 0.21913471\n",
      "Iteration 93, loss = 0.23148353\n",
      "Iteration 121, loss = 0.21811804\n",
      "Iteration 94, loss = 0.23110370\n",
      "Iteration 122, loss = 0.21718245\n",
      "Iteration 95, loss = 0.23072299\n",
      "Iteration 123, loss = 0.21765914\n",
      "Iteration 96, loss = 0.23007793\n",
      "Iteration 124, loss = 0.21752061\n",
      "Iteration 97, loss = 0.22972276\n",
      "Iteration 125, loss = 0.21640275\n",
      "Iteration 98, loss = 0.22898392\n",
      "Iteration 126, loss = 0.21575200\n",
      "Iteration 99, loss = 0.22952417\n",
      "Iteration 127, loss = 0.21654992\n",
      "Iteration 100, loss = 0.22861068\n",
      "Iteration 128, loss = 0.21460029\n",
      "Iteration 101, loss = 0.22955976\n",
      "Iteration 129, loss = 0.21600069\n",
      "Iteration 102, loss = 0.22781975\n",
      "Iteration 130, loss = 0.21453691\n",
      "Iteration 103, loss = 0.22727611\n",
      "Iteration 131, loss = 0.21493947\n",
      "Iteration 104, loss = 0.22654048\n",
      "Iteration 132, loss = 0.21327192\n",
      "Iteration 105, loss = 0.22637313\n",
      "Iteration 133, loss = 0.21468548\n",
      "Iteration 106, loss = 0.22638548\n",
      "Iteration 134, loss = 0.21215817\n",
      "Iteration 107, loss = 0.22541445\n",
      "Iteration 135, loss = 0.21301010\n",
      "Iteration 136, loss = 0.21202654\n",
      "Iteration 108, loss = 0.22482709\n",
      "Iteration 109, loss = 0.22475348\n",
      "Iteration 137, loss = 0.21235402\n",
      "Iteration 110, loss = 0.22408741\n",
      "Iteration 138, loss = 0.21121192\n",
      "Iteration 111, loss = 0.22363236\n",
      "Iteration 139, loss = 0.21182230\n",
      "Iteration 112, loss = 0.22353458\n",
      "Iteration 140, loss = 0.21046389\n",
      "Iteration 113, loss = 0.22283902\n",
      "Iteration 141, loss = 0.21034096\n",
      "Iteration 114, loss = 0.22224246\n",
      "Iteration 142, loss = 0.20947790\n",
      "Iteration 115, loss = 0.22190278\n",
      "Iteration 143, loss = 0.20939753\n",
      "Iteration 144, loss = 0.20874744Iteration 116, loss = 0.22116864\n",
      "\n",
      "Iteration 117, loss = 0.22099817\n",
      "Iteration 145, loss = 0.20899298\n",
      "Iteration 146, loss = 0.20830904Iteration 118, loss = 0.22067418\n",
      "\n",
      "Iteration 119, loss = 0.22035737Iteration 147, loss = 0.20844573\n",
      "\n",
      "Iteration 120, loss = 0.22069636\n",
      "Iteration 148, loss = 0.20705220\n",
      "Iteration 121, loss = 0.21992441\n",
      "Iteration 149, loss = 0.20796084\n",
      "Iteration 122, loss = 0.21840848\n",
      "Iteration 150, loss = 0.20703468\n",
      "Iteration 123, loss = 0.21917388\n",
      "Iteration 151, loss = 0.20705490\n",
      "Iteration 124, loss = 0.21892191\n",
      "Iteration 152, loss = 0.20727615\n",
      "Iteration 125, loss = 0.21762717\n",
      "Iteration 153, loss = 0.20617871\n",
      "Iteration 126, loss = 0.21698871\n",
      "Iteration 154, loss = 0.20560832\n",
      "Iteration 127, loss = 0.21731127\n",
      "Iteration 155, loss = 0.20529250\n",
      "Iteration 128, loss = 0.21593047\n",
      "Iteration 156, loss = 0.20566100\n",
      "Iteration 129, loss = 0.21716258Iteration 157, loss = 0.20477554\n",
      "\n",
      "Iteration 130, loss = 0.21574744\n",
      "Iteration 158, loss = 0.20440143\n",
      "Iteration 159, loss = 0.20393588\n",
      "Iteration 131, loss = 0.21604171\n",
      "Iteration 160, loss = 0.20320348\n",
      "Iteration 132, loss = 0.21498160\n",
      "Iteration 161, loss = 0.20412251\n",
      "Iteration 133, loss = 0.21649622\n",
      "Iteration 162, loss = 0.20287002\n",
      "Iteration 134, loss = 0.21390949\n",
      "Iteration 163, loss = 0.20229185\n",
      "Iteration 135, loss = 0.21434524\n",
      "Iteration 164, loss = 0.20205626\n",
      "Iteration 136, loss = 0.21369453\n",
      "Iteration 165, loss = 0.20225681\n",
      "Iteration 137, loss = 0.21270416\n",
      "Iteration 166, loss = 0.20157149\n",
      "Iteration 138, loss = 0.21246579\n",
      "Iteration 167, loss = 0.20136400\n",
      "Iteration 139, loss = 0.21275332\n",
      "Iteration 168, loss = 0.20090721\n",
      "Iteration 140, loss = 0.21124193\n",
      "Iteration 169, loss = 0.20091797\n",
      "Iteration 141, loss = 0.21139073\n",
      "Iteration 170, loss = 0.19966484\n",
      "Iteration 142, loss = 0.21057646\n",
      "Iteration 171, loss = 0.20035471\n",
      "Iteration 143, loss = 0.21040899\n",
      "Iteration 172, loss = 0.19944327\n",
      "Iteration 144, loss = 0.20966140\n",
      "Iteration 173, loss = 0.19902298\n",
      "Iteration 145, loss = 0.20974272\n",
      "Iteration 174, loss = 0.19914993\n",
      "Iteration 146, loss = 0.20876787\n",
      "Iteration 175, loss = 0.20070811\n",
      "Iteration 147, loss = 0.20906456\n",
      "Iteration 148, loss = 0.20790237Iteration 176, loss = 0.19875722\n",
      "\n",
      "Iteration 149, loss = 0.20795689Iteration 177, loss = 0.19836620\n",
      "\n",
      "Iteration 178, loss = 0.19738923\n",
      "Iteration 150, loss = 0.20823877\n",
      "Iteration 179, loss = 0.19763083\n",
      "Iteration 151, loss = 0.20754197\n",
      "Iteration 180, loss = 0.19702179\n",
      "Iteration 152, loss = 0.20653105\n",
      "Iteration 181, loss = 0.19698023\n",
      "Iteration 153, loss = 0.20631467\n",
      "Iteration 182, loss = 0.19717934\n",
      "Iteration 154, loss = 0.20656287\n",
      "Iteration 183, loss = 0.19568323\n",
      "Iteration 155, loss = 0.20621467\n",
      "Iteration 184, loss = 0.19576174\n",
      "Iteration 156, loss = 0.20577901\n",
      "Iteration 185, loss = 0.19588857\n",
      "Iteration 157, loss = 0.20570098\n",
      "Iteration 186, loss = 0.19563362\n",
      "Iteration 158, loss = 0.20491452\n",
      "Iteration 187, loss = 0.19504923\n",
      "Iteration 159, loss = 0.20443347\n",
      "Iteration 188, loss = 0.19468701\n",
      "Iteration 160, loss = 0.20377893\n",
      "Iteration 189, loss = 0.19433956\n",
      "Iteration 161, loss = 0.20455663\n",
      "Iteration 190, loss = 0.19364443\n",
      "Iteration 162, loss = 0.20302570\n",
      "Iteration 191, loss = 0.19364848\n",
      "Iteration 163, loss = 0.20293496\n",
      "Iteration 192, loss = 0.19326373\n",
      "Iteration 164, loss = 0.20227867\n",
      "Iteration 193, loss = 0.19298260\n",
      "Iteration 165, loss = 0.20219314\n",
      "Iteration 194, loss = 0.19300200\n",
      "Iteration 166, loss = 0.20159943\n",
      "Iteration 195, loss = 0.19247178\n",
      "Iteration 196, loss = 0.19249029\n",
      "Iteration 167, loss = 0.20174502\n",
      "Iteration 168, loss = 0.20122806\n",
      "Iteration 197, loss = 0.19194393\n",
      "Iteration 169, loss = 0.20149653\n",
      "Iteration 198, loss = 0.19155792\n",
      "Iteration 170, loss = 0.19993803\n",
      "Iteration 199, loss = 0.19132334\n",
      "Iteration 171, loss = 0.20110019\n",
      "Iteration 200, loss = 0.19115089\n",
      "Iteration 172, loss = 0.20000412\n",
      "Iteration 173, loss = 0.19937015\n",
      "Iteration 174, loss = 0.19972583\n",
      "Iteration 175, loss = 0.20022095\n",
      "Iteration 176, loss = 0.19909915\n",
      "Iteration 1, loss = 0.41921958\n",
      "Iteration 177, loss = 0.19899102\n",
      "Iteration 2, loss = 0.34456051\n",
      "Iteration 178, loss = 0.19797581\n",
      "Iteration 3, loss = 0.32744356\n",
      "Iteration 179, loss = 0.19756664\n",
      "Iteration 4, loss = 0.32648113\n",
      "Iteration 180, loss = 0.19747186\n",
      "Iteration 181, loss = 0.19670693\n",
      "Iteration 5, loss = 0.32382590\n",
      "Iteration 182, loss = 0.19774931\n",
      "Iteration 6, loss = 0.31932436\n",
      "Iteration 183, loss = 0.19632481Iteration 7, loss = 0.31360050\n",
      "\n",
      "Iteration 184, loss = 0.19581924Iteration 8, loss = 0.30828559\n",
      "\n",
      "Iteration 9, loss = 0.30296460Iteration 185, loss = 0.19646476\n",
      "\n",
      "Iteration 10, loss = 0.29750447Iteration 186, loss = 0.19596377\n",
      "\n",
      "Iteration 187, loss = 0.19503769Iteration 11, loss = 0.29252148\n",
      "\n",
      "Iteration 12, loss = 0.28801143\n",
      "Iteration 188, loss = 0.19460481\n",
      "Iteration 13, loss = 0.28420194\n",
      "Iteration 189, loss = 0.19417894\n",
      "Iteration 14, loss = 0.28059667\n",
      "Iteration 190, loss = 0.19402291\n",
      "Iteration 191, loss = 0.19415671Iteration 15, loss = 0.27765245\n",
      "\n",
      "Iteration 16, loss = 0.27525439Iteration 192, loss = 0.19320366\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, loss = 0.27265396\n",
      "Iteration 193, loss = 0.19335830\n",
      "Iteration 18, loss = 0.27041487\n",
      "Iteration 194, loss = 0.19271516\n",
      "Iteration 19, loss = 0.26895105\n",
      "Iteration 195, loss = 0.19267890\n",
      "Iteration 20, loss = 0.26742017\n",
      "Iteration 196, loss = 0.19195923\n",
      "Iteration 21, loss = 0.26523261\n",
      "Iteration 197, loss = 0.19174321\n",
      "Iteration 22, loss = 0.26499566\n",
      "Iteration 198, loss = 0.19125578\n",
      "Iteration 23, loss = 0.26358816\n",
      "Iteration 199, loss = 0.19149823\n",
      "Iteration 24, loss = 0.26188898\n",
      "Iteration 200, loss = 0.19083318\n",
      "Iteration 25, loss = 0.26128352\n",
      "Iteration 26, loss = 0.26021690\n",
      "Iteration 27, loss = 0.25946421\n",
      "Iteration 28, loss = 0.25879827\n",
      "Iteration 29, loss = 0.25786809\n",
      "Iteration 1, loss = 0.41977444\n",
      "Iteration 30, loss = 0.25687256\n",
      "Iteration 2, loss = 0.34501295\n",
      "Iteration 31, loss = 0.25711631\n",
      "Iteration 3, loss = 0.32776995\n",
      "Iteration 32, loss = 0.25602424\n",
      "Iteration 4, loss = 0.32698731\n",
      "Iteration 33, loss = 0.25587912\n",
      "Iteration 5, loss = 0.32445738\n",
      "Iteration 34, loss = 0.25750315\n",
      "Iteration 6, loss = 0.32011445\n",
      "Iteration 35, loss = 0.25469318\n",
      "Iteration 7, loss = 0.31477629\n",
      "Iteration 36, loss = 0.25429275\n",
      "Iteration 8, loss = 0.30966530\n",
      "Iteration 37, loss = 0.25346156\n",
      "Iteration 9, loss = 0.30470447\n",
      "Iteration 38, loss = 0.25339349\n",
      "Iteration 10, loss = 0.29933355\n",
      "Iteration 39, loss = 0.25244789\n",
      "Iteration 11, loss = 0.29408105\n",
      "Iteration 40, loss = 0.25436197\n",
      "Iteration 12, loss = 0.28958891\n",
      "Iteration 41, loss = 0.25234811\n",
      "Iteration 13, loss = 0.28535863\n",
      "Iteration 42, loss = 0.25246970\n",
      "Iteration 14, loss = 0.28174737\n",
      "Iteration 43, loss = 0.25124181\n",
      "Iteration 15, loss = 0.27825895\n",
      "Iteration 44, loss = 0.25089253\n",
      "Iteration 16, loss = 0.27565394\n",
      "Iteration 45, loss = 0.25081012\n",
      "Iteration 17, loss = 0.27301795\n",
      "Iteration 46, loss = 0.24979194\n",
      "Iteration 18, loss = 0.27075671\n",
      "Iteration 47, loss = 0.25041225\n",
      "Iteration 19, loss = 0.26860267\n",
      "Iteration 48, loss = 0.24942408\n",
      "Iteration 20, loss = 0.26660421\n",
      "Iteration 49, loss = 0.24899794\n",
      "Iteration 21, loss = 0.26517158\n",
      "Iteration 50, loss = 0.24846703\n",
      "Iteration 22, loss = 0.26359707\n",
      "Iteration 51, loss = 0.24827256\n",
      "Iteration 23, loss = 0.26217782\n",
      "Iteration 52, loss = 0.24847781\n",
      "Iteration 53, loss = 0.24715720\n",
      "Iteration 54, loss = 0.24666474\n",
      "Iteration 55, loss = 0.24670357\n",
      "Iteration 56, loss = 0.24551845\n",
      "Iteration 57, loss = 0.24500832\n",
      "Iteration 58, loss = 0.24437884\n",
      "Iteration 24, loss = 0.26153464\n",
      "Iteration 59, loss = 0.24399762\n",
      "Iteration 25, loss = 0.25999142\n",
      "Iteration 26, loss = 0.25898120\n",
      "Iteration 27, loss = 0.25829730\n",
      "Iteration 60, loss = 0.24400845Iteration 28, loss = 0.25696164\n",
      "\n",
      "Iteration 29, loss = 0.25633176\n",
      "Iteration 61, loss = 0.24311110\n",
      "Iteration 30, loss = 0.25510806\n",
      "Iteration 62, loss = 0.24232399\n",
      "Iteration 63, loss = 0.24191390\n",
      "Iteration 64, loss = 0.24169342Iteration 31, loss = 0.25529860\n",
      "\n",
      "Iteration 65, loss = 0.24113852\n",
      "Iteration 66, loss = 0.24041089\n",
      "Iteration 67, loss = 0.23969610\n",
      "Iteration 32, loss = 0.25399169\n",
      "Iteration 33, loss = 0.25394170\n",
      "Iteration 34, loss = 0.25537322\n",
      "Iteration 68, loss = 0.23888546\n",
      "Iteration 35, loss = 0.25247318\n",
      "Iteration 69, loss = 0.23843919\n",
      "Iteration 70, loss = 0.23786032\n",
      "Iteration 36, loss = 0.25276763\n",
      "Iteration 37, loss = 0.25149768\n",
      "Iteration 38, loss = 0.25123734\n",
      "Iteration 71, loss = 0.23730367\n",
      "Iteration 39, loss = 0.25110804\n",
      "Iteration 72, loss = 0.23686932\n",
      "Iteration 40, loss = 0.25118947\n",
      "Iteration 73, loss = 0.23599888Iteration 41, loss = 0.25030117\n",
      "\n",
      "Iteration 42, loss = 0.25056491\n",
      "Iteration 43, loss = 0.24901038\n",
      "Iteration 74, loss = 0.23542367\n",
      "Iteration 44, loss = 0.24869049\n",
      "Iteration 45, loss = 0.24821411\n",
      "Iteration 75, loss = 0.23492750Iteration 46, loss = 0.24780541\n",
      "Iteration 47, loss = 0.24747509\n",
      "\n",
      "Iteration 48, loss = 0.24709070\n",
      "Iteration 76, loss = 0.23452859\n",
      "Iteration 49, loss = 0.24707292\n",
      "Iteration 50, loss = 0.24662933\n",
      "Iteration 51, loss = 0.24584768Iteration 77, loss = 0.23385823\n",
      "\n",
      "Iteration 52, loss = 0.24702823Iteration 78, loss = 0.23413908\n",
      "Iteration 53, loss = 0.24541047\n",
      "\n",
      "Iteration 54, loss = 0.24503362Iteration 79, loss = 0.23335885\n",
      "Iteration 80, loss = 0.23239903\n",
      "Iteration 55, loss = 0.24524603\n",
      "Iteration 81, loss = 0.23198864\n",
      "Iteration 56, loss = 0.24360168\n",
      "Iteration 82, loss = 0.23152559\n",
      "Iteration 57, loss = 0.24367334\n",
      "\n",
      "Iteration 58, loss = 0.24309859Iteration 83, loss = 0.23179759\n",
      "Iteration 84, loss = 0.23071346\n",
      "Iteration 59, loss = 0.24252274\n",
      "Iteration 85, loss = 0.23073814\n",
      "Iteration 60, loss = 0.24247600\n",
      "\n",
      "Iteration 61, loss = 0.24177251Iteration 86, loss = 0.22986115\n",
      "Iteration 87, loss = 0.22924718\n",
      "Iteration 62, loss = 0.24127096\n",
      "Iteration 88, loss = 0.22882932\n",
      "Iteration 63, loss = 0.24126515\n",
      "Iteration 89, loss = 0.22863981\n",
      "\n",
      "Iteration 64, loss = 0.24039708\n",
      "Iteration 90, loss = 0.22809842\n",
      "Iteration 65, loss = 0.24000914\n",
      "Iteration 66, loss = 0.23933749Iteration 91, loss = 0.22766375\n",
      "\n",
      "Iteration 92, loss = 0.22722848Iteration 67, loss = 0.23952356\n",
      "Iteration 93, loss = 0.22671126\n",
      "\n",
      "Iteration 94, loss = 0.22613620Iteration 68, loss = 0.23818188\n",
      "Iteration 69, loss = 0.23818646\n",
      "Iteration 95, loss = 0.22583290\n",
      "Iteration 70, loss = 0.23729731\n",
      "\n",
      "Iteration 71, loss = 0.23702160Iteration 96, loss = 0.22564184\n",
      "\n",
      "Iteration 97, loss = 0.22482883\n",
      "Iteration 72, loss = 0.23660985\n",
      "Iteration 98, loss = 0.22401395\n",
      "Iteration 99, loss = 0.22465196\n",
      "Iteration 73, loss = 0.23548825\n",
      "Iteration 100, loss = 0.22353818\n",
      "Iteration 101, loss = 0.22415339\n",
      "Iteration 74, loss = 0.23510434\n",
      "Iteration 102, loss = 0.22288766\n",
      "Iteration 103, loss = 0.22207630\n",
      "Iteration 75, loss = 0.23444912\n",
      "Iteration 104, loss = 0.22152250\n",
      "Iteration 76, loss = 0.23432955\n",
      "Iteration 105, loss = 0.22104097\n",
      "Iteration 77, loss = 0.23331117\n",
      "Iteration 78, loss = 0.23370966Iteration 106, loss = 0.22142290\n",
      "\n",
      "Iteration 79, loss = 0.23263050\n",
      "Iteration 107, loss = 0.22075518\n",
      "Iteration 80, loss = 0.23189258\n",
      "Iteration 108, loss = 0.22000437\n",
      "Iteration 81, loss = 0.23142071\n",
      "Iteration 109, loss = 0.21984397\n",
      "Iteration 82, loss = 0.23121107\n",
      "Iteration 110, loss = 0.21893197\n",
      "Iteration 83, loss = 0.23110052\n",
      "Iteration 111, loss = 0.21844776\n",
      "Iteration 84, loss = 0.22999488\n",
      "Iteration 112, loss = 0.21839288\n",
      "Iteration 85, loss = 0.22955705\n",
      "Iteration 113, loss = 0.21820656\n",
      "Iteration 86, loss = 0.22919887\n",
      "Iteration 114, loss = 0.21713690\n",
      "Iteration 87, loss = 0.22840568\n",
      "Iteration 115, loss = 0.21685355Iteration 88, loss = 0.22799093\n",
      "\n",
      "Iteration 116, loss = 0.21634583Iteration 89, loss = 0.22711925\n",
      "\n",
      "Iteration 90, loss = 0.22677458\n",
      "Iteration 117, loss = 0.21617216\n",
      "Iteration 91, loss = 0.22624966\n",
      "Iteration 118, loss = 0.21503676\n",
      "Iteration 92, loss = 0.22585743\n",
      "Iteration 119, loss = 0.21494981\n",
      "Iteration 93, loss = 0.22519446\n",
      "Iteration 120, loss = 0.21536601Iteration 94, loss = 0.22479925\n",
      "\n",
      "Iteration 95, loss = 0.22420357\n",
      "Iteration 121, loss = 0.21424185\n",
      "Iteration 96, loss = 0.22349721\n",
      "Iteration 122, loss = 0.21309396\n",
      "Iteration 97, loss = 0.22322576\n",
      "Iteration 123, loss = 0.21307508\n",
      "Iteration 98, loss = 0.22206325\n",
      "Iteration 124, loss = 0.21307841\n",
      "Iteration 99, loss = 0.22312401\n",
      "Iteration 125, loss = 0.21185066\n",
      "Iteration 100, loss = 0.22148495\n",
      "Iteration 101, loss = 0.22221132\n",
      "Iteration 126, loss = 0.21172973\n",
      "Iteration 102, loss = 0.22095514\n",
      "Iteration 127, loss = 0.21194129\n",
      "Iteration 103, loss = 0.21996536\n",
      "Iteration 128, loss = 0.21061877\n",
      "Iteration 104, loss = 0.21970457\n",
      "Iteration 129, loss = 0.21146336\n",
      "Iteration 105, loss = 0.21875024\n",
      "Iteration 130, loss = 0.20993358\n",
      "Iteration 106, loss = 0.21931017\n",
      "Iteration 131, loss = 0.21010515\n",
      "Iteration 107, loss = 0.21857934\n",
      "Iteration 108, loss = 0.21774094Iteration 132, loss = 0.20950186\n",
      "\n",
      "Iteration 109, loss = 0.21789724\n",
      "Iteration 133, loss = 0.20979993\n",
      "Iteration 110, loss = 0.21676619\n",
      "Iteration 134, loss = 0.20795900\n",
      "Iteration 111, loss = 0.21590910\n",
      "Iteration 135, loss = 0.20862078\n",
      "Iteration 112, loss = 0.21702103\n",
      "Iteration 136, loss = 0.20795546\n",
      "Iteration 113, loss = 0.21541914\n",
      "Iteration 137, loss = 0.20735490\n",
      "Iteration 114, loss = 0.21462861\n",
      "Iteration 138, loss = 0.20637533\n",
      "Iteration 115, loss = 0.21432036\n",
      "Iteration 139, loss = 0.20718920\n",
      "Iteration 116, loss = 0.21346903\n",
      "Iteration 140, loss = 0.20536601\n",
      "Iteration 117, loss = 0.21373001\n",
      "Iteration 141, loss = 0.20558308\n",
      "Iteration 118, loss = 0.21295770\n",
      "Iteration 142, loss = 0.20446404\n",
      "Iteration 119, loss = 0.21248105\n",
      "Iteration 120, loss = 0.21352962Iteration 143, loss = 0.20437260\n",
      "\n",
      "Iteration 121, loss = 0.21201553\n",
      "Iteration 144, loss = 0.20359938\n",
      "Iteration 122, loss = 0.21076402\n",
      "Iteration 145, loss = 0.20387122\n",
      "Iteration 123, loss = 0.21052303\n",
      "Iteration 146, loss = 0.20276541\n",
      "Iteration 124, loss = 0.21023343\n",
      "Iteration 147, loss = 0.20281955\n",
      "Iteration 125, loss = 0.20946356\n",
      "Iteration 148, loss = 0.20206428\n",
      "Iteration 126, loss = 0.20916991\n",
      "Iteration 127, loss = 0.20900679\n",
      "Iteration 149, loss = 0.20185483\n",
      "Iteration 128, loss = 0.20818023\n",
      "Iteration 150, loss = 0.20192056\n",
      "Iteration 129, loss = 0.20827246\n",
      "Iteration 151, loss = 0.20152712\n",
      "Iteration 130, loss = 0.20751880\n",
      "Iteration 152, loss = 0.20074709\n",
      "Iteration 131, loss = 0.20711999\n",
      "Iteration 153, loss = 0.20018054\n",
      "Iteration 132, loss = 0.20712969\n",
      "Iteration 154, loss = 0.20024946\n",
      "Iteration 133, loss = 0.20664702\n",
      "Iteration 155, loss = 0.19965827\n",
      "Iteration 134, loss = 0.20549805\n",
      "Iteration 135, loss = 0.20590999\n",
      "Iteration 156, loss = 0.19954572\n",
      "Iteration 136, loss = 0.20533464\n",
      "Iteration 157, loss = 0.19891311\n",
      "Iteration 137, loss = 0.20456331\n",
      "Iteration 158, loss = 0.19930844\n",
      "Iteration 138, loss = 0.20402536\n",
      "Iteration 159, loss = 0.19799423\n",
      "Iteration 139, loss = 0.20414608\n",
      "Iteration 160, loss = 0.19803694\n",
      "Iteration 140, loss = 0.20327772\n",
      "Iteration 161, loss = 0.19815064\n",
      "Iteration 141, loss = 0.20295701\n",
      "Iteration 162, loss = 0.19703823\n",
      "Iteration 142, loss = 0.20237193\n",
      "Iteration 163, loss = 0.19730907\n",
      "Iteration 143, loss = 0.20218803\n",
      "Iteration 164, loss = 0.19678269\n",
      "Iteration 144, loss = 0.20167040\n",
      "Iteration 165, loss = 0.19630444\n",
      "Iteration 145, loss = 0.20205317\n",
      "Iteration 166, loss = 0.19574946\n",
      "Iteration 146, loss = 0.20108054\n",
      "Iteration 167, loss = 0.19619089\n",
      "Iteration 147, loss = 0.20041995\n",
      "Iteration 168, loss = 0.19536031\n",
      "Iteration 148, loss = 0.20015923\n",
      "Iteration 169, loss = 0.19540947\n",
      "Iteration 149, loss = 0.19984200\n",
      "Iteration 170, loss = 0.19434440\n",
      "Iteration 150, loss = 0.19939433\n",
      "Iteration 171, loss = 0.19531140\n",
      "Iteration 151, loss = 0.19888006\n",
      "Iteration 172, loss = 0.19423307\n",
      "Iteration 152, loss = 0.19834778\n",
      "Iteration 153, loss = 0.19820977\n",
      "Iteration 173, loss = 0.19331964\n",
      "Iteration 154, loss = 0.19801409\n",
      "Iteration 174, loss = 0.19374817\n",
      "Iteration 155, loss = 0.19726006\n",
      "Iteration 175, loss = 0.19463212\n",
      "Iteration 156, loss = 0.19737874\n",
      "Iteration 176, loss = 0.19278541\n",
      "Iteration 157, loss = 0.19640488\n",
      "Iteration 177, loss = 0.19293644\n",
      "Iteration 158, loss = 0.19616991\n",
      "Iteration 178, loss = 0.19187027\n",
      "Iteration 159, loss = 0.19581249\n",
      "Iteration 179, loss = 0.19164119\n",
      "Iteration 160, loss = 0.19520549\n",
      "Iteration 180, loss = 0.19128106\n",
      "Iteration 161, loss = 0.19541717\n",
      "Iteration 181, loss = 0.19062244\n",
      "Iteration 162, loss = 0.19481585\n",
      "Iteration 182, loss = 0.19118599\n",
      "Iteration 163, loss = 0.19444608\n",
      "Iteration 183, loss = 0.19007062\n",
      "Iteration 164, loss = 0.19420574\n",
      "Iteration 184, loss = 0.18990182\n",
      "Iteration 165, loss = 0.19402802\n",
      "Iteration 185, loss = 0.18994122\n",
      "Iteration 166, loss = 0.19346918\n",
      "Iteration 186, loss = 0.18972717\n",
      "Iteration 167, loss = 0.19366715\n",
      "Iteration 187, loss = 0.18917331\n",
      "Iteration 168, loss = 0.19301787\n",
      "Iteration 188, loss = 0.18900674\n",
      "Iteration 169, loss = 0.19257259\n",
      "Iteration 189, loss = 0.18781801\n",
      "Iteration 170, loss = 0.19154766\n",
      "Iteration 190, loss = 0.18840008\n",
      "Iteration 171, loss = 0.19251042\n",
      "Iteration 191, loss = 0.18922355\n",
      "Iteration 172, loss = 0.19257961\n",
      "Iteration 192, loss = 0.18810251\n",
      "Iteration 173, loss = 0.19059624\n",
      "Iteration 193, loss = 0.18772880\n",
      "Iteration 174, loss = 0.19073026\n",
      "Iteration 194, loss = 0.18696706\n",
      "Iteration 175, loss = 0.19021883\n",
      "Iteration 195, loss = 0.18689814\n",
      "Iteration 176, loss = 0.19026199\n",
      "Iteration 196, loss = 0.18621432\n",
      "Iteration 177, loss = 0.18952089\n",
      "Iteration 197, loss = 0.18588295\n",
      "Iteration 178, loss = 0.18915318\n",
      "Iteration 198, loss = 0.18545694\n",
      "Iteration 179, loss = 0.18860988\n",
      "Iteration 199, loss = 0.18524738\n",
      "Iteration 180, loss = 0.18892592\n",
      "Iteration 200, loss = 0.18488441\n",
      "Iteration 181, loss = 0.18797561\n",
      "Iteration 182, loss = 0.18863530\n",
      "Iteration 183, loss = 0.18761791\n",
      "Iteration 184, loss = 0.18717593\n",
      "Iteration 185, loss = 0.18701581\n",
      "Iteration 186, loss = 0.18687634\n",
      "Iteration 1, loss = 0.41938381\n",
      "Iteration 187, loss = 0.18674899\n",
      "Iteration 2, loss = 0.34596983\n",
      "Iteration 188, loss = 0.18642189\n",
      "Iteration 3, loss = 0.32824675\n",
      "Iteration 189, loss = 0.18614264\n",
      "Iteration 4, loss = 0.32731130\n",
      "Iteration 190, loss = 0.18537930\n",
      "Iteration 5, loss = 0.32504635\n",
      "Iteration 191, loss = 0.18534781\n",
      "Iteration 6, loss = 0.32083386\n",
      "Iteration 192, loss = 0.18457300\n",
      "Iteration 7, loss = 0.31515590\n",
      "Iteration 193, loss = 0.18435898\n",
      "Iteration 8, loss = 0.30981329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 0.30467075\n",
      "Iteration 10, loss = 0.29919742\n",
      "Iteration 194, loss = 0.18427068\n",
      "Iteration 11, loss = 0.29388898\n",
      "Iteration 12, loss = 0.28953707\n",
      "Iteration 195, loss = 0.18489298\n",
      "Iteration 13, loss = 0.28544080\n",
      "Iteration 14, loss = 0.28201571\n",
      "Iteration 196, loss = 0.18362157\n",
      "Iteration 15, loss = 0.27863220\n",
      "Iteration 197, loss = 0.18324833\n",
      "Iteration 16, loss = 0.27601903\n",
      "Iteration 17, loss = 0.27350239\n",
      "Iteration 198, loss = 0.18302622\n",
      "Iteration 199, loss = 0.18307149\n",
      "Iteration 18, loss = 0.27127044\n",
      "Iteration 200, loss = 0.18275584\n",
      "Iteration 19, loss = 0.26923572\n",
      "Iteration 20, loss = 0.26766007\n",
      "Iteration 21, loss = 0.26572921\n",
      "Iteration 22, loss = 0.26426817\n",
      "Iteration 23, loss = 0.26320683\n",
      "Iteration 1, loss = 0.41998381\n",
      "Iteration 2, loss = 0.34572781\n",
      "Iteration 24, loss = 0.26274793\n",
      "Iteration 25, loss = 0.26092751\n",
      "Iteration 3, loss = 0.32782257\n",
      "Iteration 26, loss = 0.26022829\n",
      "Iteration 27, loss = 0.25935338\n",
      "Iteration 4, loss = 0.32636598\n",
      "Iteration 28, loss = 0.25842074\n",
      "Iteration 5, loss = 0.32370642\n",
      "Iteration 29, loss = 0.25759407\n",
      "Iteration 6, loss = 0.31909876\n",
      "Iteration 30, loss = 0.25695402\n",
      "Iteration 7, loss = 0.31291936\n",
      "Iteration 31, loss = 0.25681304\n",
      "Iteration 8, loss = 0.30719360\n",
      "Iteration 32, loss = 0.25598259\n",
      "Iteration 9, loss = 0.30174891\n",
      "Iteration 33, loss = 0.25558680\n",
      "Iteration 10, loss = 0.29628140\n",
      "Iteration 34, loss = 0.25777345\n",
      "Iteration 11, loss = 0.29053257\n",
      "Iteration 35, loss = 0.25441658\n",
      "Iteration 36, loss = 0.25500763\n",
      "Iteration 12, loss = 0.28594954\n",
      "Iteration 37, loss = 0.25385086\n",
      "Iteration 13, loss = 0.28167699\n",
      "Iteration 14, loss = 0.27792641\n",
      "Iteration 38, loss = 0.25295199\n",
      "Iteration 15, loss = 0.27464163\n",
      "Iteration 39, loss = 0.25261733\n",
      "Iteration 16, loss = 0.27164034\n",
      "Iteration 40, loss = 0.25245519\n",
      "Iteration 17, loss = 0.26908195\n",
      "Iteration 41, loss = 0.25207208\n",
      "Iteration 18, loss = 0.26663816\n",
      "Iteration 19, loss = 0.26475080\n",
      "Iteration 20, loss = 0.26297772\n",
      "Iteration 21, loss = 0.26106427\n",
      "Iteration 42, loss = 0.25174920\n",
      "Iteration 22, loss = 0.25927859\n",
      "Iteration 23, loss = 0.25829032\n",
      "Iteration 24, loss = 0.25810144\n",
      "Iteration 43, loss = 0.25158468\n",
      "Iteration 25, loss = 0.25538815\n",
      "Iteration 44, loss = 0.25090804\n",
      "Iteration 26, loss = 0.25520048\n",
      "Iteration 45, loss = 0.25018984\n",
      "Iteration 27, loss = 0.25355758\n",
      "Iteration 46, loss = 0.24982332\n",
      "Iteration 28, loss = 0.25335361\n",
      "Iteration 47, loss = 0.24923120\n",
      "Iteration 29, loss = 0.25214779\n",
      "Iteration 48, loss = 0.24896628\n",
      "Iteration 30, loss = 0.25135653\n",
      "Iteration 49, loss = 0.24882378\n",
      "Iteration 31, loss = 0.25086850Iteration 50, loss = 0.24889690\n",
      "\n",
      "Iteration 51, loss = 0.24794819\n",
      "Iteration 32, loss = 0.25026905\n",
      "Iteration 52, loss = 0.24924900\n",
      "Iteration 33, loss = 0.25014063Iteration 53, loss = 0.24743094\n",
      "\n",
      "Iteration 54, loss = 0.24715165Iteration 34, loss = 0.25120830\n",
      "\n",
      "Iteration 55, loss = 0.24702693\n",
      "Iteration 35, loss = 0.24986999\n",
      "Iteration 56, loss = 0.24593057\n",
      "Iteration 36, loss = 0.24870156\n",
      "Iteration 57, loss = 0.24573771\n",
      "Iteration 37, loss = 0.24883389\n",
      "Iteration 58, loss = 0.24509853\n",
      "Iteration 38, loss = 0.24778949\n",
      "Iteration 39, loss = 0.24771940\n",
      "Iteration 40, loss = 0.24715687\n",
      "Iteration 59, loss = 0.24457701\n",
      "Iteration 41, loss = 0.24743676\n",
      "Iteration 42, loss = 0.24656596\n",
      "Iteration 43, loss = 0.24699886\n",
      "Iteration 44, loss = 0.24600490\n",
      "Iteration 45, loss = 0.24543453\n",
      "Iteration 60, loss = 0.24494966\n",
      "Iteration 61, loss = 0.24425616\n",
      "Iteration 62, loss = 0.24369930\n",
      "Iteration 46, loss = 0.24505745\n",
      "Iteration 63, loss = 0.24328924\n",
      "Iteration 64, loss = 0.24286100\n",
      "Iteration 65, loss = 0.24249625\n",
      "Iteration 66, loss = 0.24167083\n",
      "Iteration 47, loss = 0.24465436\n",
      "Iteration 48, loss = 0.24437757\n",
      "Iteration 49, loss = 0.24461627\n",
      "Iteration 50, loss = 0.24405043\n",
      "Iteration 51, loss = 0.24338735Iteration 67, loss = 0.24101485\n",
      "\n",
      "Iteration 68, loss = 0.24065174\n",
      "Iteration 52, loss = 0.24378275Iteration 69, loss = 0.24027206\n",
      "\n",
      "Iteration 70, loss = 0.23979892\n",
      "Iteration 53, loss = 0.24340568\n",
      "Iteration 71, loss = 0.23968987\n",
      "Iteration 54, loss = 0.24256239\n",
      "Iteration 55, loss = 0.24275077\n",
      "Iteration 72, loss = 0.23904471\n",
      "Iteration 56, loss = 0.24201306\n",
      "Iteration 73, loss = 0.23825172\n",
      "Iteration 57, loss = 0.24197764\n",
      "Iteration 58, loss = 0.24129902Iteration 74, loss = 0.23795848\n",
      "\n",
      "Iteration 75, loss = 0.23711768\n",
      "Iteration 59, loss = 0.24082865\n",
      "Iteration 76, loss = 0.23704181\n",
      "Iteration 77, loss = 0.23586423Iteration 60, loss = 0.24076329\n",
      "\n",
      "Iteration 61, loss = 0.24043134Iteration 78, loss = 0.23691817\n",
      "Iteration 79, loss = 0.23509808\n",
      "Iteration 80, loss = 0.23470461\n",
      "Iteration 81, loss = 0.23415442\n",
      "Iteration 82, loss = 0.23383315\n",
      "\n",
      "Iteration 83, loss = 0.23405292\n",
      "Iteration 84, loss = 0.23266938\n",
      "Iteration 85, loss = 0.23255652\n",
      "Iteration 86, loss = 0.23231004\n",
      "Iteration 62, loss = 0.24016127\n",
      "Iteration 87, loss = 0.23136776\n",
      "Iteration 88, loss = 0.23058114\n",
      "Iteration 89, loss = 0.22975528\n",
      "Iteration 90, loss = 0.22977174\n",
      "Iteration 63, loss = 0.23973265\n",
      "Iteration 91, loss = 0.22919516\n",
      "Iteration 64, loss = 0.23915504\n",
      "Iteration 92, loss = 0.22849137\n",
      "Iteration 65, loss = 0.23904715\n",
      "Iteration 66, loss = 0.23821045\n",
      "Iteration 67, loss = 0.23789820\n",
      "Iteration 93, loss = 0.22808734\n",
      "Iteration 94, loss = 0.22754808\n",
      "Iteration 95, loss = 0.22706328\n",
      "Iteration 96, loss = 0.22604974\n",
      "Iteration 68, loss = 0.23781931\n",
      "Iteration 69, loss = 0.23731490\n",
      "Iteration 70, loss = 0.23738260\n",
      "Iteration 97, loss = 0.22593153\n",
      "Iteration 98, loss = 0.22458547\n",
      "Iteration 71, loss = 0.23651988\n",
      "Iteration 99, loss = 0.22650392\n",
      "Iteration 72, loss = 0.23604401\n",
      "Iteration 73, loss = 0.23555543\n",
      "Iteration 100, loss = 0.22413862\n",
      "Iteration 74, loss = 0.23529996\n",
      "Iteration 101, loss = 0.22516405\n",
      "Iteration 75, loss = 0.23474736\n",
      "Iteration 102, loss = 0.22321690\n",
      "Iteration 76, loss = 0.23454701\n",
      "Iteration 103, loss = 0.22289768\n",
      "Iteration 77, loss = 0.23379556\n",
      "Iteration 78, loss = 0.23525078Iteration 104, loss = 0.22176248\n",
      "\n",
      "Iteration 79, loss = 0.23329791\n",
      "Iteration 105, loss = 0.22167427\n",
      "Iteration 80, loss = 0.23293509\n",
      "Iteration 106, loss = 0.22153412\n",
      "Iteration 81, loss = 0.23240821\n",
      "Iteration 107, loss = 0.22095533\n",
      "Iteration 82, loss = 0.23201970\n",
      "Iteration 108, loss = 0.21982801\n",
      "Iteration 83, loss = 0.23242211\n",
      "Iteration 109, loss = 0.21979450\n",
      "Iteration 84, loss = 0.23179991\n",
      "Iteration 110, loss = 0.21922237\n",
      "Iteration 85, loss = 0.23122065\n",
      "Iteration 111, loss = 0.21854233\n",
      "Iteration 86, loss = 0.23253456\n",
      "Iteration 112, loss = 0.21923054\n",
      "Iteration 87, loss = 0.23104447\n",
      "Iteration 113, loss = 0.21823436\n",
      "Iteration 88, loss = 0.22991262\n",
      "Iteration 89, loss = 0.22920026\n",
      "Iteration 114, loss = 0.21732758\n",
      "Iteration 90, loss = 0.22936008\n",
      "Iteration 115, loss = 0.21679698\n",
      "Iteration 91, loss = 0.22892483\n",
      "Iteration 116, loss = 0.21652342\n",
      "Iteration 92, loss = 0.22860701\n",
      "Iteration 117, loss = 0.21659822\n",
      "Iteration 93, loss = 0.22805634\n",
      "Iteration 118, loss = 0.21548879\n",
      "Iteration 94, loss = 0.22793094\n",
      "Iteration 119, loss = 0.21583878\n",
      "Iteration 95, loss = 0.22760666\n",
      "Iteration 120, loss = 0.21623587\n",
      "Iteration 96, loss = 0.22694590\n",
      "Iteration 121, loss = 0.21454662\n",
      "Iteration 97, loss = 0.22650571\n",
      "Iteration 122, loss = 0.21355544\n",
      "Iteration 98, loss = 0.22614062\n",
      "Iteration 123, loss = 0.21375643\n",
      "Iteration 99, loss = 0.22606753\n",
      "Iteration 124, loss = 0.21346084\n",
      "Iteration 100, loss = 0.22552483\n",
      "Iteration 125, loss = 0.21258556\n",
      "Iteration 101, loss = 0.22546488\n",
      "Iteration 126, loss = 0.21220243Iteration 102, loss = 0.22470189\n",
      "\n",
      "Iteration 103, loss = 0.22526559\n",
      "Iteration 127, loss = 0.21250180\n",
      "Iteration 104, loss = 0.22411959\n",
      "Iteration 128, loss = 0.21147400\n",
      "Iteration 105, loss = 0.22451613\n",
      "Iteration 129, loss = 0.21141050\n",
      "Iteration 106, loss = 0.22434161\n",
      "Iteration 130, loss = 0.21124677\n",
      "Iteration 107, loss = 0.22349536\n",
      "Iteration 131, loss = 0.21173925\n",
      "Iteration 108, loss = 0.22320685\n",
      "Iteration 132, loss = 0.21097487\n",
      "Iteration 109, loss = 0.22310396\n",
      "Iteration 133, loss = 0.21054278\n",
      "Iteration 110, loss = 0.22271227\n",
      "Iteration 134, loss = 0.20973876Iteration 111, loss = 0.22218681\n",
      "\n",
      "Iteration 112, loss = 0.22232129\n",
      "Iteration 135, loss = 0.20927389\n",
      "Iteration 113, loss = 0.22174025\n",
      "Iteration 136, loss = 0.20910798\n",
      "Iteration 114, loss = 0.22147465\n",
      "Iteration 137, loss = 0.20825244\n",
      "Iteration 115, loss = 0.22069749\n",
      "Iteration 138, loss = 0.20787336\n",
      "Iteration 116, loss = 0.22093893\n",
      "Iteration 139, loss = 0.20771894\n",
      "Iteration 117, loss = 0.22089677\n",
      "Iteration 140, loss = 0.20739680\n",
      "Iteration 118, loss = 0.22023306\n",
      "Iteration 141, loss = 0.20706714\n",
      "Iteration 119, loss = 0.22007315\n",
      "Iteration 120, loss = 0.22123663\n",
      "Iteration 142, loss = 0.20673082\n",
      "Iteration 121, loss = 0.22057232\n",
      "Iteration 143, loss = 0.20713814\n",
      "Iteration 122, loss = 0.21911680\n",
      "Iteration 144, loss = 0.20602623\n",
      "Iteration 123, loss = 0.21952591\n",
      "Iteration 145, loss = 0.20601733\n",
      "Iteration 124, loss = 0.21845492\n",
      "Iteration 146, loss = 0.20595397\n",
      "Iteration 125, loss = 0.21882001\n",
      "Iteration 147, loss = 0.20465157\n",
      "Iteration 126, loss = 0.21768707\n",
      "Iteration 148, loss = 0.20510781\n",
      "Iteration 127, loss = 0.21830535\n",
      "Iteration 149, loss = 0.20464637Iteration 128, loss = 0.21733655\n",
      "\n",
      "Iteration 129, loss = 0.21781254\n",
      "Iteration 150, loss = 0.20475538\n",
      "Iteration 130, loss = 0.21693328\n",
      "Iteration 151, loss = 0.20350205\n",
      "Iteration 131, loss = 0.21793432\n",
      "Iteration 152, loss = 0.20355975\n",
      "Iteration 132, loss = 0.21728318\n",
      "Iteration 153, loss = 0.20374173\n",
      "Iteration 133, loss = 0.21633503\n",
      "Iteration 154, loss = 0.20368420\n",
      "Iteration 134, loss = 0.21536926\n",
      "Iteration 155, loss = 0.20263337\n",
      "Iteration 135, loss = 0.21599050\n",
      "Iteration 156, loss = 0.20279974\n",
      "Iteration 136, loss = 0.21578007\n",
      "Iteration 157, loss = 0.20198318\n",
      "Iteration 137, loss = 0.21459921\n",
      "Iteration 138, loss = 0.21428792\n",
      "Iteration 158, loss = 0.20189672\n",
      "Iteration 139, loss = 0.21417134\n",
      "Iteration 159, loss = 0.20125221\n",
      "Iteration 140, loss = 0.21390891\n",
      "Iteration 160, loss = 0.20106419\n",
      "Iteration 141, loss = 0.21365307\n",
      "Iteration 161, loss = 0.20059092\n",
      "Iteration 142, loss = 0.21314287\n",
      "Iteration 162, loss = 0.20053526\n",
      "Iteration 143, loss = 0.21325857\n",
      "Iteration 163, loss = 0.19994260\n",
      "Iteration 144, loss = 0.21278819\n",
      "Iteration 164, loss = 0.19981494\n",
      "Iteration 145, loss = 0.21290645\n",
      "Iteration 165, loss = 0.19973452\n",
      "Iteration 146, loss = 0.21215415\n",
      "Iteration 166, loss = 0.19935718\n",
      "Iteration 147, loss = 0.21161113\n",
      "Iteration 167, loss = 0.19900630\n",
      "Iteration 148, loss = 0.21137088\n",
      "Iteration 168, loss = 0.19934454\n",
      "Iteration 149, loss = 0.21127682\n",
      "Iteration 169, loss = 0.19924951\n",
      "Iteration 150, loss = 0.21121261\n",
      "Iteration 170, loss = 0.19808821\n",
      "Iteration 151, loss = 0.21075895\n",
      "Iteration 152, loss = 0.21028802\n",
      "Iteration 171, loss = 0.19835589\n",
      "Iteration 153, loss = 0.21015254\n",
      "Iteration 172, loss = 0.19915417\n",
      "Iteration 154, loss = 0.21000034\n",
      "Iteration 173, loss = 0.19732122\n",
      "Iteration 155, loss = 0.20976795\n",
      "Iteration 174, loss = 0.19810802\n",
      "Iteration 156, loss = 0.20964992\n",
      "Iteration 175, loss = 0.19675622\n",
      "Iteration 157, loss = 0.20929563\n",
      "Iteration 176, loss = 0.19703178\n",
      "Iteration 158, loss = 0.20898897\n",
      "Iteration 177, loss = 0.19611196\n",
      "Iteration 159, loss = 0.20874710\n",
      "Iteration 178, loss = 0.19603498\n",
      "Iteration 160, loss = 0.20813499\n",
      "Iteration 179, loss = 0.19546526\n",
      "Iteration 161, loss = 0.20797859\n",
      "Iteration 162, loss = 0.20759927Iteration 180, loss = 0.19562408\n",
      "\n",
      "Iteration 163, loss = 0.20713112\n",
      "Iteration 181, loss = 0.19512882\n",
      "Iteration 164, loss = 0.20712459\n",
      "Iteration 182, loss = 0.19537158\n",
      "Iteration 165, loss = 0.20696445\n",
      "Iteration 183, loss = 0.19475857\n",
      "Iteration 166, loss = 0.20638447\n",
      "Iteration 184, loss = 0.19426985\n",
      "Iteration 167, loss = 0.20688420\n",
      "Iteration 185, loss = 0.19418458\n",
      "Iteration 168, loss = 0.20643073\n",
      "Iteration 186, loss = 0.19382919\n",
      "Iteration 169, loss = 0.20678368\n",
      "Iteration 187, loss = 0.19391166\n",
      "Iteration 170, loss = 0.20551857\n",
      "Iteration 188, loss = 0.19440721\n",
      "Iteration 171, loss = 0.20608521\n",
      "Iteration 172, loss = 0.20640564\n",
      "Iteration 189, loss = 0.19313012\n",
      "Iteration 173, loss = 0.20483878\n",
      "Iteration 190, loss = 0.19395056\n",
      "Iteration 174, loss = 0.20518692\n",
      "Iteration 191, loss = 0.19273518\n",
      "Iteration 175, loss = 0.20433240\n",
      "Iteration 192, loss = 0.19239850\n",
      "Iteration 176, loss = 0.20415962\n",
      "Iteration 193, loss = 0.19174566\n",
      "Iteration 177, loss = 0.20379726\n",
      "Iteration 194, loss = 0.19199384\n",
      "Iteration 178, loss = 0.20353161\n",
      "Iteration 195, loss = 0.19234116\n",
      "Iteration 179, loss = 0.20256863\n",
      "Iteration 196, loss = 0.19138217\n",
      "Iteration 180, loss = 0.20377271\n",
      "Iteration 181, loss = 0.20223520Iteration 197, loss = 0.19146454\n",
      "\n",
      "Iteration 182, loss = 0.20376680\n",
      "Iteration 198, loss = 0.19081674\n",
      "Iteration 183, loss = 0.20255844\n",
      "Iteration 199, loss = 0.19048466\n",
      "Iteration 184, loss = 0.20231825\n",
      "Iteration 200, loss = 0.19056105\n",
      "Iteration 185, loss = 0.20221089\n",
      "Iteration 186, loss = 0.20168939\n",
      "Iteration 187, loss = 0.20230756\n",
      "Iteration 188, loss = 0.20175737\n",
      "Iteration 189, loss = 0.20069498\n",
      "Iteration 1, loss = 0.42079580\n",
      "Iteration 190, loss = 0.20103903\n",
      "Iteration 2, loss = 0.34631347\n",
      "Iteration 191, loss = 0.20078126\n",
      "Iteration 3, loss = 0.32823123\n",
      "Iteration 192, loss = 0.20052187\n",
      "Iteration 4, loss = 0.32627756\n",
      "Iteration 193, loss = 0.19971231\n",
      "Iteration 194, loss = 0.20107146\n",
      "Iteration 5, loss = 0.32390046\n",
      "Iteration 195, loss = 0.20069314\n",
      "Iteration 6, loss = 0.31958132\n",
      "Iteration 196, loss = 0.19970087\n",
      "Iteration 7, loss = 0.31361638\n",
      "Iteration 197, loss = 0.19932292\n",
      "Iteration 198, loss = 0.19915787\n",
      "Iteration 199, loss = 0.19878453\n",
      "Iteration 200, loss = 0.19907989\n",
      "Iteration 8, loss = 0.30790109\n",
      "Iteration 9, loss = 0.30245048\n",
      "Iteration 10, loss = 0.29696489\n",
      "Iteration 11, loss = 0.29087882\n",
      "Iteration 1, loss = 0.42026423\n",
      "Iteration 12, loss = 0.28598710\n",
      "Iteration 2, loss = 0.34591497\n",
      "Iteration 3, loss = 0.32858262Iteration 13, loss = 0.28147986\n",
      "\n",
      "Iteration 4, loss = 0.32582092\n",
      "Iteration 14, loss = 0.27774251\n",
      "Iteration 5, loss = 0.32318656\n",
      "Iteration 15, loss = 0.27409141\n",
      "Iteration 6, loss = 0.31913058\n",
      "Iteration 16, loss = 0.27125573\n",
      "Iteration 7, loss = 0.31319147\n",
      "Iteration 17, loss = 0.26863690\n",
      "Iteration 8, loss = 0.30718534\n",
      "Iteration 9, loss = 0.30156843\n",
      "Iteration 18, loss = 0.26611007\n",
      "Iteration 10, loss = 0.29598497Iteration 19, loss = 0.26399425\n",
      "\n",
      "Iteration 11, loss = 0.28971698\n",
      "Iteration 12, loss = 0.28471432\n",
      "Iteration 20, loss = 0.26219479Iteration 13, loss = 0.28022905\n",
      "Iteration 14, loss = 0.27649161\n",
      "Iteration 15, loss = 0.27271485\n",
      "\n",
      "Iteration 16, loss = 0.26981756\n",
      "Iteration 21, loss = 0.26054410\n",
      "Iteration 17, loss = 0.26711330\n",
      "Iteration 18, loss = 0.26442099\n",
      "Iteration 22, loss = 0.25923748\n",
      "Iteration 19, loss = 0.26217942\n",
      "Iteration 23, loss = 0.25754265\n",
      "Iteration 20, loss = 0.26008894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24, loss = 0.25738886\n",
      "Iteration 21, loss = 0.25850775\n",
      "Iteration 25, loss = 0.25541502\n",
      "Iteration 22, loss = 0.25724049\n",
      "Iteration 26, loss = 0.25535077\n",
      "Iteration 23, loss = 0.25531067\n",
      "Iteration 27, loss = 0.25345806\n",
      "Iteration 24, loss = 0.25495155\n",
      "Iteration 25, loss = 0.25330420\n",
      "Iteration 28, loss = 0.25314891\n",
      "Iteration 26, loss = 0.25248721\n",
      "Iteration 29, loss = 0.25191636\n",
      "Iteration 27, loss = 0.25107350\n",
      "Iteration 30, loss = 0.25135403\n",
      "Iteration 28, loss = 0.25108467\n",
      "Iteration 31, loss = 0.25112551\n",
      "Iteration 29, loss = 0.24961533\n",
      "Iteration 32, loss = 0.25005931\n",
      "Iteration 30, loss = 0.24908022\n",
      "Iteration 33, loss = 0.25000733\n",
      "Iteration 31, loss = 0.24862097\n",
      "Iteration 34, loss = 0.24984480\n",
      "Iteration 32, loss = 0.24877497\n",
      "Iteration 33, loss = 0.24776598\n",
      "Iteration 35, loss = 0.24970532\n",
      "Iteration 34, loss = 0.24685695\n",
      "Iteration 36, loss = 0.24827731\n",
      "Iteration 35, loss = 0.24738340\n",
      "Iteration 37, loss = 0.24850978\n",
      "Iteration 36, loss = 0.24586139\n",
      "Iteration 37, loss = 0.24630594Iteration 38, loss = 0.24742572\n",
      "\n",
      "Iteration 38, loss = 0.24527188\n",
      "Iteration 39, loss = 0.24780112\n",
      "Iteration 39, loss = 0.24607588\n",
      "Iteration 40, loss = 0.24701106\n",
      "Iteration 40, loss = 0.24502668\n",
      "Iteration 41, loss = 0.24762634\n",
      "Iteration 41, loss = 0.24534207\n",
      "Iteration 42, loss = 0.24632148\n",
      "Iteration 42, loss = 0.24385253\n",
      "Iteration 43, loss = 0.24680129\n",
      "Iteration 43, loss = 0.24462753\n",
      "Iteration 44, loss = 0.24635427\n",
      "Iteration 44, loss = 0.24435705\n",
      "Iteration 45, loss = 0.24552395\n",
      "Iteration 45, loss = 0.24328462\n",
      "Iteration 46, loss = 0.24493853\n",
      "Iteration 46, loss = 0.24279577\n",
      "Iteration 47, loss = 0.24490602\n",
      "Iteration 47, loss = 0.24251227\n",
      "Iteration 48, loss = 0.24499165\n",
      "Iteration 48, loss = 0.24265451\n",
      "Iteration 49, loss = 0.24471809\n",
      "Iteration 49, loss = 0.24254907\n",
      "Iteration 50, loss = 0.24412274\n",
      "Iteration 50, loss = 0.24168886\n",
      "Iteration 51, loss = 0.24163712\n",
      "Iteration 51, loss = 0.24378033\n",
      "Iteration 52, loss = 0.24157286\n",
      "Iteration 52, loss = 0.24367263\n",
      "Iteration 53, loss = 0.24194272\n",
      "Iteration 53, loss = 0.24496915\n",
      "Iteration 54, loss = 0.24051240\n",
      "Iteration 54, loss = 0.24277208\n",
      "Iteration 55, loss = 0.24063356\n",
      "Iteration 55, loss = 0.24304093\n",
      "Iteration 56, loss = 0.24016286\n",
      "Iteration 56, loss = 0.24267541\n",
      "Iteration 57, loss = 0.24019183\n",
      "Iteration 58, loss = 0.23924502\n",
      "Iteration 57, loss = 0.24237469\n",
      "Iteration 59, loss = 0.23919670\n",
      "Iteration 58, loss = 0.24195828\n",
      "Iteration 60, loss = 0.23890517\n",
      "Iteration 61, loss = 0.23851325\n",
      "Iteration 62, loss = 0.23838950\n",
      "Iteration 59, loss = 0.24137421\n",
      "Iteration 63, loss = 0.23826075\n",
      "Iteration 60, loss = 0.24149587\n",
      "Iteration 64, loss = 0.23801248\n",
      "Iteration 61, loss = 0.24084952\n",
      "Iteration 65, loss = 0.23755568\n",
      "Iteration 62, loss = 0.24097138\n",
      "Iteration 66, loss = 0.23655770\n",
      "Iteration 63, loss = 0.24062575\n",
      "Iteration 67, loss = 0.23597434\n",
      "Iteration 64, loss = 0.23982976\n",
      "Iteration 68, loss = 0.23578306\n",
      "Iteration 65, loss = 0.23961278\n",
      "Iteration 69, loss = 0.23574667\n",
      "Iteration 66, loss = 0.23866652\n",
      "Iteration 70, loss = 0.23549285\n",
      "Iteration 67, loss = 0.23878828\n",
      "Iteration 71, loss = 0.23449254\n",
      "Iteration 72, loss = 0.23374199\n",
      "Iteration 68, loss = 0.23825861\n",
      "Iteration 73, loss = 0.23347312\n",
      "Iteration 69, loss = 0.23869442\n",
      "Iteration 74, loss = 0.23321678\n",
      "Iteration 70, loss = 0.23866343\n",
      "Iteration 75, loss = 0.23245466\n",
      "Iteration 71, loss = 0.23709897\n",
      "Iteration 76, loss = 0.23217242\n",
      "Iteration 72, loss = 0.23665231\n",
      "Iteration 77, loss = 0.23150607\n",
      "Iteration 73, loss = 0.23633751\n",
      "Iteration 78, loss = 0.23217688\n",
      "Iteration 79, loss = 0.23078496\n",
      "Iteration 74, loss = 0.23593048\n",
      "Iteration 80, loss = 0.23051181\n",
      "Iteration 75, loss = 0.23541010\n",
      "Iteration 81, loss = 0.23017694\n",
      "Iteration 76, loss = 0.23527711\n",
      "Iteration 82, loss = 0.22985020\n",
      "Iteration 77, loss = 0.23459310\n",
      "Iteration 83, loss = 0.22986841\n",
      "Iteration 78, loss = 0.23517615\n",
      "Iteration 84, loss = 0.22913058\n",
      "Iteration 79, loss = 0.23374663\n",
      "Iteration 85, loss = 0.22900977\n",
      "Iteration 86, loss = 0.23011606\n",
      "Iteration 80, loss = 0.23360780\n",
      "Iteration 87, loss = 0.22817915\n",
      "Iteration 81, loss = 0.23323478\n",
      "Iteration 88, loss = 0.22711181\n",
      "Iteration 82, loss = 0.23303274\n",
      "Iteration 89, loss = 0.22679235\n",
      "Iteration 83, loss = 0.23375292\n",
      "Iteration 90, loss = 0.22666662\n",
      "Iteration 84, loss = 0.23268628\n",
      "Iteration 91, loss = 0.22637422\n",
      "Iteration 85, loss = 0.23217449\n",
      "Iteration 92, loss = 0.22608728\n",
      "Iteration 93, loss = 0.22505040\n",
      "Iteration 86, loss = 0.23308174\n",
      "Iteration 94, loss = 0.22521331\n",
      "Iteration 87, loss = 0.23239902\n",
      "Iteration 95, loss = 0.22471247\n",
      "Iteration 96, loss = 0.22383228\n",
      "Iteration 97, loss = 0.22347286\n",
      "Iteration 98, loss = 0.22288122\n",
      "Iteration 88, loss = 0.23059113\n",
      "Iteration 99, loss = 0.22276057\n",
      "Iteration 89, loss = 0.23069634Iteration 100, loss = 0.22225845\n",
      "\n",
      "Iteration 101, loss = 0.22253190\n",
      "Iteration 102, loss = 0.22167535\n",
      "Iteration 90, loss = 0.23036096Iteration 103, loss = 0.22195155\n",
      "\n",
      "Iteration 104, loss = 0.22035068\n",
      "Iteration 91, loss = 0.23002078\n",
      "Iteration 105, loss = 0.22115721\n",
      "Iteration 92, loss = 0.22983085\n",
      "Iteration 106, loss = 0.22080612\n",
      "Iteration 93, loss = 0.22893597\n",
      "Iteration 94, loss = 0.22888575\n",
      "Iteration 95, loss = 0.22818029\n",
      "Iteration 107, loss = 0.21981998\n",
      "Iteration 96, loss = 0.22759658\n",
      "Iteration 108, loss = 0.21941446\n",
      "Iteration 97, loss = 0.22728789\n",
      "Iteration 98, loss = 0.22666366\n",
      "Iteration 99, loss = 0.22663178\n",
      "Iteration 100, loss = 0.22608636\n",
      "Iteration 109, loss = 0.21902692\n",
      "Iteration 101, loss = 0.22668699\n",
      "Iteration 110, loss = 0.21903620\n",
      "Iteration 102, loss = 0.22604133\n",
      "Iteration 111, loss = 0.21840580\n",
      "Iteration 103, loss = 0.22649715Iteration 112, loss = 0.21912188\n",
      "\n",
      "Iteration 113, loss = 0.21758540\n",
      "Iteration 114, loss = 0.21762579\n",
      "Iteration 104, loss = 0.22414523\n",
      "Iteration 115, loss = 0.21700748\n",
      "Iteration 105, loss = 0.22522051Iteration 116, loss = 0.21701377\n",
      "\n",
      "Iteration 117, loss = 0.21685746\n",
      "Iteration 118, loss = 0.21629859\n",
      "Iteration 119, loss = 0.21584257\n",
      "Iteration 120, loss = 0.21728056\n",
      "Iteration 106, loss = 0.22383498\n",
      "Iteration 121, loss = 0.21524353\n",
      "Iteration 107, loss = 0.22370894Iteration 122, loss = 0.21498896\n",
      "\n",
      "Iteration 123, loss = 0.21464867\n",
      "Iteration 108, loss = 0.22284575\n",
      "Iteration 124, loss = 0.21431546\n",
      "Iteration 125, loss = 0.21429067\n",
      "Iteration 109, loss = 0.22279862\n",
      "Iteration 110, loss = 0.22252282\n",
      "Iteration 111, loss = 0.22176210\n",
      "Iteration 126, loss = 0.21361127\n",
      "Iteration 127, loss = 0.21356775\n",
      "Iteration 128, loss = 0.21353536\n",
      "Iteration 112, loss = 0.22208510\n",
      "Iteration 129, loss = 0.21252006\n",
      "Iteration 130, loss = 0.21271085\n",
      "Iteration 113, loss = 0.22110323\n",
      "Iteration 114, loss = 0.22154536\n",
      "Iteration 115, loss = 0.22003986\n",
      "Iteration 116, loss = 0.22062057\n",
      "Iteration 117, loss = 0.22054779\n",
      "Iteration 131, loss = 0.21197632\n",
      "Iteration 132, loss = 0.21237728\n",
      "Iteration 133, loss = 0.21107429\n",
      "Iteration 118, loss = 0.21922827\n",
      "Iteration 134, loss = 0.21103493\n",
      "Iteration 119, loss = 0.21914016\n",
      "Iteration 135, loss = 0.21060566Iteration 120, loss = 0.21960230\n",
      "\n",
      "Iteration 121, loss = 0.21814526\n",
      "Iteration 136, loss = 0.21053249\n",
      "Iteration 137, loss = 0.21060793\n",
      "Iteration 122, loss = 0.21737618\n",
      "Iteration 138, loss = 0.21004157\n",
      "Iteration 123, loss = 0.21728461\n",
      "Iteration 139, loss = 0.20918921\n",
      "Iteration 124, loss = 0.21711845\n",
      "Iteration 140, loss = 0.20907857\n",
      "Iteration 125, loss = 0.21666056\n",
      "Iteration 141, loss = 0.20871156Iteration 126, loss = 0.21630554\n",
      "\n",
      "Iteration 127, loss = 0.21622524\n",
      "Iteration 142, loss = 0.20858340\n",
      "Iteration 128, loss = 0.21592869\n",
      "Iteration 143, loss = 0.20832276\n",
      "Iteration 129, loss = 0.21581006\n",
      "Iteration 144, loss = 0.20790228\n",
      "Iteration 130, loss = 0.21555602\n",
      "Iteration 145, loss = 0.20766768\n",
      "Iteration 131, loss = 0.21577895\n",
      "Iteration 146, loss = 0.20723149\n",
      "Iteration 132, loss = 0.21582907\n",
      "Iteration 147, loss = 0.20690461\n",
      "Iteration 133, loss = 0.21445223\n",
      "Iteration 148, loss = 0.20631948\n",
      "Iteration 134, loss = 0.21417338\n",
      "Iteration 149, loss = 0.20598900\n",
      "Iteration 135, loss = 0.21405005\n",
      "Iteration 136, loss = 0.21356880\n",
      "Iteration 150, loss = 0.20581843\n",
      "Iteration 137, loss = 0.21292749\n",
      "Iteration 151, loss = 0.20584078\n",
      "Iteration 138, loss = 0.21262596\n",
      "Iteration 152, loss = 0.20615289\n",
      "Iteration 139, loss = 0.21183921\n",
      "Iteration 153, loss = 0.20526797\n",
      "Iteration 140, loss = 0.21138325\n",
      "Iteration 154, loss = 0.20523453\n",
      "Iteration 141, loss = 0.21194654\n",
      "Iteration 155, loss = 0.20404319\n",
      "Iteration 142, loss = 0.21116861\n",
      "Iteration 156, loss = 0.20368356\n",
      "Iteration 143, loss = 0.21110275\n",
      "Iteration 157, loss = 0.20333733\n",
      "Iteration 144, loss = 0.21018904\n",
      "Iteration 158, loss = 0.20307785\n",
      "Iteration 145, loss = 0.21031447\n",
      "Iteration 159, loss = 0.20296762\n",
      "Iteration 146, loss = 0.20984843\n",
      "Iteration 147, loss = 0.20929295\n",
      "Iteration 160, loss = 0.20246293\n",
      "Iteration 161, loss = 0.20238703\n",
      "Iteration 148, loss = 0.20909844\n",
      "Iteration 149, loss = 0.20904439\n",
      "Iteration 162, loss = 0.20180384\n",
      "Iteration 150, loss = 0.20860975\n",
      "Iteration 163, loss = 0.20132660\n",
      "Iteration 151, loss = 0.20821973\n",
      "Iteration 164, loss = 0.20132745\n",
      "Iteration 152, loss = 0.20848402\n",
      "Iteration 165, loss = 0.20085579\n",
      "Iteration 153, loss = 0.20764786\n",
      "Iteration 166, loss = 0.20101213\n",
      "Iteration 154, loss = 0.20789006\n",
      "Iteration 167, loss = 0.20060618Iteration 155, loss = 0.20695517\n",
      "\n",
      "Iteration 156, loss = 0.20635676\n",
      "Iteration 168, loss = 0.20084504\n",
      "Iteration 157, loss = 0.20605667\n",
      "Iteration 169, loss = 0.19976305\n",
      "Iteration 158, loss = 0.20580327\n",
      "Iteration 170, loss = 0.19936584\n",
      "Iteration 159, loss = 0.20612163\n",
      "Iteration 171, loss = 0.19926545\n",
      "Iteration 160, loss = 0.20522311\n",
      "Iteration 172, loss = 0.19918880\n",
      "Iteration 161, loss = 0.20504825\n",
      "Iteration 173, loss = 0.19893632\n",
      "Iteration 162, loss = 0.20464048\n",
      "Iteration 174, loss = 0.19831344\n",
      "Iteration 163, loss = 0.20425544\n",
      "Iteration 175, loss = 0.19771158\n",
      "Iteration 164, loss = 0.20414961\n",
      "Iteration 176, loss = 0.19792509\n",
      "Iteration 165, loss = 0.20379996\n",
      "Iteration 166, loss = 0.20362036\n",
      "Iteration 177, loss = 0.19764902\n",
      "Iteration 167, loss = 0.20325437\n",
      "Iteration 178, loss = 0.19719516\n",
      "Iteration 168, loss = 0.20331952\n",
      "Iteration 179, loss = 0.19633926\n",
      "Iteration 169, loss = 0.20345237\n",
      "Iteration 180, loss = 0.19717650\n",
      "Iteration 170, loss = 0.20244166\n",
      "Iteration 181, loss = 0.19637874\n",
      "Iteration 171, loss = 0.20327619\n",
      "Iteration 182, loss = 0.19624713\n",
      "Iteration 172, loss = 0.20322549\n",
      "Iteration 183, loss = 0.19577243\n",
      "Iteration 173, loss = 0.20193016\n",
      "Iteration 184, loss = 0.19576968\n",
      "Iteration 174, loss = 0.20192070\n",
      "Iteration 185, loss = 0.19558955\n",
      "Iteration 175, loss = 0.20122477\n",
      "Iteration 186, loss = 0.19523910\n",
      "Iteration 176, loss = 0.20086554\n",
      "Iteration 187, loss = 0.19469640\n",
      "Iteration 177, loss = 0.20048727\n",
      "Iteration 188, loss = 0.19465667\n",
      "Iteration 178, loss = 0.20037661\n",
      "Iteration 189, loss = 0.19411300\n",
      "Iteration 179, loss = 0.19927418\n",
      "Iteration 180, loss = 0.20048108Iteration 190, loss = 0.19475712\n",
      "\n",
      "Iteration 181, loss = 0.19931797\n",
      "Iteration 191, loss = 0.19456290\n",
      "Iteration 182, loss = 0.20002658\n",
      "Iteration 192, loss = 0.19386578\n",
      "Iteration 183, loss = 0.19906380\n",
      "Iteration 193, loss = 0.19295656\n",
      "Iteration 184, loss = 0.19968895\n",
      "Iteration 194, loss = 0.19324427\n",
      "Iteration 185, loss = 0.19875656\n",
      "Iteration 195, loss = 0.19303081\n",
      "Iteration 186, loss = 0.19838505\n",
      "Iteration 196, loss = 0.19290700\n",
      "Iteration 187, loss = 0.19839026\n",
      "Iteration 197, loss = 0.19235534\n",
      "Iteration 188, loss = 0.19854847\n",
      "Iteration 189, loss = 0.19718767\n",
      "Iteration 190, loss = 0.19995818\n",
      "Iteration 191, loss = 0.19798992\n",
      "Iteration 192, loss = 0.19786667\n",
      "Iteration 198, loss = 0.19198820\n",
      "Iteration 193, loss = 0.19637775\n",
      "Iteration 199, loss = 0.19191279\n",
      "Iteration 194, loss = 0.19686305\n",
      "Iteration 195, loss = 0.19654083\n",
      "Iteration 200, loss = 0.19337277\n",
      "Iteration 196, loss = 0.19646469\n",
      "Iteration 197, loss = 0.19612937\n",
      "Iteration 198, loss = 0.19523282\n",
      "Iteration 199, loss = 0.19562886\n",
      "Iteration 200, loss = 0.19619249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.39039072\n",
      "Iteration 1, loss = 0.39077560\n",
      "Iteration 2, loss = 0.33359530\n",
      "Iteration 2, loss = 0.33355544\n",
      "Iteration 3, loss = 0.32698875\n",
      "Iteration 3, loss = 0.32657400\n",
      "Iteration 4, loss = 0.31580564\n",
      "Iteration 4, loss = 0.31571222\n",
      "Iteration 5, loss = 0.30390084\n",
      "Iteration 5, loss = 0.30304375\n",
      "Iteration 6, loss = 0.29337534\n",
      "Iteration 6, loss = 0.29214376\n",
      "Iteration 7, loss = 0.28556735\n",
      "Iteration 7, loss = 0.28398273\n",
      "Iteration 8, loss = 0.27935067\n",
      "Iteration 8, loss = 0.27775639\n",
      "Iteration 9, loss = 0.27477676\n",
      "Iteration 9, loss = 0.27299013\n",
      "Iteration 10, loss = 0.27155273\n",
      "Iteration 10, loss = 0.26969692\n",
      "Iteration 11, loss = 0.26874764\n",
      "Iteration 11, loss = 0.26660952\n",
      "Iteration 12, loss = 0.26677880\n",
      "Iteration 12, loss = 0.26489259\n",
      "Iteration 13, loss = 0.26498645\n",
      "Iteration 13, loss = 0.26303188\n",
      "Iteration 14, loss = 0.26405495\n",
      "Iteration 14, loss = 0.26181258\n",
      "Iteration 15, loss = 0.26305377\n",
      "Iteration 15, loss = 0.26054695\n",
      "Iteration 16, loss = 0.26189133\n",
      "Iteration 16, loss = 0.25960074\n",
      "Iteration 17, loss = 0.26088710\n",
      "Iteration 17, loss = 0.25871231\n",
      "Iteration 18, loss = 0.26086204\n",
      "Iteration 18, loss = 0.25853106\n",
      "Iteration 19, loss = 0.25965405\n",
      "Iteration 19, loss = 0.25749705\n",
      "Iteration 20, loss = 0.25922846\n",
      "Iteration 20, loss = 0.25692080\n",
      "Iteration 21, loss = 0.25874294\n",
      "Iteration 21, loss = 0.25665096\n",
      "Iteration 22, loss = 0.25868354\n",
      "Iteration 22, loss = 0.25697714\n",
      "Iteration 23, loss = 0.25599788\n",
      "Iteration 23, loss = 0.25837080\n",
      "Iteration 24, loss = 0.25554312\n",
      "Iteration 24, loss = 0.25772982\n",
      "Iteration 25, loss = 0.25497307\n",
      "Iteration 25, loss = 0.25740805\n",
      "Iteration 26, loss = 0.25481268\n",
      "Iteration 26, loss = 0.25740671\n",
      "Iteration 27, loss = 0.25418644\n",
      "Iteration 27, loss = 0.25647831\n",
      "Iteration 28, loss = 0.25392497\n",
      "Iteration 28, loss = 0.25597048\n",
      "Iteration 29, loss = 0.25328497\n",
      "Iteration 29, loss = 0.25568435\n",
      "Iteration 30, loss = 0.25324299\n",
      "Iteration 30, loss = 0.25565654\n",
      "Iteration 31, loss = 0.25275644\n",
      "Iteration 31, loss = 0.25528108\n",
      "Iteration 32, loss = 0.25298618\n",
      "Iteration 32, loss = 0.25565023\n",
      "Iteration 33, loss = 0.25275955\n",
      "Iteration 33, loss = 0.25587675\n",
      "Iteration 34, loss = 0.25155679\n",
      "Iteration 34, loss = 0.25386842\n",
      "Iteration 35, loss = 0.25201752\n",
      "Iteration 35, loss = 0.25432907\n",
      "Iteration 36, loss = 0.25107903\n",
      "Iteration 36, loss = 0.25361111\n",
      "Iteration 37, loss = 0.25075617\n",
      "Iteration 37, loss = 0.25333868\n",
      "Iteration 38, loss = 0.25034117\n",
      "Iteration 38, loss = 0.25273730\n",
      "Iteration 39, loss = 0.25035109\n",
      "Iteration 39, loss = 0.25312453\n",
      "Iteration 40, loss = 0.25007755\n",
      "Iteration 40, loss = 0.25267889\n",
      "Iteration 41, loss = 0.24900703\n",
      "Iteration 41, loss = 0.25154845\n",
      "Iteration 42, loss = 0.24868438\n",
      "Iteration 42, loss = 0.25125808\n",
      "Iteration 43, loss = 0.24923573\n",
      "Iteration 44, loss = 0.24872018Iteration 43, loss = 0.25196729\n",
      "\n",
      "Iteration 45, loss = 0.24764824Iteration 44, loss = 0.25116573\n",
      "\n",
      "Iteration 46, loss = 0.24812458\n",
      "Iteration 45, loss = 0.25034597\n",
      "Iteration 47, loss = 0.24748653\n",
      "Iteration 46, loss = 0.25019819\n",
      "Iteration 48, loss = 0.24661588\n",
      "Iteration 47, loss = 0.25008894\n",
      "Iteration 48, loss = 0.24906045\n",
      "Iteration 49, loss = 0.24705575\n",
      "Iteration 49, loss = 0.24952737\n",
      "Iteration 50, loss = 0.24631146\n",
      "Iteration 50, loss = 0.24898007\n",
      "Iteration 51, loss = 0.24683962\n",
      "Iteration 51, loss = 0.24903262\n",
      "Iteration 52, loss = 0.24678838\n",
      "Iteration 52, loss = 0.24937211\n",
      "Iteration 53, loss = 0.24576525\n",
      "Iteration 54, loss = 0.24476868\n",
      "Iteration 53, loss = 0.24821286\n",
      "Iteration 55, loss = 0.24443753\n",
      "Iteration 54, loss = 0.24720653\n",
      "Iteration 56, loss = 0.24398879\n",
      "Iteration 55, loss = 0.24697083\n",
      "Iteration 57, loss = 0.24394499\n",
      "Iteration 56, loss = 0.24650941\n",
      "Iteration 58, loss = 0.24333853\n",
      "Iteration 57, loss = 0.24667262\n",
      "Iteration 59, loss = 0.24365625\n",
      "Iteration 58, loss = 0.24608265\n",
      "Iteration 60, loss = 0.24317555\n",
      "Iteration 59, loss = 0.24589574\n",
      "Iteration 61, loss = 0.24283106\n",
      "Iteration 60, loss = 0.24593473\n",
      "Iteration 61, loss = 0.24519764\n",
      "Iteration 62, loss = 0.24207558\n",
      "Iteration 62, loss = 0.24452345\n",
      "Iteration 63, loss = 0.24185549\n",
      "Iteration 63, loss = 0.24433335\n",
      "Iteration 64, loss = 0.24187633\n",
      "Iteration 65, loss = 0.24229510Iteration 64, loss = 0.24415628\n",
      "\n",
      "Iteration 66, loss = 0.24107654\n",
      "Iteration 65, loss = 0.24383022\n",
      "Iteration 66, loss = 0.24337802Iteration 67, loss = 0.24072789\n",
      "\n",
      "Iteration 67, loss = 0.24299744\n",
      "Iteration 68, loss = 0.24041253\n",
      "Iteration 68, loss = 0.24259845\n",
      "Iteration 69, loss = 0.24008000\n",
      "Iteration 70, loss = 0.24010405Iteration 69, loss = 0.24223315\n",
      "\n",
      "Iteration 70, loss = 0.24233130Iteration 71, loss = 0.23973256\n",
      "\n",
      "Iteration 72, loss = 0.23950522\n",
      "Iteration 71, loss = 0.24171871\n",
      "Iteration 73, loss = 0.23901365\n",
      "Iteration 72, loss = 0.24147794\n",
      "Iteration 74, loss = 0.23858076\n",
      "Iteration 73, loss = 0.24091913\n",
      "Iteration 75, loss = 0.23832645Iteration 74, loss = 0.24048484\n",
      "\n",
      "Iteration 75, loss = 0.24044652\n",
      "Iteration 76, loss = 0.23829341\n",
      "Iteration 77, loss = 0.23810538\n",
      "Iteration 78, loss = 0.23809004\n",
      "Iteration 79, loss = 0.23750465\n",
      "Iteration 76, loss = 0.23998513\n",
      "Iteration 80, loss = 0.23726900\n",
      "Iteration 77, loss = 0.23995614\n",
      "Iteration 81, loss = 0.23652964Iteration 78, loss = 0.24008842\n",
      "\n",
      "Iteration 82, loss = 0.23773028Iteration 79, loss = 0.23930747\n",
      "\n",
      "Iteration 80, loss = 0.23922091\n",
      "Iteration 83, loss = 0.23651477Iteration 81, loss = 0.23857281\n",
      "\n",
      "Iteration 82, loss = 0.23878859\n",
      "Iteration 84, loss = 0.23608040\n",
      "Iteration 85, loss = 0.23560731\n",
      "Iteration 83, loss = 0.23798451\n",
      "Iteration 86, loss = 0.23540341\n",
      "Iteration 87, loss = 0.23538172\n",
      "Iteration 84, loss = 0.23746381\n",
      "Iteration 88, loss = 0.23515170\n",
      "Iteration 85, loss = 0.23731936\n",
      "Iteration 89, loss = 0.23511112\n",
      "Iteration 86, loss = 0.23713915\n",
      "Iteration 87, loss = 0.23691394\n",
      "Iteration 90, loss = 0.23486389Iteration 88, loss = 0.23672608\n",
      "\n",
      "Iteration 89, loss = 0.23646686\n",
      "Iteration 91, loss = 0.23411024\n",
      "Iteration 90, loss = 0.23613578\n",
      "Iteration 91, loss = 0.23553176\n",
      "Iteration 92, loss = 0.23416708Iteration 92, loss = 0.23537707\n",
      "\n",
      "Iteration 93, loss = 0.23523055\n",
      "Iteration 93, loss = 0.23407107\n",
      "Iteration 94, loss = 0.23350302\n",
      "Iteration 94, loss = 0.23489467\n",
      "Iteration 95, loss = 0.23306449\n",
      "Iteration 96, loss = 0.23311593\n",
      "Iteration 95, loss = 0.23421480\n",
      "Iteration 97, loss = 0.23317275\n",
      "Iteration 98, loss = 0.23297438\n",
      "Iteration 96, loss = 0.23420838\n",
      "Iteration 99, loss = 0.23233415\n",
      "Iteration 100, loss = 0.23209987\n",
      "Iteration 97, loss = 0.23418053\n",
      "Iteration 101, loss = 0.23173658\n",
      "Iteration 98, loss = 0.23368140\n",
      "Iteration 102, loss = 0.23177465\n",
      "Iteration 99, loss = 0.23343026\n",
      "Iteration 103, loss = 0.23179797\n",
      "Iteration 100, loss = 0.23307516\n",
      "Iteration 104, loss = 0.23162521\n",
      "Iteration 105, loss = 0.23138706\n",
      "Iteration 101, loss = 0.23273958\n",
      "Iteration 106, loss = 0.23088765\n",
      "Iteration 102, loss = 0.23264021\n",
      "Iteration 107, loss = 0.23063100\n",
      "Iteration 103, loss = 0.23253317\n",
      "Iteration 104, loss = 0.23231721\n",
      "Iteration 108, loss = 0.23079567\n",
      "Iteration 109, loss = 0.23034691\n",
      "Iteration 105, loss = 0.23197028\n",
      "Iteration 110, loss = 0.23044919\n",
      "Iteration 106, loss = 0.23148966\n",
      "Iteration 111, loss = 0.22999574\n",
      "Iteration 107, loss = 0.23134171\n",
      "Iteration 112, loss = 0.22967060\n",
      "Iteration 108, loss = 0.23157546\n",
      "Iteration 113, loss = 0.22982819\n",
      "Iteration 109, loss = 0.23083492\n",
      "Iteration 114, loss = 0.22982040\n",
      "Iteration 110, loss = 0.23061195\n",
      "Iteration 115, loss = 0.22901552\n",
      "Iteration 111, loss = 0.23025671\n",
      "Iteration 116, loss = 0.22894862\n",
      "Iteration 112, loss = 0.22954759\n",
      "Iteration 117, loss = 0.22853682\n",
      "Iteration 113, loss = 0.23034371\n",
      "Iteration 118, loss = 0.22863840\n",
      "Iteration 114, loss = 0.23091219\n",
      "Iteration 119, loss = 0.22840191\n",
      "Iteration 115, loss = 0.22901408\n",
      "Iteration 120, loss = 0.22818723\n",
      "Iteration 116, loss = 0.22877072\n",
      "Iteration 121, loss = 0.22822319\n",
      "Iteration 117, loss = 0.22855228\n",
      "Iteration 122, loss = 0.22775253\n",
      "Iteration 118, loss = 0.22851483\n",
      "Iteration 123, loss = 0.22754293\n",
      "Iteration 119, loss = 0.22799186\n",
      "Iteration 124, loss = 0.22786069\n",
      "Iteration 120, loss = 0.22754658\n",
      "Iteration 125, loss = 0.22732480\n",
      "Iteration 126, loss = 0.22794739\n",
      "Iteration 121, loss = 0.22791101\n",
      "Iteration 122, loss = 0.22707033\n",
      "Iteration 127, loss = 0.22686441\n",
      "Iteration 123, loss = 0.22684450\n",
      "Iteration 124, loss = 0.22695712\n",
      "Iteration 128, loss = 0.22666375\n",
      "Iteration 125, loss = 0.22676002\n",
      "Iteration 129, loss = 0.22689272\n",
      "Iteration 126, loss = 0.22749171\n",
      "Iteration 127, loss = 0.22582564\n",
      "Iteration 130, loss = 0.22649778\n",
      "Iteration 131, loss = 0.22626075\n",
      "Iteration 132, loss = 0.22678504\n",
      "Iteration 128, loss = 0.22560597\n",
      "Iteration 133, loss = 0.22638038\n",
      "Iteration 134, loss = 0.22723249\n",
      "Iteration 129, loss = 0.22635651\n",
      "Iteration 135, loss = 0.22578402\n",
      "Iteration 130, loss = 0.22529104\n",
      "Iteration 136, loss = 0.22571247\n",
      "Iteration 131, loss = 0.22528561\n",
      "Iteration 137, loss = 0.22528978\n",
      "Iteration 132, loss = 0.22562876\n",
      "Iteration 133, loss = 0.22511629\n",
      "Iteration 138, loss = 0.22502191\n",
      "Iteration 139, loss = 0.22510384\n",
      "Iteration 140, loss = 0.22486493\n",
      "Iteration 134, loss = 0.22568082\n",
      "Iteration 141, loss = 0.22484585\n",
      "Iteration 135, loss = 0.22457171\n",
      "Iteration 142, loss = 0.22457181\n",
      "Iteration 136, loss = 0.22455946\n",
      "Iteration 143, loss = 0.22498601\n",
      "Iteration 137, loss = 0.22420853\n",
      "Iteration 144, loss = 0.22465108\n",
      "Iteration 138, loss = 0.22375415\n",
      "Iteration 145, loss = 0.22408908\n",
      "Iteration 139, loss = 0.22372240\n",
      "Iteration 146, loss = 0.22395118\n",
      "Iteration 140, loss = 0.22323718\n",
      "Iteration 141, loss = 0.22368466\n",
      "Iteration 142, loss = 0.22294183\n",
      "Iteration 147, loss = 0.22382829\n",
      "Iteration 143, loss = 0.22330353\n",
      "Iteration 148, loss = 0.22354832\n",
      "Iteration 144, loss = 0.22301782\n",
      "Iteration 149, loss = 0.22344719\n",
      "Iteration 145, loss = 0.22240269\n",
      "Iteration 150, loss = 0.22314616\n",
      "Iteration 146, loss = 0.22214698\n",
      "Iteration 151, loss = 0.22315157\n",
      "Iteration 152, loss = 0.22298902\n",
      "Iteration 147, loss = 0.22171764\n",
      "Iteration 153, loss = 0.22274496\n",
      "Iteration 148, loss = 0.22143153\n",
      "Iteration 154, loss = 0.22302925\n",
      "Iteration 149, loss = 0.22167967\n",
      "Iteration 155, loss = 0.22257661\n",
      "Iteration 150, loss = 0.22096797\n",
      "Iteration 156, loss = 0.22259551\n",
      "Iteration 151, loss = 0.22093021\n",
      "Iteration 157, loss = 0.22256004\n",
      "Iteration 152, loss = 0.22073884\n",
      "Iteration 158, loss = 0.22262886\n",
      "Iteration 153, loss = 0.22050481\n",
      "Iteration 159, loss = 0.22247180\n",
      "Iteration 154, loss = 0.22042848\n",
      "Iteration 160, loss = 0.22253676\n",
      "Iteration 155, loss = 0.22009373\n",
      "Iteration 161, loss = 0.22204391\n",
      "Iteration 156, loss = 0.22016930\n",
      "Iteration 162, loss = 0.22232380\n",
      "Iteration 157, loss = 0.21994595\n",
      "Iteration 163, loss = 0.22208057\n",
      "Iteration 158, loss = 0.22001313\n",
      "Iteration 164, loss = 0.22196434\n",
      "Iteration 159, loss = 0.21959703\n",
      "Iteration 165, loss = 0.22129858\n",
      "Iteration 160, loss = 0.21964686\n",
      "Iteration 166, loss = 0.22124029\n",
      "Iteration 161, loss = 0.21920025\n",
      "Iteration 167, loss = 0.22116965\n",
      "Iteration 162, loss = 0.21895674\n",
      "Iteration 168, loss = 0.22103192\n",
      "Iteration 163, loss = 0.21931716\n",
      "Iteration 169, loss = 0.22125770\n",
      "Iteration 164, loss = 0.21891419\n",
      "Iteration 170, loss = 0.22080932\n",
      "Iteration 165, loss = 0.21844797\n",
      "Iteration 171, loss = 0.22042238\n",
      "Iteration 166, loss = 0.21837028\n",
      "Iteration 172, loss = 0.22032316\n",
      "Iteration 167, loss = 0.21801323\n",
      "Iteration 173, loss = 0.22060905\n",
      "Iteration 168, loss = 0.21785911\n",
      "Iteration 174, loss = 0.22032064\n",
      "Iteration 169, loss = 0.21822864\n",
      "Iteration 175, loss = 0.22063637\n",
      "Iteration 176, loss = 0.21967269\n",
      "Iteration 177, loss = 0.21968176\n",
      "Iteration 170, loss = 0.21750129\n",
      "Iteration 178, loss = 0.22000825\n",
      "Iteration 171, loss = 0.21737917\n",
      "Iteration 179, loss = 0.21940882\n",
      "Iteration 172, loss = 0.21704486\n",
      "Iteration 180, loss = 0.21952289\n",
      "Iteration 173, loss = 0.21691739\n",
      "Iteration 181, loss = 0.21941782\n",
      "Iteration 174, loss = 0.21668002\n",
      "Iteration 182, loss = 0.21899519\n",
      "Iteration 183, loss = 0.21927900\n",
      "Iteration 175, loss = 0.21716191\n",
      "Iteration 184, loss = 0.21893676\n",
      "Iteration 176, loss = 0.21647251\n",
      "Iteration 185, loss = 0.21859103\n",
      "Iteration 177, loss = 0.21615171\n",
      "Iteration 178, loss = 0.21628514\n",
      "Iteration 186, loss = 0.21850664\n",
      "Iteration 187, loss = 0.21873523Iteration 179, loss = 0.21586825\n",
      "\n",
      "Iteration 188, loss = 0.21884781\n",
      "Iteration 180, loss = 0.21602246\n",
      "Iteration 189, loss = 0.21841054\n",
      "Iteration 181, loss = 0.21577362\n",
      "Iteration 190, loss = 0.21788145\n",
      "Iteration 182, loss = 0.21538090\n",
      "Iteration 191, loss = 0.21782081\n",
      "Iteration 183, loss = 0.21598604\n",
      "Iteration 192, loss = 0.21803892\n",
      "Iteration 184, loss = 0.21562303\n",
      "Iteration 193, loss = 0.21750798\n",
      "Iteration 185, loss = 0.21529692\n",
      "Iteration 194, loss = 0.21717226\n",
      "Iteration 186, loss = 0.21486310\n",
      "Iteration 195, loss = 0.21705929\n",
      "Iteration 196, loss = 0.21726923\n",
      "Iteration 187, loss = 0.21503713\n",
      "Iteration 197, loss = 0.21691986\n",
      "Iteration 188, loss = 0.21518474\n",
      "Iteration 198, loss = 0.21699199\n",
      "Iteration 189, loss = 0.21508787\n",
      "Iteration 199, loss = 0.21658480\n",
      "Iteration 190, loss = 0.21447178\n",
      "Iteration 200, loss = 0.21634185\n",
      "Iteration 191, loss = 0.21481554\n",
      "Iteration 192, loss = 0.21452226\n",
      "Iteration 193, loss = 0.21404115\n",
      "Iteration 194, loss = 0.21419225\n",
      "Iteration 195, loss = 0.21382819\n",
      "Iteration 1, loss = 0.39081193\n",
      "Iteration 196, loss = 0.21383501\n",
      "Iteration 2, loss = 0.33367500\n",
      "Iteration 3, loss = 0.32666584\n",
      "Iteration 197, loss = 0.21339311Iteration 4, loss = 0.31630430\n",
      "\n",
      "Iteration 198, loss = 0.21374529Iteration 5, loss = 0.30398154\n",
      "\n",
      "Iteration 6, loss = 0.29335423\n",
      "Iteration 199, loss = 0.21338219\n",
      "Iteration 7, loss = 0.28536990\n",
      "Iteration 200, loss = 0.21296774\n",
      "Iteration 8, loss = 0.27899472\n",
      "Iteration 1, loss = 0.39011155\n",
      "Iteration 2, loss = 0.33489214\n",
      "Iteration 9, loss = 0.27466659\n",
      "Iteration 3, loss = 0.32744380\n",
      "Iteration 10, loss = 0.27084802\n",
      "Iteration 4, loss = 0.31754864\n",
      "Iteration 11, loss = 0.26788208\n",
      "Iteration 12, loss = 0.26586853\n",
      "Iteration 5, loss = 0.30542109\n",
      "Iteration 13, loss = 0.26374952\n",
      "Iteration 14, loss = 0.26267026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, loss = 0.26131657\n",
      "Iteration 6, loss = 0.29473801\n",
      "Iteration 16, loss = 0.26052217\n",
      "Iteration 7, loss = 0.28660203\n",
      "Iteration 17, loss = 0.25964160\n",
      "Iteration 8, loss = 0.28047077\n",
      "Iteration 9, loss = 0.27623805\n",
      "Iteration 18, loss = 0.25897106\n",
      "Iteration 10, loss = 0.27264505\n",
      "Iteration 11, loss = 0.26992194\n",
      "Iteration 19, loss = 0.25834803\n",
      "Iteration 12, loss = 0.26795549\n",
      "Iteration 13, loss = 0.26602350\n",
      "Iteration 14, loss = 0.26496143\n",
      "Iteration 20, loss = 0.25782593\n",
      "Iteration 15, loss = 0.26378460\n",
      "Iteration 16, loss = 0.26336147\n",
      "Iteration 17, loss = 0.26209332\n",
      "Iteration 21, loss = 0.25765691\n",
      "Iteration 18, loss = 0.26176644\n",
      "Iteration 22, loss = 0.25740840\n",
      "Iteration 19, loss = 0.26137696\n",
      "Iteration 23, loss = 0.25681740\n",
      "Iteration 24, loss = 0.25688375\n",
      "Iteration 20, loss = 0.26070119Iteration 25, loss = 0.25620857\n",
      "\n",
      "Iteration 26, loss = 0.25677669\n",
      "Iteration 27, loss = 0.25566592\n",
      "Iteration 21, loss = 0.26035772\n",
      "Iteration 22, loss = 0.26046512Iteration 28, loss = 0.25522811\n",
      "\n",
      "Iteration 29, loss = 0.25478281\n",
      "Iteration 23, loss = 0.25989810\n",
      "Iteration 30, loss = 0.25442910\n",
      "Iteration 31, loss = 0.25443910\n",
      "Iteration 24, loss = 0.26025563\n",
      "Iteration 32, loss = 0.25449269\n",
      "Iteration 25, loss = 0.25919110\n",
      "Iteration 33, loss = 0.25452432\n",
      "Iteration 26, loss = 0.26026387\n",
      "Iteration 34, loss = 0.25317042\n",
      "Iteration 35, loss = 0.25342825\n",
      "Iteration 27, loss = 0.25891957Iteration 36, loss = 0.25314394\n",
      "\n",
      "Iteration 37, loss = 0.25286844\n",
      "Iteration 28, loss = 0.25838502\n",
      "Iteration 38, loss = 0.25257155\n",
      "Iteration 29, loss = 0.25779477\n",
      "Iteration 30, loss = 0.25776706\n",
      "Iteration 39, loss = 0.25272757\n",
      "Iteration 31, loss = 0.25756113\n",
      "Iteration 40, loss = 0.25196433\n",
      "Iteration 41, loss = 0.25113977\n",
      "Iteration 42, loss = 0.25073145\n",
      "Iteration 32, loss = 0.25708360\n",
      "Iteration 43, loss = 0.25112083\n",
      "Iteration 33, loss = 0.25750639\n",
      "Iteration 44, loss = 0.25043097\n",
      "Iteration 34, loss = 0.25656281\n",
      "Iteration 45, loss = 0.25007064\n",
      "Iteration 35, loss = 0.25624846\n",
      "Iteration 46, loss = 0.24989901\n",
      "Iteration 36, loss = 0.25602851\n",
      "Iteration 47, loss = 0.25011824\n",
      "Iteration 37, loss = 0.25560462\n",
      "Iteration 48, loss = 0.24976069\n",
      "Iteration 38, loss = 0.25518447\n",
      "Iteration 49, loss = 0.25001250\n",
      "Iteration 39, loss = 0.25528232\n",
      "Iteration 50, loss = 0.24876702\n",
      "Iteration 40, loss = 0.25517476\n",
      "Iteration 51, loss = 0.24943429\n",
      "Iteration 41, loss = 0.25409555\n",
      "Iteration 52, loss = 0.25025380\n",
      "Iteration 42, loss = 0.25365194\n",
      "Iteration 43, loss = 0.25433460\n",
      "Iteration 53, loss = 0.24836773\n",
      "Iteration 44, loss = 0.25369253\n",
      "Iteration 54, loss = 0.24699437\n",
      "Iteration 45, loss = 0.25328470\n",
      "Iteration 55, loss = 0.24702014\n",
      "Iteration 46, loss = 0.25299440\n",
      "Iteration 56, loss = 0.24646893\n",
      "Iteration 47, loss = 0.25387458\n",
      "Iteration 57, loss = 0.24595560\n",
      "Iteration 48, loss = 0.25271039\n",
      "Iteration 58, loss = 0.24570269\n",
      "Iteration 59, loss = 0.24563383\n",
      "Iteration 49, loss = 0.25276721Iteration 60, loss = 0.24522933\n",
      "\n",
      "Iteration 61, loss = 0.24538457\n",
      "Iteration 62, loss = 0.24456727\n",
      "Iteration 50, loss = 0.25169705\n",
      "Iteration 63, loss = 0.24428089\n",
      "Iteration 51, loss = 0.25203353\n",
      "Iteration 64, loss = 0.24417808\n",
      "Iteration 52, loss = 0.25175872\n",
      "Iteration 65, loss = 0.24458568\n",
      "Iteration 53, loss = 0.25088463\n",
      "Iteration 54, loss = 0.24970269\n",
      "Iteration 66, loss = 0.24357918\n",
      "Iteration 55, loss = 0.24942985\n",
      "Iteration 67, loss = 0.24301046\n",
      "Iteration 56, loss = 0.24899802\n",
      "Iteration 68, loss = 0.24257523\n",
      "Iteration 57, loss = 0.24853789\n",
      "Iteration 69, loss = 0.24270928\n",
      "Iteration 58, loss = 0.24816348\n",
      "Iteration 70, loss = 0.24267418\n",
      "Iteration 59, loss = 0.24825467\n",
      "Iteration 71, loss = 0.24214949\n",
      "Iteration 60, loss = 0.24767437\n",
      "Iteration 72, loss = 0.24160050\n",
      "Iteration 61, loss = 0.24806436\n",
      "Iteration 73, loss = 0.24116800\n",
      "Iteration 62, loss = 0.24699659\n",
      "Iteration 74, loss = 0.24096830\n",
      "Iteration 63, loss = 0.24654461\n",
      "Iteration 75, loss = 0.24095262\n",
      "Iteration 64, loss = 0.24626612\n",
      "Iteration 76, loss = 0.24064858\n",
      "Iteration 65, loss = 0.24655488\n",
      "Iteration 77, loss = 0.24047865\n",
      "Iteration 66, loss = 0.24527362\n",
      "Iteration 78, loss = 0.24027245\n",
      "Iteration 67, loss = 0.24503067\n",
      "Iteration 79, loss = 0.23967860\n",
      "Iteration 68, loss = 0.24452694\n",
      "Iteration 80, loss = 0.23956398\n",
      "Iteration 69, loss = 0.24461506\n",
      "Iteration 81, loss = 0.23895837\n",
      "Iteration 70, loss = 0.24430734\n",
      "Iteration 82, loss = 0.23952062\n",
      "Iteration 71, loss = 0.24413583\n",
      "Iteration 83, loss = 0.23882615\n",
      "Iteration 72, loss = 0.24355372\n",
      "Iteration 84, loss = 0.23819769\n",
      "Iteration 73, loss = 0.24293975\n",
      "Iteration 85, loss = 0.23792646\n",
      "Iteration 86, loss = 0.23788499\n",
      "Iteration 74, loss = 0.24274202Iteration 87, loss = 0.23747836\n",
      "\n",
      "Iteration 88, loss = 0.23745080\n",
      "Iteration 75, loss = 0.24245748\n",
      "Iteration 89, loss = 0.23746998\n",
      "Iteration 90, loss = 0.23711698\n",
      "Iteration 76, loss = 0.24226475\n",
      "Iteration 91, loss = 0.23623907Iteration 77, loss = 0.24216885\n",
      "Iteration 92, loss = 0.23642647\n",
      "\n",
      "Iteration 93, loss = 0.23619060Iteration 78, loss = 0.24209249\n",
      "Iteration 79, loss = 0.24149223\n",
      "\n",
      "Iteration 94, loss = 0.23560333\n",
      "Iteration 95, loss = 0.23507821\n",
      "Iteration 80, loss = 0.24123220\n",
      "Iteration 96, loss = 0.23498507\n",
      "Iteration 97, loss = 0.23474298\n",
      "Iteration 98, loss = 0.23416217\n",
      "Iteration 81, loss = 0.24076672\n",
      "Iteration 99, loss = 0.23422322\n",
      "Iteration 100, loss = 0.23369350\n",
      "Iteration 82, loss = 0.24187316Iteration 101, loss = 0.23347498\n",
      "Iteration 83, loss = 0.24065624\n",
      "\n",
      "Iteration 84, loss = 0.24047204Iteration 102, loss = 0.23347148\n",
      "\n",
      "Iteration 103, loss = 0.23326168\n",
      "Iteration 85, loss = 0.23973621\n",
      "Iteration 104, loss = 0.23248120\n",
      "Iteration 86, loss = 0.24014245\n",
      "Iteration 105, loss = 0.23295013\n",
      "Iteration 87, loss = 0.23980634\n",
      "Iteration 106, loss = 0.23221470\n",
      "Iteration 88, loss = 0.23927899\n",
      "Iteration 107, loss = 0.23205779\n",
      "Iteration 89, loss = 0.24020638\n",
      "Iteration 108, loss = 0.23195500\n",
      "Iteration 90, loss = 0.23901535\n",
      "Iteration 109, loss = 0.23162631\n",
      "Iteration 91, loss = 0.23892837\n",
      "Iteration 110, loss = 0.23201854\n",
      "Iteration 92, loss = 0.23886778\n",
      "Iteration 111, loss = 0.23138986\n",
      "Iteration 93, loss = 0.23828586\n",
      "Iteration 112, loss = 0.23048410\n",
      "Iteration 94, loss = 0.23790806\n",
      "Iteration 113, loss = 0.23116119\n",
      "Iteration 95, loss = 0.23725677\n",
      "Iteration 114, loss = 0.23066242\n",
      "Iteration 96, loss = 0.23707118\n",
      "Iteration 115, loss = 0.22977244\n",
      "Iteration 97, loss = 0.23695419\n",
      "Iteration 116, loss = 0.22965761\n",
      "Iteration 98, loss = 0.23680242\n",
      "Iteration 117, loss = 0.22932252\n",
      "Iteration 99, loss = 0.23640456\n",
      "Iteration 118, loss = 0.22936861\n",
      "Iteration 100, loss = 0.23620102\n",
      "Iteration 119, loss = 0.22888532\n",
      "Iteration 101, loss = 0.23600937\n",
      "Iteration 120, loss = 0.22860624\n",
      "Iteration 102, loss = 0.23574054\n",
      "Iteration 121, loss = 0.22807568\n",
      "Iteration 103, loss = 0.23567766\n",
      "Iteration 122, loss = 0.22795686\n",
      "Iteration 104, loss = 0.23547272\n",
      "Iteration 123, loss = 0.22772037\n",
      "Iteration 105, loss = 0.23570325\n",
      "Iteration 124, loss = 0.22812389\n",
      "Iteration 106, loss = 0.23483466\n",
      "Iteration 125, loss = 0.22716676\n",
      "Iteration 107, loss = 0.23436412\n",
      "Iteration 126, loss = 0.22812530\n",
      "Iteration 108, loss = 0.23433098\n",
      "Iteration 127, loss = 0.22660330\n",
      "Iteration 109, loss = 0.23428144\n",
      "Iteration 110, loss = 0.23441564\n",
      "Iteration 111, loss = 0.23389051\n",
      "Iteration 112, loss = 0.23360284\n",
      "Iteration 128, loss = 0.22660872\n",
      "Iteration 113, loss = 0.23372608\n",
      "Iteration 129, loss = 0.22689412\n",
      "Iteration 114, loss = 0.23339945Iteration 130, loss = 0.22585717\n",
      "\n",
      "Iteration 131, loss = 0.22569029\n",
      "Iteration 115, loss = 0.23282721Iteration 132, loss = 0.22560078\n",
      "\n",
      "Iteration 133, loss = 0.22542604\n",
      "Iteration 116, loss = 0.23270955\n",
      "Iteration 134, loss = 0.22530720\n",
      "Iteration 117, loss = 0.23222651\n",
      "Iteration 135, loss = 0.22468023\n",
      "Iteration 136, loss = 0.22453458\n",
      "Iteration 118, loss = 0.23232109\n",
      "Iteration 137, loss = 0.22425589\n",
      "Iteration 119, loss = 0.23179405Iteration 138, loss = 0.22371721\n",
      "\n",
      "Iteration 120, loss = 0.23195591\n",
      "Iteration 139, loss = 0.22399140\n",
      "Iteration 121, loss = 0.23166815\n",
      "Iteration 122, loss = 0.23144462\n",
      "Iteration 123, loss = 0.23109233\n",
      "Iteration 124, loss = 0.23126483\n",
      "Iteration 125, loss = 0.23090242\n",
      "Iteration 140, loss = 0.22353703\n",
      "Iteration 126, loss = 0.23162115\n",
      "Iteration 127, loss = 0.23022537\n",
      "Iteration 141, loss = 0.22335934\n",
      "Iteration 128, loss = 0.22990329\n",
      "Iteration 142, loss = 0.22309726\n",
      "Iteration 129, loss = 0.23020681\n",
      "Iteration 130, loss = 0.22971916\n",
      "Iteration 131, loss = 0.22938582\n",
      "Iteration 143, loss = 0.22293958\n",
      "Iteration 132, loss = 0.22925013Iteration 144, loss = 0.22261480\n",
      "\n",
      "Iteration 145, loss = 0.22237913\n",
      "Iteration 133, loss = 0.22909657Iteration 146, loss = 0.22200320\n",
      "\n",
      "Iteration 147, loss = 0.22167692\n",
      "Iteration 134, loss = 0.22913347\n",
      "Iteration 148, loss = 0.22201024\n",
      "Iteration 149, loss = 0.22165716\n",
      "Iteration 135, loss = 0.22852580\n",
      "Iteration 150, loss = 0.22092860\n",
      "Iteration 136, loss = 0.22824667\n",
      "Iteration 151, loss = 0.22108626\n",
      "Iteration 137, loss = 0.22851252\n",
      "Iteration 138, loss = 0.22799360\n",
      "Iteration 152, loss = 0.22064295\n",
      "Iteration 139, loss = 0.22783438\n",
      "Iteration 153, loss = 0.22076000\n",
      "Iteration 140, loss = 0.22793297\n",
      "Iteration 154, loss = 0.22042599\n",
      "Iteration 141, loss = 0.22770395\n",
      "Iteration 155, loss = 0.22029269\n",
      "Iteration 142, loss = 0.22760478\n",
      "Iteration 156, loss = 0.22000743\n",
      "Iteration 143, loss = 0.22724485\n",
      "Iteration 157, loss = 0.21979509\n",
      "Iteration 144, loss = 0.22718813\n",
      "Iteration 158, loss = 0.21999446\n",
      "Iteration 145, loss = 0.22695107\n",
      "Iteration 159, loss = 0.21994468\n",
      "Iteration 146, loss = 0.22659775\n",
      "Iteration 160, loss = 0.21919442\n",
      "Iteration 147, loss = 0.22654180\n",
      "Iteration 161, loss = 0.21930415\n",
      "Iteration 148, loss = 0.22682496\n",
      "Iteration 162, loss = 0.21912702\n",
      "Iteration 149, loss = 0.22643115\n",
      "Iteration 163, loss = 0.21973105\n",
      "Iteration 150, loss = 0.22607345\n",
      "Iteration 164, loss = 0.21864728\n",
      "Iteration 165, loss = 0.21814443\n",
      "Iteration 151, loss = 0.22603958\n",
      "Iteration 166, loss = 0.21801710\n",
      "Iteration 152, loss = 0.22566283\n",
      "Iteration 167, loss = 0.21787527\n",
      "Iteration 153, loss = 0.22540782\n",
      "Iteration 168, loss = 0.21842710\n",
      "Iteration 154, loss = 0.22559235\n",
      "Iteration 169, loss = 0.22030961\n",
      "Iteration 155, loss = 0.22502662\n",
      "Iteration 170, loss = 0.21851732\n",
      "Iteration 156, loss = 0.22485307\n",
      "Iteration 171, loss = 0.21710670\n",
      "Iteration 157, loss = 0.22485806\n",
      "Iteration 172, loss = 0.21699022\n",
      "Iteration 158, loss = 0.22511003\n",
      "Iteration 173, loss = 0.21714459\n",
      "Iteration 159, loss = 0.22529951\n",
      "Iteration 160, loss = 0.22450939\n",
      "Iteration 174, loss = 0.21683496\n",
      "Iteration 161, loss = 0.22465270\n",
      "Iteration 162, loss = 0.22457728\n",
      "Iteration 163, loss = 0.22511659\n",
      "Iteration 175, loss = 0.21677482\n",
      "Iteration 164, loss = 0.22439032\n",
      "Iteration 176, loss = 0.21640269\n",
      "Iteration 165, loss = 0.22337790\n",
      "Iteration 177, loss = 0.21657763\n",
      "Iteration 166, loss = 0.22337727\n",
      "Iteration 178, loss = 0.21635837\n",
      "Iteration 167, loss = 0.22377391\n",
      "Iteration 179, loss = 0.21622453\n",
      "Iteration 168, loss = 0.22397959\n",
      "Iteration 180, loss = 0.21604461\n",
      "Iteration 169, loss = 0.22507127\n",
      "Iteration 181, loss = 0.21609691\n",
      "Iteration 170, loss = 0.22418281\n",
      "Iteration 182, loss = 0.21597493\n",
      "Iteration 171, loss = 0.22273833\n",
      "Iteration 183, loss = 0.21590296\n",
      "Iteration 172, loss = 0.22258418\n",
      "Iteration 184, loss = 0.21602001\n",
      "Iteration 173, loss = 0.22251322\n",
      "Iteration 185, loss = 0.21511453\n",
      "Iteration 174, loss = 0.22274135\n",
      "Iteration 186, loss = 0.21515999\n",
      "Iteration 175, loss = 0.22263817\n",
      "Iteration 187, loss = 0.21510234\n",
      "Iteration 176, loss = 0.22236050\n",
      "Iteration 177, loss = 0.22149579\n",
      "Iteration 188, loss = 0.21470388\n",
      "Iteration 178, loss = 0.22213507\n",
      "Iteration 189, loss = 0.21468190\n",
      "Iteration 179, loss = 0.22150040\n",
      "Iteration 190, loss = 0.21427438\n",
      "Iteration 180, loss = 0.22143896\n",
      "Iteration 191, loss = 0.21450291\n",
      "Iteration 181, loss = 0.22159931\n",
      "Iteration 192, loss = 0.21477953\n",
      "Iteration 182, loss = 0.22203111\n",
      "Iteration 193, loss = 0.21422215\n",
      "Iteration 183, loss = 0.22106638\n",
      "Iteration 194, loss = 0.21427665\n",
      "Iteration 184, loss = 0.22098583\n",
      "Iteration 195, loss = 0.21380381\n",
      "Iteration 185, loss = 0.22040176\n",
      "Iteration 196, loss = 0.21389050\n",
      "Iteration 186, loss = 0.22032764\n",
      "Iteration 197, loss = 0.21369049\n",
      "Iteration 187, loss = 0.22021469\n",
      "Iteration 198, loss = 0.21352367\n",
      "Iteration 188, loss = 0.22015401\n",
      "Iteration 199, loss = 0.21343416\n",
      "Iteration 189, loss = 0.21996887\n",
      "Iteration 200, loss = 0.21322466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 190, loss = 0.22006653\n",
      "Iteration 191, loss = 0.22001142\n",
      "Iteration 192, loss = 0.22006147\n",
      "Iteration 1, loss = 0.38611524\n",
      "Iteration 193, loss = 0.22015092\n",
      "Iteration 2, loss = 0.33516992\n",
      "Iteration 194, loss = 0.21947941\n",
      "Iteration 3, loss = 0.32683164\n",
      "Iteration 195, loss = 0.21921637\n",
      "Iteration 4, loss = 0.31429402\n",
      "Iteration 196, loss = 0.21880739\n",
      "Iteration 5, loss = 0.30217911\n",
      "Iteration 197, loss = 0.21871227\n",
      "Iteration 6, loss = 0.29104247\n",
      "Iteration 198, loss = 0.21847035\n",
      "Iteration 7, loss = 0.28232622\n",
      "Iteration 199, loss = 0.21836317\n",
      "Iteration 8, loss = 0.27568737\n",
      "Iteration 9, loss = 0.27071283\n",
      "Iteration 10, loss = 0.26699455\n",
      "Iteration 200, loss = 0.21821212\n",
      "Iteration 11, loss = 0.26519298\n",
      "Iteration 1, loss = 0.38509188\n",
      "Iteration 12, loss = 0.26371978\n",
      "Iteration 2, loss = 0.33541514\n",
      "Iteration 13, loss = 0.26072812\n",
      "Iteration 3, loss = 0.32737113\n",
      "Iteration 14, loss = 0.25935487\n",
      "Iteration 4, loss = 0.31567146\n",
      "Iteration 15, loss = 0.25808315\n",
      "Iteration 5, loss = 0.30348722\n",
      "Iteration 16, loss = 0.25675770\n",
      "Iteration 6, loss = 0.29235407\n",
      "Iteration 17, loss = 0.25609047\n",
      "Iteration 7, loss = 0.28399074\n",
      "Iteration 18, loss = 0.25516707\n",
      "Iteration 8, loss = 0.27768043\n",
      "Iteration 19, loss = 0.25461470\n",
      "Iteration 9, loss = 0.27298687\n",
      "Iteration 10, loss = 0.26968465\n",
      "Iteration 20, loss = 0.25474190\n",
      "Iteration 11, loss = 0.26742366\n",
      "Iteration 21, loss = 0.25388984\n",
      "Iteration 12, loss = 0.26645323\n",
      "Iteration 22, loss = 0.25340967\n",
      "Iteration 13, loss = 0.26404369\n",
      "Iteration 23, loss = 0.25274897\n",
      "Iteration 14, loss = 0.26263104\n",
      "Iteration 24, loss = 0.25236642\n",
      "Iteration 15, loss = 0.26146009\n",
      "Iteration 25, loss = 0.25212224\n",
      "Iteration 16, loss = 0.26059893\n",
      "Iteration 26, loss = 0.25184367\n",
      "Iteration 17, loss = 0.25989388\n",
      "Iteration 27, loss = 0.25122177\n",
      "Iteration 18, loss = 0.25932127\n",
      "Iteration 28, loss = 0.25122223\n",
      "Iteration 19, loss = 0.25867113\n",
      "Iteration 29, loss = 0.25089830\n",
      "Iteration 20, loss = 0.25863668\n",
      "Iteration 30, loss = 0.25047088\n",
      "Iteration 21, loss = 0.25795644\n",
      "Iteration 31, loss = 0.25053601\n",
      "Iteration 22, loss = 0.25792097\n",
      "Iteration 32, loss = 0.25038910\n",
      "Iteration 23, loss = 0.25724935\n",
      "Iteration 33, loss = 0.25036377\n",
      "Iteration 24, loss = 0.25730411\n",
      "Iteration 34, loss = 0.24977491\n",
      "Iteration 25, loss = 0.25689526\n",
      "Iteration 26, loss = 0.25699014\n",
      "Iteration 35, loss = 0.24899406\n",
      "Iteration 27, loss = 0.25651915\n",
      "Iteration 36, loss = 0.24785849\n",
      "Iteration 28, loss = 0.25647347\n",
      "Iteration 29, loss = 0.25572090\n",
      "Iteration 37, loss = 0.24776250\n",
      "Iteration 30, loss = 0.25532918\n",
      "Iteration 31, loss = 0.25581985\n",
      "Iteration 38, loss = 0.24754029\n",
      "Iteration 32, loss = 0.25523852\n",
      "Iteration 33, loss = 0.25556642\n",
      "Iteration 39, loss = 0.24716229\n",
      "Iteration 34, loss = 0.25490059\n",
      "Iteration 35, loss = 0.25405492\n",
      "Iteration 40, loss = 0.24739020\n",
      "Iteration 41, loss = 0.24621464\n",
      "Iteration 36, loss = 0.25326212\n",
      "Iteration 42, loss = 0.24601349\n",
      "Iteration 37, loss = 0.25311726\n",
      "Iteration 43, loss = 0.24616539\n",
      "Iteration 38, loss = 0.25285399\n",
      "Iteration 44, loss = 0.24532882\n",
      "Iteration 39, loss = 0.25242186\n",
      "Iteration 45, loss = 0.24495778\n",
      "Iteration 40, loss = 0.25254325\n",
      "Iteration 46, loss = 0.24478141\n",
      "Iteration 41, loss = 0.25178656\n",
      "Iteration 47, loss = 0.24459641\n",
      "Iteration 42, loss = 0.25181855\n",
      "Iteration 48, loss = 0.24427388\n",
      "Iteration 43, loss = 0.25264795\n",
      "Iteration 49, loss = 0.24341311\n",
      "Iteration 44, loss = 0.25177029\n",
      "Iteration 50, loss = 0.24328250\n",
      "Iteration 45, loss = 0.25099478\n",
      "Iteration 51, loss = 0.24247920Iteration 46, loss = 0.25091240\n",
      "\n",
      "Iteration 47, loss = 0.25072346\n",
      "Iteration 52, loss = 0.24246290\n",
      "Iteration 48, loss = 0.25027203Iteration 53, loss = 0.24208423\n",
      "\n",
      "Iteration 49, loss = 0.24937019\n",
      "Iteration 54, loss = 0.24152239\n",
      "Iteration 50, loss = 0.24917507\n",
      "Iteration 55, loss = 0.24151309\n",
      "Iteration 51, loss = 0.24885636\n",
      "Iteration 56, loss = 0.24059984\n",
      "Iteration 52, loss = 0.24924120\n",
      "Iteration 57, loss = 0.24011333\n",
      "Iteration 53, loss = 0.24811310\n",
      "Iteration 58, loss = 0.23973117\n",
      "Iteration 54, loss = 0.24765665\n",
      "Iteration 59, loss = 0.23938576\n",
      "Iteration 55, loss = 0.24744644\n",
      "Iteration 60, loss = 0.23960967\n",
      "Iteration 56, loss = 0.24702262\n",
      "Iteration 61, loss = 0.23908047\n",
      "Iteration 62, loss = 0.23956194\n",
      "Iteration 57, loss = 0.24660234\n",
      "Iteration 63, loss = 0.23898085\n",
      "Iteration 58, loss = 0.24601763\n",
      "Iteration 64, loss = 0.23786683\n",
      "Iteration 65, loss = 0.23748751\n",
      "Iteration 66, loss = 0.23748294\n",
      "Iteration 67, loss = 0.23696715\n",
      "Iteration 59, loss = 0.24551459\n",
      "Iteration 68, loss = 0.23668195\n",
      "Iteration 60, loss = 0.24551835\n",
      "Iteration 69, loss = 0.23645048\n",
      "Iteration 61, loss = 0.24529035\n",
      "Iteration 70, loss = 0.23615115\n",
      "Iteration 71, loss = 0.23674862\n",
      "Iteration 72, loss = 0.23562721\n",
      "Iteration 62, loss = 0.24535958\n",
      "Iteration 73, loss = 0.23511719\n",
      "Iteration 63, loss = 0.24539362\n",
      "Iteration 64, loss = 0.24395063\n",
      "Iteration 74, loss = 0.23514356Iteration 65, loss = 0.24333544\n",
      "\n",
      "Iteration 66, loss = 0.24331620\n",
      "Iteration 67, loss = 0.24281869\n",
      "Iteration 68, loss = 0.24299042\n",
      "Iteration 75, loss = 0.23488205\n",
      "Iteration 69, loss = 0.24245469\n",
      "Iteration 70, loss = 0.24181276\n",
      "Iteration 71, loss = 0.24211538\n",
      "Iteration 76, loss = 0.23418352\n",
      "Iteration 72, loss = 0.24112989\n",
      "Iteration 77, loss = 0.23433705\n",
      "Iteration 73, loss = 0.24071889\n",
      "Iteration 74, loss = 0.24060940\n",
      "Iteration 78, loss = 0.23424727Iteration 75, loss = 0.24036933\n",
      "\n",
      "Iteration 76, loss = 0.23956385\n",
      "Iteration 79, loss = 0.23430965\n",
      "Iteration 77, loss = 0.23994427\n",
      "Iteration 80, loss = 0.23366275Iteration 78, loss = 0.23959170\n",
      "\n",
      "Iteration 79, loss = 0.23892036\n",
      "Iteration 80, loss = 0.23858067\n",
      "Iteration 81, loss = 0.23338241\n",
      "Iteration 81, loss = 0.23817303\n",
      "Iteration 82, loss = 0.23735879\n",
      "Iteration 82, loss = 0.23296199\n",
      "Iteration 83, loss = 0.23745578\n",
      "Iteration 83, loss = 0.23231937\n",
      "Iteration 84, loss = 0.23716010\n",
      "Iteration 84, loss = 0.23203702Iteration 85, loss = 0.23693019\n",
      "\n",
      "Iteration 86, loss = 0.23641541\n",
      "Iteration 85, loss = 0.23204227\n",
      "Iteration 87, loss = 0.23649124\n",
      "Iteration 86, loss = 0.23192014\n",
      "Iteration 88, loss = 0.23701504\n",
      "Iteration 87, loss = 0.23131925\n",
      "Iteration 89, loss = 0.23602754\n",
      "Iteration 88, loss = 0.23155484\n",
      "Iteration 90, loss = 0.23568177\n",
      "Iteration 89, loss = 0.23146928\n",
      "Iteration 91, loss = 0.23522472\n",
      "Iteration 90, loss = 0.23085534\n",
      "Iteration 92, loss = 0.23480554\n",
      "Iteration 91, loss = 0.23052861\n",
      "Iteration 93, loss = 0.23500573\n",
      "Iteration 92, loss = 0.23021482\n",
      "Iteration 94, loss = 0.23465883\n",
      "Iteration 93, loss = 0.23058210\n",
      "Iteration 95, loss = 0.23501691\n",
      "Iteration 94, loss = 0.22978151\n",
      "Iteration 96, loss = 0.23401363\n",
      "Iteration 95, loss = 0.23036922\n",
      "Iteration 97, loss = 0.23358394\n",
      "Iteration 96, loss = 0.22996620\n",
      "Iteration 98, loss = 0.23375238\n",
      "Iteration 97, loss = 0.22940254\n",
      "Iteration 99, loss = 0.23358025\n",
      "Iteration 98, loss = 0.22900920\n",
      "Iteration 100, loss = 0.23320463\n",
      "Iteration 99, loss = 0.22932842\n",
      "Iteration 101, loss = 0.23275508\n",
      "Iteration 100, loss = 0.22914549\n",
      "Iteration 102, loss = 0.23243000\n",
      "Iteration 101, loss = 0.22888686\n",
      "Iteration 103, loss = 0.23219197\n",
      "Iteration 102, loss = 0.22851777\n",
      "Iteration 104, loss = 0.23181246\n",
      "Iteration 103, loss = 0.22787814\n",
      "Iteration 105, loss = 0.23127452\n",
      "Iteration 104, loss = 0.22770105\n",
      "Iteration 106, loss = 0.23183438\n",
      "Iteration 105, loss = 0.22753260\n",
      "Iteration 107, loss = 0.23179816\n",
      "Iteration 106, loss = 0.22803091\n",
      "Iteration 108, loss = 0.23061479\n",
      "Iteration 107, loss = 0.22830853\n",
      "Iteration 109, loss = 0.23076148\n",
      "Iteration 108, loss = 0.22718172\n",
      "Iteration 110, loss = 0.23055440\n",
      "Iteration 109, loss = 0.22710486\n",
      "Iteration 111, loss = 0.23073024\n",
      "Iteration 110, loss = 0.22661293\n",
      "Iteration 112, loss = 0.22965504\n",
      "Iteration 111, loss = 0.22745347\n",
      "Iteration 113, loss = 0.22938351\n",
      "Iteration 114, loss = 0.22933619\n",
      "Iteration 112, loss = 0.22622356\n",
      "Iteration 115, loss = 0.22903259\n",
      "Iteration 113, loss = 0.22574214\n",
      "Iteration 116, loss = 0.22869990\n",
      "Iteration 114, loss = 0.22621746\n",
      "Iteration 117, loss = 0.22854962\n",
      "Iteration 115, loss = 0.22586164\n",
      "Iteration 118, loss = 0.22833800\n",
      "Iteration 116, loss = 0.22560160\n",
      "Iteration 119, loss = 0.22827205\n",
      "Iteration 117, loss = 0.22518754\n",
      "Iteration 120, loss = 0.22791087\n",
      "Iteration 118, loss = 0.22531989\n",
      "Iteration 121, loss = 0.22820094\n",
      "Iteration 119, loss = 0.22494847\n",
      "Iteration 122, loss = 0.22765241\n",
      "Iteration 120, loss = 0.22460599\n",
      "Iteration 123, loss = 0.22714731\n",
      "Iteration 121, loss = 0.22465875\n",
      "Iteration 124, loss = 0.22679327\n",
      "Iteration 122, loss = 0.22447995\n",
      "Iteration 123, loss = 0.22451595\n",
      "Iteration 125, loss = 0.22702890\n",
      "Iteration 124, loss = 0.22385123\n",
      "Iteration 126, loss = 0.22698026\n",
      "Iteration 125, loss = 0.22418740\n",
      "Iteration 127, loss = 0.22637101\n",
      "Iteration 126, loss = 0.22390086\n",
      "Iteration 127, loss = 0.22356061\n",
      "Iteration 128, loss = 0.22719047\n",
      "Iteration 128, loss = 0.22382614\n",
      "Iteration 129, loss = 0.22620254\n",
      "Iteration 130, loss = 0.22583902\n",
      "Iteration 129, loss = 0.22315844\n",
      "Iteration 131, loss = 0.22594977\n",
      "Iteration 130, loss = 0.22302710\n",
      "Iteration 132, loss = 0.22592153\n",
      "Iteration 131, loss = 0.22285537\n",
      "Iteration 133, loss = 0.22559391\n",
      "Iteration 132, loss = 0.22289823\n",
      "Iteration 134, loss = 0.22516106\n",
      "Iteration 133, loss = 0.22259384\n",
      "Iteration 135, loss = 0.22469472\n",
      "Iteration 134, loss = 0.22224822\n",
      "Iteration 136, loss = 0.22451855\n",
      "Iteration 135, loss = 0.22229025\n",
      "Iteration 137, loss = 0.22448110\n",
      "Iteration 136, loss = 0.22225315\n",
      "Iteration 138, loss = 0.22452541\n",
      "Iteration 137, loss = 0.22182782\n",
      "Iteration 139, loss = 0.22402781\n",
      "Iteration 138, loss = 0.22171163\n",
      "Iteration 140, loss = 0.22378935\n",
      "Iteration 139, loss = 0.22148927\n",
      "Iteration 141, loss = 0.22384558\n",
      "Iteration 140, loss = 0.22115338Iteration 142, loss = 0.22371785\n",
      "\n",
      "Iteration 143, loss = 0.22337288\n",
      "Iteration 141, loss = 0.22172785\n",
      "Iteration 144, loss = 0.22341666\n",
      "Iteration 142, loss = 0.22086722\n",
      "Iteration 145, loss = 0.22289346\n",
      "Iteration 143, loss = 0.22067473\n",
      "Iteration 146, loss = 0.22296161\n",
      "Iteration 144, loss = 0.22087997\n",
      "Iteration 147, loss = 0.22284408\n",
      "Iteration 145, loss = 0.22028557\n",
      "Iteration 148, loss = 0.22242320\n",
      "Iteration 146, loss = 0.22036634\n",
      "Iteration 149, loss = 0.22228422\n",
      "Iteration 147, loss = 0.22008693\n",
      "Iteration 150, loss = 0.22197660\n",
      "Iteration 148, loss = 0.21992272\n",
      "Iteration 151, loss = 0.22213962\n",
      "Iteration 149, loss = 0.21987339\n",
      "Iteration 152, loss = 0.22161942\n",
      "Iteration 150, loss = 0.21959269\n",
      "Iteration 153, loss = 0.22160725\n",
      "Iteration 154, loss = 0.22153720\n",
      "Iteration 151, loss = 0.21982109\n",
      "Iteration 155, loss = 0.22143439\n",
      "Iteration 152, loss = 0.21930605\n",
      "Iteration 156, loss = 0.22132465\n",
      "Iteration 153, loss = 0.21913390\n",
      "Iteration 157, loss = 0.22139972\n",
      "Iteration 154, loss = 0.21891380\n",
      "Iteration 158, loss = 0.22093620\n",
      "Iteration 155, loss = 0.21922517\n",
      "Iteration 159, loss = 0.22075760\n",
      "Iteration 156, loss = 0.21909000\n",
      "Iteration 160, loss = 0.22027552\n",
      "Iteration 157, loss = 0.21860516\n",
      "Iteration 161, loss = 0.22031113\n",
      "Iteration 158, loss = 0.21827595\n",
      "Iteration 162, loss = 0.22025094\n",
      "Iteration 159, loss = 0.21877848\n",
      "Iteration 163, loss = 0.22042401\n",
      "Iteration 160, loss = 0.21813104\n",
      "Iteration 164, loss = 0.22016107\n",
      "Iteration 161, loss = 0.21787080\n",
      "Iteration 165, loss = 0.21948175\n",
      "Iteration 162, loss = 0.21760609\n",
      "Iteration 166, loss = 0.21962778\n",
      "Iteration 167, loss = 0.22010831\n",
      "Iteration 163, loss = 0.21792936\n",
      "Iteration 168, loss = 0.21959924\n",
      "Iteration 164, loss = 0.21815703\n",
      "Iteration 169, loss = 0.21918815\n",
      "Iteration 165, loss = 0.21729392\n",
      "Iteration 170, loss = 0.21888981\n",
      "Iteration 166, loss = 0.21720495\n",
      "Iteration 171, loss = 0.21864145\n",
      "Iteration 167, loss = 0.21727621\n",
      "Iteration 172, loss = 0.21908895\n",
      "Iteration 168, loss = 0.21654502\n",
      "Iteration 173, loss = 0.21880262\n",
      "Iteration 169, loss = 0.21643432\n",
      "Iteration 174, loss = 0.21949317\n",
      "Iteration 170, loss = 0.21613129\n",
      "Iteration 175, loss = 0.21843308\n",
      "Iteration 171, loss = 0.21604111\n",
      "Iteration 176, loss = 0.21795078\n",
      "Iteration 172, loss = 0.21606412\n",
      "Iteration 177, loss = 0.21832413\n",
      "Iteration 173, loss = 0.21631709\n",
      "Iteration 178, loss = 0.21777718\n",
      "Iteration 179, loss = 0.21724431\n",
      "Iteration 174, loss = 0.21602400\n",
      "Iteration 180, loss = 0.21756476\n",
      "Iteration 181, loss = 0.21752598\n",
      "Iteration 175, loss = 0.21565534\n",
      "Iteration 176, loss = 0.21539035\n",
      "Iteration 177, loss = 0.21603317\n",
      "Iteration 182, loss = 0.21706073\n",
      "Iteration 178, loss = 0.21567652\n",
      "Iteration 183, loss = 0.21746746\n",
      "Iteration 179, loss = 0.21466000\n",
      "Iteration 184, loss = 0.21670669\n",
      "Iteration 180, loss = 0.21502343Iteration 185, loss = 0.21666074\n",
      "\n",
      "Iteration 186, loss = 0.21656314\n",
      "Iteration 187, loss = 0.21645705\n",
      "Iteration 181, loss = 0.21439549\n",
      "Iteration 188, loss = 0.21675858\n",
      "Iteration 182, loss = 0.21443499\n",
      "Iteration 189, loss = 0.21618479\n",
      "Iteration 183, loss = 0.21488183\n",
      "Iteration 190, loss = 0.21619721\n",
      "Iteration 191, loss = 0.21569832\n",
      "Iteration 192, loss = 0.21549482\n",
      "Iteration 184, loss = 0.21387032\n",
      "Iteration 193, loss = 0.21546999\n",
      "Iteration 194, loss = 0.21562538\n",
      "Iteration 195, loss = 0.21481254\n",
      "Iteration 185, loss = 0.21383547Iteration 196, loss = 0.21502974\n",
      "\n",
      "Iteration 197, loss = 0.21469698\n",
      "Iteration 198, loss = 0.21460752\n",
      "Iteration 186, loss = 0.21402417\n",
      "Iteration 187, loss = 0.21378159\n",
      "Iteration 188, loss = 0.21450455\n",
      "Iteration 199, loss = 0.21480329\n",
      "Iteration 189, loss = 0.21390812\n",
      "Iteration 200, loss = 0.21473906\n",
      "Iteration 190, loss = 0.21293897\n",
      "Iteration 191, loss = 0.21299107\n",
      "Iteration 192, loss = 0.21295984\n",
      "Iteration 193, loss = 0.21327653\n",
      "Iteration 194, loss = 0.21263310\n",
      "Iteration 1, loss = 0.38525696\n",
      "Iteration 195, loss = 0.21245220\n",
      "Iteration 2, loss = 0.33453551\n",
      "Iteration 196, loss = 0.21276449\n",
      "Iteration 3, loss = 0.32638079\n",
      "Iteration 197, loss = 0.21299697\n",
      "Iteration 198, loss = 0.21220345\n",
      "Iteration 4, loss = 0.31337082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 199, loss = 0.21219005\n",
      "Iteration 200, loss = 0.21278953\n",
      "Iteration 5, loss = 0.30118716\n",
      "Iteration 6, loss = 0.28967880\n",
      "Iteration 7, loss = 0.28137415\n",
      "Iteration 8, loss = 0.27481572Iteration 1, loss = 0.38621108\n",
      "\n",
      "Iteration 9, loss = 0.27067518\n",
      "Iteration 2, loss = 0.33537363\n",
      "Iteration 10, loss = 0.26681114\n",
      "Iteration 3, loss = 0.32628674\n",
      "Iteration 11, loss = 0.26436659\n",
      "Iteration 4, loss = 0.31319277\n",
      "Iteration 12, loss = 0.26332563\n",
      "Iteration 5, loss = 0.30039331\n",
      "Iteration 13, loss = 0.26070248\n",
      "Iteration 6, loss = 0.28824616\n",
      "Iteration 14, loss = 0.25901982\n",
      "Iteration 7, loss = 0.27963328\n",
      "Iteration 15, loss = 0.25792296\n",
      "Iteration 8, loss = 0.27258958\n",
      "Iteration 16, loss = 0.25690903\n",
      "Iteration 9, loss = 0.26848341\n",
      "Iteration 17, loss = 0.25608586\n",
      "Iteration 10, loss = 0.26448718\n",
      "Iteration 18, loss = 0.25537465\n",
      "Iteration 11, loss = 0.26178921\n",
      "Iteration 19, loss = 0.25449669\n",
      "Iteration 12, loss = 0.25985989\n",
      "Iteration 20, loss = 0.25451733\n",
      "Iteration 13, loss = 0.25766296\n",
      "Iteration 21, loss = 0.25368478\n",
      "Iteration 22, loss = 0.25367531\n",
      "Iteration 14, loss = 0.25632020\n",
      "Iteration 23, loss = 0.25283019\n",
      "Iteration 15, loss = 0.25516565\n",
      "Iteration 24, loss = 0.25340031\n",
      "Iteration 16, loss = 0.25439532\n",
      "Iteration 25, loss = 0.25234641\n",
      "Iteration 17, loss = 0.25366374\n",
      "Iteration 26, loss = 0.25183099\n",
      "Iteration 18, loss = 0.25291735\n",
      "Iteration 27, loss = 0.25163612\n",
      "Iteration 19, loss = 0.25209764\n",
      "Iteration 28, loss = 0.25147376\n",
      "Iteration 20, loss = 0.25213535\n",
      "Iteration 29, loss = 0.25076988\n",
      "Iteration 30, loss = 0.25047673\n",
      "Iteration 21, loss = 0.25135568\n",
      "Iteration 31, loss = 0.25036046\n",
      "Iteration 22, loss = 0.25130053\n",
      "Iteration 32, loss = 0.24947731\n",
      "Iteration 33, loss = 0.24975490\n",
      "Iteration 23, loss = 0.25010132\n",
      "Iteration 34, loss = 0.24953355\n",
      "Iteration 24, loss = 0.25028868\n",
      "Iteration 35, loss = 0.24874581\n",
      "Iteration 25, loss = 0.24983050\n",
      "Iteration 36, loss = 0.24797297\n",
      "Iteration 26, loss = 0.24937661\n",
      "Iteration 37, loss = 0.24757636\n",
      "Iteration 27, loss = 0.24889092\n",
      "Iteration 38, loss = 0.24722388\n",
      "Iteration 28, loss = 0.24895604\n",
      "Iteration 39, loss = 0.24669746\n",
      "Iteration 29, loss = 0.24834757\n",
      "Iteration 40, loss = 0.24683055\n",
      "Iteration 30, loss = 0.24828891Iteration 41, loss = 0.24604549\n",
      "\n",
      "Iteration 42, loss = 0.24627740\n",
      "Iteration 31, loss = 0.24764983\n",
      "Iteration 43, loss = 0.24668004\n",
      "Iteration 32, loss = 0.24722046\n",
      "Iteration 33, loss = 0.24808913\n",
      "Iteration 44, loss = 0.24548352\n",
      "Iteration 34, loss = 0.24702562\n",
      "Iteration 45, loss = 0.24489625\n",
      "Iteration 46, loss = 0.24489765Iteration 35, loss = 0.24702988\n",
      "\n",
      "Iteration 47, loss = 0.24476630\n",
      "Iteration 36, loss = 0.24581586\n",
      "Iteration 48, loss = 0.24463052\n",
      "Iteration 37, loss = 0.24578052\n",
      "Iteration 49, loss = 0.24345717\n",
      "Iteration 38, loss = 0.24539258\n",
      "Iteration 50, loss = 0.24287817\n",
      "Iteration 39, loss = 0.24505191\n",
      "Iteration 51, loss = 0.24338699\n",
      "Iteration 40, loss = 0.24566186\n",
      "Iteration 52, loss = 0.24300087\n",
      "Iteration 41, loss = 0.24457273\n",
      "Iteration 53, loss = 0.24217186\n",
      "Iteration 42, loss = 0.24471568\n",
      "Iteration 54, loss = 0.24176218\n",
      "Iteration 43, loss = 0.24511356\n",
      "Iteration 55, loss = 0.24190050\n",
      "Iteration 44, loss = 0.24348275\n",
      "Iteration 56, loss = 0.24108410\n",
      "Iteration 45, loss = 0.24289039\n",
      "Iteration 57, loss = 0.24072957\n",
      "Iteration 46, loss = 0.24305139\n",
      "Iteration 58, loss = 0.24024888\n",
      "Iteration 47, loss = 0.24314189\n",
      "Iteration 59, loss = 0.24002692\n",
      "Iteration 60, loss = 0.24007946\n",
      "Iteration 48, loss = 0.24252335\n",
      "Iteration 61, loss = 0.24019210\n",
      "Iteration 49, loss = 0.24149824\n",
      "Iteration 62, loss = 0.23967385\n",
      "Iteration 50, loss = 0.24079116\n",
      "Iteration 63, loss = 0.23927469\n",
      "Iteration 51, loss = 0.24120819\n",
      "Iteration 64, loss = 0.23883065\n",
      "Iteration 52, loss = 0.24062841\n",
      "Iteration 65, loss = 0.23811449\n",
      "Iteration 53, loss = 0.24065585\n",
      "Iteration 66, loss = 0.23814085\n",
      "Iteration 54, loss = 0.23987653\n",
      "Iteration 67, loss = 0.23773258\n",
      "Iteration 55, loss = 0.23972021\n",
      "Iteration 68, loss = 0.23784424\n",
      "Iteration 56, loss = 0.23891218\n",
      "Iteration 69, loss = 0.23790117\n",
      "Iteration 57, loss = 0.23934315Iteration 70, loss = 0.23688283\n",
      "\n",
      "Iteration 71, loss = 0.23717320Iteration 58, loss = 0.23878071\n",
      "\n",
      "Iteration 72, loss = 0.23631716\n",
      "Iteration 73, loss = 0.23588550\n",
      "Iteration 74, loss = 0.23581729\n",
      "Iteration 59, loss = 0.23873354\n",
      "Iteration 75, loss = 0.23533996\n",
      "Iteration 76, loss = 0.23497027\n",
      "Iteration 60, loss = 0.23811831\n",
      "Iteration 77, loss = 0.23539336\n",
      "Iteration 61, loss = 0.23822274Iteration 78, loss = 0.23534115\n",
      "Iteration 79, loss = 0.23442566\n",
      "\n",
      "Iteration 80, loss = 0.23468826\n",
      "Iteration 62, loss = 0.23692482Iteration 81, loss = 0.23355386\n",
      "\n",
      "Iteration 82, loss = 0.23312141\n",
      "Iteration 83, loss = 0.23282360\n",
      "Iteration 63, loss = 0.23680457\n",
      "Iteration 64, loss = 0.23655309\n",
      "Iteration 84, loss = 0.23260767\n",
      "Iteration 65, loss = 0.23597140\n",
      "Iteration 66, loss = 0.23568750\n",
      "Iteration 85, loss = 0.23238900Iteration 67, loss = 0.23534031\n",
      "\n",
      "Iteration 68, loss = 0.23552697\n",
      "Iteration 86, loss = 0.23208301\n",
      "Iteration 69, loss = 0.23520669\n",
      "Iteration 87, loss = 0.23211070\n",
      "Iteration 70, loss = 0.23428910\n",
      "Iteration 88, loss = 0.23213683\n",
      "Iteration 71, loss = 0.23473900\n",
      "Iteration 89, loss = 0.23138669\n",
      "Iteration 72, loss = 0.23388580\n",
      "Iteration 90, loss = 0.23119519\n",
      "Iteration 73, loss = 0.23344139\n",
      "Iteration 91, loss = 0.23067169\n",
      "Iteration 92, loss = 0.23020912\n",
      "Iteration 74, loss = 0.23335087\n",
      "Iteration 93, loss = 0.23033883\n",
      "Iteration 75, loss = 0.23279226\n",
      "Iteration 76, loss = 0.23258474\n",
      "Iteration 94, loss = 0.23027068\n",
      "Iteration 77, loss = 0.23244394\n",
      "Iteration 95, loss = 0.23024596\n",
      "Iteration 78, loss = 0.23269899\n",
      "Iteration 96, loss = 0.22937579\n",
      "Iteration 79, loss = 0.23174060\n",
      "Iteration 97, loss = 0.22897722\n",
      "Iteration 80, loss = 0.23189077\n",
      "Iteration 98, loss = 0.22881778\n",
      "Iteration 81, loss = 0.23098150\n",
      "Iteration 82, loss = 0.23062288\n",
      "Iteration 99, loss = 0.22872589\n",
      "Iteration 83, loss = 0.23059058\n",
      "Iteration 100, loss = 0.22798255\n",
      "Iteration 84, loss = 0.22995298\n",
      "Iteration 101, loss = 0.22747532\n",
      "Iteration 85, loss = 0.22987656\n",
      "Iteration 102, loss = 0.22732700\n",
      "Iteration 86, loss = 0.22964908\n",
      "Iteration 103, loss = 0.22811785\n",
      "Iteration 87, loss = 0.22960338\n",
      "Iteration 104, loss = 0.22693536\n",
      "Iteration 88, loss = 0.22924476\n",
      "Iteration 105, loss = 0.22595714\n",
      "Iteration 89, loss = 0.22894661\n",
      "Iteration 106, loss = 0.22615860\n",
      "Iteration 90, loss = 0.22877597\n",
      "Iteration 107, loss = 0.22587927\n",
      "Iteration 91, loss = 0.22812244\n",
      "Iteration 108, loss = 0.22543912\n",
      "Iteration 92, loss = 0.22774828\n",
      "Iteration 109, loss = 0.22514816\n",
      "Iteration 93, loss = 0.22773312\n",
      "Iteration 110, loss = 0.22439603\n",
      "Iteration 94, loss = 0.22729888\n",
      "Iteration 111, loss = 0.22460319\n",
      "Iteration 95, loss = 0.22758537\n",
      "Iteration 112, loss = 0.22404763\n",
      "Iteration 96, loss = 0.22687119\n",
      "Iteration 113, loss = 0.22332832\n",
      "Iteration 97, loss = 0.22688760\n",
      "Iteration 114, loss = 0.22335420\n",
      "Iteration 98, loss = 0.22622189\n",
      "Iteration 115, loss = 0.22294757\n",
      "Iteration 99, loss = 0.22614008\n",
      "Iteration 116, loss = 0.22263281\n",
      "Iteration 100, loss = 0.22635699\n",
      "Iteration 117, loss = 0.22210343\n",
      "Iteration 118, loss = 0.22165155\n",
      "Iteration 101, loss = 0.22555615\n",
      "Iteration 119, loss = 0.22168422\n",
      "Iteration 102, loss = 0.22552485\n",
      "Iteration 103, loss = 0.22635458\n",
      "Iteration 120, loss = 0.22138766\n",
      "Iteration 104, loss = 0.22526213\n",
      "Iteration 121, loss = 0.22144783\n",
      "Iteration 122, loss = 0.22059930Iteration 105, loss = 0.22445099\n",
      "\n",
      "Iteration 123, loss = 0.22025694\n",
      "Iteration 106, loss = 0.22434840\n",
      "Iteration 124, loss = 0.21980228\n",
      "Iteration 107, loss = 0.22520519\n",
      "Iteration 108, loss = 0.22371262\n",
      "Iteration 125, loss = 0.22009393\n",
      "Iteration 109, loss = 0.22368664\n",
      "Iteration 126, loss = 0.21976768\n",
      "Iteration 110, loss = 0.22323623\n",
      "Iteration 127, loss = 0.21905072\n",
      "Iteration 111, loss = 0.22326145\n",
      "Iteration 128, loss = 0.21929052\n",
      "Iteration 112, loss = 0.22293308\n",
      "Iteration 129, loss = 0.21861911\n",
      "Iteration 130, loss = 0.21823748\n",
      "Iteration 113, loss = 0.22246704\n",
      "Iteration 131, loss = 0.21826665\n",
      "Iteration 114, loss = 0.22278531\n",
      "Iteration 132, loss = 0.21759900\n",
      "Iteration 115, loss = 0.22306283\n",
      "Iteration 133, loss = 0.21739365\n",
      "Iteration 116, loss = 0.22260480\n",
      "Iteration 134, loss = 0.21718282\n",
      "Iteration 117, loss = 0.22163817\n",
      "Iteration 135, loss = 0.21681504\n",
      "Iteration 118, loss = 0.22128073\n",
      "Iteration 119, loss = 0.22116371\n",
      "Iteration 136, loss = 0.21710644\n",
      "Iteration 137, loss = 0.21668491\n",
      "Iteration 120, loss = 0.22166640\n",
      "Iteration 121, loss = 0.22177875\n",
      "Iteration 138, loss = 0.21626213\n",
      "Iteration 122, loss = 0.22031698\n",
      "Iteration 139, loss = 0.21592918\n",
      "Iteration 123, loss = 0.22016328\n",
      "Iteration 140, loss = 0.21558291\n",
      "Iteration 124, loss = 0.21994845Iteration 141, loss = 0.21568189\n",
      "\n",
      "Iteration 142, loss = 0.21534949\n",
      "Iteration 125, loss = 0.22003692\n",
      "Iteration 143, loss = 0.21477974\n",
      "Iteration 126, loss = 0.22063696\n",
      "Iteration 144, loss = 0.21511848\n",
      "Iteration 127, loss = 0.21949157\n",
      "Iteration 145, loss = 0.21476185\n",
      "Iteration 128, loss = 0.21959753\n",
      "Iteration 146, loss = 0.21420487\n",
      "Iteration 129, loss = 0.21894016\n",
      "Iteration 147, loss = 0.21382238\n",
      "Iteration 130, loss = 0.21867445\n",
      "Iteration 148, loss = 0.21358426\n",
      "Iteration 149, loss = 0.21319244\n",
      "Iteration 150, loss = 0.21288238\n",
      "Iteration 131, loss = 0.21878227\n",
      "Iteration 151, loss = 0.21304579\n",
      "Iteration 132, loss = 0.21860518\n",
      "Iteration 152, loss = 0.21250126\n",
      "Iteration 153, loss = 0.21255503\n",
      "Iteration 133, loss = 0.21848482\n",
      "Iteration 154, loss = 0.21228720\n",
      "Iteration 134, loss = 0.21801290\n",
      "Iteration 135, loss = 0.21760502\n",
      "Iteration 155, loss = 0.21202938\n",
      "Iteration 136, loss = 0.21768062\n",
      "Iteration 137, loss = 0.21785279\n",
      "Iteration 138, loss = 0.21744090\n",
      "Iteration 156, loss = 0.21182044\n",
      "Iteration 139, loss = 0.21703796\n",
      "Iteration 157, loss = 0.21169498\n",
      "Iteration 140, loss = 0.21674916\n",
      "Iteration 141, loss = 0.21694192\n",
      "Iteration 158, loss = 0.21150790\n",
      "Iteration 142, loss = 0.21653635\n",
      "Iteration 159, loss = 0.21150802Iteration 143, loss = 0.21587143\n",
      "\n",
      "Iteration 144, loss = 0.21609707\n",
      "Iteration 160, loss = 0.21119075\n",
      "Iteration 161, loss = 0.21068404\n",
      "Iteration 145, loss = 0.21598429\n",
      "Iteration 162, loss = 0.21038580\n",
      "Iteration 163, loss = 0.21034368\n",
      "Iteration 164, loss = 0.21013263\n",
      "Iteration 146, loss = 0.21552518\n",
      "Iteration 165, loss = 0.20954135\n",
      "Iteration 166, loss = 0.20950907\n",
      "Iteration 147, loss = 0.21546052\n",
      "Iteration 167, loss = 0.20966973\n",
      "Iteration 168, loss = 0.20936824\n",
      "Iteration 169, loss = 0.20890979\n",
      "Iteration 148, loss = 0.21523616\n",
      "Iteration 170, loss = 0.20887904\n",
      "Iteration 171, loss = 0.20892406\n",
      "Iteration 149, loss = 0.21489166\n",
      "Iteration 150, loss = 0.21439798\n",
      "Iteration 172, loss = 0.20954681Iteration 151, loss = 0.21443922\n",
      "\n",
      "Iteration 152, loss = 0.21397818\n",
      "Iteration 153, loss = 0.21406911\n",
      "Iteration 173, loss = 0.20938926\n",
      "Iteration 154, loss = 0.21358034\n",
      "Iteration 174, loss = 0.20893781\n",
      "Iteration 155, loss = 0.21381895\n",
      "Iteration 156, loss = 0.21328605\n",
      "Iteration 175, loss = 0.20806507\n",
      "Iteration 157, loss = 0.21311344\n",
      "Iteration 176, loss = 0.20790673\n",
      "Iteration 158, loss = 0.21333860\n",
      "Iteration 177, loss = 0.20755121\n",
      "Iteration 159, loss = 0.21306982\n",
      "Iteration 178, loss = 0.20737210\n",
      "Iteration 160, loss = 0.21274507\n",
      "Iteration 179, loss = 0.20707437\n",
      "Iteration 161, loss = 0.21277751\n",
      "Iteration 180, loss = 0.20693228\n",
      "Iteration 162, loss = 0.21215126\n",
      "Iteration 181, loss = 0.20724045\n",
      "Iteration 163, loss = 0.21230022\n",
      "Iteration 182, loss = 0.20639938\n",
      "Iteration 164, loss = 0.21218815\n",
      "Iteration 183, loss = 0.20729003\n",
      "Iteration 165, loss = 0.21151381\n",
      "Iteration 184, loss = 0.20665954\n",
      "Iteration 166, loss = 0.21127932\n",
      "Iteration 185, loss = 0.20611780\n",
      "Iteration 167, loss = 0.21104789\n",
      "Iteration 186, loss = 0.20631131\n",
      "Iteration 168, loss = 0.21089589\n",
      "Iteration 187, loss = 0.20583131\n",
      "Iteration 169, loss = 0.21111596\n",
      "Iteration 188, loss = 0.20624835\n",
      "Iteration 170, loss = 0.21104643\n",
      "Iteration 171, loss = 0.21083086\n",
      "Iteration 189, loss = 0.20660740\n",
      "Iteration 172, loss = 0.21075757\n",
      "Iteration 190, loss = 0.20624828\n",
      "Iteration 173, loss = 0.21072008\n",
      "Iteration 191, loss = 0.20520374\n",
      "Iteration 174, loss = 0.21063473\n",
      "Iteration 192, loss = 0.20522095\n",
      "Iteration 175, loss = 0.20995779\n",
      "Iteration 193, loss = 0.20556216\n",
      "Iteration 176, loss = 0.21000320\n",
      "Iteration 194, loss = 0.20599176\n",
      "Iteration 177, loss = 0.20969350\n",
      "Iteration 195, loss = 0.20480907\n",
      "Iteration 178, loss = 0.20930981\n",
      "Iteration 196, loss = 0.20505154\n",
      "Iteration 179, loss = 0.20898367\n",
      "Iteration 197, loss = 0.20465943\n",
      "Iteration 180, loss = 0.20865582\n",
      "Iteration 198, loss = 0.20460014\n",
      "Iteration 181, loss = 0.20931385\n",
      "Iteration 199, loss = 0.20435135\n",
      "Iteration 182, loss = 0.20841072\n",
      "Iteration 200, loss = 0.20448044\n",
      "Iteration 183, loss = 0.20875337\n",
      "Iteration 184, loss = 0.20841883\n",
      "Iteration 185, loss = 0.20807993\n",
      "Iteration 1, loss = 0.38568373\n",
      "Iteration 186, loss = 0.20918087\n",
      "Iteration 2, loss = 0.33563591\n",
      "Iteration 187, loss = 0.20824358\n",
      "Iteration 3, loss = 0.32710248\n",
      "Iteration 188, loss = 0.20791223\n",
      "Iteration 4, loss = 0.31486208\n",
      "Iteration 189, loss = 0.20771923\n",
      "Iteration 190, loss = 0.20760293\n",
      "Iteration 5, loss = 0.30261168\n",
      "Iteration 191, loss = 0.20820434\n",
      "Iteration 6, loss = 0.29125496\n",
      "Iteration 192, loss = 0.20701596\n",
      "Iteration 7, loss = 0.28308869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 193, loss = 0.20716472\n",
      "Iteration 8, loss = 0.27697898\n",
      "Iteration 194, loss = 0.20797816\n",
      "Iteration 9, loss = 0.27264211\n",
      "Iteration 195, loss = 0.20690074\n",
      "Iteration 10, loss = 0.26896078\n",
      "Iteration 196, loss = 0.20636533\n",
      "Iteration 11, loss = 0.26654866\n",
      "Iteration 197, loss = 0.20624524\n",
      "Iteration 12, loss = 0.26508465\n",
      "Iteration 198, loss = 0.20657656\n",
      "Iteration 13, loss = 0.26324671\n",
      "Iteration 199, loss = 0.20616925\n",
      "Iteration 14, loss = 0.26173899\n",
      "Iteration 200, loss = 0.20655325\n",
      "Iteration 15, loss = 0.26112224\n",
      "Iteration 16, loss = 0.26001468\n",
      "Iteration 17, loss = 0.25900603\n",
      "Iteration 1, loss = 0.38671330\n",
      "Iteration 18, loss = 0.25848832\n",
      "Iteration 19, loss = 0.25765003\n",
      "Iteration 2, loss = 0.33538060\n",
      "Iteration 20, loss = 0.25759051\n",
      "Iteration 3, loss = 0.32622347\n",
      "Iteration 21, loss = 0.25693576\n",
      "Iteration 4, loss = 0.31386609\n",
      "Iteration 22, loss = 0.25695517Iteration 5, loss = 0.30094679\n",
      "\n",
      "Iteration 23, loss = 0.25605178\n",
      "Iteration 6, loss = 0.28908080\n",
      "Iteration 7, loss = 0.28028067\n",
      "Iteration 24, loss = 0.25599436\n",
      "Iteration 25, loss = 0.25576288\n",
      "Iteration 8, loss = 0.27339135\n",
      "Iteration 26, loss = 0.25544882\n",
      "Iteration 9, loss = 0.26884829\n",
      "Iteration 27, loss = 0.25504907\n",
      "Iteration 10, loss = 0.26459853\n",
      "Iteration 28, loss = 0.25483935\n",
      "Iteration 11, loss = 0.26178347\n",
      "Iteration 29, loss = 0.25460078\n",
      "Iteration 12, loss = 0.25980211\n",
      "Iteration 30, loss = 0.25434225\n",
      "Iteration 13, loss = 0.25749941\n",
      "Iteration 31, loss = 0.25378426\n",
      "Iteration 14, loss = 0.25626648\n",
      "Iteration 32, loss = 0.25340471\n",
      "Iteration 15, loss = 0.25479955\n",
      "Iteration 33, loss = 0.25449275\n",
      "Iteration 16, loss = 0.25385890\n",
      "Iteration 34, loss = 0.25318481\n",
      "Iteration 17, loss = 0.25303808\n",
      "Iteration 35, loss = 0.25284451\n",
      "Iteration 18, loss = 0.25233539\n",
      "Iteration 36, loss = 0.25200260\n",
      "Iteration 19, loss = 0.25146340\n",
      "Iteration 37, loss = 0.25172849\n",
      "Iteration 20, loss = 0.25085478\n",
      "Iteration 38, loss = 0.25155906\n",
      "Iteration 21, loss = 0.25059126\n",
      "Iteration 39, loss = 0.25118379\n",
      "Iteration 22, loss = 0.25027788\n",
      "Iteration 40, loss = 0.25146091\n",
      "Iteration 23, loss = 0.24927977\n",
      "Iteration 41, loss = 0.25091152\n",
      "Iteration 24, loss = 0.24898423\n",
      "Iteration 42, loss = 0.25059161\n",
      "Iteration 25, loss = 0.24899546\n",
      "Iteration 43, loss = 0.25050554\n",
      "Iteration 26, loss = 0.24823654\n",
      "Iteration 44, loss = 0.24988298\n",
      "Iteration 27, loss = 0.24801069\n",
      "Iteration 45, loss = 0.24977321\n",
      "Iteration 28, loss = 0.24755858\n",
      "Iteration 46, loss = 0.24924968\n",
      "Iteration 29, loss = 0.24745955\n",
      "Iteration 47, loss = 0.24935553\n",
      "Iteration 30, loss = 0.24649050\n",
      "Iteration 48, loss = 0.24917676\n",
      "Iteration 31, loss = 0.24673284\n",
      "Iteration 49, loss = 0.24811861\n",
      "Iteration 32, loss = 0.24590867\n",
      "Iteration 50, loss = 0.24756104\n",
      "Iteration 51, loss = 0.24734903\n",
      "Iteration 33, loss = 0.24644790\n",
      "Iteration 52, loss = 0.24735715\n",
      "Iteration 53, loss = 0.24716912\n",
      "Iteration 34, loss = 0.24528466\n",
      "Iteration 54, loss = 0.24666224\n",
      "Iteration 35, loss = 0.24483325Iteration 55, loss = 0.24753123\n",
      "\n",
      "Iteration 56, loss = 0.24664811\n",
      "Iteration 36, loss = 0.24409116\n",
      "Iteration 37, loss = 0.24376269\n",
      "Iteration 57, loss = 0.24571288\n",
      "Iteration 38, loss = 0.24350229\n",
      "Iteration 39, loss = 0.24315132\n",
      "Iteration 58, loss = 0.24533339\n",
      "Iteration 40, loss = 0.24348521\n",
      "Iteration 59, loss = 0.24501961\n",
      "Iteration 41, loss = 0.24260506\n",
      "Iteration 42, loss = 0.24256177\n",
      "Iteration 60, loss = 0.24536587\n",
      "Iteration 43, loss = 0.24202957Iteration 61, loss = 0.24530408\n",
      "\n",
      "Iteration 62, loss = 0.24359481\n",
      "Iteration 63, loss = 0.24353527\n",
      "Iteration 44, loss = 0.24158153Iteration 64, loss = 0.24325075\n",
      "\n",
      "Iteration 65, loss = 0.24271826\n",
      "Iteration 66, loss = 0.24277543\n",
      "Iteration 45, loss = 0.24215594Iteration 67, loss = 0.24211960\n",
      "\n",
      "Iteration 68, loss = 0.24245420\n",
      "Iteration 69, loss = 0.24193980\n",
      "Iteration 70, loss = 0.24120554\n",
      "Iteration 46, loss = 0.24262464Iteration 71, loss = 0.24112458\n",
      "\n",
      "Iteration 72, loss = 0.24060705\n",
      "Iteration 73, loss = 0.24044870\n",
      "Iteration 47, loss = 0.24323421\n",
      "Iteration 74, loss = 0.24013714\n",
      "Iteration 48, loss = 0.24234306\n",
      "Iteration 49, loss = 0.24035149\n",
      "Iteration 75, loss = 0.23954865\n",
      "Iteration 50, loss = 0.24018968\n",
      "Iteration 51, loss = 0.23962894\n",
      "Iteration 76, loss = 0.23920133\n",
      "Iteration 52, loss = 0.23918344\n",
      "Iteration 53, loss = 0.23930313\n",
      "Iteration 77, loss = 0.23936440\n",
      "Iteration 54, loss = 0.23896386\n",
      "Iteration 78, loss = 0.23903363\n",
      "Iteration 79, loss = 0.23838692\n",
      "Iteration 55, loss = 0.23897731\n",
      "Iteration 80, loss = 0.23894200\n",
      "Iteration 56, loss = 0.23801490\n",
      "Iteration 81, loss = 0.23765075\n",
      "Iteration 57, loss = 0.23735710\n",
      "Iteration 82, loss = 0.23763014\n",
      "Iteration 58, loss = 0.23694857\n",
      "Iteration 83, loss = 0.23702515\n",
      "Iteration 59, loss = 0.23696073\n",
      "Iteration 84, loss = 0.23689739\n",
      "Iteration 60, loss = 0.23714191\n",
      "Iteration 85, loss = 0.23676432\n",
      "Iteration 61, loss = 0.23733126\n",
      "Iteration 86, loss = 0.23655101\n",
      "Iteration 62, loss = 0.23536336\n",
      "Iteration 63, loss = 0.23526925\n",
      "Iteration 87, loss = 0.23645841\n",
      "Iteration 88, loss = 0.23575037\n",
      "Iteration 89, loss = 0.23560791\n",
      "Iteration 64, loss = 0.23489998\n",
      "Iteration 90, loss = 0.23555673\n",
      "Iteration 65, loss = 0.23453228\n",
      "Iteration 91, loss = 0.23512863\n",
      "Iteration 66, loss = 0.23432553\n",
      "Iteration 92, loss = 0.23499380\n",
      "Iteration 93, loss = 0.23481308\n",
      "Iteration 67, loss = 0.23371835\n",
      "Iteration 94, loss = 0.23416720\n",
      "Iteration 95, loss = 0.23415828\n",
      "Iteration 68, loss = 0.23363863Iteration 96, loss = 0.23368077\n",
      "\n",
      "Iteration 97, loss = 0.23432620\n",
      "Iteration 98, loss = 0.23336317\n",
      "Iteration 69, loss = 0.23332647\n",
      "Iteration 99, loss = 0.23301623\n",
      "Iteration 100, loss = 0.23258824\n",
      "Iteration 70, loss = 0.23262274\n",
      "Iteration 71, loss = 0.23249043\n",
      "Iteration 101, loss = 0.23242314\n",
      "Iteration 72, loss = 0.23188626\n",
      "Iteration 102, loss = 0.23210952\n",
      "Iteration 73, loss = 0.23187376\n",
      "Iteration 103, loss = 0.23243669\n",
      "Iteration 104, loss = 0.23193301\n",
      "Iteration 74, loss = 0.23158129\n",
      "Iteration 105, loss = 0.23137288\n",
      "Iteration 106, loss = 0.23131920\n",
      "Iteration 75, loss = 0.23090395Iteration 107, loss = 0.23126930\n",
      "\n",
      "Iteration 108, loss = 0.23047296\n",
      "Iteration 109, loss = 0.23073507\n",
      "Iteration 76, loss = 0.23041085\n",
      "Iteration 110, loss = 0.23021684\n",
      "Iteration 77, loss = 0.23065620\n",
      "Iteration 111, loss = 0.23011584\n",
      "Iteration 78, loss = 0.23011196\n",
      "Iteration 79, loss = 0.22980203\n",
      "Iteration 112, loss = 0.22974961\n",
      "Iteration 80, loss = 0.23032063\n",
      "Iteration 81, loss = 0.22882467\n",
      "Iteration 113, loss = 0.22956522Iteration 82, loss = 0.22920479\n",
      "\n",
      "Iteration 83, loss = 0.22904873\n",
      "Iteration 114, loss = 0.22966066\n",
      "Iteration 84, loss = 0.22820297\n",
      "Iteration 115, loss = 0.23031298\n",
      "Iteration 85, loss = 0.22791128\n",
      "Iteration 116, loss = 0.22981352\n",
      "Iteration 86, loss = 0.22831377\n",
      "Iteration 117, loss = 0.22878610\n",
      "Iteration 87, loss = 0.22798597\n",
      "Iteration 118, loss = 0.22821039\n",
      "Iteration 119, loss = 0.22820244\n",
      "Iteration 88, loss = 0.22718803\n",
      "Iteration 120, loss = 0.22832449\n",
      "Iteration 89, loss = 0.22678028\n",
      "Iteration 121, loss = 0.22805045\n",
      "Iteration 90, loss = 0.22688600\n",
      "Iteration 122, loss = 0.22751167\n",
      "Iteration 91, loss = 0.22651932\n",
      "Iteration 123, loss = 0.22715714\n",
      "Iteration 92, loss = 0.22619156\n",
      "Iteration 124, loss = 0.22689628\n",
      "Iteration 93, loss = 0.22643748\n",
      "Iteration 125, loss = 0.22697820\n",
      "Iteration 94, loss = 0.22576292\n",
      "Iteration 126, loss = 0.22702077\n",
      "Iteration 95, loss = 0.22552361\n",
      "Iteration 127, loss = 0.22641963\n",
      "Iteration 96, loss = 0.22517080\n",
      "Iteration 128, loss = 0.22607924\n",
      "Iteration 97, loss = 0.22644983\n",
      "Iteration 129, loss = 0.22571543\n",
      "Iteration 98, loss = 0.22515742\n",
      "Iteration 130, loss = 0.22530000\n",
      "Iteration 99, loss = 0.22448566Iteration 131, loss = 0.22508427\n",
      "\n",
      "Iteration 132, loss = 0.22508789\n",
      "Iteration 100, loss = 0.22425889\n",
      "Iteration 133, loss = 0.22464435\n",
      "Iteration 101, loss = 0.22480076\n",
      "Iteration 134, loss = 0.22482526\n",
      "Iteration 102, loss = 0.22431627\n",
      "Iteration 135, loss = 0.22459551\n",
      "Iteration 103, loss = 0.22502974\n",
      "Iteration 136, loss = 0.22423465\n",
      "Iteration 104, loss = 0.22437647Iteration 137, loss = 0.22384526\n",
      "\n",
      "Iteration 138, loss = 0.22344810\n",
      "Iteration 105, loss = 0.22323046\n",
      "Iteration 139, loss = 0.22332495\n",
      "Iteration 106, loss = 0.22352329\n",
      "Iteration 140, loss = 0.22296934\n",
      "Iteration 107, loss = 0.22335181\n",
      "Iteration 141, loss = 0.22263779\n",
      "Iteration 108, loss = 0.22233063\n",
      "Iteration 142, loss = 0.22265482\n",
      "Iteration 109, loss = 0.22270487\n",
      "Iteration 143, loss = 0.22187520\n",
      "Iteration 110, loss = 0.22249413\n",
      "Iteration 144, loss = 0.22214388\n",
      "Iteration 111, loss = 0.22191250\n",
      "Iteration 145, loss = 0.22181477\n",
      "Iteration 112, loss = 0.22232081\n",
      "Iteration 146, loss = 0.22164065\n",
      "Iteration 113, loss = 0.22174620\n",
      "Iteration 147, loss = 0.22154074\n",
      "Iteration 114, loss = 0.22152655\n",
      "Iteration 148, loss = 0.22106950\n",
      "Iteration 115, loss = 0.22289091\n",
      "Iteration 149, loss = 0.22085699\n",
      "Iteration 116, loss = 0.22339515\n",
      "Iteration 150, loss = 0.22022790\n",
      "Iteration 117, loss = 0.22177686\n",
      "Iteration 151, loss = 0.22024790\n",
      "Iteration 152, loss = 0.21969551Iteration 118, loss = 0.22071606\n",
      "\n",
      "Iteration 119, loss = 0.22122973Iteration 153, loss = 0.21982934\n",
      "\n",
      "Iteration 154, loss = 0.21955095\n",
      "Iteration 120, loss = 0.22117344\n",
      "Iteration 155, loss = 0.21933360\n",
      "Iteration 121, loss = 0.22059616\n",
      "Iteration 156, loss = 0.21887648\n",
      "Iteration 122, loss = 0.22022433\n",
      "Iteration 157, loss = 0.21865898\n",
      "Iteration 123, loss = 0.21990434\n",
      "Iteration 158, loss = 0.21884815\n",
      "Iteration 124, loss = 0.21993966\n",
      "Iteration 159, loss = 0.21838480\n",
      "Iteration 125, loss = 0.21944767\n",
      "Iteration 160, loss = 0.21806775\n",
      "Iteration 126, loss = 0.22002976\n",
      "Iteration 161, loss = 0.21829942\n",
      "Iteration 162, loss = 0.21762493\n",
      "Iteration 127, loss = 0.21953182\n",
      "Iteration 163, loss = 0.21788749\n",
      "Iteration 128, loss = 0.21901814\n",
      "Iteration 164, loss = 0.21799469\n",
      "Iteration 129, loss = 0.21890767\n",
      "Iteration 165, loss = 0.21700930\n",
      "Iteration 130, loss = 0.21860428\n",
      "Iteration 166, loss = 0.21705744\n",
      "Iteration 131, loss = 0.21838490\n",
      "Iteration 167, loss = 0.21663686\n",
      "Iteration 132, loss = 0.21822411\n",
      "Iteration 168, loss = 0.21658292\n",
      "Iteration 133, loss = 0.21828289\n",
      "Iteration 169, loss = 0.21644545\n",
      "Iteration 134, loss = 0.21875497\n",
      "Iteration 170, loss = 0.21639279\n",
      "Iteration 135, loss = 0.21879802\n",
      "Iteration 171, loss = 0.21694167\n",
      "Iteration 136, loss = 0.21792388\n",
      "Iteration 172, loss = 0.21706068\n",
      "Iteration 137, loss = 0.21770357\n",
      "Iteration 173, loss = 0.21605708\n",
      "Iteration 138, loss = 0.21735633\n",
      "Iteration 174, loss = 0.21561272\n",
      "Iteration 175, loss = 0.21541064\n",
      "Iteration 139, loss = 0.21737674\n",
      "Iteration 176, loss = 0.21572074\n",
      "Iteration 140, loss = 0.21724490\n",
      "Iteration 177, loss = 0.21495252\n",
      "Iteration 141, loss = 0.21708417\n",
      "Iteration 178, loss = 0.21495111\n",
      "Iteration 142, loss = 0.21734508\n",
      "Iteration 179, loss = 0.21471356\n",
      "Iteration 143, loss = 0.21637213\n",
      "Iteration 180, loss = 0.21529923\n",
      "Iteration 144, loss = 0.21643927\n",
      "Iteration 181, loss = 0.21577071\n",
      "Iteration 145, loss = 0.21622242\n",
      "Iteration 182, loss = 0.21438731\n",
      "Iteration 146, loss = 0.21588807\n",
      "Iteration 183, loss = 0.21428828\n",
      "Iteration 147, loss = 0.21590895\n",
      "Iteration 184, loss = 0.21438733\n",
      "Iteration 148, loss = 0.21535828\n",
      "Iteration 185, loss = 0.21366955\n",
      "Iteration 149, loss = 0.21573375\n",
      "Iteration 186, loss = 0.21442512\n",
      "Iteration 150, loss = 0.21521636\n",
      "Iteration 187, loss = 0.21376309\n",
      "Iteration 151, loss = 0.21526796\n",
      "Iteration 188, loss = 0.21330381\n",
      "Iteration 189, loss = 0.21302200\n",
      "Iteration 152, loss = 0.21510690\n",
      "Iteration 190, loss = 0.21287391\n",
      "Iteration 191, loss = 0.21332250\n",
      "Iteration 153, loss = 0.21479444\n",
      "Iteration 192, loss = 0.21268970\n",
      "Iteration 154, loss = 0.21445164\n",
      "Iteration 193, loss = 0.21253618\n",
      "Iteration 155, loss = 0.21471463\n",
      "Iteration 194, loss = 0.21262664\n",
      "Iteration 156, loss = 0.21389614\n",
      "Iteration 195, loss = 0.21236774\n",
      "Iteration 157, loss = 0.21463933\n",
      "Iteration 196, loss = 0.21239571\n",
      "Iteration 158, loss = 0.21519880\n",
      "Iteration 197, loss = 0.21201042\n",
      "Iteration 159, loss = 0.21431138\n",
      "Iteration 198, loss = 0.21184584\n",
      "Iteration 160, loss = 0.21395141\n",
      "Iteration 199, loss = 0.21177304\n",
      "Iteration 161, loss = 0.21355011\n",
      "Iteration 200, loss = 0.21162643\n",
      "Iteration 162, loss = 0.21346294\n",
      "Iteration 163, loss = 0.21353064\n",
      "Iteration 164, loss = 0.21362250\n",
      "Iteration 165, loss = 0.21261893\n",
      "Iteration 166, loss = 0.21257314\n",
      "Iteration 167, loss = 0.21262915\n",
      "Iteration 168, loss = 0.21254692\n",
      "Iteration 169, loss = 0.21225450\n",
      "Iteration 170, loss = 0.21224393\n",
      "Iteration 171, loss = 0.21200308\n",
      "Iteration 172, loss = 0.21182679\n",
      "Iteration 173, loss = 0.21199152\n",
      "Iteration 174, loss = 0.21149452\n",
      "Iteration 175, loss = 0.21149525\n",
      "Iteration 176, loss = 0.21179970\n",
      "Iteration 177, loss = 0.21111970\n",
      "Iteration 178, loss = 0.21105807\n",
      "Iteration 179, loss = 0.21082062\n",
      "Iteration 180, loss = 0.21118668\n",
      "Iteration 181, loss = 0.21104966\n",
      "Iteration 182, loss = 0.21046484\n",
      "Iteration 183, loss = 0.21023874\n",
      "Iteration 184, loss = 0.21031970\n",
      "Iteration 185, loss = 0.21003900\n",
      "Iteration 186, loss = 0.21122416\n",
      "Iteration 187, loss = 0.21011698\n",
      "Iteration 188, loss = 0.21009821\n",
      "Iteration 189, loss = 0.20953853\n",
      "Iteration 190, loss = 0.20947542\n",
      "Iteration 191, loss = 0.20920989\n",
      "Iteration 192, loss = 0.20949364\n",
      "Iteration 193, loss = 0.20947811\n",
      "Iteration 194, loss = 0.20875728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 195, loss = 0.20939903\n",
      "Iteration 196, loss = 0.20878770\n",
      "Iteration 197, loss = 0.20871099\n",
      "Iteration 198, loss = 0.20859117\n",
      "Iteration 199, loss = 0.20836730\n",
      "Iteration 200, loss = 0.20846984\n",
      "Iteration 1, loss = 0.36860705\n",
      "Iteration 2, loss = 0.32900367\n",
      "Iteration 1, loss = 0.36892552\n",
      "Iteration 3, loss = 0.31640025\n",
      "Iteration 2, loss = 0.32933481\n",
      "Iteration 4, loss = 0.30430665\n",
      "Iteration 3, loss = 0.31661902\n",
      "Iteration 4, loss = 0.30352845\n",
      "Iteration 5, loss = 0.29403032\n",
      "Iteration 5, loss = 0.29235853\n",
      "Iteration 6, loss = 0.28790837\n",
      "Iteration 6, loss = 0.28556069\n",
      "Iteration 7, loss = 0.28076160\n",
      "Iteration 7, loss = 0.28363490\n",
      "Iteration 8, loss = 0.27797557\n",
      "Iteration 9, loss = 0.27527968\n",
      "Iteration 8, loss = 0.28104785\n",
      "Iteration 9, loss = 0.27882110\n",
      "Iteration 10, loss = 0.27750158\n",
      "Iteration 11, loss = 0.27612390\n",
      "Iteration 10, loss = 0.27440883\n",
      "Iteration 12, loss = 0.27452896\n",
      "Iteration 11, loss = 0.27239851Iteration 13, loss = 0.27465535\n",
      "\n",
      "Iteration 14, loss = 0.27348047\n",
      "Iteration 15, loss = 0.27277904\n",
      "Iteration 16, loss = 0.27216804\n",
      "Iteration 12, loss = 0.27110592\n",
      "Iteration 17, loss = 0.27188202\n",
      "Iteration 13, loss = 0.27030268\n",
      "Iteration 14, loss = 0.27059659\n",
      "Iteration 18, loss = 0.27113826\n",
      "Iteration 15, loss = 0.26928469\n",
      "Iteration 16, loss = 0.26820018\n",
      "Iteration 19, loss = 0.27106840\n",
      "Iteration 17, loss = 0.26775103\n",
      "Iteration 20, loss = 0.27078662\n",
      "Iteration 18, loss = 0.26721321\n",
      "Iteration 19, loss = 0.26693057\n",
      "Iteration 21, loss = 0.27057729\n",
      "Iteration 22, loss = 0.27029666\n",
      "Iteration 20, loss = 0.26693884\n",
      "Iteration 23, loss = 0.26994023\n",
      "Iteration 21, loss = 0.26658866\n",
      "Iteration 24, loss = 0.26976116\n",
      "Iteration 22, loss = 0.26623525\n",
      "Iteration 25, loss = 0.26969978\n",
      "Iteration 23, loss = 0.26582668\n",
      "Iteration 26, loss = 0.26948257\n",
      "Iteration 24, loss = 0.26536196\n",
      "Iteration 27, loss = 0.26921974\n",
      "Iteration 25, loss = 0.26487992\n",
      "Iteration 28, loss = 0.26860062\n",
      "Iteration 26, loss = 0.26499681\n",
      "Iteration 29, loss = 0.26870959\n",
      "Iteration 27, loss = 0.26425044\n",
      "Iteration 30, loss = 0.26961460\n",
      "Iteration 28, loss = 0.26411362\n",
      "Iteration 31, loss = 0.26805582\n",
      "Iteration 29, loss = 0.26409269\n",
      "Iteration 32, loss = 0.26807224\n",
      "Iteration 30, loss = 0.26422569\n",
      "Iteration 33, loss = 0.26746361\n",
      "Iteration 31, loss = 0.26354183\n",
      "Iteration 34, loss = 0.26746302\n",
      "Iteration 32, loss = 0.26402187\n",
      "Iteration 35, loss = 0.26753309\n",
      "Iteration 36, loss = 0.26677467\n",
      "Iteration 33, loss = 0.26283708\n",
      "Iteration 37, loss = 0.26664820\n",
      "Iteration 38, loss = 0.26700022\n",
      "Iteration 34, loss = 0.26290444\n",
      "Iteration 39, loss = 0.26661563\n",
      "Iteration 35, loss = 0.26240534\n",
      "Iteration 40, loss = 0.26625400\n",
      "Iteration 41, loss = 0.26589309\n",
      "Iteration 36, loss = 0.26201980\n",
      "Iteration 42, loss = 0.26559943\n",
      "Iteration 37, loss = 0.26185860\n",
      "Iteration 43, loss = 0.26553691\n",
      "Iteration 38, loss = 0.26166725\n",
      "Iteration 44, loss = 0.26629970\n",
      "Iteration 39, loss = 0.26241766\n",
      "Iteration 45, loss = 0.26551751\n",
      "Iteration 40, loss = 0.26151642\n",
      "Iteration 46, loss = 0.26433698\n",
      "Iteration 41, loss = 0.26089564\n",
      "Iteration 47, loss = 0.26395966\n",
      "Iteration 42, loss = 0.26064454\n",
      "Iteration 48, loss = 0.26382073\n",
      "Iteration 43, loss = 0.26049680\n",
      "Iteration 49, loss = 0.26373398\n",
      "Iteration 44, loss = 0.26098117\n",
      "Iteration 50, loss = 0.26334741\n",
      "Iteration 45, loss = 0.26037452\n",
      "Iteration 51, loss = 0.26330253\n",
      "Iteration 46, loss = 0.25929435\n",
      "Iteration 47, loss = 0.25905714\n",
      "Iteration 48, loss = 0.25891891\n",
      "Iteration 52, loss = 0.26382604\n",
      "Iteration 49, loss = 0.25882758\n",
      "Iteration 53, loss = 0.26276027\n",
      "Iteration 50, loss = 0.25849912\n",
      "Iteration 54, loss = 0.26271200\n",
      "Iteration 51, loss = 0.25827252\n",
      "Iteration 55, loss = 0.26289870\n",
      "Iteration 52, loss = 0.25841448\n",
      "Iteration 53, loss = 0.25795021\n",
      "Iteration 56, loss = 0.26267936\n",
      "Iteration 57, loss = 0.26246176\n",
      "Iteration 54, loss = 0.25768940\n",
      "Iteration 58, loss = 0.26208178\n",
      "Iteration 59, loss = 0.26177781\n",
      "Iteration 55, loss = 0.25699594\n",
      "Iteration 60, loss = 0.26137196\n",
      "Iteration 61, loss = 0.26085942Iteration 56, loss = 0.25710075\n",
      "\n",
      "Iteration 57, loss = 0.25690388Iteration 62, loss = 0.26156227\n",
      "\n",
      "Iteration 58, loss = 0.25663900Iteration 63, loss = 0.26037643\n",
      "\n",
      "Iteration 64, loss = 0.26029400Iteration 59, loss = 0.25637038\n",
      "\n",
      "Iteration 65, loss = 0.26069629Iteration 60, loss = 0.25544484\n",
      "\n",
      "Iteration 66, loss = 0.25988938Iteration 61, loss = 0.25536652\n",
      "\n",
      "Iteration 62, loss = 0.25610071Iteration 67, loss = 0.25985325\n",
      "\n",
      "Iteration 63, loss = 0.25475148Iteration 68, loss = 0.25947866\n",
      "\n",
      "Iteration 64, loss = 0.25470380Iteration 69, loss = 0.25959832\n",
      "\n",
      "Iteration 70, loss = 0.25965491Iteration 65, loss = 0.25447769\n",
      "\n",
      "Iteration 71, loss = 0.25913797\n",
      "Iteration 66, loss = 0.25410223\n",
      "Iteration 72, loss = 0.25873311\n",
      "Iteration 67, loss = 0.25441490\n",
      "Iteration 73, loss = 0.25870778\n",
      "Iteration 68, loss = 0.25376636\n",
      "Iteration 74, loss = 0.25879983\n",
      "Iteration 75, loss = 0.25822670\n",
      "Iteration 69, loss = 0.25363569\n",
      "Iteration 76, loss = 0.25800204\n",
      "Iteration 77, loss = 0.25821487\n",
      "Iteration 78, loss = 0.25810014\n",
      "Iteration 70, loss = 0.25336982\n",
      "Iteration 79, loss = 0.25780241\n",
      "Iteration 71, loss = 0.25291809\n",
      "Iteration 80, loss = 0.25729158\n",
      "Iteration 72, loss = 0.25238210\n",
      "Iteration 81, loss = 0.25793210\n",
      "Iteration 73, loss = 0.25298836\n",
      "Iteration 82, loss = 0.25718640\n",
      "Iteration 74, loss = 0.25255278\n",
      "Iteration 83, loss = 0.25669480\n",
      "Iteration 75, loss = 0.25203225\n",
      "Iteration 76, loss = 0.25197509\n",
      "Iteration 84, loss = 0.25711102\n",
      "Iteration 77, loss = 0.25209816\n",
      "Iteration 85, loss = 0.25702554\n",
      "Iteration 78, loss = 0.25185820\n",
      "Iteration 86, loss = 0.25653526\n",
      "Iteration 79, loss = 0.25133030\n",
      "Iteration 87, loss = 0.25701797\n",
      "Iteration 80, loss = 0.25107001\n",
      "Iteration 88, loss = 0.25654263\n",
      "Iteration 81, loss = 0.25132938\n",
      "Iteration 89, loss = 0.25589993\n",
      "Iteration 90, loss = 0.25594605\n",
      "Iteration 82, loss = 0.25080357\n",
      "Iteration 91, loss = 0.25608159\n",
      "Iteration 83, loss = 0.25034522\n",
      "Iteration 92, loss = 0.25568997\n",
      "Iteration 84, loss = 0.25105566\n",
      "Iteration 93, loss = 0.25563822\n",
      "Iteration 85, loss = 0.25017073\n",
      "Iteration 94, loss = 0.25522402\n",
      "Iteration 86, loss = 0.25002653\n",
      "Iteration 95, loss = 0.25540525\n",
      "Iteration 87, loss = 0.25045372\n",
      "Iteration 96, loss = 0.25539501\n",
      "Iteration 88, loss = 0.25053587\n",
      "Iteration 97, loss = 0.25529471\n",
      "Iteration 89, loss = 0.24974932\n",
      "Iteration 98, loss = 0.25482257\n",
      "Iteration 90, loss = 0.24959877\n",
      "Iteration 99, loss = 0.25526215\n",
      "Iteration 91, loss = 0.24911209\n",
      "Iteration 100, loss = 0.25480373\n",
      "Iteration 92, loss = 0.24900141\n",
      "Iteration 101, loss = 0.25540245\n",
      "Iteration 93, loss = 0.24897130\n",
      "Iteration 102, loss = 0.25471594\n",
      "Iteration 94, loss = 0.24875796\n",
      "Iteration 103, loss = 0.25420456\n",
      "Iteration 95, loss = 0.24871127\n",
      "Iteration 104, loss = 0.25408089\n",
      "Iteration 96, loss = 0.24860705\n",
      "Iteration 105, loss = 0.25372764\n",
      "Iteration 97, loss = 0.24845085\n",
      "Iteration 106, loss = 0.25370860\n",
      "Iteration 98, loss = 0.24816570\n",
      "Iteration 107, loss = 0.25347346\n",
      "Iteration 99, loss = 0.24888486\n",
      "Iteration 108, loss = 0.25320714\n",
      "Iteration 100, loss = 0.24796823\n",
      "Iteration 109, loss = 0.25340862\n",
      "Iteration 101, loss = 0.24887547\n",
      "Iteration 110, loss = 0.25298996\n",
      "Iteration 102, loss = 0.24857539\n",
      "Iteration 111, loss = 0.25325290\n",
      "Iteration 112, loss = 0.25364535\n",
      "Iteration 103, loss = 0.24736664\n",
      "Iteration 113, loss = 0.25270725\n",
      "Iteration 104, loss = 0.24748070\n",
      "Iteration 114, loss = 0.25303871\n",
      "Iteration 105, loss = 0.24721852\n",
      "Iteration 115, loss = 0.25222080\n",
      "Iteration 106, loss = 0.24736683\n",
      "Iteration 116, loss = 0.25284752\n",
      "Iteration 107, loss = 0.24727815\n",
      "Iteration 117, loss = 0.25235092\n",
      "Iteration 108, loss = 0.24714806\n",
      "Iteration 118, loss = 0.25195434\n",
      "Iteration 109, loss = 0.24668915\n",
      "Iteration 119, loss = 0.25163326\n",
      "Iteration 110, loss = 0.24633319\n",
      "Iteration 120, loss = 0.25165271\n",
      "Iteration 111, loss = 0.24769162\n",
      "Iteration 121, loss = 0.25162607\n",
      "Iteration 112, loss = 0.24760983\n",
      "Iteration 122, loss = 0.25136733\n",
      "Iteration 113, loss = 0.24658481\n",
      "Iteration 123, loss = 0.25113188\n",
      "Iteration 114, loss = 0.24686990\n",
      "Iteration 124, loss = 0.25088588\n",
      "Iteration 115, loss = 0.24608994\n",
      "Iteration 125, loss = 0.25117622\n",
      "Iteration 116, loss = 0.24633512\n",
      "Iteration 126, loss = 0.25091369\n",
      "Iteration 117, loss = 0.24630882\n",
      "Iteration 127, loss = 0.25051595\n",
      "Iteration 118, loss = 0.24608600\n",
      "Iteration 128, loss = 0.25159500\n",
      "Iteration 119, loss = 0.24555541\n",
      "Iteration 129, loss = 0.25072600\n",
      "Iteration 130, loss = 0.25021934\n",
      "Iteration 120, loss = 0.24577165\n",
      "Iteration 131, loss = 0.25000241\n",
      "Iteration 132, loss = 0.25001071\n",
      "Iteration 121, loss = 0.24561631\n",
      "Iteration 133, loss = 0.24962387\n",
      "Iteration 122, loss = 0.24534818\n",
      "Iteration 134, loss = 0.24980470\n",
      "Iteration 123, loss = 0.24514879\n",
      "Iteration 135, loss = 0.24945580\n",
      "Iteration 124, loss = 0.24521933\n",
      "Iteration 136, loss = 0.24927896\n",
      "Iteration 125, loss = 0.24483777\n",
      "Iteration 137, loss = 0.24912950\n",
      "Iteration 126, loss = 0.24553918\n",
      "Iteration 138, loss = 0.25002459\n",
      "Iteration 127, loss = 0.24484005\n",
      "Iteration 139, loss = 0.24906003\n",
      "Iteration 128, loss = 0.24532847\n",
      "Iteration 140, loss = 0.24903969\n",
      "Iteration 129, loss = 0.24453683\n",
      "Iteration 141, loss = 0.24872699\n",
      "Iteration 130, loss = 0.24491416\n",
      "Iteration 142, loss = 0.24842905\n",
      "Iteration 131, loss = 0.24436598\n",
      "Iteration 143, loss = 0.24815083\n",
      "Iteration 132, loss = 0.24448206\n",
      "Iteration 144, loss = 0.24875897\n",
      "Iteration 133, loss = 0.24424139\n",
      "Iteration 145, loss = 0.24871834\n",
      "Iteration 134, loss = 0.24418727\n",
      "Iteration 146, loss = 0.24794511\n",
      "Iteration 135, loss = 0.24382342Iteration 147, loss = 0.24774156\n",
      "\n",
      "Iteration 148, loss = 0.24770129\n",
      "Iteration 136, loss = 0.24379413\n",
      "Iteration 149, loss = 0.24749130\n",
      "Iteration 137, loss = 0.24370736\n",
      "Iteration 150, loss = 0.24811297\n",
      "Iteration 138, loss = 0.24395736\n",
      "Iteration 139, loss = 0.24359085\n",
      "Iteration 151, loss = 0.24734445\n",
      "Iteration 140, loss = 0.24342146\n",
      "Iteration 152, loss = 0.24722782\n",
      "Iteration 141, loss = 0.24342371\n",
      "Iteration 153, loss = 0.24758888\n",
      "Iteration 142, loss = 0.24343351\n",
      "Iteration 154, loss = 0.24749216\n",
      "Iteration 155, loss = 0.24724527\n",
      "Iteration 143, loss = 0.24303975\n",
      "Iteration 156, loss = 0.24640555\n",
      "Iteration 144, loss = 0.24340910\n",
      "Iteration 145, loss = 0.24313988\n",
      "Iteration 157, loss = 0.24657363\n",
      "Iteration 146, loss = 0.24245524\n",
      "Iteration 158, loss = 0.24656775\n",
      "Iteration 147, loss = 0.24256482\n",
      "Iteration 159, loss = 0.24627851\n",
      "Iteration 148, loss = 0.24253850\n",
      "Iteration 160, loss = 0.24595935\n",
      "Iteration 149, loss = 0.24253944\n",
      "Iteration 161, loss = 0.24617532\n",
      "Iteration 150, loss = 0.24233023\n",
      "Iteration 162, loss = 0.24605733\n",
      "Iteration 151, loss = 0.24211093\n",
      "Iteration 163, loss = 0.24596877\n",
      "Iteration 152, loss = 0.24184989\n",
      "Iteration 164, loss = 0.24593864\n",
      "Iteration 165, loss = 0.24561513\n",
      "Iteration 153, loss = 0.24250536\n",
      "Iteration 154, loss = 0.24217229\n",
      "Iteration 166, loss = 0.24588246\n",
      "Iteration 155, loss = 0.24165521\n",
      "Iteration 167, loss = 0.24539464\n",
      "Iteration 168, loss = 0.24550567\n",
      "Iteration 156, loss = 0.24156178\n",
      "Iteration 169, loss = 0.24542896\n",
      "Iteration 157, loss = 0.24175873\n",
      "Iteration 158, loss = 0.24143666Iteration 170, loss = 0.24481729\n",
      "\n",
      "Iteration 171, loss = 0.24492875\n",
      "Iteration 159, loss = 0.24155643\n",
      "Iteration 172, loss = 0.24455742\n",
      "Iteration 160, loss = 0.24115392\n",
      "Iteration 173, loss = 0.24449951\n",
      "Iteration 161, loss = 0.24112229\n",
      "Iteration 174, loss = 0.24461881\n",
      "Iteration 162, loss = 0.24103729\n",
      "Iteration 175, loss = 0.24467996\n",
      "Iteration 163, loss = 0.24104934\n",
      "Iteration 176, loss = 0.24441773\n",
      "Iteration 164, loss = 0.24099236\n",
      "Iteration 177, loss = 0.24493012\n",
      "Iteration 165, loss = 0.24054880\n",
      "Iteration 178, loss = 0.24415291\n",
      "Iteration 166, loss = 0.24067599\n",
      "Iteration 179, loss = 0.24424071\n",
      "Iteration 167, loss = 0.24061346\n",
      "Iteration 180, loss = 0.24454978\n",
      "Iteration 168, loss = 0.24085421\n",
      "Iteration 181, loss = 0.24455394\n",
      "Iteration 169, loss = 0.24034401\n",
      "Iteration 182, loss = 0.24409925\n",
      "Iteration 170, loss = 0.24002553\n",
      "Iteration 183, loss = 0.24394842\n",
      "Iteration 171, loss = 0.24002894\n",
      "Iteration 184, loss = 0.24380884\n",
      "Iteration 172, loss = 0.24021784\n",
      "Iteration 185, loss = 0.24348586\n",
      "Iteration 173, loss = 0.24011961\n",
      "Iteration 186, loss = 0.24350825\n",
      "Iteration 174, loss = 0.23964047\n",
      "Iteration 187, loss = 0.24352240\n",
      "Iteration 175, loss = 0.23967965\n",
      "Iteration 188, loss = 0.24355077\n",
      "Iteration 176, loss = 0.23957108\n",
      "Iteration 189, loss = 0.24315923\n",
      "Iteration 177, loss = 0.24055651\n",
      "Iteration 190, loss = 0.24299828\n",
      "Iteration 178, loss = 0.24000924\n",
      "Iteration 191, loss = 0.24314243\n",
      "Iteration 179, loss = 0.23950199\n",
      "Iteration 192, loss = 0.24313479\n",
      "Iteration 193, loss = 0.24314027\n",
      "Iteration 180, loss = 0.24009318\n",
      "Iteration 194, loss = 0.24278754\n",
      "Iteration 181, loss = 0.23956663\n",
      "Iteration 182, loss = 0.23903345\n",
      "Iteration 195, loss = 0.24275160\n",
      "Iteration 183, loss = 0.23908893\n",
      "Iteration 196, loss = 0.24299388\n",
      "Iteration 197, loss = 0.24263826\n",
      "Iteration 184, loss = 0.23854966\n",
      "Iteration 198, loss = 0.24253944\n",
      "Iteration 199, loss = 0.24278259\n",
      "Iteration 185, loss = 0.23869989\n",
      "Iteration 200, loss = 0.24256529\n",
      "Iteration 186, loss = 0.23858073\n",
      "Iteration 187, loss = 0.23877616\n",
      "Iteration 188, loss = 0.23852305\n",
      "Iteration 189, loss = 0.23849110Iteration 1, loss = 0.36900719\n",
      "\n",
      "Iteration 2, loss = 0.32891609\n",
      "Iteration 3, loss = 0.31617460"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 190, loss = 0.23826317\n",
      "Iteration 4, loss = 0.30320875\n",
      "Iteration 5, loss = 0.29254681\n",
      "Iteration 191, loss = 0.23808972\n",
      "Iteration 6, loss = 0.28613612\n",
      "Iteration 192, loss = 0.23829932\n",
      "Iteration 7, loss = 0.28144636\n",
      "Iteration 193, loss = 0.23825356\n",
      "Iteration 8, loss = 0.27898933\n",
      "Iteration 9, loss = 0.27643739\n",
      "Iteration 194, loss = 0.23764261\n",
      "Iteration 10, loss = 0.27509162\n",
      "Iteration 195, loss = 0.23760227\n",
      "Iteration 11, loss = 0.27341276\n",
      "Iteration 12, loss = 0.27248232\n",
      "Iteration 196, loss = 0.23782682\n",
      "Iteration 13, loss = 0.27149883\n",
      "Iteration 197, loss = 0.23769613\n",
      "Iteration 14, loss = 0.27130534\n",
      "Iteration 198, loss = 0.23722377\n",
      "Iteration 15, loss = 0.27028864\n",
      "Iteration 16, loss = 0.26939233\n",
      "Iteration 199, loss = 0.23755975\n",
      "Iteration 17, loss = 0.26933176\n",
      "Iteration 18, loss = 0.26833682\n",
      "Iteration 200, loss = 0.23746095\n",
      "Iteration 19, loss = 0.26824600\n",
      "Iteration 20, loss = 0.26818217\n",
      "Iteration 21, loss = 0.26762451\n",
      "Iteration 22, loss = 0.26708652\n",
      "Iteration 23, loss = 0.26731212\n",
      "Iteration 1, loss = 0.36822817\n",
      "Iteration 24, loss = 0.26669005\n",
      "Iteration 25, loss = 0.26603658\n",
      "Iteration 26, loss = 0.26614369\n",
      "Iteration 2, loss = 0.32870094\n",
      "Iteration 27, loss = 0.26555547\n",
      "Iteration 28, loss = 0.26559192\n",
      "Iteration 3, loss = 0.31516527\n",
      "Iteration 29, loss = 0.26555443\n",
      "Iteration 4, loss = 0.30154489\n",
      "Iteration 30, loss = 0.26549395\n",
      "Iteration 31, loss = 0.26477074\n",
      "Iteration 5, loss = 0.29048702\n",
      "Iteration 32, loss = 0.26588739\n",
      "Iteration 6, loss = 0.28378205\n",
      "Iteration 33, loss = 0.26423844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34, loss = 0.26477856\n",
      "Iteration 7, loss = 0.27891026\n",
      "Iteration 35, loss = 0.26406981\n",
      "Iteration 8, loss = 0.27622262\n",
      "Iteration 36, loss = 0.26385182\n",
      "Iteration 9, loss = 0.27314058\n",
      "Iteration 37, loss = 0.26366436\n",
      "Iteration 10, loss = 0.27154521\n",
      "Iteration 38, loss = 0.26361937\n",
      "Iteration 11, loss = 0.27005384\n",
      "Iteration 39, loss = 0.26387653\n",
      "Iteration 12, loss = 0.26824930\n",
      "Iteration 40, loss = 0.26361459\n",
      "Iteration 13, loss = 0.26772279\n",
      "Iteration 41, loss = 0.26266497\n",
      "Iteration 14, loss = 0.26781217\n",
      "Iteration 42, loss = 0.26218959\n",
      "Iteration 15, loss = 0.26682474\n",
      "Iteration 16, loss = 0.26556293\n",
      "Iteration 43, loss = 0.26218287\n",
      "Iteration 17, loss = 0.26519531\n",
      "Iteration 44, loss = 0.26251866\n",
      "Iteration 45, loss = 0.26255691\n",
      "Iteration 18, loss = 0.26484339\n",
      "Iteration 46, loss = 0.26160297\n",
      "Iteration 19, loss = 0.26440603\n",
      "Iteration 47, loss = 0.26159940\n",
      "Iteration 48, loss = 0.26102213\n",
      "Iteration 20, loss = 0.26501610\n",
      "Iteration 49, loss = 0.26071745\n",
      "Iteration 21, loss = 0.26421260\n",
      "Iteration 50, loss = 0.26067020\n",
      "Iteration 22, loss = 0.26351505\n",
      "Iteration 23, loss = 0.26419137\n",
      "Iteration 51, loss = 0.26036287\n",
      "Iteration 24, loss = 0.26357303\n",
      "Iteration 25, loss = 0.26290038\n",
      "Iteration 26, loss = 0.26278973\n",
      "Iteration 52, loss = 0.26044948\n",
      "Iteration 27, loss = 0.26218228\n",
      "Iteration 53, loss = 0.25969431\n",
      "Iteration 28, loss = 0.26219883\n",
      "Iteration 54, loss = 0.25957693\n",
      "Iteration 29, loss = 0.26221841\n",
      "Iteration 55, loss = 0.25919944\n",
      "Iteration 56, loss = 0.25906956\n",
      "Iteration 30, loss = 0.26221692\n",
      "Iteration 57, loss = 0.25945152\n",
      "Iteration 58, loss = 0.25953061\n",
      "Iteration 31, loss = 0.26134181\n",
      "Iteration 59, loss = 0.25865806\n",
      "Iteration 32, loss = 0.26277103\n",
      "Iteration 60, loss = 0.25793514\n",
      "Iteration 33, loss = 0.26075924\n",
      "Iteration 61, loss = 0.25761157\n",
      "Iteration 34, loss = 0.26138245\n",
      "Iteration 62, loss = 0.25806886\n",
      "Iteration 35, loss = 0.26044914\n",
      "Iteration 63, loss = 0.25701804\n",
      "Iteration 36, loss = 0.26053429\n",
      "Iteration 64, loss = 0.25694437\n",
      "Iteration 37, loss = 0.26001966\n",
      "Iteration 65, loss = 0.25698340\n",
      "Iteration 38, loss = 0.26014102\n",
      "Iteration 66, loss = 0.25624206\n",
      "Iteration 39, loss = 0.26022675\n",
      "Iteration 67, loss = 0.25679762\n",
      "Iteration 40, loss = 0.26053210\n",
      "Iteration 68, loss = 0.25640513\n",
      "Iteration 41, loss = 0.25898591\n",
      "Iteration 69, loss = 0.25639175\n",
      "Iteration 42, loss = 0.25886747\n",
      "Iteration 70, loss = 0.25585449\n",
      "Iteration 43, loss = 0.25853112\n",
      "Iteration 71, loss = 0.25524938\n",
      "Iteration 44, loss = 0.25899390\n",
      "Iteration 72, loss = 0.25492657\n",
      "Iteration 45, loss = 0.25868680\n",
      "Iteration 73, loss = 0.25579247\n",
      "Iteration 46, loss = 0.25772874\n",
      "Iteration 74, loss = 0.25517655\n",
      "Iteration 47, loss = 0.25787310\n",
      "Iteration 75, loss = 0.25502630\n",
      "Iteration 48, loss = 0.25747563\n",
      "Iteration 76, loss = 0.25436209\n",
      "Iteration 49, loss = 0.25679712\n",
      "Iteration 77, loss = 0.25431030\n",
      "Iteration 50, loss = 0.25665762\n",
      "Iteration 78, loss = 0.25414212\n",
      "Iteration 51, loss = 0.25651914\n",
      "Iteration 79, loss = 0.25360466\n",
      "Iteration 52, loss = 0.25640274\n",
      "Iteration 80, loss = 0.25369126\n",
      "Iteration 53, loss = 0.25589344\n",
      "Iteration 81, loss = 0.25408562\n",
      "Iteration 54, loss = 0.25582050\n",
      "Iteration 82, loss = 0.25341599\n",
      "Iteration 55, loss = 0.25541473\n",
      "Iteration 83, loss = 0.25245609\n",
      "Iteration 56, loss = 0.25516985\n",
      "Iteration 84, loss = 0.25366308\n",
      "Iteration 57, loss = 0.25545862\n",
      "Iteration 85, loss = 0.25267282\n",
      "Iteration 58, loss = 0.25626998\n",
      "Iteration 86, loss = 0.25236114\n",
      "Iteration 59, loss = 0.25539203\n",
      "Iteration 87, loss = 0.25275681\n",
      "Iteration 60, loss = 0.25414899\n",
      "Iteration 88, loss = 0.25219547\n",
      "Iteration 61, loss = 0.25389145\n",
      "Iteration 89, loss = 0.25177362\n",
      "Iteration 62, loss = 0.25427989\n",
      "Iteration 90, loss = 0.25171856\n",
      "Iteration 63, loss = 0.25378688\n",
      "Iteration 91, loss = 0.25143210\n",
      "Iteration 64, loss = 0.25338163\n",
      "Iteration 92, loss = 0.25112070\n",
      "Iteration 65, loss = 0.25310531\n",
      "Iteration 93, loss = 0.25090002\n",
      "Iteration 66, loss = 0.25234528\n",
      "Iteration 94, loss = 0.25083464\n",
      "Iteration 67, loss = 0.25305840\n",
      "Iteration 95, loss = 0.25093349\n",
      "Iteration 68, loss = 0.25262391\n",
      "Iteration 96, loss = 0.25055673\n",
      "Iteration 69, loss = 0.25223674\n",
      "Iteration 97, loss = 0.25047504\n",
      "Iteration 70, loss = 0.25206380\n",
      "Iteration 98, loss = 0.25014280\n",
      "Iteration 71, loss = 0.25148505\n",
      "Iteration 99, loss = 0.25057369\n",
      "Iteration 100, loss = 0.24999466\n",
      "Iteration 101, loss = 0.25041493\n",
      "Iteration 72, loss = 0.25110076\n",
      "Iteration 102, loss = 0.25022927\n",
      "Iteration 73, loss = 0.25138170\n",
      "Iteration 103, loss = 0.24913437\n",
      "Iteration 74, loss = 0.25093319\n",
      "Iteration 104, loss = 0.24959158\n",
      "Iteration 75, loss = 0.25081422\n",
      "Iteration 105, loss = 0.24871778\n",
      "Iteration 76, loss = 0.25043322\n",
      "Iteration 106, loss = 0.24960545\n",
      "Iteration 77, loss = 0.25074607\n",
      "Iteration 107, loss = 0.24900691\n",
      "Iteration 78, loss = 0.25019549\n",
      "Iteration 108, loss = 0.24851587\n",
      "Iteration 79, loss = 0.24966016\n",
      "Iteration 80, loss = 0.25010405\n",
      "Iteration 109, loss = 0.24867625\n",
      "Iteration 81, loss = 0.24943690\n",
      "Iteration 110, loss = 0.24826116\n",
      "Iteration 82, loss = 0.24938946\n",
      "Iteration 111, loss = 0.24923528\n",
      "Iteration 83, loss = 0.24847078\n",
      "Iteration 112, loss = 0.24921140\n",
      "Iteration 84, loss = 0.24901012\n",
      "Iteration 113, loss = 0.24815024\n",
      "Iteration 85, loss = 0.24848013\n",
      "Iteration 114, loss = 0.24843261\n",
      "Iteration 86, loss = 0.24869063\n",
      "Iteration 115, loss = 0.24780271\n",
      "Iteration 87, loss = 0.24878017\n",
      "Iteration 116, loss = 0.24808840\n",
      "Iteration 88, loss = 0.24773640\n",
      "Iteration 117, loss = 0.24766373\n",
      "Iteration 118, loss = 0.24786504Iteration 89, loss = 0.24755857\n",
      "\n",
      "Iteration 119, loss = 0.24727097\n",
      "Iteration 120, loss = 0.24738269\n",
      "Iteration 90, loss = 0.24771105\n",
      "Iteration 121, loss = 0.24715018\n",
      "Iteration 91, loss = 0.24745407\n",
      "Iteration 122, loss = 0.24705231\n",
      "Iteration 92, loss = 0.24724752\n",
      "Iteration 123, loss = 0.24669339\n",
      "Iteration 93, loss = 0.24721308\n",
      "Iteration 124, loss = 0.24659377\n",
      "Iteration 94, loss = 0.24672859\n",
      "Iteration 125, loss = 0.24643961\n",
      "Iteration 95, loss = 0.24687012\n",
      "Iteration 126, loss = 0.24639726\n",
      "Iteration 96, loss = 0.24652920\n",
      "Iteration 127, loss = 0.24621928\n",
      "Iteration 97, loss = 0.24698670\n",
      "Iteration 128, loss = 0.24638145\n",
      "Iteration 98, loss = 0.24606355\n",
      "Iteration 129, loss = 0.24591621\n",
      "Iteration 99, loss = 0.24614233\n",
      "Iteration 130, loss = 0.24668954\n",
      "Iteration 100, loss = 0.24606178\n",
      "Iteration 131, loss = 0.24587690\n",
      "Iteration 101, loss = 0.24618994\n",
      "Iteration 132, loss = 0.24576992\n",
      "Iteration 102, loss = 0.24565999\n",
      "Iteration 133, loss = 0.24542355\n",
      "Iteration 103, loss = 0.24481830\n",
      "Iteration 134, loss = 0.24552310\n",
      "Iteration 104, loss = 0.24523437\n",
      "Iteration 135, loss = 0.24528350\n",
      "Iteration 105, loss = 0.24428693\n",
      "Iteration 136, loss = 0.24501864\n",
      "Iteration 137, loss = 0.24501394\n",
      "Iteration 106, loss = 0.24517735\n",
      "Iteration 138, loss = 0.24513217\n",
      "Iteration 139, loss = 0.24451208\n",
      "Iteration 107, loss = 0.24454249\n",
      "Iteration 140, loss = 0.24477397\n",
      "Iteration 108, loss = 0.24367760\n",
      "Iteration 141, loss = 0.24491160\n",
      "Iteration 142, loss = 0.24443732\n",
      "Iteration 109, loss = 0.24440399\n",
      "Iteration 143, loss = 0.24466798\n",
      "Iteration 110, loss = 0.24402417\n",
      "Iteration 144, loss = 0.24447580\n",
      "Iteration 111, loss = 0.24415894\n",
      "Iteration 145, loss = 0.24400876\n",
      "Iteration 112, loss = 0.24434338\n",
      "Iteration 146, loss = 0.24370024\n",
      "Iteration 113, loss = 0.24357029\n",
      "Iteration 147, loss = 0.24411504\n",
      "Iteration 114, loss = 0.24347253\n",
      "Iteration 148, loss = 0.24376601\n",
      "Iteration 115, loss = 0.24330385\n",
      "Iteration 149, loss = 0.24401813\n",
      "Iteration 116, loss = 0.24333282\n",
      "Iteration 150, loss = 0.24327761\n",
      "Iteration 117, loss = 0.24273324\n",
      "Iteration 151, loss = 0.24347598\n",
      "Iteration 118, loss = 0.24250961\n",
      "Iteration 152, loss = 0.24343715\n",
      "Iteration 119, loss = 0.24280191\n",
      "Iteration 153, loss = 0.24356640\n",
      "Iteration 120, loss = 0.24231305\n",
      "Iteration 154, loss = 0.24349213\n",
      "Iteration 121, loss = 0.24195998\n",
      "Iteration 122, loss = 0.24184394\n",
      "Iteration 155, loss = 0.24300252\n",
      "Iteration 123, loss = 0.24188945\n",
      "Iteration 156, loss = 0.24293470\n",
      "Iteration 124, loss = 0.24149263\n",
      "Iteration 157, loss = 0.24264786\n",
      "Iteration 125, loss = 0.24141549\n",
      "Iteration 158, loss = 0.24262154\n",
      "Iteration 126, loss = 0.24137614\n",
      "Iteration 159, loss = 0.24297896\n",
      "Iteration 127, loss = 0.24116028\n",
      "Iteration 160, loss = 0.24222857\n",
      "Iteration 128, loss = 0.24138380\n",
      "Iteration 161, loss = 0.24266437\n",
      "Iteration 129, loss = 0.24099151\n",
      "Iteration 162, loss = 0.24220593\n",
      "Iteration 130, loss = 0.24205620\n",
      "Iteration 163, loss = 0.24275496\n",
      "Iteration 131, loss = 0.24114584\n",
      "Iteration 164, loss = 0.24218178\n",
      "Iteration 132, loss = 0.24046340\n",
      "Iteration 165, loss = 0.24200597\n",
      "Iteration 133, loss = 0.24057943\n",
      "Iteration 134, loss = 0.24002124\n",
      "Iteration 166, loss = 0.24165523\n",
      "Iteration 135, loss = 0.24029403\n",
      "Iteration 167, loss = 0.24160639\n",
      "Iteration 168, loss = 0.24187095\n",
      "Iteration 136, loss = 0.24018293\n",
      "Iteration 169, loss = 0.24171567\n",
      "Iteration 137, loss = 0.23959348\n",
      "Iteration 170, loss = 0.24140270\n",
      "Iteration 138, loss = 0.23939032\n",
      "Iteration 171, loss = 0.24126936\n",
      "Iteration 139, loss = 0.23974802\n",
      "Iteration 172, loss = 0.24169593\n",
      "Iteration 140, loss = 0.23961392\n",
      "Iteration 173, loss = 0.24141445\n",
      "Iteration 141, loss = 0.23939219\n",
      "Iteration 174, loss = 0.24072972\n",
      "Iteration 142, loss = 0.23905972\n",
      "Iteration 175, loss = 0.24147754\n",
      "Iteration 143, loss = 0.23901610\n",
      "Iteration 176, loss = 0.24072953\n",
      "Iteration 144, loss = 0.23917414\n",
      "Iteration 177, loss = 0.24108149\n",
      "Iteration 145, loss = 0.23827250\n",
      "Iteration 178, loss = 0.24073287\n",
      "Iteration 146, loss = 0.23826140\n",
      "Iteration 179, loss = 0.24080661\n",
      "Iteration 180, loss = 0.24095229\n",
      "Iteration 181, loss = 0.24068072\n",
      "Iteration 147, loss = 0.23808357\n",
      "Iteration 182, loss = 0.24057859\n",
      "Iteration 148, loss = 0.23790100\n",
      "Iteration 149, loss = 0.23910525\n",
      "Iteration 183, loss = 0.24056482\n",
      "Iteration 150, loss = 0.23747880\n",
      "Iteration 151, loss = 0.23758170\n",
      "Iteration 184, loss = 0.24011951\n",
      "Iteration 185, loss = 0.23993854Iteration 152, loss = 0.23723773\n",
      "\n",
      "Iteration 153, loss = 0.23724284\n",
      "Iteration 186, loss = 0.23971458\n",
      "Iteration 154, loss = 0.23732642\n",
      "Iteration 187, loss = 0.23988078\n",
      "Iteration 155, loss = 0.23673056\n",
      "Iteration 156, loss = 0.23724917\n",
      "Iteration 188, loss = 0.24003414\n",
      "Iteration 189, loss = 0.24015104\n",
      "Iteration 190, loss = 0.24004344\n",
      "Iteration 157, loss = 0.23638422\n",
      "Iteration 191, loss = 0.23996776Iteration 158, loss = 0.23639797\n",
      "\n",
      "Iteration 159, loss = 0.23637280\n",
      "Iteration 192, loss = 0.23945040\n",
      "Iteration 160, loss = 0.23597671\n",
      "Iteration 193, loss = 0.23970558\n",
      "Iteration 161, loss = 0.23597985\n",
      "Iteration 194, loss = 0.23940367\n",
      "Iteration 162, loss = 0.23558798\n",
      "Iteration 195, loss = 0.23969188\n",
      "Iteration 163, loss = 0.23576160\n",
      "Iteration 196, loss = 0.23965032\n",
      "Iteration 164, loss = 0.23508869\n",
      "Iteration 197, loss = 0.23976369\n",
      "Iteration 165, loss = 0.23489115\n",
      "Iteration 198, loss = 0.23876020\n",
      "Iteration 166, loss = 0.23468431\n",
      "Iteration 199, loss = 0.23901956\n",
      "Iteration 167, loss = 0.23452620\n",
      "Iteration 200, loss = 0.23912616\n",
      "Iteration 168, loss = 0.23414851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 169, loss = 0.23437582\n",
      "Iteration 170, loss = 0.23375341\n",
      "Iteration 1, loss = 0.36906012\n",
      "Iteration 171, loss = 0.23374098\n",
      "Iteration 2, loss = 0.32870357\n",
      "Iteration 172, loss = 0.23382330\n",
      "Iteration 173, loss = 0.23345710\n",
      "Iteration 3, loss = 0.31570882\n",
      "Iteration 174, loss = 0.23336090\n",
      "Iteration 4, loss = 0.30218840\n",
      "Iteration 175, loss = 0.23286009\n",
      "Iteration 5, loss = 0.29152124\n",
      "Iteration 176, loss = 0.23287669\n",
      "Iteration 6, loss = 0.28478211\n",
      "Iteration 177, loss = 0.23343758\n",
      "Iteration 7, loss = 0.28030286\n",
      "Iteration 178, loss = 0.23225683\n",
      "Iteration 8, loss = 0.27759156\n",
      "Iteration 179, loss = 0.23183706\n",
      "Iteration 9, loss = 0.27456467\n",
      "Iteration 180, loss = 0.23281524\n",
      "Iteration 10, loss = 0.27312309\n",
      "Iteration 181, loss = 0.23199289\n",
      "Iteration 182, loss = 0.23196458\n",
      "Iteration 11, loss = 0.27187005\n",
      "Iteration 183, loss = 0.23197840\n",
      "Iteration 12, loss = 0.26987264\n",
      "Iteration 184, loss = 0.23123390\n",
      "Iteration 13, loss = 0.26945435\n",
      "Iteration 185, loss = 0.23107181\n",
      "Iteration 14, loss = 0.26910934\n",
      "Iteration 186, loss = 0.23102952\n",
      "Iteration 15, loss = 0.26907111\n",
      "Iteration 187, loss = 0.23112138\n",
      "Iteration 16, loss = 0.26803159\n",
      "Iteration 188, loss = 0.23103095\n",
      "Iteration 17, loss = 0.26687946\n",
      "Iteration 189, loss = 0.23144176\n",
      "Iteration 18, loss = 0.26656221\n",
      "Iteration 190, loss = 0.23105457\n",
      "Iteration 19, loss = 0.26619678\n",
      "Iteration 191, loss = 0.23052067\n",
      "Iteration 192, loss = 0.23045639\n",
      "Iteration 20, loss = 0.26635331\n",
      "Iteration 21, loss = 0.26598952\n",
      "Iteration 22, loss = 0.26519969\n",
      "Iteration 193, loss = 0.22998805\n",
      "Iteration 23, loss = 0.26534632Iteration 194, loss = 0.23038427\n",
      "\n",
      "Iteration 195, loss = 0.23031416\n",
      "Iteration 24, loss = 0.26560749\n",
      "Iteration 196, loss = 0.22957821\n",
      "Iteration 25, loss = 0.26408595\n",
      "Iteration 197, loss = 0.23021135\n",
      "Iteration 198, loss = 0.22975714\n",
      "Iteration 26, loss = 0.26405370Iteration 199, loss = 0.22961533\n",
      "\n",
      "Iteration 200, loss = 0.22943898\n",
      "Iteration 27, loss = 0.26361303\n",
      "Iteration 28, loss = 0.26355521\n",
      "Iteration 29, loss = 0.26301524\n",
      "Iteration 30, loss = 0.26325573\n",
      "Iteration 31, loss = 0.26278023\n",
      "Iteration 32, loss = 0.26321094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 33, loss = 0.26182489\n",
      "Iteration 1, loss = 0.36904628\n",
      "Iteration 34, loss = 0.26196673\n",
      "Iteration 35, loss = 0.26167802\n",
      "Iteration 2, loss = 0.33001613\n",
      "Iteration 36, loss = 0.26150209\n",
      "Iteration 37, loss = 0.26144168\n",
      "Iteration 3, loss = 0.31912843\n",
      "Iteration 4, loss = 0.30724307\n",
      "Iteration 38, loss = 0.26096141\n",
      "Iteration 5, loss = 0.29743245\n",
      "Iteration 39, loss = 0.26088401\n",
      "Iteration 6, loss = 0.29116786\n",
      "Iteration 40, loss = 0.26102547\n",
      "Iteration 7, loss = 0.28693561\n",
      "Iteration 41, loss = 0.26001860\n",
      "Iteration 8, loss = 0.28380833\n",
      "Iteration 42, loss = 0.25987099\n",
      "Iteration 9, loss = 0.28136250\n",
      "Iteration 43, loss = 0.25969471\n",
      "Iteration 10, loss = 0.27951611\n",
      "Iteration 44, loss = 0.25969031\n",
      "Iteration 11, loss = 0.27858780\n",
      "Iteration 45, loss = 0.25965593\n",
      "Iteration 12, loss = 0.27643153\n",
      "Iteration 46, loss = 0.25894640\n",
      "Iteration 13, loss = 0.27615939\n",
      "Iteration 47, loss = 0.25980468\n",
      "Iteration 14, loss = 0.27571168\n",
      "Iteration 48, loss = 0.25848077\n",
      "Iteration 15, loss = 0.27485449\n",
      "Iteration 49, loss = 0.25798099\n",
      "Iteration 16, loss = 0.27455441\n",
      "Iteration 50, loss = 0.25779777\n",
      "Iteration 17, loss = 0.27340089\n",
      "Iteration 51, loss = 0.25785597\n",
      "Iteration 18, loss = 0.27292891\n",
      "Iteration 52, loss = 0.25778048\n",
      "Iteration 19, loss = 0.27242472\n",
      "Iteration 53, loss = 0.25704521\n",
      "Iteration 20, loss = 0.27262873\n",
      "Iteration 54, loss = 0.25709450\n",
      "Iteration 21, loss = 0.27233649\n",
      "Iteration 55, loss = 0.25651689\n",
      "Iteration 22, loss = 0.27146034\n",
      "Iteration 56, loss = 0.25662182\n",
      "Iteration 57, loss = 0.25680943\n",
      "Iteration 23, loss = 0.27134234\n",
      "Iteration 58, loss = 0.25775433\n",
      "Iteration 24, loss = 0.27139397\n",
      "Iteration 59, loss = 0.25616256Iteration 25, loss = 0.27089854\n",
      "\n",
      "Iteration 26, loss = 0.27007256\n",
      "Iteration 60, loss = 0.25539102\n",
      "Iteration 27, loss = 0.27040133\n",
      "Iteration 28, loss = 0.26995295\n",
      "Iteration 61, loss = 0.25513994\n",
      "Iteration 29, loss = 0.26904964\n",
      "Iteration 30, loss = 0.26958903\n",
      "Iteration 62, loss = 0.25624556\n",
      "Iteration 31, loss = 0.26884070\n",
      "Iteration 63, loss = 0.25465276\n",
      "Iteration 32, loss = 0.26875459\n",
      "Iteration 64, loss = 0.25455051\n",
      "Iteration 33, loss = 0.26812943\n",
      "Iteration 65, loss = 0.25475789\n",
      "Iteration 34, loss = 0.26811172\n",
      "Iteration 66, loss = 0.25376395\n",
      "Iteration 35, loss = 0.26769927\n",
      "Iteration 67, loss = 0.25392360\n",
      "Iteration 36, loss = 0.26748328\n",
      "Iteration 68, loss = 0.25417834\n",
      "Iteration 37, loss = 0.26783670\n",
      "Iteration 69, loss = 0.25363155\n",
      "Iteration 38, loss = 0.26705218\n",
      "Iteration 70, loss = 0.25303281\n",
      "Iteration 39, loss = 0.26750826\n",
      "Iteration 71, loss = 0.25292795\n",
      "Iteration 40, loss = 0.26702900\n",
      "Iteration 72, loss = 0.25216571\n",
      "Iteration 41, loss = 0.26603449\n",
      "Iteration 73, loss = 0.25253014\n",
      "Iteration 42, loss = 0.26579334\n",
      "Iteration 74, loss = 0.25188574\n",
      "Iteration 43, loss = 0.26558900\n",
      "Iteration 75, loss = 0.25173687\n",
      "Iteration 44, loss = 0.26582174\n",
      "Iteration 76, loss = 0.25171850\n",
      "Iteration 45, loss = 0.26547098\n",
      "Iteration 77, loss = 0.25197123\n",
      "Iteration 46, loss = 0.26481158\n",
      "Iteration 78, loss = 0.25138629\n",
      "Iteration 47, loss = 0.26529756\n",
      "Iteration 79, loss = 0.25085130\n",
      "Iteration 48, loss = 0.26439409\n",
      "Iteration 80, loss = 0.25049261\n",
      "Iteration 49, loss = 0.26413270\n",
      "Iteration 81, loss = 0.25127597\n",
      "Iteration 50, loss = 0.26393039\n",
      "Iteration 82, loss = 0.25049434\n",
      "Iteration 51, loss = 0.26405272\n",
      "Iteration 83, loss = 0.25016624\n",
      "Iteration 52, loss = 0.26344179\n",
      "Iteration 84, loss = 0.24971669\n",
      "Iteration 53, loss = 0.26317491\n",
      "Iteration 85, loss = 0.24967270\n",
      "Iteration 54, loss = 0.26344315\n",
      "Iteration 86, loss = 0.24940307\n",
      "Iteration 55, loss = 0.26282984\n",
      "Iteration 56, loss = 0.26284078\n",
      "Iteration 87, loss = 0.24985495\n",
      "Iteration 57, loss = 0.26294957\n",
      "Iteration 88, loss = 0.24873890\n",
      "Iteration 89, loss = 0.24880080\n",
      "Iteration 58, loss = 0.26302968\n",
      "Iteration 90, loss = 0.24854776\n",
      "Iteration 59, loss = 0.26225926\n",
      "Iteration 91, loss = 0.24825779\n",
      "Iteration 92, loss = 0.24819787\n",
      "Iteration 60, loss = 0.26171922\n",
      "Iteration 93, loss = 0.24822924\n",
      "Iteration 61, loss = 0.26171335\n",
      "Iteration 94, loss = 0.24774448\n",
      "Iteration 62, loss = 0.26222524\n",
      "Iteration 95, loss = 0.24796268\n",
      "Iteration 63, loss = 0.26109141\n",
      "Iteration 96, loss = 0.24722456\n",
      "Iteration 64, loss = 0.26140371\n",
      "Iteration 97, loss = 0.24794657\n",
      "Iteration 65, loss = 0.26093595\n",
      "Iteration 98, loss = 0.24685542\n",
      "Iteration 66, loss = 0.26032703\n",
      "Iteration 99, loss = 0.24688851\n",
      "Iteration 67, loss = 0.26072926\n",
      "Iteration 100, loss = 0.24697481\n",
      "Iteration 68, loss = 0.26044840\n",
      "Iteration 101, loss = 0.24778878\n",
      "Iteration 69, loss = 0.26062029\n",
      "Iteration 102, loss = 0.24643122\n",
      "Iteration 70, loss = 0.25961345\n",
      "Iteration 103, loss = 0.24579816\n",
      "Iteration 71, loss = 0.25978216\n",
      "Iteration 104, loss = 0.24562567\n",
      "Iteration 72, loss = 0.25922507\n",
      "Iteration 105, loss = 0.24511473\n",
      "Iteration 73, loss = 0.25952455\n",
      "Iteration 106, loss = 0.24577636\n",
      "Iteration 74, loss = 0.25903359\n",
      "Iteration 107, loss = 0.24522310\n",
      "Iteration 75, loss = 0.25899123\n",
      "Iteration 76, loss = 0.25907176\n",
      "Iteration 77, loss = 0.25878619\n",
      "Iteration 108, loss = 0.24428522\n",
      "Iteration 78, loss = 0.25886779\n",
      "Iteration 109, loss = 0.24547729\n",
      "Iteration 110, loss = 0.24464766\n",
      "Iteration 79, loss = 0.25825846\n",
      "Iteration 80, loss = 0.25805735\n",
      "Iteration 111, loss = 0.24471223\n",
      "Iteration 81, loss = 0.25804140\n",
      "Iteration 112, loss = 0.24489179\n",
      "Iteration 82, loss = 0.25776712\n",
      "Iteration 113, loss = 0.24405197\n",
      "Iteration 83, loss = 0.25715819\n",
      "Iteration 114, loss = 0.24441949\n",
      "Iteration 84, loss = 0.25754038\n",
      "Iteration 115, loss = 0.24382663\n",
      "Iteration 116, loss = 0.24355163\n",
      "Iteration 85, loss = 0.25707629\n",
      "Iteration 86, loss = 0.25721850\n",
      "Iteration 117, loss = 0.24350966\n",
      "Iteration 87, loss = 0.25812469\n",
      "Iteration 118, loss = 0.24290050\n",
      "Iteration 88, loss = 0.25637437\n",
      "Iteration 119, loss = 0.24349154\n",
      "Iteration 89, loss = 0.25624622\n",
      "Iteration 120, loss = 0.24306718\n",
      "Iteration 90, loss = 0.25651523\n",
      "Iteration 121, loss = 0.24258614\n",
      "Iteration 91, loss = 0.25603969\n",
      "Iteration 122, loss = 0.24250026\n",
      "Iteration 92, loss = 0.25607708\n",
      "Iteration 123, loss = 0.24248284\n",
      "Iteration 93, loss = 0.25582003\n",
      "Iteration 124, loss = 0.24251115\n",
      "Iteration 94, loss = 0.25552175\n",
      "Iteration 125, loss = 0.24241957\n",
      "Iteration 126, loss = 0.24198791\n",
      "Iteration 95, loss = 0.25547527\n",
      "Iteration 127, loss = 0.24185150\n",
      "Iteration 128, loss = 0.24189371\n",
      "Iteration 96, loss = 0.25511398\n",
      "Iteration 129, loss = 0.24160528\n",
      "Iteration 130, loss = 0.24189663\n",
      "Iteration 97, loss = 0.25588087\n",
      "Iteration 131, loss = 0.24157023\n",
      "Iteration 132, loss = 0.24149755\n",
      "Iteration 98, loss = 0.25482766\n",
      "Iteration 133, loss = 0.24110910\n",
      "Iteration 134, loss = 0.24103346\n",
      "Iteration 135, loss = 0.24090707\n",
      "Iteration 99, loss = 0.25507075\n",
      "Iteration 136, loss = 0.24035834\n",
      "Iteration 137, loss = 0.24027641\n",
      "Iteration 138, loss = 0.24031648\n",
      "Iteration 100, loss = 0.25466533\n",
      "Iteration 139, loss = 0.24028649\n",
      "Iteration 101, loss = 0.25485737Iteration 140, loss = 0.24016917\n",
      "\n",
      "Iteration 141, loss = 0.23987045\n",
      "Iteration 102, loss = 0.25402657Iteration 142, loss = 0.23941699\n",
      "\n",
      "Iteration 143, loss = 0.23940117\n",
      "Iteration 103, loss = 0.25414031\n",
      "Iteration 144, loss = 0.23982150\n",
      "Iteration 145, loss = 0.23924448\n",
      "Iteration 104, loss = 0.25367417\n",
      "Iteration 146, loss = 0.23913695\n",
      "Iteration 147, loss = 0.23918936\n",
      "Iteration 105, loss = 0.25340578\n",
      "Iteration 148, loss = 0.23851104\n",
      "Iteration 149, loss = 0.23951795\n",
      "Iteration 106, loss = 0.25354048\n",
      "Iteration 107, loss = 0.25331307\n",
      "Iteration 108, loss = 0.25279401\n",
      "Iteration 150, loss = 0.23832504\n",
      "Iteration 109, loss = 0.25335928\n",
      "Iteration 151, loss = 0.23825174\n",
      "Iteration 152, loss = 0.23843139\n",
      "Iteration 110, loss = 0.25266871\n",
      "Iteration 153, loss = 0.23814591\n",
      "Iteration 111, loss = 0.25286415\n",
      "Iteration 112, loss = 0.25315484\n",
      "Iteration 154, loss = 0.23844460\n",
      "Iteration 113, loss = 0.25251588\n",
      "Iteration 155, loss = 0.23806228\n",
      "Iteration 114, loss = 0.25358355\n",
      "Iteration 156, loss = 0.23829503\n",
      "Iteration 157, loss = 0.23749893\n",
      "Iteration 158, loss = 0.23766288\n",
      "Iteration 115, loss = 0.25233591\n",
      "Iteration 116, loss = 0.25220821\n",
      "Iteration 159, loss = 0.23767571\n",
      "Iteration 117, loss = 0.25184439\n",
      "Iteration 160, loss = 0.23729447\n",
      "Iteration 161, loss = 0.23752802\n",
      "Iteration 118, loss = 0.25125670\n",
      "Iteration 162, loss = 0.23734052\n",
      "Iteration 163, loss = 0.23750442\n",
      "Iteration 119, loss = 0.25137392\n",
      "Iteration 164, loss = 0.23695395\n",
      "Iteration 120, loss = 0.25149202\n",
      "Iteration 165, loss = 0.23687521\n",
      "Iteration 121, loss = 0.25112442\n",
      "Iteration 122, loss = 0.25079540\n",
      "Iteration 166, loss = 0.23670219\n",
      "Iteration 123, loss = 0.25069756\n",
      "Iteration 167, loss = 0.23679928\n",
      "Iteration 124, loss = 0.25127984Iteration 168, loss = 0.23691758\n",
      "\n",
      "Iteration 169, loss = 0.23659074\n",
      "Iteration 170, loss = 0.23632864\n",
      "Iteration 125, loss = 0.25057058Iteration 171, loss = 0.23643883\n",
      "\n",
      "Iteration 172, loss = 0.23667290\n",
      "Iteration 126, loss = 0.25020527\n",
      "Iteration 173, loss = 0.23609555\n",
      "Iteration 127, loss = 0.25006545\n",
      "Iteration 128, loss = 0.25021163\n",
      "Iteration 174, loss = 0.23609899\n",
      "Iteration 129, loss = 0.25027317\n",
      "Iteration 130, loss = 0.25005399\n",
      "Iteration 175, loss = 0.23572002\n",
      "Iteration 131, loss = 0.24969426\n",
      "Iteration 132, loss = 0.24934420\n",
      "Iteration 176, loss = 0.23567635\n",
      "Iteration 133, loss = 0.24922079\n",
      "Iteration 177, loss = 0.23583733\n",
      "Iteration 178, loss = 0.23562946\n",
      "Iteration 134, loss = 0.24897033\n",
      "Iteration 135, loss = 0.24889602\n",
      "Iteration 179, loss = 0.23544108\n",
      "Iteration 136, loss = 0.24866218\n",
      "Iteration 180, loss = 0.23572504\n",
      "Iteration 137, loss = 0.24835690\n",
      "Iteration 181, loss = 0.23525032\n",
      "Iteration 182, loss = 0.23575997\n",
      "Iteration 138, loss = 0.24845640\n",
      "Iteration 183, loss = 0.23523269\n",
      "Iteration 139, loss = 0.24821513\n",
      "Iteration 184, loss = 0.23537814\n",
      "Iteration 140, loss = 0.24853533\n",
      "Iteration 185, loss = 0.23458481\n",
      "Iteration 141, loss = 0.24841159\n",
      "Iteration 186, loss = 0.23452212\n",
      "Iteration 142, loss = 0.24752376\n",
      "Iteration 187, loss = 0.23490926\n",
      "Iteration 143, loss = 0.24733640\n",
      "Iteration 188, loss = 0.23454030\n",
      "Iteration 144, loss = 0.24773297\n",
      "Iteration 189, loss = 0.23512717\n",
      "Iteration 145, loss = 0.24740179\n",
      "Iteration 190, loss = 0.23483641\n",
      "Iteration 146, loss = 0.24710128\n",
      "Iteration 191, loss = 0.23409983\n",
      "Iteration 147, loss = 0.24701049\n",
      "Iteration 192, loss = 0.23432984\n",
      "Iteration 148, loss = 0.24683871\n",
      "Iteration 193, loss = 0.23435654\n",
      "Iteration 149, loss = 0.24728805\n",
      "Iteration 194, loss = 0.23412941\n",
      "Iteration 150, loss = 0.24611262\n",
      "Iteration 195, loss = 0.23382181\n",
      "Iteration 151, loss = 0.24648532\n",
      "Iteration 196, loss = 0.23439464\n",
      "Iteration 152, loss = 0.24677678\n",
      "Iteration 197, loss = 0.23387683\n",
      "Iteration 153, loss = 0.24621080\n",
      "Iteration 198, loss = 0.23421034\n",
      "Iteration 154, loss = 0.24659793\n",
      "Iteration 199, loss = 0.23341812\n",
      "Iteration 155, loss = 0.24650246\n",
      "Iteration 200, loss = 0.23359580\n",
      "Iteration 156, loss = 0.24617188\n",
      "Iteration 157, loss = 0.24555896\n",
      "Iteration 1, loss = 0.37049642\n",
      "Iteration 158, loss = 0.24558960\n",
      "Iteration 159, loss = 0.24564419\n",
      "Iteration 2, loss = 0.32891406\n",
      "Iteration 160, loss = 0.24503218\n",
      "Iteration 3, loss = 0.31610313\n",
      "Iteration 161, loss = 0.24531009\n",
      "Iteration 4, loss = 0.30233703\n",
      "Iteration 5, loss = 0.29125562\n",
      "Iteration 162, loss = 0.24504954\n",
      "Iteration 6, loss = 0.28502673\n",
      "Iteration 7, loss = 0.28028772\n",
      "Iteration 8, loss = 0.27756279\n",
      "Iteration 163, loss = 0.24474442\n",
      "Iteration 9, loss = 0.27475044\n",
      "Iteration 10, loss = 0.27383956\n",
      "Iteration 164, loss = 0.24498298\n",
      "Iteration 11, loss = 0.27154435\n",
      "Iteration 12, loss = 0.27058283\n",
      "Iteration 165, loss = 0.24506667\n",
      "Iteration 13, loss = 0.26984071\n",
      "Iteration 166, loss = 0.24447506\n",
      "Iteration 167, loss = 0.24453391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14, loss = 0.26971631\n",
      "Iteration 168, loss = 0.24437446\n",
      "Iteration 15, loss = 0.26872438\n",
      "Iteration 169, loss = 0.24416577\n",
      "Iteration 170, loss = 0.24432221\n",
      "Iteration 16, loss = 0.26823666\n",
      "Iteration 171, loss = 0.24414292\n",
      "Iteration 17, loss = 0.26751911\n",
      "Iteration 172, loss = 0.24437659\n",
      "Iteration 18, loss = 0.26763367\n",
      "Iteration 173, loss = 0.24386387\n",
      "Iteration 19, loss = 0.26713101\n",
      "Iteration 174, loss = 0.24366284\n",
      "Iteration 20, loss = 0.26717111\n",
      "Iteration 175, loss = 0.24396705\n",
      "Iteration 21, loss = 0.26753829\n",
      "Iteration 176, loss = 0.24339960\n",
      "Iteration 22, loss = 0.26608193\n",
      "Iteration 177, loss = 0.24327127\n",
      "Iteration 178, loss = 0.24318246\n",
      "Iteration 23, loss = 0.26552900\n",
      "Iteration 179, loss = 0.24318245\n",
      "Iteration 24, loss = 0.26558035\n",
      "Iteration 180, loss = 0.24411031\n",
      "Iteration 25, loss = 0.26509465\n",
      "Iteration 181, loss = 0.24300262\n",
      "Iteration 26, loss = 0.26506580\n",
      "Iteration 182, loss = 0.24282835\n",
      "Iteration 27, loss = 0.26524808\n",
      "Iteration 183, loss = 0.24257915\n",
      "Iteration 28, loss = 0.26626946\n",
      "Iteration 184, loss = 0.24277227\n",
      "Iteration 185, loss = 0.24216498\n",
      "Iteration 29, loss = 0.26399028\n",
      "Iteration 30, loss = 0.26380799\n",
      "Iteration 186, loss = 0.24225254\n",
      "Iteration 31, loss = 0.26336119\n",
      "Iteration 187, loss = 0.24235552\n",
      "Iteration 32, loss = 0.26337516\n",
      "Iteration 188, loss = 0.24219582\n",
      "Iteration 33, loss = 0.26389431\n",
      "Iteration 189, loss = 0.24319154\n",
      "Iteration 34, loss = 0.26262670\n",
      "Iteration 190, loss = 0.24236303\n",
      "Iteration 35, loss = 0.26236571\n",
      "Iteration 191, loss = 0.24150476\n",
      "Iteration 36, loss = 0.26240392\n",
      "Iteration 192, loss = 0.24156394\n",
      "Iteration 37, loss = 0.26198397\n",
      "Iteration 193, loss = 0.24180911\n",
      "Iteration 38, loss = 0.26285727\n",
      "Iteration 194, loss = 0.24120459\n",
      "Iteration 39, loss = 0.26238798\n",
      "Iteration 195, loss = 0.24115208\n",
      "Iteration 40, loss = 0.26154527\n",
      "Iteration 196, loss = 0.24128526\n",
      "Iteration 41, loss = 0.26165144\n",
      "Iteration 197, loss = 0.24105499\n",
      "Iteration 42, loss = 0.26125547\n",
      "Iteration 198, loss = 0.24130787\n",
      "Iteration 43, loss = 0.26059005\n",
      "Iteration 199, loss = 0.24060028\n",
      "Iteration 44, loss = 0.26069301\n",
      "Iteration 200, loss = 0.24080268\n",
      "Iteration 45, loss = 0.26053220\n",
      "Iteration 46, loss = 0.26031111\n",
      "Iteration 47, loss = 0.26002269\n",
      "Iteration 1, loss = 0.37048704\n",
      "Iteration 48, loss = 0.25981529\n",
      "Iteration 2, loss = 0.32933663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 49, loss = 0.25940112\n",
      "Iteration 3, loss = 0.31673233\n",
      "Iteration 50, loss = 0.25903021\n",
      "Iteration 4, loss = 0.30362141\n",
      "Iteration 51, loss = 0.25909618\n",
      "Iteration 5, loss = 0.29263179\n",
      "Iteration 52, loss = 0.25859230\n",
      "Iteration 6, loss = 0.28616844\n",
      "Iteration 53, loss = 0.25829413\n",
      "Iteration 7, loss = 0.28138855\n",
      "Iteration 54, loss = 0.26003683\n",
      "Iteration 8, loss = 0.27840354\n",
      "Iteration 55, loss = 0.25814840\n",
      "Iteration 9, loss = 0.27601990\n",
      "Iteration 56, loss = 0.25742102\n",
      "Iteration 10, loss = 0.27460701\n",
      "Iteration 57, loss = 0.25725781\n",
      "Iteration 11, loss = 0.27253205\n",
      "Iteration 58, loss = 0.25698908\n",
      "Iteration 12, loss = 0.27161547\n",
      "Iteration 59, loss = 0.25710696\n",
      "Iteration 13, loss = 0.27055636\n",
      "Iteration 60, loss = 0.25688547\n",
      "Iteration 14, loss = 0.27058168\n",
      "Iteration 61, loss = 0.25668450\n",
      "Iteration 15, loss = 0.26928211\n",
      "Iteration 62, loss = 0.25637457\n",
      "Iteration 16, loss = 0.26897155\n",
      "Iteration 63, loss = 0.25594350\n",
      "Iteration 17, loss = 0.26822457\n",
      "Iteration 64, loss = 0.25545599\n",
      "Iteration 18, loss = 0.26793910\n",
      "Iteration 65, loss = 0.25541376\n",
      "Iteration 19, loss = 0.26757481\n",
      "Iteration 66, loss = 0.25542288\n",
      "Iteration 20, loss = 0.26752494\n",
      "Iteration 67, loss = 0.25482783\n",
      "Iteration 21, loss = 0.26737265\n",
      "Iteration 68, loss = 0.25508486\n",
      "Iteration 22, loss = 0.26631101\n",
      "Iteration 69, loss = 0.25468024\n",
      "Iteration 23, loss = 0.26588144\n",
      "Iteration 70, loss = 0.25488195\n",
      "Iteration 24, loss = 0.26563498\n",
      "Iteration 71, loss = 0.25461253\n",
      "Iteration 25, loss = 0.26549540\n",
      "Iteration 72, loss = 0.25398926\n",
      "Iteration 26, loss = 0.26507119\n",
      "Iteration 73, loss = 0.25420682\n",
      "Iteration 27, loss = 0.26539979\n",
      "Iteration 74, loss = 0.25349443\n",
      "Iteration 28, loss = 0.26638089\n",
      "Iteration 75, loss = 0.25353118\n",
      "Iteration 29, loss = 0.26445538\n",
      "Iteration 76, loss = 0.25367821\n",
      "Iteration 30, loss = 0.26457845\n",
      "Iteration 77, loss = 0.25311613\n",
      "Iteration 31, loss = 0.26380317\n",
      "Iteration 78, loss = 0.25326904\n",
      "Iteration 32, loss = 0.26376547\n",
      "Iteration 79, loss = 0.25321091\n",
      "Iteration 33, loss = 0.26501507\n",
      "Iteration 80, loss = 0.25315472\n",
      "Iteration 34, loss = 0.26353104\n",
      "Iteration 81, loss = 0.25344753\n",
      "Iteration 35, loss = 0.26312753\n",
      "Iteration 82, loss = 0.25214191\n",
      "Iteration 36, loss = 0.26314622\n",
      "Iteration 83, loss = 0.25190000\n",
      "Iteration 37, loss = 0.26278732\n",
      "Iteration 84, loss = 0.25158695\n",
      "Iteration 38, loss = 0.26307984\n",
      "Iteration 85, loss = 0.25201017\n",
      "Iteration 39, loss = 0.26306854\n",
      "Iteration 86, loss = 0.25159617\n",
      "Iteration 40, loss = 0.26227065\n",
      "Iteration 87, loss = 0.25226064\n",
      "Iteration 41, loss = 0.26299710\n",
      "Iteration 88, loss = 0.25126196\n",
      "Iteration 42, loss = 0.26187983\n",
      "Iteration 89, loss = 0.25124290\n",
      "Iteration 43, loss = 0.26171827\n",
      "Iteration 44, loss = 0.26226336\n",
      "Iteration 90, loss = 0.25089831\n",
      "Iteration 45, loss = 0.26130049\n",
      "Iteration 91, loss = 0.25085050\n",
      "Iteration 46, loss = 0.26088332\n",
      "Iteration 92, loss = 0.25053954\n",
      "Iteration 47, loss = 0.26092103\n",
      "Iteration 93, loss = 0.25040469\n",
      "Iteration 48, loss = 0.26103087\n",
      "Iteration 94, loss = 0.25040572\n",
      "Iteration 49, loss = 0.26051836\n",
      "Iteration 95, loss = 0.25003149\n",
      "Iteration 50, loss = 0.26013089\n",
      "Iteration 96, loss = 0.25010673\n",
      "Iteration 51, loss = 0.25968115\n",
      "Iteration 97, loss = 0.24978717\n",
      "Iteration 52, loss = 0.25943600\n",
      "Iteration 98, loss = 0.24954727\n",
      "Iteration 53, loss = 0.25946614\n",
      "Iteration 99, loss = 0.24937391\n",
      "Iteration 54, loss = 0.25999382\n",
      "Iteration 100, loss = 0.24991618\n",
      "Iteration 55, loss = 0.25906876\n",
      "Iteration 101, loss = 0.24896149\n",
      "Iteration 56, loss = 0.25835438\n",
      "Iteration 102, loss = 0.24922608\n",
      "Iteration 57, loss = 0.25830914\n",
      "Iteration 103, loss = 0.24942714\n",
      "Iteration 58, loss = 0.25792531\n",
      "Iteration 104, loss = 0.24929390\n",
      "Iteration 59, loss = 0.25793050\n",
      "Iteration 105, loss = 0.24878363\n",
      "Iteration 60, loss = 0.25779854\n",
      "Iteration 106, loss = 0.24823867\n",
      "Iteration 61, loss = 0.25732293\n",
      "Iteration 107, loss = 0.24809021\n",
      "Iteration 62, loss = 0.25716852\n",
      "Iteration 108, loss = 0.24866257\n",
      "Iteration 63, loss = 0.25716215\n",
      "Iteration 109, loss = 0.24755161\n",
      "Iteration 64, loss = 0.25657969\n",
      "Iteration 110, loss = 0.24770748\n",
      "Iteration 65, loss = 0.25632114\n",
      "Iteration 66, loss = 0.25620793\n",
      "Iteration 111, loss = 0.24841686\n",
      "Iteration 67, loss = 0.25562067\n",
      "Iteration 68, loss = 0.25572893\n",
      "Iteration 112, loss = 0.24779483\n",
      "Iteration 69, loss = 0.25540506\n",
      "Iteration 113, loss = 0.24688694\n",
      "Iteration 70, loss = 0.25525469\n",
      "Iteration 114, loss = 0.24808111\n",
      "Iteration 71, loss = 0.25508919\n",
      "Iteration 115, loss = 0.24712409\n",
      "Iteration 72, loss = 0.25449967\n",
      "Iteration 116, loss = 0.24661145\n",
      "Iteration 73, loss = 0.25472049\n",
      "Iteration 117, loss = 0.24662837\n",
      "Iteration 74, loss = 0.25417082\n",
      "Iteration 118, loss = 0.24672267\n",
      "Iteration 119, loss = 0.24613462\n",
      "Iteration 75, loss = 0.25443985\n",
      "Iteration 120, loss = 0.24704970\n",
      "Iteration 76, loss = 0.25371301\n",
      "Iteration 121, loss = 0.24552884\n",
      "Iteration 77, loss = 0.25353910\n",
      "Iteration 78, loss = 0.25320672\n",
      "Iteration 122, loss = 0.24601696\n",
      "Iteration 79, loss = 0.25299104\n",
      "Iteration 123, loss = 0.24587702\n",
      "Iteration 80, loss = 0.25320264\n",
      "Iteration 124, loss = 0.24612423\n",
      "Iteration 81, loss = 0.25329414\n",
      "Iteration 125, loss = 0.24569700\n",
      "Iteration 82, loss = 0.25260373\n",
      "Iteration 126, loss = 0.24527391\n",
      "Iteration 127, loss = 0.24726947\n",
      "Iteration 83, loss = 0.25216612\n",
      "Iteration 84, loss = 0.25222077\n",
      "Iteration 128, loss = 0.24513921\n",
      "Iteration 85, loss = 0.25185232\n",
      "Iteration 86, loss = 0.25199229\n",
      "Iteration 129, loss = 0.24442639\n",
      "Iteration 87, loss = 0.25161819\n",
      "Iteration 130, loss = 0.24435196\n",
      "Iteration 88, loss = 0.25190382\n",
      "Iteration 89, loss = 0.25130773\n",
      "Iteration 131, loss = 0.24441745\n",
      "Iteration 90, loss = 0.25106262\n",
      "Iteration 132, loss = 0.24427054\n",
      "Iteration 91, loss = 0.25038581Iteration 133, loss = 0.24391317\n",
      "\n",
      "Iteration 134, loss = 0.24398634\n",
      "Iteration 92, loss = 0.25040478\n",
      "Iteration 135, loss = 0.24350889\n",
      "Iteration 93, loss = 0.25005393\n",
      "Iteration 136, loss = 0.24404396\n",
      "Iteration 94, loss = 0.24987390\n",
      "Iteration 137, loss = 0.24377511\n",
      "Iteration 95, loss = 0.24988298\n",
      "Iteration 138, loss = 0.24406234\n",
      "Iteration 96, loss = 0.24943317\n",
      "Iteration 139, loss = 0.24329338\n",
      "Iteration 97, loss = 0.24887373\n",
      "Iteration 140, loss = 0.24390379\n",
      "Iteration 98, loss = 0.24881687\n",
      "Iteration 141, loss = 0.24307098Iteration 99, loss = 0.24858730\n",
      "\n",
      "Iteration 142, loss = 0.24282636\n",
      "Iteration 100, loss = 0.24890494\n",
      "Iteration 143, loss = 0.24260118\n",
      "Iteration 101, loss = 0.24800496\n",
      "Iteration 144, loss = 0.24240151\n",
      "Iteration 102, loss = 0.24798412\n",
      "Iteration 145, loss = 0.24223639\n",
      "Iteration 103, loss = 0.24818287\n",
      "Iteration 146, loss = 0.24204369\n",
      "Iteration 104, loss = 0.24820063\n",
      "Iteration 105, loss = 0.24759237\n",
      "Iteration 147, loss = 0.24258592\n",
      "Iteration 148, loss = 0.24191634Iteration 106, loss = 0.24705280\n",
      "\n",
      "Iteration 149, loss = 0.24207830\n",
      "Iteration 107, loss = 0.24669063\n",
      "Iteration 150, loss = 0.24158331\n",
      "Iteration 108, loss = 0.24718208\n",
      "Iteration 151, loss = 0.24179820\n",
      "Iteration 152, loss = 0.24137839\n",
      "Iteration 109, loss = 0.24621921\n",
      "Iteration 153, loss = 0.24095849\n",
      "Iteration 110, loss = 0.24646754Iteration 154, loss = 0.24108628\n",
      "\n",
      "Iteration 111, loss = 0.24673812\n",
      "Iteration 112, loss = 0.24573480Iteration 155, loss = 0.24085562\n",
      "\n",
      "Iteration 156, loss = 0.24062722\n",
      "Iteration 157, loss = 0.24094049\n",
      "Iteration 113, loss = 0.24547000\n",
      "Iteration 158, loss = 0.24049176\n",
      "Iteration 159, loss = 0.24028162\n",
      "Iteration 114, loss = 0.24627683\n",
      "Iteration 160, loss = 0.24009692\n",
      "Iteration 161, loss = 0.24039120\n",
      "Iteration 115, loss = 0.24533115\n",
      "Iteration 162, loss = 0.23978729\n",
      "Iteration 116, loss = 0.24543194\n",
      "Iteration 117, loss = 0.24473568\n",
      "Iteration 163, loss = 0.24023324\n",
      "Iteration 118, loss = 0.24466471\n",
      "Iteration 119, loss = 0.24435039\n",
      "Iteration 164, loss = 0.23962804\n",
      "Iteration 120, loss = 0.24499050\n",
      "Iteration 165, loss = 0.23937687Iteration 121, loss = 0.24382265\n",
      "\n",
      "Iteration 122, loss = 0.24448288\n",
      "Iteration 166, loss = 0.23985080\n",
      "Iteration 123, loss = 0.24367297\n",
      "Iteration 167, loss = 0.23925568\n",
      "Iteration 124, loss = 0.24376841\n",
      "Iteration 168, loss = 0.23952523\n",
      "Iteration 125, loss = 0.24373067\n",
      "Iteration 169, loss = 0.23915824\n",
      "Iteration 126, loss = 0.24274348\n",
      "Iteration 170, loss = 0.23914499\n",
      "Iteration 127, loss = 0.24340759\n",
      "Iteration 171, loss = 0.23894423\n",
      "Iteration 128, loss = 0.24276862\n",
      "Iteration 172, loss = 0.23909201\n",
      "Iteration 129, loss = 0.24245971\n",
      "Iteration 173, loss = 0.23870585\n",
      "Iteration 130, loss = 0.24214672\n",
      "Iteration 174, loss = 0.23878801\n",
      "Iteration 131, loss = 0.24260419\n",
      "Iteration 175, loss = 0.23864522\n",
      "Iteration 132, loss = 0.24211685\n",
      "Iteration 176, loss = 0.23843409\n",
      "Iteration 133, loss = 0.24167828\n",
      "Iteration 177, loss = 0.23820133\n",
      "Iteration 134, loss = 0.24183386\n",
      "Iteration 178, loss = 0.23835515\n",
      "Iteration 135, loss = 0.24142014\n",
      "Iteration 136, loss = 0.24119282\n",
      "Iteration 137, loss = 0.24230514\n",
      "Iteration 179, loss = 0.23842201\n",
      "Iteration 180, loss = 0.23846060\n",
      "Iteration 181, loss = 0.23790233\n",
      "Iteration 138, loss = 0.24190915\n",
      "Iteration 182, loss = 0.23770220\n",
      "Iteration 139, loss = 0.24075816\n",
      "Iteration 183, loss = 0.23806255\n",
      "Iteration 140, loss = 0.24102647\n",
      "Iteration 184, loss = 0.23771815\n",
      "Iteration 185, loss = 0.23767104\n",
      "Iteration 141, loss = 0.24075751\n",
      "Iteration 142, loss = 0.24058283\n",
      "Iteration 186, loss = 0.23745547\n",
      "Iteration 187, loss = 0.23780323\n",
      "Iteration 143, loss = 0.24013988\n",
      "Iteration 188, loss = 0.23709098\n",
      "Iteration 189, loss = 0.23698605\n",
      "Iteration 190, loss = 0.23715446\n",
      "Iteration 144, loss = 0.24022595\n",
      "Iteration 191, loss = 0.23690252\n",
      "Iteration 145, loss = 0.24002343\n",
      "Iteration 192, loss = 0.23664094\n",
      "Iteration 146, loss = 0.23970850\n",
      "Iteration 193, loss = 0.23684997Iteration 147, loss = 0.24004971\n",
      "\n",
      "Iteration 148, loss = 0.23956919\n",
      "Iteration 194, loss = 0.23666884\n",
      "Iteration 149, loss = 0.23994657\n",
      "Iteration 195, loss = 0.23648077\n",
      "Iteration 150, loss = 0.23914432\n",
      "Iteration 196, loss = 0.23657006\n",
      "Iteration 151, loss = 0.23972395\n",
      "Iteration 197, loss = 0.23692389\n",
      "Iteration 152, loss = 0.23897411\n",
      "Iteration 198, loss = 0.23716690\n",
      "Iteration 153, loss = 0.23897456\n",
      "Iteration 199, loss = 0.23670948\n",
      "Iteration 154, loss = 0.23873862\n",
      "Iteration 200, loss = 0.23623710\n",
      "Iteration 155, loss = 0.23854327\n",
      "Iteration 156, loss = 0.23865217\n",
      "Iteration 157, loss = 0.23962979\n",
      "Iteration 1, loss = 0.37044133\n",
      "Iteration 158, loss = 0.23814564\n",
      "Iteration 2, loss = 0.32921287\n",
      "Iteration 159, loss = 0.23807121\n",
      "Iteration 3, loss = 0.31637879\n",
      "Iteration 160, loss = 0.23803019\n",
      "Iteration 4, loss = 0.30320704\n",
      "Iteration 161, loss = 0.23822053\n",
      "Iteration 5, loss = 0.29252908\n",
      "Iteration 162, loss = 0.23778804\n",
      "Iteration 6, loss = 0.28589511\n",
      "Iteration 163, loss = 0.23808101\n",
      "Iteration 7, loss = 0.28130873\n",
      "Iteration 164, loss = 0.23750860\n",
      "Iteration 8, loss = 0.27821722\n",
      "Iteration 165, loss = 0.23722242\n",
      "Iteration 9, loss = 0.27545484\n",
      "Iteration 166, loss = 0.23769671\n",
      "Iteration 10, loss = 0.27396379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 167, loss = 0.23775866\n",
      "Iteration 11, loss = 0.27248044\n",
      "Iteration 168, loss = 0.23740511\n",
      "Iteration 12, loss = 0.27103332\n",
      "Iteration 13, loss = 0.27021701\n",
      "Iteration 169, loss = 0.23714322\n",
      "Iteration 14, loss = 0.26975338\n",
      "Iteration 170, loss = 0.23757813\n",
      "Iteration 15, loss = 0.26866062Iteration 171, loss = 0.23747000\n",
      "\n",
      "Iteration 172, loss = 0.23712461\n",
      "Iteration 16, loss = 0.26814654\n",
      "Iteration 173, loss = 0.23705609\n",
      "Iteration 17, loss = 0.26741005\n",
      "Iteration 174, loss = 0.23679199\n",
      "Iteration 18, loss = 0.26722406\n",
      "Iteration 19, loss = 0.26662940\n",
      "Iteration 175, loss = 0.23659421\n",
      "Iteration 20, loss = 0.26630506\n",
      "Iteration 176, loss = 0.23664703\n",
      "Iteration 21, loss = 0.26587744\n",
      "Iteration 177, loss = 0.23640836\n",
      "Iteration 22, loss = 0.26536622\n",
      "Iteration 178, loss = 0.23670624\n",
      "Iteration 23, loss = 0.26507337\n",
      "Iteration 179, loss = 0.23616621\n",
      "Iteration 24, loss = 0.26499887Iteration 180, loss = 0.23663408\n",
      "\n",
      "Iteration 181, loss = 0.23643074\n",
      "Iteration 25, loss = 0.26472580\n",
      "Iteration 182, loss = 0.23603753\n",
      "Iteration 26, loss = 0.26438465\n",
      "Iteration 183, loss = 0.23593868\n",
      "Iteration 27, loss = 0.26439455\n",
      "Iteration 28, loss = 0.26483921\n",
      "Iteration 184, loss = 0.23641675\n",
      "Iteration 185, loss = 0.23591993\n",
      "Iteration 29, loss = 0.26384619\n",
      "Iteration 186, loss = 0.23560746\n",
      "Iteration 30, loss = 0.26383788\n",
      "Iteration 187, loss = 0.23575793\n",
      "Iteration 31, loss = 0.26332577\n",
      "Iteration 188, loss = 0.23568285\n",
      "Iteration 32, loss = 0.26317841\n",
      "Iteration 189, loss = 0.23532842\n",
      "Iteration 33, loss = 0.26411473\n",
      "Iteration 190, loss = 0.23534177\n",
      "Iteration 34, loss = 0.26331134\n",
      "Iteration 191, loss = 0.23543199\n",
      "Iteration 35, loss = 0.26274755\n",
      "Iteration 192, loss = 0.23516343\n",
      "Iteration 36, loss = 0.26253502\n",
      "Iteration 193, loss = 0.23499250\n",
      "Iteration 37, loss = 0.26222964\n",
      "Iteration 194, loss = 0.23507693\n",
      "Iteration 38, loss = 0.26243635\n",
      "Iteration 195, loss = 0.23501390\n",
      "Iteration 39, loss = 0.26209335\n",
      "Iteration 196, loss = 0.23472721\n",
      "Iteration 40, loss = 0.26163091\n",
      "Iteration 197, loss = 0.23570302\n",
      "Iteration 41, loss = 0.26247940\n",
      "Iteration 198, loss = 0.23577423\n",
      "Iteration 42, loss = 0.26123650\n",
      "Iteration 199, loss = 0.23509179\n",
      "Iteration 43, loss = 0.26109094\n",
      "Iteration 200, loss = 0.23479544\n",
      "Iteration 44, loss = 0.26156977\n",
      "Iteration 45, loss = 0.26078409\n",
      "Iteration 46, loss = 0.26019019\n",
      "Iteration 1, loss = 0.37048263\n",
      "Iteration 47, loss = 0.26038000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.32909498\n",
      "Iteration 48, loss = 0.26067726\n",
      "Iteration 3, loss = 0.31651493\n",
      "Iteration 49, loss = 0.25999806\n",
      "Iteration 4, loss = 0.30287050\n",
      "Iteration 5, loss = 0.29215113Iteration 50, loss = 0.25918229\n",
      "\n",
      "Iteration 6, loss = 0.28543176\n",
      "Iteration 51, loss = 0.25887627\n",
      "Iteration 7, loss = 0.28119470\n",
      "Iteration 52, loss = 0.25882238\n",
      "Iteration 8, loss = 0.27764893\n",
      "Iteration 53, loss = 0.25853396\n",
      "Iteration 9, loss = 0.27546412\n",
      "Iteration 10, loss = 0.27434724\n",
      "Iteration 54, loss = 0.25914674\n",
      "Iteration 55, loss = 0.25859519\n",
      "Iteration 11, loss = 0.27239797\n",
      "Iteration 56, loss = 0.25746488\n",
      "Iteration 12, loss = 0.27075204\n",
      "Iteration 13, loss = 0.27006284\n",
      "Iteration 57, loss = 0.25768367\n",
      "Iteration 14, loss = 0.26942207\n",
      "Iteration 58, loss = 0.25704286\n",
      "Iteration 15, loss = 0.26852583\n",
      "Iteration 59, loss = 0.25709887\n",
      "Iteration 16, loss = 0.26849103\n",
      "Iteration 60, loss = 0.25710429\n",
      "Iteration 17, loss = 0.26740614\n",
      "Iteration 61, loss = 0.25626271\n",
      "Iteration 18, loss = 0.26702052\n",
      "Iteration 62, loss = 0.25600403\n",
      "Iteration 19, loss = 0.26653229\n",
      "Iteration 63, loss = 0.25702242\n",
      "Iteration 20, loss = 0.26610559\n",
      "Iteration 64, loss = 0.25589996\n",
      "Iteration 21, loss = 0.26579126\n",
      "Iteration 65, loss = 0.25509476\n",
      "Iteration 22, loss = 0.26529393\n",
      "Iteration 66, loss = 0.25588438\n",
      "Iteration 23, loss = 0.26486940\n",
      "Iteration 67, loss = 0.25484332\n",
      "Iteration 24, loss = 0.26509616\n",
      "Iteration 68, loss = 0.25506419\n",
      "Iteration 25, loss = 0.26460267\n",
      "Iteration 69, loss = 0.25469900\n",
      "Iteration 26, loss = 0.26445824\n",
      "Iteration 70, loss = 0.25416108\n",
      "Iteration 27, loss = 0.26412251\n",
      "Iteration 71, loss = 0.25407241\n",
      "Iteration 28, loss = 0.26460510\n",
      "Iteration 29, loss = 0.26342068\n",
      "Iteration 72, loss = 0.25350038\n",
      "Iteration 30, loss = 0.26362984\n",
      "Iteration 31, loss = 0.26272304\n",
      "Iteration 73, loss = 0.25430259\n",
      "Iteration 32, loss = 0.26236343\n",
      "Iteration 74, loss = 0.25347861\n",
      "Iteration 33, loss = 0.26400299\n",
      "Iteration 75, loss = 0.25322353\n",
      "Iteration 34, loss = 0.26235250\n",
      "Iteration 76, loss = 0.25295899\n",
      "Iteration 35, loss = 0.26207978\n",
      "Iteration 77, loss = 0.25254959\n",
      "Iteration 36, loss = 0.26183546\n",
      "Iteration 78, loss = 0.25212045\n",
      "Iteration 37, loss = 0.26155713\n",
      "Iteration 79, loss = 0.25211172\n",
      "Iteration 80, loss = 0.25192094\n",
      "Iteration 38, loss = 0.26163546\n",
      "Iteration 39, loss = 0.26112068\n",
      "Iteration 81, loss = 0.25285056\n",
      "Iteration 40, loss = 0.26101870\n",
      "Iteration 82, loss = 0.25160611\n",
      "Iteration 41, loss = 0.26125954\n",
      "Iteration 83, loss = 0.25089438\n",
      "Iteration 42, loss = 0.26022217\n",
      "Iteration 84, loss = 0.25098999\n",
      "Iteration 43, loss = 0.26000757\n",
      "Iteration 85, loss = 0.25117491\n",
      "Iteration 44, loss = 0.26097705\n",
      "Iteration 86, loss = 0.25126030\n",
      "Iteration 45, loss = 0.25995050\n",
      "Iteration 46, loss = 0.25913695\n",
      "Iteration 87, loss = 0.25058689\n",
      "Iteration 88, loss = 0.25001299Iteration 47, loss = 0.25973866\n",
      "\n",
      "Iteration 48, loss = 0.25949959\n",
      "Iteration 89, loss = 0.25010181\n",
      "Iteration 49, loss = 0.25880683\n",
      "Iteration 90, loss = 0.24990041\n",
      "Iteration 50, loss = 0.25848786\n",
      "Iteration 91, loss = 0.24935724\n",
      "Iteration 51, loss = 0.25811815\n",
      "Iteration 92, loss = 0.24948417\n",
      "Iteration 52, loss = 0.25863364\n",
      "Iteration 93, loss = 0.24931613\n",
      "Iteration 53, loss = 0.25766530\n",
      "Iteration 94, loss = 0.24875182\n",
      "Iteration 54, loss = 0.25823647\n",
      "Iteration 95, loss = 0.24861998\n",
      "Iteration 55, loss = 0.25786378\n",
      "Iteration 96, loss = 0.24838531\n",
      "Iteration 56, loss = 0.25701344\n",
      "Iteration 97, loss = 0.24834284\n",
      "Iteration 57, loss = 0.25677310\n",
      "Iteration 98, loss = 0.24774661\n",
      "Iteration 58, loss = 0.25630167\n",
      "Iteration 59, loss = 0.25643888\n",
      "Iteration 99, loss = 0.24776499\n",
      "Iteration 60, loss = 0.25625682\n",
      "Iteration 100, loss = 0.24814372\n",
      "Iteration 61, loss = 0.25577019\n",
      "Iteration 101, loss = 0.24721651\n",
      "Iteration 62, loss = 0.25568594\n",
      "Iteration 102, loss = 0.24809123\n",
      "Iteration 63, loss = 0.25660641\n",
      "Iteration 103, loss = 0.24720824\n",
      "Iteration 64, loss = 0.25493508\n",
      "Iteration 104, loss = 0.24705324\n",
      "Iteration 65, loss = 0.25482072\n",
      "Iteration 105, loss = 0.24675270\n",
      "Iteration 66, loss = 0.25506659\n",
      "Iteration 106, loss = 0.24648441\n",
      "Iteration 67, loss = 0.25465288\n",
      "Iteration 107, loss = 0.24629199\n",
      "Iteration 68, loss = 0.25455533\n",
      "Iteration 108, loss = 0.24655984\n",
      "Iteration 69, loss = 0.25475357\n",
      "Iteration 109, loss = 0.24623632\n",
      "Iteration 70, loss = 0.25390899\n",
      "Iteration 110, loss = 0.24567023\n",
      "Iteration 71, loss = 0.25396379\n",
      "Iteration 111, loss = 0.24588739Iteration 72, loss = 0.25311682\n",
      "\n",
      "Iteration 73, loss = 0.25468381\n",
      "Iteration 112, loss = 0.24552437\n",
      "Iteration 113, loss = 0.24545473\n",
      "Iteration 74, loss = 0.25367978\n",
      "Iteration 114, loss = 0.24585150\n",
      "Iteration 75, loss = 0.25285645\n",
      "Iteration 115, loss = 0.24516798\n",
      "Iteration 76, loss = 0.25258029\n",
      "Iteration 77, loss = 0.25233208\n",
      "Iteration 116, loss = 0.24491387\n",
      "Iteration 78, loss = 0.25191987\n",
      "Iteration 117, loss = 0.24450958\n",
      "Iteration 79, loss = 0.25234197\n",
      "Iteration 118, loss = 0.24461437\n",
      "Iteration 80, loss = 0.25220219\n",
      "Iteration 119, loss = 0.24431196\n",
      "Iteration 81, loss = 0.25169669\n",
      "Iteration 120, loss = 0.24455570\n",
      "Iteration 82, loss = 0.25155249\n",
      "Iteration 121, loss = 0.24398147\n",
      "Iteration 83, loss = 0.25118270\n",
      "Iteration 122, loss = 0.24398255\n",
      "Iteration 84, loss = 0.25123449\n",
      "Iteration 123, loss = 0.24385569\n",
      "Iteration 124, loss = 0.24367063\n",
      "Iteration 85, loss = 0.25108227\n",
      "Iteration 86, loss = 0.25117035\n",
      "Iteration 87, loss = 0.25058755\n",
      "Iteration 125, loss = 0.24363699\n",
      "Iteration 88, loss = 0.25043336\n",
      "Iteration 89, loss = 0.25031828\n",
      "Iteration 126, loss = 0.24307902\n",
      "Iteration 90, loss = 0.25018430\n",
      "Iteration 91, loss = 0.24975099\n",
      "Iteration 127, loss = 0.24339327Iteration 92, loss = 0.25027789\n",
      "\n",
      "Iteration 93, loss = 0.24959106\n",
      "Iteration 128, loss = 0.24301035\n",
      "Iteration 94, loss = 0.24919697\n",
      "Iteration 95, loss = 0.24906946\n",
      "Iteration 129, loss = 0.24278500\n",
      "Iteration 96, loss = 0.24902013\n",
      "Iteration 130, loss = 0.24275869\n",
      "Iteration 131, loss = 0.24271563\n",
      "Iteration 97, loss = 0.24911345\n",
      "Iteration 132, loss = 0.24274473\n",
      "Iteration 98, loss = 0.24859280\n",
      "Iteration 133, loss = 0.24204758\n",
      "Iteration 99, loss = 0.24840423\n",
      "Iteration 134, loss = 0.24307215\n",
      "Iteration 100, loss = 0.24843049\n",
      "Iteration 101, loss = 0.24798898\n",
      "Iteration 135, loss = 0.24204538\n",
      "Iteration 102, loss = 0.24821109\n",
      "Iteration 136, loss = 0.24215901\n",
      "Iteration 103, loss = 0.24802405\n",
      "Iteration 137, loss = 0.24206269\n",
      "Iteration 104, loss = 0.24770863\n",
      "Iteration 105, loss = 0.24776941\n",
      "Iteration 138, loss = 0.24262093\n",
      "Iteration 106, loss = 0.24729731\n",
      "Iteration 139, loss = 0.24223056\n",
      "Iteration 107, loss = 0.24718577\n",
      "Iteration 140, loss = 0.24174049\n",
      "Iteration 108, loss = 0.24747099\n",
      "Iteration 141, loss = 0.24175454\n",
      "Iteration 109, loss = 0.24700711\n",
      "Iteration 142, loss = 0.24140745\n",
      "Iteration 110, loss = 0.24644814\n",
      "Iteration 143, loss = 0.24120305\n",
      "Iteration 111, loss = 0.24660120\n",
      "Iteration 144, loss = 0.24106457\n",
      "Iteration 112, loss = 0.24619179\n",
      "Iteration 145, loss = 0.24099352\n",
      "Iteration 113, loss = 0.24622841\n",
      "Iteration 146, loss = 0.24089026\n",
      "Iteration 147, loss = 0.24096193\n",
      "Iteration 114, loss = 0.24731506\n",
      "Iteration 115, loss = 0.24612271\n",
      "Iteration 148, loss = 0.24077655\n",
      "Iteration 116, loss = 0.24605128\n",
      "Iteration 117, loss = 0.24551468\n",
      "Iteration 149, loss = 0.24073403\n",
      "Iteration 150, loss = 0.24053380\n",
      "Iteration 118, loss = 0.24583536\n",
      "Iteration 151, loss = 0.24092392\n",
      "Iteration 119, loss = 0.24551144\n",
      "Iteration 120, loss = 0.24553977\n",
      "Iteration 121, loss = 0.24520836\n",
      "Iteration 152, loss = 0.24029523\n",
      "Iteration 122, loss = 0.24504505\n",
      "Iteration 123, loss = 0.24507383\n",
      "Iteration 153, loss = 0.23996308\n",
      "Iteration 124, loss = 0.24490575\n",
      "Iteration 154, loss = 0.24019564\n",
      "Iteration 125, loss = 0.24468598\n",
      "Iteration 126, loss = 0.24446392\n",
      "Iteration 155, loss = 0.24017260\n",
      "Iteration 127, loss = 0.24467956\n",
      "Iteration 128, loss = 0.24425806\n",
      "Iteration 156, loss = 0.23991635\n",
      "Iteration 129, loss = 0.24423161\n",
      "Iteration 130, loss = 0.24405714\n",
      "Iteration 157, loss = 0.24210404\n",
      "Iteration 131, loss = 0.24407435\n",
      "Iteration 132, loss = 0.24416476\n",
      "Iteration 158, loss = 0.23954492Iteration 133, loss = 0.24355339\n",
      "\n",
      "Iteration 134, loss = 0.24391639\n",
      "Iteration 135, loss = 0.24351941\n",
      "Iteration 159, loss = 0.23937074\n",
      "Iteration 136, loss = 0.24334082\n",
      "Iteration 137, loss = 0.24317588\n",
      "Iteration 160, loss = 0.23943681\n",
      "Iteration 161, loss = 0.23943463\n",
      "Iteration 138, loss = 0.24399019\n",
      "Iteration 162, loss = 0.23917112\n",
      "Iteration 139, loss = 0.24348808\n",
      "Iteration 163, loss = 0.23981989\n",
      "Iteration 164, loss = 0.23896560\n",
      "Iteration 140, loss = 0.24312622\n",
      "Iteration 165, loss = 0.23890836\n",
      "Iteration 166, loss = 0.23941004\n",
      "Iteration 141, loss = 0.24327609\n",
      "Iteration 167, loss = 0.23886965\n",
      "Iteration 142, loss = 0.24315833\n",
      "Iteration 168, loss = 0.23873569\n",
      "Iteration 143, loss = 0.24256455\n",
      "Iteration 169, loss = 0.23880441\n",
      "Iteration 144, loss = 0.24275657\n",
      "Iteration 170, loss = 0.23865111\n",
      "Iteration 145, loss = 0.24256587\n",
      "Iteration 171, loss = 0.23830935\n",
      "Iteration 146, loss = 0.24241907\n",
      "Iteration 172, loss = 0.23833139\n",
      "Iteration 147, loss = 0.24223652\n",
      "Iteration 173, loss = 0.23779127\n",
      "Iteration 148, loss = 0.24221234\n",
      "Iteration 174, loss = 0.23798779\n",
      "Iteration 149, loss = 0.24212048\n",
      "Iteration 175, loss = 0.23774867\n",
      "Iteration 150, loss = 0.24223672\n",
      "Iteration 176, loss = 0.23816330\n",
      "Iteration 151, loss = 0.24186946\n",
      "Iteration 177, loss = 0.23761285\n",
      "Iteration 152, loss = 0.24170897\n",
      "Iteration 178, loss = 0.23765290\n",
      "Iteration 153, loss = 0.24177174\n",
      "Iteration 179, loss = 0.23774028\n",
      "Iteration 154, loss = 0.24171558\n",
      "Iteration 180, loss = 0.23777014\n",
      "Iteration 155, loss = 0.24184399\n",
      "Iteration 181, loss = 0.23743737\n",
      "Iteration 156, loss = 0.24180535\n",
      "Iteration 182, loss = 0.23718083\n",
      "Iteration 157, loss = 0.24237282\n",
      "Iteration 183, loss = 0.23746200\n",
      "Iteration 158, loss = 0.24152910\n",
      "Iteration 184, loss = 0.23730964\n",
      "Iteration 185, loss = 0.23754216\n",
      "Iteration 159, loss = 0.24107308\n",
      "Iteration 186, loss = 0.23695529\n",
      "Iteration 160, loss = 0.24127023\n",
      "Iteration 187, loss = 0.23712682\n",
      "Iteration 161, loss = 0.24087992\n",
      "Iteration 188, loss = 0.23684982\n",
      "Iteration 162, loss = 0.24081486\n",
      "Iteration 189, loss = 0.23658086\n",
      "Iteration 163, loss = 0.24126690\n",
      "Iteration 190, loss = 0.23632089\n",
      "Iteration 164, loss = 0.24048588\n",
      "Iteration 191, loss = 0.23640628\n",
      "Iteration 165, loss = 0.24038577\n",
      "Iteration 192, loss = 0.23621683\n",
      "Iteration 166, loss = 0.24111966\n",
      "Iteration 193, loss = 0.23677645\n",
      "Iteration 167, loss = 0.24041570\n",
      "Iteration 194, loss = 0.23583635\n",
      "Iteration 168, loss = 0.24008906\n",
      "Iteration 195, loss = 0.23662968\n",
      "Iteration 169, loss = 0.24008544\n",
      "Iteration 196, loss = 0.23603438\n",
      "Iteration 170, loss = 0.24008493\n",
      "Iteration 197, loss = 0.23616061\n",
      "Iteration 171, loss = 0.23994112\n",
      "Iteration 198, loss = 0.23702407\n",
      "Iteration 172, loss = 0.23996768\n",
      "Iteration 199, loss = 0.23617604\n",
      "Iteration 173, loss = 0.23984752\n",
      "Iteration 200, loss = 0.23584386\n",
      "Iteration 174, loss = 0.23957781\n",
      "Iteration 175, loss = 0.23922911\n",
      "Iteration 176, loss = 0.23991644\n",
      "Iteration 177, loss = 0.23921818\n",
      "Iteration 178, loss = 0.23950586\n",
      "Iteration 179, loss = 0.23928422\n",
      "Iteration 180, loss = 0.23899445\n",
      "Iteration 181, loss = 0.23897450\n",
      "Iteration 182, loss = 0.23871076\n",
      "Iteration 183, loss = 0.23894367\n",
      "Iteration 184, loss = 0.23936253\n",
      "Iteration 185, loss = 0.23882270\n",
      "Iteration 186, loss = 0.23858563\n",
      "Iteration 187, loss = 0.23850780\n",
      "Iteration 188, loss = 0.23859851\n",
      "Iteration 189, loss = 0.23850611\n",
      "Iteration 190, loss = 0.23800037\n",
      "Iteration 191, loss = 0.23875379\n",
      "Iteration 192, loss = 0.23812347\n",
      "Iteration 193, loss = 0.23848881\n",
      "Iteration 194, loss = 0.23782946\n",
      "Iteration 195, loss = 0.23849498\n",
      "Iteration 196, loss = 0.23780308\n",
      "Iteration 197, loss = 0.23813551\n",
      "Iteration 198, loss = 0.23818532\n",
      "Iteration 199, loss = 0.23758205\n",
      "Iteration 200, loss = 0.23799009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.37687530\n",
      "Iteration 2, loss = 0.33995049\n",
      "Iteration 1, loss = 0.37728711\n",
      "Iteration 3, loss = 0.31897479\n",
      "Iteration 2, loss = 0.34074795\n",
      "Iteration 4, loss = 0.30378526\n",
      "Iteration 3, loss = 0.32066170\n",
      "Iteration 5, loss = 0.29551208\n",
      "Iteration 4, loss = 0.30602473\n",
      "Iteration 6, loss = 0.29047954\n",
      "Iteration 5, loss = 0.29795121\n",
      "Iteration 7, loss = 0.28782884\n",
      "Iteration 8, loss = 0.28598585\n",
      "Iteration 6, loss = 0.29338977\n",
      "Iteration 7, loss = 0.29095387\n",
      "Iteration 9, loss = 0.28448448\n",
      "Iteration 8, loss = 0.28854655\n",
      "Iteration 10, loss = 0.28367094\n",
      "Iteration 11, loss = 0.28246406\n",
      "Iteration 9, loss = 0.28727459\n",
      "Iteration 12, loss = 0.28263187\n",
      "Iteration 13, loss = 0.28198505\n",
      "Iteration 10, loss = 0.28637102\n",
      "Iteration 14, loss = 0.28142449\n",
      "Iteration 15, loss = 0.28062085\n",
      "Iteration 11, loss = 0.28612232\n",
      "Iteration 16, loss = 0.28064454\n",
      "Iteration 12, loss = 0.28548727\n",
      "Iteration 13, loss = 0.28467128\n",
      "Iteration 17, loss = 0.28002415\n",
      "Iteration 14, loss = 0.28477566\n",
      "Iteration 18, loss = 0.27993458\n",
      "Iteration 15, loss = 0.28389150\n",
      "Iteration 19, loss = 0.27963182\n",
      "Iteration 16, loss = 0.28328860\n",
      "Iteration 20, loss = 0.27975816\n",
      "Iteration 17, loss = 0.28336257\n",
      "Iteration 21, loss = 0.27992223\n",
      "Iteration 18, loss = 0.28308371\n",
      "Iteration 22, loss = 0.27856583\n",
      "Iteration 19, loss = 0.28267017\n",
      "Iteration 23, loss = 0.27865898\n",
      "Iteration 20, loss = 0.28291620\n",
      "Iteration 24, loss = 0.27871646\n",
      "Iteration 21, loss = 0.28322753\n",
      "Iteration 25, loss = 0.27949569\n",
      "Iteration 22, loss = 0.28179185\n",
      "Iteration 26, loss = 0.27788983\n",
      "Iteration 23, loss = 0.28187648\n",
      "Iteration 27, loss = 0.27787567Iteration 24, loss = 0.28227724\n",
      "\n",
      "Iteration 25, loss = 0.28309919\n",
      "Iteration 28, loss = 0.27706511\n",
      "Iteration 26, loss = 0.28116460\n",
      "Iteration 29, loss = 0.27708194\n",
      "Iteration 27, loss = 0.28154679\n",
      "Iteration 30, loss = 0.27694251\n",
      "Iteration 28, loss = 0.28081371\n",
      "Iteration 31, loss = 0.27653813\n",
      "Iteration 29, loss = 0.28046489\n",
      "Iteration 32, loss = 0.27607822\n",
      "Iteration 30, loss = 0.28031253\n",
      "Iteration 33, loss = 0.27620401\n",
      "Iteration 31, loss = 0.28006332\n",
      "Iteration 34, loss = 0.27527966\n",
      "Iteration 32, loss = 0.27952787\n",
      "Iteration 35, loss = 0.27510942\n",
      "Iteration 33, loss = 0.27957901\n",
      "Iteration 34, loss = 0.27883698\n",
      "Iteration 36, loss = 0.27465643\n",
      "Iteration 35, loss = 0.27901918\n",
      "Iteration 37, loss = 0.27554228\n",
      "Iteration 36, loss = 0.27834409\n",
      "Iteration 38, loss = 0.27384629\n",
      "Iteration 37, loss = 0.27879356\n",
      "Iteration 39, loss = 0.27484501\n",
      "Iteration 38, loss = 0.27732711\n",
      "Iteration 40, loss = 0.27358617\n",
      "Iteration 39, loss = 0.27852234\n",
      "Iteration 41, loss = 0.27303947\n",
      "Iteration 40, loss = 0.27696816\n",
      "Iteration 42, loss = 0.27284550\n",
      "Iteration 41, loss = 0.27663558\n",
      "Iteration 43, loss = 0.27255476\n",
      "Iteration 42, loss = 0.27632331\n",
      "Iteration 44, loss = 0.27195450Iteration 43, loss = 0.27657844\n",
      "\n",
      "Iteration 44, loss = 0.27591685\n",
      "Iteration 45, loss = 0.27330625\n",
      "Iteration 45, loss = 0.27698767\n",
      "Iteration 46, loss = 0.27182895\n",
      "Iteration 46, loss = 0.27541087\n",
      "Iteration 47, loss = 0.27110247\n",
      "Iteration 47, loss = 0.27468862\n",
      "Iteration 48, loss = 0.27035864\n",
      "Iteration 48, loss = 0.27408904\n",
      "Iteration 49, loss = 0.27032025\n",
      "Iteration 49, loss = 0.27390127\n",
      "Iteration 50, loss = 0.27045151\n",
      "Iteration 50, loss = 0.27385848\n",
      "Iteration 51, loss = 0.26916472\n",
      "Iteration 51, loss = 0.27324009\n",
      "Iteration 52, loss = 0.27014831\n",
      "Iteration 52, loss = 0.27383551\n",
      "Iteration 53, loss = 0.27287257\n",
      "Iteration 53, loss = 0.26913903\n",
      "Iteration 54, loss = 0.27225840\n",
      "Iteration 54, loss = 0.26858408\n",
      "Iteration 55, loss = 0.27202991\n",
      "Iteration 55, loss = 0.26880117\n",
      "Iteration 56, loss = 0.27193051\n",
      "Iteration 57, loss = 0.27152333\n",
      "Iteration 58, loss = 0.27074228\n",
      "Iteration 56, loss = 0.26804371\n",
      "Iteration 57, loss = 0.26773404\n",
      "Iteration 59, loss = 0.27032993\n",
      "Iteration 60, loss = 0.27023304\n",
      "Iteration 58, loss = 0.26726837Iteration 61, loss = 0.26980838\n",
      "\n",
      "Iteration 59, loss = 0.26665997\n",
      "Iteration 62, loss = 0.26939813\n",
      "Iteration 60, loss = 0.26673602\n",
      "Iteration 63, loss = 0.26942548\n",
      "Iteration 61, loss = 0.26645480\n",
      "Iteration 64, loss = 0.26889359\n",
      "Iteration 65, loss = 0.26829816\n",
      "Iteration 62, loss = 0.26569343\n",
      "Iteration 66, loss = 0.26803294\n",
      "Iteration 67, loss = 0.26748149\n",
      "Iteration 68, loss = 0.26729821\n",
      "Iteration 69, loss = 0.26750883\n",
      "Iteration 63, loss = 0.26547784\n",
      "Iteration 70, loss = 0.26637871\n",
      "Iteration 71, loss = 0.26702972\n",
      "Iteration 64, loss = 0.26508415\n",
      "Iteration 65, loss = 0.26450697\n",
      "Iteration 72, loss = 0.26594373\n",
      "Iteration 66, loss = 0.26414641\n",
      "Iteration 73, loss = 0.26564207\n",
      "Iteration 67, loss = 0.26409357\n",
      "Iteration 74, loss = 0.26518475\n",
      "Iteration 75, loss = 0.26616418\n",
      "Iteration 68, loss = 0.26353697\n",
      "Iteration 76, loss = 0.26478132\n",
      "Iteration 69, loss = 0.26379124\n",
      "Iteration 70, loss = 0.26327546\n",
      "Iteration 77, loss = 0.26415009\n",
      "Iteration 78, loss = 0.26504243\n",
      "Iteration 71, loss = 0.26296977\n",
      "Iteration 79, loss = 0.26388042\n",
      "Iteration 72, loss = 0.26252425Iteration 80, loss = 0.26365596\n",
      "\n",
      "Iteration 81, loss = 0.26310863\n",
      "Iteration 82, loss = 0.26318164\n",
      "Iteration 83, loss = 0.26381723\n",
      "Iteration 73, loss = 0.26197350\n",
      "Iteration 84, loss = 0.26278781\n",
      "Iteration 85, loss = 0.26277047\n",
      "Iteration 74, loss = 0.26162549\n",
      "Iteration 75, loss = 0.26207323\n",
      "Iteration 76, loss = 0.26117588\n",
      "Iteration 86, loss = 0.26268756\n",
      "Iteration 77, loss = 0.26064297\n",
      "Iteration 87, loss = 0.26167759\n",
      "Iteration 88, loss = 0.26181190\n",
      "Iteration 78, loss = 0.26127830\n",
      "Iteration 89, loss = 0.26171788\n",
      "Iteration 79, loss = 0.26075649\n",
      "Iteration 90, loss = 0.26127047\n",
      "Iteration 91, loss = 0.26143216\n",
      "Iteration 80, loss = 0.25999168\n",
      "Iteration 92, loss = 0.26077418\n",
      "Iteration 93, loss = 0.26234759\n",
      "Iteration 81, loss = 0.25953592\n",
      "Iteration 94, loss = 0.26125870\n",
      "Iteration 82, loss = 0.25959066\n",
      "Iteration 95, loss = 0.26032467\n",
      "Iteration 83, loss = 0.26035031\n",
      "Iteration 96, loss = 0.26020507\n",
      "Iteration 84, loss = 0.25940647\n",
      "Iteration 97, loss = 0.26016327\n",
      "Iteration 85, loss = 0.25848979\n",
      "Iteration 98, loss = 0.26018936\n",
      "Iteration 86, loss = 0.25886080\n",
      "Iteration 99, loss = 0.25982206\n",
      "Iteration 87, loss = 0.25811631\n",
      "Iteration 100, loss = 0.26034698\n",
      "Iteration 88, loss = 0.25816270\n",
      "Iteration 101, loss = 0.25911884\n",
      "Iteration 89, loss = 0.25831342\n",
      "Iteration 102, loss = 0.25955195\n",
      "Iteration 90, loss = 0.25785781\n",
      "Iteration 103, loss = 0.25979062\n",
      "Iteration 91, loss = 0.25821367\n",
      "Iteration 104, loss = 0.25940932\n",
      "Iteration 92, loss = 0.25722817\n",
      "Iteration 105, loss = 0.25879380\n",
      "Iteration 93, loss = 0.25844558\n",
      "Iteration 106, loss = 0.25879217\n",
      "Iteration 94, loss = 0.25701839\n",
      "Iteration 107, loss = 0.25954339\n",
      "Iteration 95, loss = 0.25656916\n",
      "Iteration 108, loss = 0.25828187\n",
      "Iteration 96, loss = 0.25657764\n",
      "Iteration 109, loss = 0.25829785\n",
      "Iteration 97, loss = 0.25665843\n",
      "Iteration 110, loss = 0.25799507\n",
      "Iteration 98, loss = 0.25604627\n",
      "Iteration 111, loss = 0.25781894\n",
      "Iteration 99, loss = 0.25618811\n",
      "Iteration 112, loss = 0.25782010\n",
      "Iteration 100, loss = 0.25591784\n",
      "Iteration 113, loss = 0.25714626\n",
      "Iteration 101, loss = 0.25518119\n",
      "Iteration 114, loss = 0.25741674\n",
      "Iteration 102, loss = 0.25639647\n",
      "Iteration 115, loss = 0.25722009\n",
      "Iteration 103, loss = 0.25516628\n",
      "Iteration 116, loss = 0.25716619\n",
      "Iteration 104, loss = 0.25519095\n",
      "Iteration 117, loss = 0.25710703\n",
      "Iteration 118, loss = 0.25718083\n",
      "Iteration 105, loss = 0.25479496\n",
      "Iteration 119, loss = 0.25728581\n",
      "Iteration 106, loss = 0.25515799\n",
      "Iteration 120, loss = 0.25675553\n",
      "Iteration 107, loss = 0.25470814\n",
      "Iteration 121, loss = 0.25645334\n",
      "Iteration 108, loss = 0.25398270\n",
      "Iteration 122, loss = 0.25635999\n",
      "Iteration 109, loss = 0.25450646\n",
      "Iteration 123, loss = 0.25634815\n",
      "Iteration 110, loss = 0.25405536\n",
      "Iteration 124, loss = 0.25620333\n",
      "Iteration 111, loss = 0.25332551Iteration 125, loss = 0.25604812\n",
      "\n",
      "Iteration 126, loss = 0.25570824\n",
      "Iteration 112, loss = 0.25388625\n",
      "Iteration 113, loss = 0.25376576\n",
      "Iteration 127, loss = 0.25645025\n",
      "Iteration 114, loss = 0.25336686\n",
      "Iteration 128, loss = 0.25596857\n",
      "Iteration 115, loss = 0.25337311\n",
      "Iteration 129, loss = 0.25555238\n",
      "Iteration 116, loss = 0.25307297\n",
      "Iteration 130, loss = 0.25577162\n",
      "Iteration 117, loss = 0.25302515\n",
      "Iteration 131, loss = 0.25565188\n",
      "Iteration 118, loss = 0.25307406\n",
      "Iteration 132, loss = 0.25688870\n",
      "Iteration 119, loss = 0.25265814\n",
      "Iteration 133, loss = 0.25540060\n",
      "Iteration 120, loss = 0.25228658\n",
      "Iteration 134, loss = 0.25507789\n",
      "Iteration 121, loss = 0.25197006\n",
      "Iteration 135, loss = 0.25522345\n",
      "Iteration 122, loss = 0.25187686\n",
      "Iteration 136, loss = 0.25574648\n",
      "Iteration 123, loss = 0.25194948\n",
      "Iteration 137, loss = 0.25457235\n",
      "Iteration 124, loss = 0.25181350\n",
      "Iteration 138, loss = 0.25520645\n",
      "Iteration 125, loss = 0.25185009\n",
      "Iteration 139, loss = 0.25431904\n",
      "Iteration 126, loss = 0.25144535\n",
      "Iteration 140, loss = 0.25477614\n",
      "Iteration 127, loss = 0.25151188\n",
      "Iteration 141, loss = 0.25461030\n",
      "Iteration 128, loss = 0.25149693\n",
      "Iteration 142, loss = 0.25530623\n",
      "Iteration 129, loss = 0.25210753\n",
      "Iteration 143, loss = 0.25430287\n",
      "Iteration 130, loss = 0.25122015\n",
      "Iteration 144, loss = 0.25462537\n",
      "Iteration 131, loss = 0.25132582\n",
      "Iteration 145, loss = 0.25451682\n",
      "Iteration 132, loss = 0.25219456\n",
      "Iteration 146, loss = 0.25444812\n",
      "Iteration 133, loss = 0.25106356\n",
      "Iteration 147, loss = 0.25369645\n",
      "Iteration 134, loss = 0.25068236\n",
      "Iteration 148, loss = 0.25359637\n",
      "Iteration 135, loss = 0.25072612\n",
      "Iteration 149, loss = 0.25413593\n",
      "Iteration 136, loss = 0.25108432\n",
      "Iteration 150, loss = 0.25394336\n",
      "Iteration 137, loss = 0.24936131\n",
      "Iteration 151, loss = 0.25400887\n",
      "Iteration 138, loss = 0.25152140\n",
      "Iteration 152, loss = 0.25346785\n",
      "Iteration 139, loss = 0.25017911\n",
      "Iteration 153, loss = 0.25341947\n",
      "Iteration 140, loss = 0.25037739\n",
      "Iteration 154, loss = 0.25360907\n",
      "Iteration 141, loss = 0.25014313\n",
      "Iteration 155, loss = 0.25333130\n",
      "Iteration 142, loss = 0.25084422\n",
      "Iteration 156, loss = 0.25303884\n",
      "Iteration 143, loss = 0.24969200\n",
      "Iteration 157, loss = 0.25298793\n",
      "Iteration 144, loss = 0.24950143\n",
      "Iteration 158, loss = 0.25303708\n",
      "Iteration 145, loss = 0.24944929\n",
      "Iteration 159, loss = 0.25252669\n",
      "Iteration 146, loss = 0.24975688\n",
      "Iteration 160, loss = 0.25274217\n",
      "Iteration 147, loss = 0.24933365\n",
      "Iteration 161, loss = 0.25245212\n",
      "Iteration 148, loss = 0.24970293\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 162, loss = 0.25292498\n",
      "Iteration 163, loss = 0.25322815\n",
      "Iteration 164, loss = 0.25283560\n",
      "Iteration 1, loss = 0.37578092\n",
      "Iteration 165, loss = 0.25274127\n",
      "Iteration 2, loss = 0.34129814\n",
      "Iteration 166, loss = 0.25226935\n",
      "Iteration 3, loss = 0.32094923\n",
      "Iteration 167, loss = 0.25239548\n",
      "Iteration 4, loss = 0.30653901\n",
      "Iteration 168, loss = 0.25240453\n",
      "Iteration 5, loss = 0.29888517\n",
      "Iteration 169, loss = 0.25197060\n",
      "Iteration 6, loss = 0.29444431\n",
      "Iteration 170, loss = 0.25230329\n",
      "Iteration 7, loss = 0.29199394\n",
      "Iteration 171, loss = 0.25188000\n",
      "Iteration 8, loss = 0.28985765\n",
      "Iteration 172, loss = 0.25211722\n",
      "Iteration 9, loss = 0.28878088\n",
      "Iteration 173, loss = 0.25189315\n",
      "Iteration 10, loss = 0.28760225\n",
      "Iteration 174, loss = 0.25191823\n",
      "Iteration 11, loss = 0.28793739\n",
      "Iteration 175, loss = 0.25178581\n",
      "Iteration 12, loss = 0.28690152\n",
      "Iteration 176, loss = 0.25200443\n",
      "Iteration 13, loss = 0.28619562\n",
      "Iteration 177, loss = 0.25121486\n",
      "Iteration 14, loss = 0.28619498\n",
      "Iteration 178, loss = 0.25148602\n",
      "Iteration 15, loss = 0.28640266\n",
      "Iteration 179, loss = 0.25133053\n",
      "Iteration 16, loss = 0.28493629\n",
      "Iteration 180, loss = 0.25105808\n",
      "Iteration 17, loss = 0.28503599\n",
      "Iteration 181, loss = 0.25131532\n",
      "Iteration 18, loss = 0.28474000\n",
      "Iteration 182, loss = 0.25114158\n",
      "Iteration 19, loss = 0.28472132\n",
      "Iteration 183, loss = 0.25135139\n",
      "Iteration 20, loss = 0.28414793\n",
      "Iteration 184, loss = 0.25074086\n",
      "Iteration 21, loss = 0.28517259\n",
      "Iteration 185, loss = 0.25147300\n",
      "Iteration 186, loss = 0.25095485\n",
      "Iteration 22, loss = 0.28363396\n",
      "Iteration 23, loss = 0.28333962Iteration 187, loss = 0.25096884\n",
      "\n",
      "Iteration 188, loss = 0.25046001\n",
      "Iteration 189, loss = 0.25089093\n",
      "Iteration 24, loss = 0.28325315\n",
      "Iteration 190, loss = 0.25039857\n",
      "Iteration 25, loss = 0.28396162\n",
      "Iteration 191, loss = 0.25058112\n",
      "Iteration 26, loss = 0.28239947\n",
      "Iteration 192, loss = 0.25018886\n",
      "Iteration 27, loss = 0.28338162\n",
      "Iteration 193, loss = 0.25054533\n",
      "Iteration 28, loss = 0.28222950\n",
      "Iteration 194, loss = 0.25034101\n",
      "Iteration 29, loss = 0.28112000\n",
      "Iteration 195, loss = 0.25033700\n",
      "Iteration 30, loss = 0.28150171\n",
      "Iteration 31, loss = 0.28092821\n",
      "Iteration 196, loss = 0.25011227\n",
      "Iteration 197, loss = 0.25007418Iteration 32, loss = 0.28043979\n",
      "\n",
      "Iteration 33, loss = 0.28041515Iteration 198, loss = 0.25019915\n",
      "\n",
      "Iteration 199, loss = 0.24998648Iteration 34, loss = 0.27981814\n",
      "\n",
      "Iteration 200, loss = 0.25002177Iteration 35, loss = 0.27998799\n",
      "\n",
      "Iteration 36, loss = 0.27957694\n",
      "Iteration 37, loss = 0.27910616\n",
      "Iteration 1, loss = 0.37556207\n",
      "Iteration 38, loss = 0.27832829\n",
      "Iteration 2, loss = 0.34149094\n",
      "Iteration 39, loss = 0.27895455\n",
      "Iteration 3, loss = 0.32082640\n",
      "Iteration 40, loss = 0.27765124\n",
      "Iteration 4, loss = 0.30585587\n",
      "Iteration 41, loss = 0.27767493\n",
      "Iteration 5, loss = 0.29782122\n",
      "Iteration 42, loss = 0.27704321\n",
      "Iteration 6, loss = 0.29310652\n",
      "Iteration 43, loss = 0.27701868\n",
      "Iteration 7, loss = 0.29036774\n",
      "Iteration 44, loss = 0.27626077\n",
      "Iteration 8, loss = 0.28779200\n",
      "Iteration 9, loss = 0.28638416\n",
      "Iteration 45, loss = 0.27664016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 0.28512245\n",
      "Iteration 11, loss = 0.28460769\n",
      "Iteration 46, loss = 0.27614589\n",
      "Iteration 47, loss = 0.27593563\n",
      "Iteration 12, loss = 0.28386239\n",
      "Iteration 48, loss = 0.27498421\n",
      "Iteration 49, loss = 0.27483915\n",
      "Iteration 13, loss = 0.28365869\n",
      "Iteration 50, loss = 0.27467803\n",
      "Iteration 14, loss = 0.28334024\n",
      "Iteration 15, loss = 0.28258072\n",
      "Iteration 51, loss = 0.27445864\n",
      "Iteration 16, loss = 0.28200645\n",
      "Iteration 17, loss = 0.28169270\n",
      "Iteration 18, loss = 0.28189417\n",
      "Iteration 52, loss = 0.27460102\n",
      "Iteration 19, loss = 0.28137612\n",
      "Iteration 20, loss = 0.28084750\n",
      "Iteration 21, loss = 0.28138811\n",
      "Iteration 22, loss = 0.28058996Iteration 53, loss = 0.27391029\n",
      "\n",
      "Iteration 23, loss = 0.28027111\n",
      "Iteration 54, loss = 0.27390731\n",
      "Iteration 24, loss = 0.28050529Iteration 55, loss = 0.27336885\n",
      "\n",
      "Iteration 25, loss = 0.28070259Iteration 56, loss = 0.27281640\n",
      "\n",
      "Iteration 57, loss = 0.27280258\n",
      "Iteration 26, loss = 0.27926102\n",
      "Iteration 27, loss = 0.28004588\n",
      "Iteration 58, loss = 0.27224500\n",
      "Iteration 28, loss = 0.27912300\n",
      "Iteration 29, loss = 0.27845045\n",
      "Iteration 30, loss = 0.27864919\n",
      "Iteration 59, loss = 0.27193379\n",
      "Iteration 60, loss = 0.27192080\n",
      "Iteration 31, loss = 0.27799037\n",
      "Iteration 61, loss = 0.27190229\n",
      "Iteration 62, loss = 0.27096428\n",
      "Iteration 32, loss = 0.27772005\n",
      "Iteration 63, loss = 0.27114537\n",
      "Iteration 33, loss = 0.27765312\n",
      "Iteration 64, loss = 0.27009033\n",
      "Iteration 34, loss = 0.27723987\n",
      "Iteration 65, loss = 0.27031160\n",
      "Iteration 35, loss = 0.27755821\n",
      "Iteration 66, loss = 0.26987511\n",
      "Iteration 36, loss = 0.27697090\n",
      "Iteration 67, loss = 0.26989501\n",
      "Iteration 37, loss = 0.27634729Iteration 68, loss = 0.26902672\n",
      "\n",
      "Iteration 38, loss = 0.27640598\n",
      "Iteration 69, loss = 0.26941472\n",
      "Iteration 39, loss = 0.27680283\n",
      "Iteration 70, loss = 0.26864745\n",
      "Iteration 71, loss = 0.26860555\n",
      "Iteration 40, loss = 0.27558100\n",
      "Iteration 72, loss = 0.26798201\n",
      "Iteration 41, loss = 0.27588142\n",
      "Iteration 73, loss = 0.26756564\n",
      "Iteration 42, loss = 0.27517865\n",
      "Iteration 74, loss = 0.26722102\n",
      "Iteration 43, loss = 0.27496090\n",
      "Iteration 75, loss = 0.26745955\n",
      "Iteration 44, loss = 0.27415071\n",
      "Iteration 76, loss = 0.26655436\n",
      "Iteration 45, loss = 0.27493923\n",
      "Iteration 77, loss = 0.26652059\n",
      "Iteration 46, loss = 0.27402867\n",
      "Iteration 78, loss = 0.26720153\n",
      "Iteration 47, loss = 0.27432597\n",
      "Iteration 79, loss = 0.26571844\n",
      "Iteration 48, loss = 0.27315914\n",
      "Iteration 80, loss = 0.26526562\n",
      "Iteration 81, loss = 0.26500588\n",
      "Iteration 49, loss = 0.27293442\n",
      "Iteration 82, loss = 0.26516713\n",
      "Iteration 50, loss = 0.27313567\n",
      "Iteration 51, loss = 0.27281378\n",
      "Iteration 83, loss = 0.26489394\n",
      "Iteration 52, loss = 0.27261093\n",
      "Iteration 84, loss = 0.26427920\n",
      "Iteration 53, loss = 0.27273010\n",
      "Iteration 85, loss = 0.26384587\n",
      "Iteration 54, loss = 0.27222447\n",
      "Iteration 86, loss = 0.26393744\n",
      "Iteration 55, loss = 0.27164628\n",
      "Iteration 87, loss = 0.26296305\n",
      "Iteration 56, loss = 0.27138967\n",
      "Iteration 88, loss = 0.26269335\n",
      "Iteration 57, loss = 0.27126921\n",
      "Iteration 89, loss = 0.26298269\n",
      "Iteration 58, loss = 0.27065196\n",
      "Iteration 90, loss = 0.26219550\n",
      "Iteration 59, loss = 0.27055680\n",
      "Iteration 91, loss = 0.26184005\n",
      "Iteration 60, loss = 0.27073259\n",
      "Iteration 92, loss = 0.26195106\n",
      "Iteration 61, loss = 0.27033666\n",
      "Iteration 93, loss = 0.26243511\n",
      "Iteration 62, loss = 0.26962439\n",
      "Iteration 94, loss = 0.26133391\n",
      "Iteration 63, loss = 0.26994865\n",
      "Iteration 95, loss = 0.26090421\n",
      "Iteration 64, loss = 0.26906996\n",
      "Iteration 96, loss = 0.26061474\n",
      "Iteration 65, loss = 0.26938888\n",
      "Iteration 97, loss = 0.26020650\n",
      "Iteration 66, loss = 0.26890289\n",
      "Iteration 98, loss = 0.26064258\n",
      "Iteration 67, loss = 0.26884395\n",
      "Iteration 99, loss = 0.26025298\n",
      "Iteration 68, loss = 0.26833324\n",
      "Iteration 100, loss = 0.26078901\n",
      "Iteration 69, loss = 0.26833940\n",
      "Iteration 101, loss = 0.25951428\n",
      "Iteration 102, loss = 0.25947990\n",
      "Iteration 70, loss = 0.26772797\n",
      "Iteration 103, loss = 0.25969455\n",
      "Iteration 71, loss = 0.26822638\n",
      "Iteration 104, loss = 0.25919003\n",
      "Iteration 72, loss = 0.26753742\n",
      "Iteration 73, loss = 0.26744426\n",
      "Iteration 105, loss = 0.25861818\n",
      "Iteration 74, loss = 0.26690536\n",
      "Iteration 75, loss = 0.26722322\n",
      "Iteration 106, loss = 0.25846213\n",
      "Iteration 76, loss = 0.26649214\n",
      "Iteration 77, loss = 0.26635382\n",
      "Iteration 107, loss = 0.25850695\n",
      "Iteration 108, loss = 0.25793823\n",
      "Iteration 78, loss = 0.26662682\n",
      "Iteration 109, loss = 0.25838068\n",
      "Iteration 79, loss = 0.26614550\n",
      "Iteration 110, loss = 0.25788092\n",
      "Iteration 80, loss = 0.26561832\n",
      "Iteration 111, loss = 0.25733546\n",
      "Iteration 81, loss = 0.26554797\n",
      "Iteration 112, loss = 0.25717192\n",
      "Iteration 82, loss = 0.26571419\n",
      "Iteration 113, loss = 0.25667393\n",
      "Iteration 83, loss = 0.26561297\n",
      "Iteration 114, loss = 0.25681796\n",
      "Iteration 84, loss = 0.26531394\n",
      "Iteration 115, loss = 0.25680041\n",
      "Iteration 85, loss = 0.26479134\n",
      "Iteration 116, loss = 0.25714693\n",
      "Iteration 86, loss = 0.26495526\n",
      "Iteration 117, loss = 0.25618798Iteration 87, loss = 0.26431588\n",
      "\n",
      "Iteration 88, loss = 0.26438849\n",
      "Iteration 118, loss = 0.25628643\n",
      "Iteration 89, loss = 0.26459577\n",
      "Iteration 119, loss = 0.25605316\n",
      "Iteration 90, loss = 0.26389158Iteration 120, loss = 0.25574619\n",
      "\n",
      "Iteration 121, loss = 0.25535037\n",
      "Iteration 91, loss = 0.26382145\n",
      "Iteration 122, loss = 0.25581109\n",
      "Iteration 92, loss = 0.26355528\n",
      "Iteration 123, loss = 0.25507998\n",
      "Iteration 93, loss = 0.26409504\n",
      "Iteration 124, loss = 0.25556871\n",
      "Iteration 94, loss = 0.26376637\n",
      "Iteration 125, loss = 0.25522029\n",
      "Iteration 95, loss = 0.26361438\n",
      "Iteration 126, loss = 0.25473523\n",
      "Iteration 96, loss = 0.26319215\n",
      "Iteration 127, loss = 0.25462391\n",
      "Iteration 97, loss = 0.26288080\n",
      "Iteration 128, loss = 0.25440144\n",
      "Iteration 98, loss = 0.26283653\n",
      "Iteration 129, loss = 0.25445866\n",
      "Iteration 99, loss = 0.26327720\n",
      "Iteration 100, loss = 0.26322140\n",
      "Iteration 130, loss = 0.25485564\n",
      "Iteration 101, loss = 0.26229838\n",
      "Iteration 131, loss = 0.25404282\n",
      "Iteration 102, loss = 0.26198554\n",
      "Iteration 132, loss = 0.25501003\n",
      "Iteration 103, loss = 0.26308269\n",
      "Iteration 104, loss = 0.26217209\n",
      "Iteration 133, loss = 0.25390033\n",
      "Iteration 105, loss = 0.26202348\n",
      "Iteration 106, loss = 0.26226372\n",
      "Iteration 134, loss = 0.25388214\n",
      "Iteration 107, loss = 0.26170447\n",
      "Iteration 135, loss = 0.25358139\n",
      "Iteration 108, loss = 0.26147620\n",
      "Iteration 136, loss = 0.25469571\n",
      "Iteration 109, loss = 0.26210128\n",
      "Iteration 137, loss = 0.25326958\n",
      "Iteration 110, loss = 0.26124361\n",
      "Iteration 138, loss = 0.25349635\n",
      "Iteration 111, loss = 0.26130401\n",
      "Iteration 139, loss = 0.25264110\n",
      "Iteration 112, loss = 0.26085340\n",
      "Iteration 140, loss = 0.25275371\n",
      "Iteration 113, loss = 0.26061693\n",
      "Iteration 141, loss = 0.25291088\n",
      "Iteration 114, loss = 0.26074424\n",
      "Iteration 142, loss = 0.25373383\n",
      "Iteration 115, loss = 0.26091250\n",
      "Iteration 143, loss = 0.25283229\n",
      "Iteration 116, loss = 0.26060365\n",
      "Iteration 117, loss = 0.26072094\n",
      "Iteration 144, loss = 0.25281478\n",
      "Iteration 118, loss = 0.26074692\n",
      "Iteration 145, loss = 0.25291832\n",
      "Iteration 119, loss = 0.25999949\n",
      "Iteration 146, loss = 0.25272360\n",
      "Iteration 120, loss = 0.25967525\n",
      "Iteration 147, loss = 0.25229946\n",
      "Iteration 121, loss = 0.25981658\n",
      "Iteration 148, loss = 0.25180745\n",
      "Iteration 122, loss = 0.25996597\n",
      "Iteration 149, loss = 0.25225068\n",
      "Iteration 123, loss = 0.25935008\n",
      "Iteration 150, loss = 0.25231807\n",
      "Iteration 124, loss = 0.25962591\n",
      "Iteration 151, loss = 0.25214957\n",
      "Iteration 125, loss = 0.25974986\n",
      "Iteration 152, loss = 0.25163258\n",
      "Iteration 126, loss = 0.25889982\n",
      "Iteration 153, loss = 0.25169046\n",
      "Iteration 127, loss = 0.25946701\n",
      "Iteration 154, loss = 0.25178995\n",
      "Iteration 128, loss = 0.25941864\n",
      "Iteration 155, loss = 0.25150167\n",
      "Iteration 129, loss = 0.25873461\n",
      "Iteration 156, loss = 0.25132066\n",
      "Iteration 130, loss = 0.25884619\n",
      "Iteration 157, loss = 0.25114343\n",
      "Iteration 131, loss = 0.25858775\n",
      "Iteration 158, loss = 0.25101003\n",
      "Iteration 132, loss = 0.25978675\n",
      "Iteration 159, loss = 0.25110404\n",
      "Iteration 133, loss = 0.25822006\n",
      "Iteration 160, loss = 0.25097512\n",
      "Iteration 134, loss = 0.25840537\n",
      "Iteration 161, loss = 0.25101299\n",
      "Iteration 135, loss = 0.25835324\n",
      "Iteration 162, loss = 0.25112794\n",
      "Iteration 136, loss = 0.25902157\n",
      "Iteration 163, loss = 0.25140894\n",
      "Iteration 137, loss = 0.25772379\n",
      "Iteration 164, loss = 0.25060320\n",
      "Iteration 138, loss = 0.25819332\n",
      "Iteration 165, loss = 0.25093971\n",
      "Iteration 139, loss = 0.25747453\n",
      "Iteration 166, loss = 0.25023364\n",
      "Iteration 140, loss = 0.25733312\n",
      "Iteration 167, loss = 0.25055645\n",
      "Iteration 141, loss = 0.25749536\n",
      "Iteration 168, loss = 0.25066408\n",
      "Iteration 142, loss = 0.25773171\n",
      "Iteration 169, loss = 0.25032307\n",
      "Iteration 143, loss = 0.25794873\n",
      "Iteration 170, loss = 0.25032270\n",
      "Iteration 144, loss = 0.25789119\n",
      "Iteration 171, loss = 0.25017518\n",
      "Iteration 145, loss = 0.25767964\n",
      "Iteration 172, loss = 0.25039959\n",
      "Iteration 146, loss = 0.25831458\n",
      "Iteration 173, loss = 0.25046750\n",
      "Iteration 147, loss = 0.25668289\n",
      "Iteration 174, loss = 0.25025366\n",
      "Iteration 148, loss = 0.25673393\n",
      "Iteration 175, loss = 0.25007174\n",
      "Iteration 149, loss = 0.25645288\n",
      "Iteration 176, loss = 0.24994892\n",
      "Iteration 150, loss = 0.25662354\n",
      "Iteration 177, loss = 0.24968141\n",
      "Iteration 151, loss = 0.25667037\n",
      "Iteration 178, loss = 0.24999756\n",
      "Iteration 152, loss = 0.25624354\n",
      "Iteration 179, loss = 0.24974959\n",
      "Iteration 153, loss = 0.25635991\n",
      "Iteration 180, loss = 0.24944644\n",
      "Iteration 154, loss = 0.25626208\n",
      "Iteration 181, loss = 0.24932073\n",
      "Iteration 155, loss = 0.25613825\n",
      "Iteration 182, loss = 0.24942904\n",
      "Iteration 156, loss = 0.25593588\n",
      "Iteration 183, loss = 0.24910426\n",
      "Iteration 157, loss = 0.25627564\n",
      "Iteration 184, loss = 0.24894003\n",
      "Iteration 158, loss = 0.25587257\n",
      "Iteration 185, loss = 0.24896890\n",
      "Iteration 159, loss = 0.25576212\n",
      "Iteration 186, loss = 0.24936328\n",
      "Iteration 160, loss = 0.25549192\n",
      "Iteration 161, loss = 0.25582415\n",
      "Iteration 187, loss = 0.24949360\n",
      "Iteration 188, loss = 0.24905348\n",
      "Iteration 162, loss = 0.25635177\n",
      "Iteration 189, loss = 0.24909230\n",
      "Iteration 190, loss = 0.24881400\n",
      "Iteration 163, loss = 0.25550495\n",
      "Iteration 191, loss = 0.24870669\n",
      "Iteration 164, loss = 0.25519588\n",
      "Iteration 165, loss = 0.25519012\n",
      "Iteration 192, loss = 0.24848416\n",
      "Iteration 166, loss = 0.25494980\n",
      "Iteration 167, loss = 0.25479833\n",
      "Iteration 193, loss = 0.24847711\n",
      "Iteration 168, loss = 0.25505412\n",
      "Iteration 194, loss = 0.24903165\n",
      "Iteration 195, loss = 0.24831052\n",
      "Iteration 169, loss = 0.25473968\n",
      "Iteration 196, loss = 0.24855847\n",
      "Iteration 170, loss = 0.25478732\n",
      "Iteration 171, loss = 0.25451163\n",
      "Iteration 197, loss = 0.24866801\n",
      "Iteration 172, loss = 0.25545666\n",
      "Iteration 173, loss = 0.25478577\n",
      "Iteration 198, loss = 0.24837249\n",
      "Iteration 174, loss = 0.25463690\n",
      "Iteration 199, loss = 0.24871155\n",
      "Iteration 175, loss = 0.25590210\n",
      "Iteration 200, loss = 0.24832401\n",
      "Iteration 176, loss = 0.25463908\n",
      "Iteration 177, loss = 0.25420832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 178, loss = 0.25475233\n",
      "Iteration 1, loss = 0.37678095\n",
      "Iteration 179, loss = 0.25378678\n",
      "Iteration 2, loss = 0.34152203\n",
      "Iteration 180, loss = 0.25400601\n",
      "Iteration 181, loss = 0.25391810\n",
      "Iteration 3, loss = 0.32069966\n",
      "Iteration 182, loss = 0.25420502\n",
      "Iteration 4, loss = 0.30603909\n",
      "Iteration 183, loss = 0.25446333\n",
      "Iteration 5, loss = 0.29794327\n",
      "Iteration 184, loss = 0.25351692\n",
      "Iteration 6, loss = 0.29321904\n",
      "Iteration 185, loss = 0.25346661\n",
      "Iteration 7, loss = 0.29076588\n",
      "Iteration 186, loss = 0.25340963\n",
      "Iteration 8, loss = 0.28847714\n",
      "Iteration 187, loss = 0.25335514\n",
      "Iteration 9, loss = 0.28689389\n",
      "Iteration 188, loss = 0.25324167\n",
      "Iteration 10, loss = 0.28570694\n",
      "Iteration 189, loss = 0.25331045\n",
      "Iteration 11, loss = 0.28561350\n",
      "Iteration 190, loss = 0.25362080\n",
      "Iteration 12, loss = 0.28454747\n",
      "Iteration 191, loss = 0.25317196\n",
      "Iteration 192, loss = 0.25289032\n",
      "Iteration 13, loss = 0.28463256\n",
      "Iteration 193, loss = 0.25297291\n",
      "Iteration 14, loss = 0.28389014\n",
      "Iteration 194, loss = 0.25318635\n",
      "Iteration 195, loss = 0.25310890\n",
      "Iteration 15, loss = 0.28325405\n",
      "Iteration 196, loss = 0.25297191\n",
      "Iteration 16, loss = 0.28303146\n",
      "Iteration 197, loss = 0.25269176\n",
      "Iteration 17, loss = 0.28262494\n",
      "Iteration 198, loss = 0.25275686\n",
      "Iteration 18, loss = 0.28280453\n",
      "Iteration 199, loss = 0.25300661\n",
      "Iteration 19, loss = 0.28229748\n",
      "Iteration 200, loss = 0.25239579\n",
      "Iteration 20, loss = 0.28174898\n",
      "Iteration 21, loss = 0.28214038\n",
      "Iteration 22, loss = 0.28148722\n",
      "Iteration 1, loss = 0.37575678\n",
      "Iteration 23, loss = 0.28130159\n",
      "Iteration 2, loss = 0.33981523\n",
      "Iteration 24, loss = 0.28151882\n",
      "Iteration 3, loss = 0.31780650\n",
      "Iteration 25, loss = 0.28103572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.30195034\n",
      "Iteration 26, loss = 0.28035153\n",
      "Iteration 5, loss = 0.29309483\n",
      "Iteration 27, loss = 0.28086004\n",
      "Iteration 6, loss = 0.28788325\n",
      "Iteration 28, loss = 0.27995330\n",
      "Iteration 7, loss = 0.28512701\n",
      "Iteration 29, loss = 0.27941615\n",
      "Iteration 8, loss = 0.28243569\n",
      "Iteration 30, loss = 0.27948067\n",
      "Iteration 9, loss = 0.28106984\n",
      "Iteration 31, loss = 0.27882457\n",
      "Iteration 10, loss = 0.27965616\n",
      "Iteration 32, loss = 0.27867035\n",
      "Iteration 11, loss = 0.27916177\n",
      "Iteration 33, loss = 0.27833361\n",
      "Iteration 12, loss = 0.27801660\n",
      "Iteration 34, loss = 0.27822131\n",
      "Iteration 13, loss = 0.27875255\n",
      "Iteration 35, loss = 0.27826106\n",
      "Iteration 14, loss = 0.27771857\n",
      "Iteration 36, loss = 0.27775280\n",
      "Iteration 15, loss = 0.27689212\n",
      "Iteration 37, loss = 0.27773137\n",
      "Iteration 16, loss = 0.27691191\n",
      "Iteration 38, loss = 0.27669488\n",
      "Iteration 17, loss = 0.27644141\n",
      "Iteration 39, loss = 0.27695606\n",
      "Iteration 18, loss = 0.27698909\n",
      "Iteration 19, loss = 0.27626019\n",
      "Iteration 20, loss = 0.27559092\n",
      "Iteration 40, loss = 0.27602355\n",
      "Iteration 41, loss = 0.27579945\n",
      "Iteration 21, loss = 0.27605645\n",
      "Iteration 22, loss = 0.27520233\n",
      "Iteration 23, loss = 0.27517180\n",
      "Iteration 24, loss = 0.27504471\n",
      "Iteration 42, loss = 0.27580429\n",
      "Iteration 25, loss = 0.27422974\n",
      "Iteration 26, loss = 0.27385688\n",
      "Iteration 43, loss = 0.27545290\n",
      "Iteration 27, loss = 0.27453648\n",
      "Iteration 28, loss = 0.27399200\n",
      "Iteration 44, loss = 0.27470718\n",
      "Iteration 29, loss = 0.27332828Iteration 45, loss = 0.27529653\n",
      "\n",
      "Iteration 46, loss = 0.27446868\n",
      "Iteration 30, loss = 0.27314993\n",
      "Iteration 47, loss = 0.27479528\n",
      "Iteration 31, loss = 0.27236597\n",
      "Iteration 48, loss = 0.27371356\n",
      "Iteration 32, loss = 0.27203910\n",
      "Iteration 49, loss = 0.27338495\n",
      "Iteration 33, loss = 0.27197120\n",
      "Iteration 50, loss = 0.27309204\n",
      "Iteration 34, loss = 0.27217650\n",
      "Iteration 51, loss = 0.27275140\n",
      "Iteration 35, loss = 0.27154792\n",
      "Iteration 52, loss = 0.27317693\n",
      "Iteration 36, loss = 0.27228036\n",
      "Iteration 53, loss = 0.27246738\n",
      "Iteration 37, loss = 0.27123501\n",
      "Iteration 54, loss = 0.27210891\n",
      "Iteration 38, loss = 0.27026140\n",
      "Iteration 55, loss = 0.27156196\n",
      "Iteration 39, loss = 0.27038295\n",
      "Iteration 56, loss = 0.27121746\n",
      "Iteration 40, loss = 0.26971254\n",
      "Iteration 57, loss = 0.27086223\n",
      "Iteration 41, loss = 0.26954615\n",
      "Iteration 42, loss = 0.26884553\n",
      "Iteration 58, loss = 0.27035571\n",
      "Iteration 43, loss = 0.26826422\n",
      "Iteration 59, loss = 0.27007380\n",
      "Iteration 60, loss = 0.26952823\n",
      "Iteration 44, loss = 0.26797034\n",
      "Iteration 61, loss = 0.26998045\n",
      "Iteration 45, loss = 0.26841503\n",
      "Iteration 62, loss = 0.26886281\n",
      "Iteration 46, loss = 0.26781412\n",
      "Iteration 63, loss = 0.26920210\n",
      "Iteration 47, loss = 0.26757419\n",
      "Iteration 64, loss = 0.26845678\n",
      "Iteration 48, loss = 0.26667577\n",
      "Iteration 65, loss = 0.26778326\n",
      "Iteration 49, loss = 0.26643440\n",
      "Iteration 66, loss = 0.26747023\n",
      "Iteration 50, loss = 0.26625827\n",
      "Iteration 67, loss = 0.26758276\n",
      "Iteration 51, loss = 0.26573169\n",
      "Iteration 68, loss = 0.26710361\n",
      "Iteration 52, loss = 0.26563841\n",
      "Iteration 69, loss = 0.26754421\n",
      "Iteration 53, loss = 0.26572467\n",
      "Iteration 70, loss = 0.26644609\n",
      "Iteration 54, loss = 0.26466278\n",
      "Iteration 71, loss = 0.26667505\n",
      "Iteration 55, loss = 0.26419041\n",
      "Iteration 72, loss = 0.26643352\n",
      "Iteration 56, loss = 0.26436471\n",
      "Iteration 73, loss = 0.26563363\n",
      "Iteration 57, loss = 0.26392179\n",
      "Iteration 74, loss = 0.26519755\n",
      "Iteration 58, loss = 0.26341427\n",
      "Iteration 75, loss = 0.26554736\n",
      "Iteration 59, loss = 0.26282278\n",
      "Iteration 76, loss = 0.26445650\n",
      "Iteration 60, loss = 0.26236774\n",
      "Iteration 77, loss = 0.26435134\n",
      "Iteration 61, loss = 0.26244775\n",
      "Iteration 78, loss = 0.26551628\n",
      "Iteration 62, loss = 0.26160704\n",
      "Iteration 79, loss = 0.26446114\n",
      "Iteration 63, loss = 0.26165836\n",
      "Iteration 80, loss = 0.26386616\n",
      "Iteration 64, loss = 0.26109841\n",
      "Iteration 81, loss = 0.26394001\n",
      "Iteration 65, loss = 0.26069295\n",
      "Iteration 82, loss = 0.26381703\n",
      "Iteration 66, loss = 0.26073322\n",
      "Iteration 83, loss = 0.26331289\n",
      "Iteration 67, loss = 0.26035350\n",
      "Iteration 84, loss = 0.26375698\n",
      "Iteration 68, loss = 0.26075781\n",
      "Iteration 85, loss = 0.26331539\n",
      "Iteration 69, loss = 0.26035779\n",
      "Iteration 86, loss = 0.26292770\n",
      "Iteration 70, loss = 0.25903427\n",
      "Iteration 87, loss = 0.26247606\n",
      "Iteration 71, loss = 0.25928418\n",
      "Iteration 88, loss = 0.26249605\n",
      "Iteration 72, loss = 0.25881405\n",
      "Iteration 89, loss = 0.26234296\n",
      "Iteration 73, loss = 0.25844871\n",
      "Iteration 90, loss = 0.26191130\n",
      "Iteration 74, loss = 0.25795752\n",
      "Iteration 91, loss = 0.26194873\n",
      "Iteration 75, loss = 0.25851344\n",
      "Iteration 92, loss = 0.26143894\n",
      "Iteration 76, loss = 0.25747632\n",
      "Iteration 93, loss = 0.26240527\n",
      "Iteration 77, loss = 0.25725169\n",
      "Iteration 94, loss = 0.26195891\n",
      "Iteration 78, loss = 0.25741404\n",
      "Iteration 95, loss = 0.26153061\n",
      "Iteration 79, loss = 0.25689494\n",
      "Iteration 96, loss = 0.26162566\n",
      "Iteration 80, loss = 0.25659854\n",
      "Iteration 97, loss = 0.26106623\n",
      "Iteration 81, loss = 0.25633280\n",
      "Iteration 98, loss = 0.26090735\n",
      "Iteration 82, loss = 0.25709861\n",
      "Iteration 99, loss = 0.26191069\n",
      "Iteration 83, loss = 0.25598588\n",
      "Iteration 100, loss = 0.26117513\n",
      "Iteration 84, loss = 0.25621203\n",
      "Iteration 101, loss = 0.26023751\n",
      "Iteration 85, loss = 0.25550901\n",
      "Iteration 102, loss = 0.26011084\n",
      "Iteration 86, loss = 0.25530487\n",
      "Iteration 103, loss = 0.26101335\n",
      "Iteration 87, loss = 0.25507928\n",
      "Iteration 104, loss = 0.26061978\n",
      "Iteration 88, loss = 0.25478697\n",
      "Iteration 105, loss = 0.26065923\n",
      "Iteration 89, loss = 0.25507442\n",
      "Iteration 106, loss = 0.26026314\n",
      "Iteration 90, loss = 0.25438125\n",
      "Iteration 107, loss = 0.26000655\n",
      "Iteration 91, loss = 0.25421116\n",
      "Iteration 108, loss = 0.25973071\n",
      "Iteration 92, loss = 0.25401808\n",
      "Iteration 109, loss = 0.25978860\n",
      "Iteration 93, loss = 0.25411864\n",
      "Iteration 110, loss = 0.25950600Iteration 94, loss = 0.25375572\n",
      "\n",
      "Iteration 111, loss = 0.25910238Iteration 95, loss = 0.25350259\n",
      "\n",
      "Iteration 96, loss = 0.25394406\n",
      "Iteration 112, loss = 0.25905704\n",
      "Iteration 97, loss = 0.25304820\n",
      "Iteration 113, loss = 0.25875752\n",
      "Iteration 98, loss = 0.25296799\n",
      "Iteration 114, loss = 0.25936251\n",
      "Iteration 99, loss = 0.25486812\n",
      "Iteration 115, loss = 0.25891112\n",
      "Iteration 100, loss = 0.25373095\n",
      "Iteration 116, loss = 0.25873168\n",
      "Iteration 117, loss = 0.25866009\n",
      "Iteration 101, loss = 0.25227606\n",
      "Iteration 118, loss = 0.25857242\n",
      "Iteration 102, loss = 0.25218501\n",
      "Iteration 119, loss = 0.25819822\n",
      "Iteration 103, loss = 0.25285794\n",
      "Iteration 120, loss = 0.25867838\n",
      "Iteration 104, loss = 0.25188103\n",
      "Iteration 121, loss = 0.25837579\n",
      "Iteration 105, loss = 0.25266917\n",
      "Iteration 122, loss = 0.25794916\n",
      "Iteration 106, loss = 0.25206273\n",
      "Iteration 123, loss = 0.25782293\n",
      "Iteration 107, loss = 0.25165484\n",
      "Iteration 124, loss = 0.25791101\n",
      "Iteration 108, loss = 0.25185647\n",
      "Iteration 125, loss = 0.25844789\n",
      "Iteration 109, loss = 0.25157375\n",
      "Iteration 126, loss = 0.25757680\n",
      "Iteration 110, loss = 0.25124296\n",
      "Iteration 127, loss = 0.25793607\n",
      "Iteration 111, loss = 0.25088086\n",
      "Iteration 128, loss = 0.25772979\n",
      "Iteration 112, loss = 0.25112489\n",
      "Iteration 129, loss = 0.25726902\n",
      "Iteration 113, loss = 0.25070090\n",
      "Iteration 130, loss = 0.25744969Iteration 114, loss = 0.25132018\n",
      "\n",
      "Iteration 115, loss = 0.25039798\n",
      "Iteration 131, loss = 0.25773678Iteration 116, loss = 0.25000308\n",
      "\n",
      "Iteration 132, loss = 0.25838277\n",
      "Iteration 117, loss = 0.25002829\n",
      "Iteration 133, loss = 0.25704555\n",
      "Iteration 118, loss = 0.24996741\n",
      "Iteration 134, loss = 0.25671087\n",
      "Iteration 119, loss = 0.24970020\n",
      "Iteration 135, loss = 0.25702950Iteration 120, loss = 0.24967551\n",
      "\n",
      "Iteration 121, loss = 0.24988567\n",
      "Iteration 136, loss = 0.25788312\n",
      "Iteration 122, loss = 0.24920986\n",
      "Iteration 137, loss = 0.25705993\n",
      "Iteration 123, loss = 0.24941699\n",
      "Iteration 138, loss = 0.25708166\n",
      "Iteration 124, loss = 0.24882543\n",
      "Iteration 139, loss = 0.25685241\n",
      "Iteration 125, loss = 0.24941000\n",
      "Iteration 140, loss = 0.25650018\n",
      "Iteration 126, loss = 0.24881547\n",
      "Iteration 141, loss = 0.25637907\n",
      "Iteration 127, loss = 0.24908686\n",
      "Iteration 142, loss = 0.25700817\n",
      "Iteration 128, loss = 0.24853893\n",
      "Iteration 143, loss = 0.25667960\n",
      "Iteration 129, loss = 0.24863557\n",
      "Iteration 144, loss = 0.25659466\n",
      "Iteration 130, loss = 0.24827404\n",
      "Iteration 145, loss = 0.25679790\n",
      "Iteration 131, loss = 0.24880695\n",
      "Iteration 146, loss = 0.25740268\n",
      "Iteration 132, loss = 0.24876633\n",
      "Iteration 147, loss = 0.25593812\n",
      "Iteration 133, loss = 0.24805864\n",
      "Iteration 148, loss = 0.25561664\n",
      "Iteration 134, loss = 0.24779977\n",
      "Iteration 149, loss = 0.25609247\n",
      "Iteration 135, loss = 0.24796664\n",
      "Iteration 150, loss = 0.25593331\n",
      "Iteration 136, loss = 0.24820826\n",
      "Iteration 151, loss = 0.25610819\n",
      "Iteration 137, loss = 0.24714721\n",
      "Iteration 152, loss = 0.25548002\n",
      "Iteration 138, loss = 0.24779146\n",
      "Iteration 153, loss = 0.25597026\n",
      "Iteration 139, loss = 0.24706082\n",
      "Iteration 154, loss = 0.25528094\n",
      "Iteration 140, loss = 0.24732662\n",
      "Iteration 155, loss = 0.25555043\n",
      "Iteration 141, loss = 0.24709365\n",
      "Iteration 156, loss = 0.25526490\n",
      "Iteration 142, loss = 0.24711558\n",
      "Iteration 157, loss = 0.25500571\n",
      "Iteration 143, loss = 0.24711071\n",
      "Iteration 158, loss = 0.25527990\n",
      "Iteration 144, loss = 0.24716657\n",
      "Iteration 159, loss = 0.25559033\n",
      "Iteration 145, loss = 0.24707981\n",
      "Iteration 160, loss = 0.25501221\n",
      "Iteration 146, loss = 0.24704647\n",
      "Iteration 161, loss = 0.25514324\n",
      "Iteration 147, loss = 0.24647684\n",
      "Iteration 162, loss = 0.25634897\n",
      "Iteration 163, loss = 0.25565395\n",
      "Iteration 148, loss = 0.24611629\n",
      "Iteration 164, loss = 0.25462770\n",
      "Iteration 149, loss = 0.24591555\n",
      "Iteration 165, loss = 0.25533240\n",
      "Iteration 150, loss = 0.24618622\n",
      "Iteration 166, loss = 0.25456574\n",
      "Iteration 151, loss = 0.24590690\n",
      "Iteration 167, loss = 0.25457599\n",
      "Iteration 152, loss = 0.24583642\n",
      "Iteration 168, loss = 0.25432803\n",
      "Iteration 153, loss = 0.24560320\n",
      "Iteration 169, loss = 0.25411790\n",
      "Iteration 154, loss = 0.24571164\n",
      "Iteration 170, loss = 0.25523829\n",
      "Iteration 155, loss = 0.24590408\n",
      "Iteration 171, loss = 0.25412692\n",
      "Iteration 156, loss = 0.24535906\n",
      "Iteration 172, loss = 0.25395762\n",
      "Iteration 157, loss = 0.24555638\n",
      "Iteration 173, loss = 0.25427500\n",
      "Iteration 158, loss = 0.24569396\n",
      "Iteration 159, loss = 0.24543474\n",
      "Iteration 174, loss = 0.25461774\n",
      "Iteration 160, loss = 0.24523873\n",
      "Iteration 175, loss = 0.25535344\n",
      "Iteration 161, loss = 0.24502621\n",
      "Iteration 176, loss = 0.25366203\n",
      "Iteration 162, loss = 0.24560563\n",
      "Iteration 177, loss = 0.25362420\n",
      "Iteration 163, loss = 0.24496354\n",
      "Iteration 178, loss = 0.25338708\n",
      "Iteration 164, loss = 0.24530503\n",
      "Iteration 179, loss = 0.25332314\n",
      "Iteration 165, loss = 0.24498398\n",
      "Iteration 180, loss = 0.25301352\n",
      "Iteration 166, loss = 0.24480902\n",
      "Iteration 181, loss = 0.25351143\n",
      "Iteration 167, loss = 0.24466050\n",
      "Iteration 182, loss = 0.25348834\n",
      "Iteration 168, loss = 0.24520448\n",
      "Iteration 183, loss = 0.25395241\n",
      "Iteration 169, loss = 0.24440806\n",
      "Iteration 184, loss = 0.25309009\n",
      "Iteration 170, loss = 0.24466535\n",
      "Iteration 185, loss = 0.25243920\n",
      "Iteration 171, loss = 0.24428167\n",
      "Iteration 186, loss = 0.25270526\n",
      "Iteration 172, loss = 0.24469776\n",
      "Iteration 187, loss = 0.25271702\n",
      "Iteration 173, loss = 0.24400185\n",
      "Iteration 188, loss = 0.25263308\n",
      "Iteration 174, loss = 0.24421302\n",
      "Iteration 189, loss = 0.25255535\n",
      "Iteration 175, loss = 0.24504162\n",
      "Iteration 190, loss = 0.25254972\n",
      "Iteration 176, loss = 0.24451356\n",
      "Iteration 191, loss = 0.25258776\n",
      "Iteration 177, loss = 0.24398146\n",
      "Iteration 192, loss = 0.25200538\n",
      "Iteration 178, loss = 0.24426981\n",
      "Iteration 193, loss = 0.25215668\n",
      "Iteration 179, loss = 0.24376283\n",
      "Iteration 194, loss = 0.25245698\n",
      "Iteration 195, loss = 0.25187194\n",
      "Iteration 180, loss = 0.24389997\n",
      "Iteration 196, loss = 0.25218432\n",
      "Iteration 181, loss = 0.24351069\n",
      "Iteration 197, loss = 0.25182314\n",
      "Iteration 182, loss = 0.24396356\n",
      "Iteration 198, loss = 0.25261172Iteration 183, loss = 0.24448040\n",
      "\n",
      "Iteration 184, loss = 0.24340318\n",
      "Iteration 199, loss = 0.25226274\n",
      "Iteration 200, loss = 0.25155086\n",
      "Iteration 185, loss = 0.24345701\n",
      "Iteration 186, loss = 0.24424512\n",
      "Iteration 187, loss = 0.24340402\n",
      "Iteration 1, loss = 0.37571833\n",
      "Iteration 188, loss = 0.24383917\n",
      "Iteration 2, loss = 0.34058429\n",
      "Iteration 189, loss = 0.24373547\n",
      "Iteration 3, loss = 0.31984001\n",
      "Iteration 190, loss = 0.24387600\n",
      "Iteration 4, loss = 0.30499403\n",
      "Iteration 191, loss = 0.24338276\n",
      "Iteration 5, loss = 0.29718710\n",
      "Iteration 192, loss = 0.24288607\n",
      "Iteration 6, loss = 0.29235928\n",
      "Iteration 193, loss = 0.24275634\n",
      "Iteration 7, loss = 0.28952845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 194, loss = 0.24326771\n",
      "Iteration 8, loss = 0.28767747\n",
      "Iteration 195, loss = 0.24294984\n",
      "Iteration 9, loss = 0.28617733\n",
      "Iteration 196, loss = 0.24289636\n",
      "Iteration 10, loss = 0.28496036\n",
      "Iteration 197, loss = 0.24253072\n",
      "Iteration 11, loss = 0.28503280\n",
      "Iteration 198, loss = 0.24373001\n",
      "Iteration 12, loss = 0.28381684\n",
      "Iteration 199, loss = 0.24369834\n",
      "Iteration 13, loss = 0.28433035\n",
      "Iteration 200, loss = 0.24291645\n",
      "Iteration 14, loss = 0.28325870\n",
      "Iteration 15, loss = 0.28277993\n",
      "Iteration 16, loss = 0.28259970\n",
      "Iteration 1, loss = 0.37596449\n",
      "Iteration 17, loss = 0.28223166\n",
      "Iteration 2, loss = 0.34031667\n",
      "Iteration 18, loss = 0.28323055\n",
      "Iteration 3, loss = 0.31943640\n",
      "Iteration 19, loss = 0.28205693\n",
      "Iteration 4, loss = 0.30444975\n",
      "Iteration 20, loss = 0.28144107\n",
      "Iteration 5, loss = 0.29753565\n",
      "Iteration 21, loss = 0.28198956\n",
      "Iteration 6, loss = 0.29229146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22, loss = 0.28138461\n",
      "Iteration 7, loss = 0.28951068\n",
      "Iteration 23, loss = 0.28141930\n",
      "Iteration 8, loss = 0.28751293\n",
      "Iteration 24, loss = 0.28080519\n",
      "Iteration 9, loss = 0.28621753\n",
      "Iteration 25, loss = 0.28013097\n",
      "Iteration 10, loss = 0.28527326\n",
      "Iteration 26, loss = 0.27995583\n",
      "Iteration 11, loss = 0.28484003\n",
      "Iteration 27, loss = 0.28039083\n",
      "Iteration 12, loss = 0.28382639\n",
      "Iteration 28, loss = 0.27955968\n",
      "Iteration 13, loss = 0.28417472\n",
      "Iteration 29, loss = 0.27967531\n",
      "Iteration 14, loss = 0.28313262\n",
      "Iteration 30, loss = 0.27929455\n",
      "Iteration 15, loss = 0.28269007\n",
      "Iteration 31, loss = 0.27847522\n",
      "Iteration 16, loss = 0.28263510\n",
      "Iteration 32, loss = 0.27916123\n",
      "Iteration 17, loss = 0.28195088\n",
      "Iteration 33, loss = 0.27868283\n",
      "Iteration 18, loss = 0.28270725\n",
      "Iteration 34, loss = 0.27811635\n",
      "Iteration 19, loss = 0.28179351\n",
      "Iteration 35, loss = 0.27750969\n",
      "Iteration 20, loss = 0.28132855\n",
      "Iteration 36, loss = 0.27781063\n",
      "Iteration 21, loss = 0.28255001\n",
      "Iteration 37, loss = 0.27718698\n",
      "Iteration 22, loss = 0.28118679\n",
      "Iteration 38, loss = 0.27614422\n",
      "Iteration 23, loss = 0.28115007\n",
      "Iteration 39, loss = 0.27601288\n",
      "Iteration 24, loss = 0.28027589\n",
      "Iteration 40, loss = 0.27574798\n",
      "Iteration 25, loss = 0.28002599\n",
      "Iteration 41, loss = 0.27536764\n",
      "Iteration 26, loss = 0.27967179\n",
      "Iteration 42, loss = 0.27506115\n",
      "Iteration 27, loss = 0.28019103\n",
      "Iteration 43, loss = 0.27427367\n",
      "Iteration 28, loss = 0.27926639\n",
      "Iteration 44, loss = 0.27375630\n",
      "Iteration 29, loss = 0.27956475\n",
      "Iteration 45, loss = 0.27350845\n",
      "Iteration 30, loss = 0.27945414\n",
      "Iteration 46, loss = 0.27336572\n",
      "Iteration 31, loss = 0.27839797\n",
      "Iteration 47, loss = 0.27331853\n",
      "Iteration 32, loss = 0.27927293\n",
      "Iteration 33, loss = 0.27842505\n",
      "Iteration 48, loss = 0.27254857\n",
      "Iteration 34, loss = 0.27814789\n",
      "Iteration 49, loss = 0.27262309\n",
      "Iteration 35, loss = 0.27764740\n",
      "Iteration 50, loss = 0.27219101\n",
      "Iteration 51, loss = 0.27147072\n",
      "Iteration 36, loss = 0.27776996\n",
      "Iteration 37, loss = 0.27736524\n",
      "Iteration 52, loss = 0.27131621\n",
      "Iteration 38, loss = 0.27654437\n",
      "Iteration 53, loss = 0.27248137\n",
      "Iteration 39, loss = 0.27632877\n",
      "Iteration 54, loss = 0.27103315\n",
      "Iteration 40, loss = 0.27618603\n",
      "Iteration 55, loss = 0.27050897\n",
      "Iteration 41, loss = 0.27576543\n",
      "Iteration 56, loss = 0.27058098\n",
      "Iteration 42, loss = 0.27609136\n",
      "Iteration 57, loss = 0.27012763\n",
      "Iteration 43, loss = 0.27502334\n",
      "Iteration 58, loss = 0.26959932\n",
      "Iteration 44, loss = 0.27453226\n",
      "Iteration 59, loss = 0.26941666\n",
      "Iteration 45, loss = 0.27426473\n",
      "Iteration 60, loss = 0.26911912\n",
      "Iteration 61, loss = 0.26957169\n",
      "Iteration 46, loss = 0.27422499\n",
      "Iteration 62, loss = 0.26856788\n",
      "Iteration 47, loss = 0.27463309\n",
      "Iteration 63, loss = 0.26832033\n",
      "Iteration 64, loss = 0.26834418\n",
      "Iteration 65, loss = 0.26776069\n",
      "Iteration 48, loss = 0.27331754\n",
      "Iteration 49, loss = 0.27317031\n",
      "Iteration 50, loss = 0.27289711\n",
      "Iteration 66, loss = 0.26795772\n",
      "Iteration 51, loss = 0.27237924\n",
      "Iteration 67, loss = 0.26755539\n",
      "Iteration 68, loss = 0.26773534\n",
      "Iteration 52, loss = 0.27187579\n",
      "Iteration 69, loss = 0.26745599\n",
      "Iteration 53, loss = 0.27306628\n",
      "Iteration 70, loss = 0.26704901\n",
      "Iteration 71, loss = 0.26682838\n",
      "Iteration 54, loss = 0.27176829\n",
      "Iteration 55, loss = 0.27118714\n",
      "Iteration 72, loss = 0.26686101\n",
      "Iteration 56, loss = 0.27130552\n",
      "Iteration 57, loss = 0.27094873\n",
      "Iteration 73, loss = 0.26623113\n",
      "Iteration 58, loss = 0.27022662\n",
      "Iteration 74, loss = 0.26593738\n",
      "Iteration 59, loss = 0.27014045\n",
      "Iteration 75, loss = 0.26648197\n",
      "Iteration 60, loss = 0.27006918\n",
      "Iteration 76, loss = 0.26566847\n",
      "Iteration 61, loss = 0.27045207\n",
      "Iteration 62, loss = 0.26962161\n",
      "Iteration 77, loss = 0.26520989\n",
      "Iteration 63, loss = 0.26961977\n",
      "Iteration 78, loss = 0.26557277\n",
      "Iteration 64, loss = 0.26944217\n",
      "Iteration 65, loss = 0.26866925\n",
      "Iteration 66, loss = 0.26847802\n",
      "Iteration 79, loss = 0.26554779\n",
      "Iteration 67, loss = 0.26826071\n",
      "Iteration 80, loss = 0.26460654\n",
      "Iteration 68, loss = 0.26935520\n",
      "Iteration 81, loss = 0.26456008\n",
      "Iteration 69, loss = 0.26842631\n",
      "Iteration 82, loss = 0.26545630\n",
      "Iteration 70, loss = 0.26763883\n",
      "Iteration 83, loss = 0.26439884\n",
      "Iteration 71, loss = 0.26757266\n",
      "Iteration 84, loss = 0.26504282\n",
      "Iteration 72, loss = 0.26756566\n",
      "Iteration 85, loss = 0.26382110\n",
      "Iteration 73, loss = 0.26714251\n",
      "Iteration 86, loss = 0.26389115\n",
      "Iteration 74, loss = 0.26672059\n",
      "Iteration 87, loss = 0.26398735\n",
      "Iteration 75, loss = 0.26726161\n",
      "Iteration 88, loss = 0.26357555\n",
      "Iteration 76, loss = 0.26652382\n",
      "Iteration 89, loss = 0.26357408\n",
      "Iteration 90, loss = 0.26338325\n",
      "Iteration 77, loss = 0.26623116\n",
      "Iteration 91, loss = 0.26329031\n",
      "Iteration 78, loss = 0.26649468\n",
      "Iteration 92, loss = 0.26290201\n",
      "Iteration 79, loss = 0.26590706\n",
      "Iteration 93, loss = 0.26273724\n",
      "Iteration 80, loss = 0.26554334\n",
      "Iteration 94, loss = 0.26276027\n",
      "Iteration 81, loss = 0.26566631\n",
      "Iteration 95, loss = 0.26235494\n",
      "Iteration 82, loss = 0.26593718\n",
      "Iteration 96, loss = 0.26278247\n",
      "Iteration 83, loss = 0.26528902\n",
      "Iteration 97, loss = 0.26235699\n",
      "Iteration 84, loss = 0.26510514\n",
      "Iteration 98, loss = 0.26173715\n",
      "Iteration 85, loss = 0.26456570\n",
      "Iteration 99, loss = 0.26254994\n",
      "Iteration 86, loss = 0.26514480\n",
      "Iteration 100, loss = 0.26225507\n",
      "Iteration 87, loss = 0.26432979\n",
      "Iteration 101, loss = 0.26165346\n",
      "Iteration 88, loss = 0.26416478\n",
      "Iteration 102, loss = 0.26119930\n",
      "Iteration 89, loss = 0.26445551\n",
      "Iteration 103, loss = 0.26171274\n",
      "Iteration 104, loss = 0.26087672\n",
      "Iteration 90, loss = 0.26404011\n",
      "Iteration 105, loss = 0.26185811\n",
      "Iteration 91, loss = 0.26389353\n",
      "Iteration 106, loss = 0.26076940\n",
      "Iteration 92, loss = 0.26350312\n",
      "Iteration 93, loss = 0.26321362\n",
      "Iteration 107, loss = 0.26145171\n",
      "Iteration 108, loss = 0.26122312\n",
      "Iteration 94, loss = 0.26406011\n",
      "Iteration 109, loss = 0.26063699\n",
      "Iteration 95, loss = 0.26320747\n",
      "Iteration 110, loss = 0.26035748\n",
      "Iteration 111, loss = 0.25985186\n",
      "Iteration 96, loss = 0.26330890\n",
      "Iteration 112, loss = 0.26055046\n",
      "Iteration 97, loss = 0.26310368\n",
      "Iteration 113, loss = 0.26008720\n",
      "Iteration 98, loss = 0.26245139\n",
      "Iteration 114, loss = 0.26011428\n",
      "Iteration 99, loss = 0.26303261\n",
      "Iteration 115, loss = 0.25971786\n",
      "Iteration 100, loss = 0.26251438\n",
      "Iteration 116, loss = 0.25905461\n",
      "Iteration 101, loss = 0.26175713\n",
      "Iteration 117, loss = 0.25886526\n",
      "Iteration 102, loss = 0.26206705\n",
      "Iteration 118, loss = 0.25896583\n",
      "Iteration 103, loss = 0.26180323\n",
      "Iteration 119, loss = 0.25882013\n",
      "Iteration 104, loss = 0.26160317\n",
      "Iteration 120, loss = 0.25910153\n",
      "Iteration 105, loss = 0.26170929\n",
      "Iteration 106, loss = 0.26163603\n",
      "Iteration 121, loss = 0.25936022\n",
      "Iteration 107, loss = 0.26181162\n",
      "Iteration 122, loss = 0.25845072\n",
      "Iteration 108, loss = 0.26141310\n",
      "Iteration 123, loss = 0.25879260\n",
      "Iteration 109, loss = 0.26081657\n",
      "Iteration 124, loss = 0.25794215\n",
      "Iteration 110, loss = 0.26075812\n",
      "Iteration 125, loss = 0.25796698\n",
      "Iteration 111, loss = 0.26042857\n",
      "Iteration 126, loss = 0.25760201\n",
      "Iteration 112, loss = 0.26050790\n",
      "Iteration 127, loss = 0.25782830\n",
      "Iteration 113, loss = 0.26019298\n",
      "Iteration 128, loss = 0.25732190\n",
      "Iteration 129, loss = 0.25707596\n",
      "Iteration 114, loss = 0.26100700\n",
      "Iteration 130, loss = 0.25708488\n",
      "Iteration 115, loss = 0.26041922\n",
      "Iteration 131, loss = 0.25704007\n",
      "Iteration 116, loss = 0.25954895\n",
      "Iteration 132, loss = 0.25729919\n",
      "Iteration 117, loss = 0.25923945\n",
      "Iteration 118, loss = 0.25940573\n",
      "Iteration 133, loss = 0.25643592\n",
      "Iteration 119, loss = 0.25952580\n",
      "Iteration 134, loss = 0.25662677\n",
      "Iteration 120, loss = 0.25932286\n",
      "Iteration 135, loss = 0.25668082\n",
      "Iteration 121, loss = 0.25989264\n",
      "Iteration 136, loss = 0.25691756\n",
      "Iteration 122, loss = 0.25910002\n",
      "Iteration 137, loss = 0.25578077\n",
      "Iteration 138, loss = 0.25634398\n",
      "Iteration 123, loss = 0.25918021\n",
      "Iteration 124, loss = 0.25847447\n",
      "Iteration 139, loss = 0.25570669\n",
      "Iteration 140, loss = 0.25584748\n",
      "Iteration 125, loss = 0.25873617\n",
      "Iteration 141, loss = 0.25554616\n",
      "Iteration 126, loss = 0.25823010\n",
      "Iteration 142, loss = 0.25563053\n",
      "Iteration 127, loss = 0.25845576\n",
      "Iteration 143, loss = 0.25473252\n",
      "Iteration 128, loss = 0.25814110\n",
      "Iteration 144, loss = 0.25547984\n",
      "Iteration 129, loss = 0.25804612\n",
      "Iteration 145, loss = 0.25546961\n",
      "Iteration 130, loss = 0.25789489\n",
      "Iteration 146, loss = 0.25516437\n",
      "Iteration 131, loss = 0.25793947\n",
      "Iteration 147, loss = 0.25451769\n",
      "Iteration 132, loss = 0.25796775\n",
      "Iteration 148, loss = 0.25440837\n",
      "Iteration 133, loss = 0.25720915\n",
      "Iteration 149, loss = 0.25424828\n",
      "Iteration 134, loss = 0.25761769\n",
      "Iteration 150, loss = 0.25423183\n",
      "Iteration 135, loss = 0.25761164\n",
      "Iteration 151, loss = 0.25386431\n",
      "Iteration 136, loss = 0.25764854\n",
      "Iteration 152, loss = 0.25383974\n",
      "Iteration 137, loss = 0.25693224Iteration 153, loss = 0.25406754\n",
      "\n",
      "Iteration 154, loss = 0.25367521\n",
      "Iteration 138, loss = 0.25691422Iteration 155, loss = 0.25342234\n",
      "\n",
      "Iteration 156, loss = 0.25292611\n",
      "Iteration 157, loss = 0.25402389\n",
      "Iteration 139, loss = 0.25702948\n",
      "Iteration 158, loss = 0.25307637\n",
      "Iteration 140, loss = 0.25684083\n",
      "Iteration 141, loss = 0.25701871\n",
      "Iteration 159, loss = 0.25351695\n",
      "Iteration 142, loss = 0.25694400\n",
      "Iteration 143, loss = 0.25622508\n",
      "Iteration 160, loss = 0.25297575\n",
      "Iteration 144, loss = 0.25680966\n",
      "Iteration 161, loss = 0.25263276\n",
      "Iteration 145, loss = 0.25672542\n",
      "Iteration 162, loss = 0.25252360\n",
      "Iteration 146, loss = 0.25642437\n",
      "Iteration 163, loss = 0.25199979\n",
      "Iteration 147, loss = 0.25600437\n",
      "Iteration 164, loss = 0.25240465\n",
      "Iteration 148, loss = 0.25605500\n",
      "Iteration 165, loss = 0.25188382\n",
      "Iteration 149, loss = 0.25600399\n",
      "Iteration 166, loss = 0.25232080\n",
      "Iteration 150, loss = 0.25574620\n",
      "Iteration 167, loss = 0.25171315\n",
      "Iteration 151, loss = 0.25546053\n",
      "Iteration 168, loss = 0.25225802\n",
      "Iteration 152, loss = 0.25563778\n",
      "Iteration 169, loss = 0.25143968\n",
      "Iteration 153, loss = 0.25562108\n",
      "Iteration 170, loss = 0.25245691\n",
      "Iteration 154, loss = 0.25556348\n",
      "Iteration 171, loss = 0.25111452\n",
      "Iteration 155, loss = 0.25538120\n",
      "Iteration 172, loss = 0.25149433\n",
      "Iteration 156, loss = 0.25508614\n",
      "Iteration 173, loss = 0.25121765\n",
      "Iteration 157, loss = 0.25553249\n",
      "Iteration 174, loss = 0.25118719\n",
      "Iteration 158, loss = 0.25554873\n",
      "Iteration 175, loss = 0.25229944\n",
      "Iteration 176, loss = 0.25145144\n",
      "Iteration 177, loss = 0.25111770Iteration 159, loss = 0.25642395\n",
      "\n",
      "Iteration 178, loss = 0.25095525\n",
      "Iteration 160, loss = 0.25531995\n",
      "Iteration 179, loss = 0.25059382\n",
      "Iteration 161, loss = 0.25504900\n",
      "Iteration 180, loss = 0.25102812\n",
      "Iteration 162, loss = 0.25516829\n",
      "Iteration 181, loss = 0.25053710\n",
      "Iteration 163, loss = 0.25444850\n",
      "Iteration 182, loss = 0.25087670\n",
      "Iteration 164, loss = 0.25498580\n",
      "Iteration 183, loss = 0.25258214\n",
      "Iteration 165, loss = 0.25439160\n",
      "Iteration 184, loss = 0.25021693\n",
      "Iteration 166, loss = 0.25459811\n",
      "Iteration 185, loss = 0.25025275\n",
      "Iteration 167, loss = 0.25416815\n",
      "Iteration 186, loss = 0.25020093\n",
      "Iteration 168, loss = 0.25498892\n",
      "Iteration 187, loss = 0.25001487\n",
      "Iteration 169, loss = 0.25434639\n",
      "Iteration 188, loss = 0.25039353\n",
      "Iteration 170, loss = 0.25522814\n",
      "Iteration 189, loss = 0.25018999\n",
      "Iteration 171, loss = 0.25392465\n",
      "Iteration 190, loss = 0.25035177\n",
      "Iteration 172, loss = 0.25442775\n",
      "Iteration 191, loss = 0.25018117\n",
      "Iteration 173, loss = 0.25414245\n",
      "Iteration 192, loss = 0.24950857\n",
      "Iteration 174, loss = 0.25414721\n",
      "Iteration 193, loss = 0.24967629\n",
      "Iteration 194, loss = 0.25014873\n",
      "Iteration 175, loss = 0.25466089\n",
      "Iteration 195, loss = 0.24951513\n",
      "Iteration 176, loss = 0.25401956\n",
      "Iteration 196, loss = 0.24993868\n",
      "Iteration 177, loss = 0.25358807\n",
      "Iteration 197, loss = 0.24975834\n",
      "Iteration 178, loss = 0.25372520\n",
      "Iteration 198, loss = 0.25029931\n",
      "Iteration 179, loss = 0.25365109\n",
      "Iteration 199, loss = 0.24968608\n",
      "Iteration 180, loss = 0.25360240\n",
      "Iteration 200, loss = 0.24936494\n",
      "Iteration 181, loss = 0.25355628\n",
      "Iteration 182, loss = 0.25338492\n",
      "Iteration 183, loss = 0.25456986\n",
      "Iteration 1, loss = 0.37640798\n",
      "Iteration 2, loss = 0.34099980\n",
      "Iteration 3, loss = 0.31994247\n",
      "Iteration 184, loss = 0.25336297\n",
      "Iteration 4, loss = 0.30461920\n",
      "Iteration 185, loss = 0.25368951\n",
      "Iteration 186, loss = 0.25370375\n",
      "Iteration 5, loss = 0.29739443\n",
      "Iteration 187, loss = 0.25353833\n",
      "Iteration 6, loss = 0.29180241\n",
      "Iteration 188, loss = 0.25352246\n",
      "Iteration 189, loss = 0.25296074\n",
      "Iteration 7, loss = 0.28873780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.28652392\n",
      "Iteration 190, loss = 0.25302400\n",
      "Iteration 9, loss = 0.28523322\n",
      "Iteration 191, loss = 0.25377351\n",
      "Iteration 10, loss = 0.28442534\n",
      "Iteration 192, loss = 0.25251634\n",
      "Iteration 193, loss = 0.25245643\n",
      "Iteration 194, loss = 0.25269223\n",
      "Iteration 11, loss = 0.28384687\n",
      "Iteration 12, loss = 0.28284396\n",
      "Iteration 13, loss = 0.28315661\n",
      "Iteration 195, loss = 0.25247099\n",
      "Iteration 14, loss = 0.28184939\n",
      "Iteration 196, loss = 0.25278077\n",
      "Iteration 15, loss = 0.28199583\n",
      "Iteration 197, loss = 0.25296263\n",
      "Iteration 16, loss = 0.28151043\n",
      "Iteration 17, loss = 0.28092868\n",
      "Iteration 198, loss = 0.25223500\n",
      "Iteration 18, loss = 0.28154372\n",
      "Iteration 19, loss = 0.28079098\n",
      "Iteration 199, loss = 0.25245931\n",
      "Iteration 20, loss = 0.28060259\n",
      "Iteration 200, loss = 0.25221894\n",
      "Iteration 21, loss = 0.28141374\n",
      "Iteration 22, loss = 0.28018395Iteration 1, loss = 0.37749010\n",
      "\n",
      "Iteration 2, loss = 0.34166351\n",
      "Iteration 3, loss = 0.32130727\n",
      "Iteration 23, loss = 0.28021831\n",
      "Iteration 4, loss = 0.30652185\n",
      "Iteration 24, loss = 0.27941883Iteration 5, loss = 0.29799226\n",
      "\n",
      "Iteration 6, loss = 0.29305344\n",
      "Iteration 25, loss = 0.27917975Iteration 7, loss = 0.28979190\n",
      "\n",
      "Iteration 8, loss = 0.28760826\n",
      "Iteration 26, loss = 0.27881367\n",
      "Iteration 9, loss = 0.28638397\n",
      "Iteration 27, loss = 0.27951983\n",
      "Iteration 10, loss = 0.28517845\n",
      "Iteration 28, loss = 0.27848609\n",
      "Iteration 11, loss = 0.28455745\n",
      "Iteration 29, loss = 0.27888734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 0.28417885\n",
      "Iteration 30, loss = 0.27852294\n",
      "Iteration 13, loss = 0.28348263\n",
      "Iteration 31, loss = 0.27735628\n",
      "Iteration 14, loss = 0.28343939\n",
      "Iteration 32, loss = 0.27803025\n",
      "Iteration 15, loss = 0.28244314\n",
      "Iteration 33, loss = 0.27721095\n",
      "Iteration 16, loss = 0.28221617\n",
      "Iteration 34, loss = 0.27717255\n",
      "Iteration 17, loss = 0.28304671\n",
      "Iteration 35, loss = 0.27709339\n",
      "Iteration 18, loss = 0.28229860\n",
      "Iteration 36, loss = 0.27676088\n",
      "Iteration 19, loss = 0.28184428\n",
      "Iteration 37, loss = 0.27591097\n",
      "Iteration 20, loss = 0.28167389\n",
      "Iteration 21, loss = 0.28149936\n",
      "Iteration 38, loss = 0.27581908\n",
      "Iteration 22, loss = 0.28101673\n",
      "Iteration 23, loss = 0.28066327Iteration 39, loss = 0.27498438\n",
      "\n",
      "Iteration 24, loss = 0.28015341\n",
      "Iteration 40, loss = 0.27476557\n",
      "Iteration 25, loss = 0.27987353\n",
      "Iteration 41, loss = 0.27457104\n",
      "Iteration 26, loss = 0.27951143\n",
      "Iteration 42, loss = 0.27467227\n",
      "Iteration 27, loss = 0.28010903\n",
      "Iteration 43, loss = 0.27403079\n",
      "Iteration 44, loss = 0.27330069\n",
      "Iteration 28, loss = 0.27910640\n",
      "Iteration 45, loss = 0.27293811\n",
      "Iteration 29, loss = 0.27886349\n",
      "Iteration 46, loss = 0.27284782\n",
      "Iteration 30, loss = 0.27861065\n",
      "Iteration 47, loss = 0.27283179\n",
      "Iteration 31, loss = 0.27870030\n",
      "Iteration 48, loss = 0.27228826\n",
      "Iteration 32, loss = 0.27860396\n",
      "Iteration 49, loss = 0.27214025\n",
      "Iteration 33, loss = 0.27781502\n",
      "Iteration 50, loss = 0.27165411\n",
      "Iteration 34, loss = 0.27732271\n",
      "Iteration 51, loss = 0.27132547\n",
      "Iteration 35, loss = 0.27682002\n",
      "Iteration 52, loss = 0.27029651\n",
      "Iteration 36, loss = 0.27651229\n",
      "Iteration 53, loss = 0.27125457\n",
      "Iteration 37, loss = 0.27621554\n",
      "Iteration 54, loss = 0.26993390\n",
      "Iteration 38, loss = 0.27574908\n",
      "Iteration 55, loss = 0.26934866\n",
      "Iteration 39, loss = 0.27544824\n",
      "Iteration 56, loss = 0.26932836\n",
      "Iteration 40, loss = 0.27563431\n",
      "Iteration 57, loss = 0.26916722Iteration 41, loss = 0.27643292\n",
      "\n",
      "Iteration 58, loss = 0.26807873\n",
      "Iteration 42, loss = 0.27425785\n",
      "Iteration 43, loss = 0.27436145\n",
      "Iteration 59, loss = 0.26789325\n",
      "Iteration 44, loss = 0.27361808\n",
      "Iteration 60, loss = 0.26815423\n",
      "Iteration 45, loss = 0.27327620\n",
      "Iteration 61, loss = 0.26876285\n",
      "Iteration 46, loss = 0.27292775\n",
      "Iteration 62, loss = 0.26703453\n",
      "Iteration 47, loss = 0.27245135\n",
      "Iteration 63, loss = 0.26659402\n",
      "Iteration 48, loss = 0.27146352\n",
      "Iteration 64, loss = 0.26701061\n",
      "Iteration 49, loss = 0.27147748\n",
      "Iteration 65, loss = 0.26632834\n",
      "Iteration 50, loss = 0.27121017\n",
      "Iteration 66, loss = 0.26588889\n",
      "Iteration 51, loss = 0.27053119\n",
      "Iteration 67, loss = 0.26563064\n",
      "Iteration 52, loss = 0.27026397\n",
      "Iteration 68, loss = 0.26580845\n",
      "Iteration 53, loss = 0.26993657\n",
      "Iteration 69, loss = 0.26491107\n",
      "Iteration 54, loss = 0.27037591\n",
      "Iteration 70, loss = 0.26473935\n",
      "Iteration 55, loss = 0.27015781\n",
      "Iteration 71, loss = 0.26456995\n",
      "Iteration 56, loss = 0.26922528\n",
      "Iteration 72, loss = 0.26446465\n",
      "Iteration 57, loss = 0.26893420\n",
      "Iteration 73, loss = 0.26356289\n",
      "Iteration 58, loss = 0.26910331\n",
      "Iteration 74, loss = 0.26287190\n",
      "Iteration 59, loss = 0.26905501\n",
      "Iteration 60, loss = 0.26766455\n",
      "Iteration 75, loss = 0.26367613\n",
      "Iteration 61, loss = 0.26710849\n",
      "Iteration 76, loss = 0.26280572\n",
      "Iteration 62, loss = 0.26742322\n",
      "Iteration 77, loss = 0.26249396\n",
      "Iteration 78, loss = 0.26257238\n",
      "Iteration 63, loss = 0.26701628\n",
      "Iteration 79, loss = 0.26192841\n",
      "Iteration 64, loss = 0.26658532\n",
      "Iteration 65, loss = 0.26640747\n",
      "Iteration 80, loss = 0.26159140\n",
      "Iteration 66, loss = 0.26645888\n",
      "Iteration 81, loss = 0.26143592\n",
      "Iteration 67, loss = 0.26609693\n",
      "Iteration 82, loss = 0.26210784\n",
      "Iteration 68, loss = 0.26552280\n",
      "Iteration 83, loss = 0.26217222\n",
      "Iteration 69, loss = 0.26528296\n",
      "Iteration 84, loss = 0.26103598\n",
      "Iteration 70, loss = 0.26457510\n",
      "Iteration 85, loss = 0.26067412\n",
      "Iteration 71, loss = 0.26434561\n",
      "Iteration 86, loss = 0.26132109\n",
      "Iteration 72, loss = 0.26477303\n",
      "Iteration 87, loss = 0.26023847\n",
      "Iteration 73, loss = 0.26384519\n",
      "Iteration 88, loss = 0.26014170\n",
      "Iteration 74, loss = 0.26389410\n",
      "Iteration 89, loss = 0.26002337\n",
      "Iteration 75, loss = 0.26393154\n",
      "Iteration 90, loss = 0.25949610\n",
      "Iteration 76, loss = 0.26325137\n",
      "Iteration 91, loss = 0.25937059\n",
      "Iteration 77, loss = 0.26283242\n",
      "Iteration 92, loss = 0.25920946\n",
      "Iteration 78, loss = 0.26344890\n",
      "Iteration 93, loss = 0.25890095\n",
      "Iteration 79, loss = 0.26277888\n",
      "Iteration 94, loss = 0.25904343\n",
      "Iteration 80, loss = 0.26226614\n",
      "Iteration 95, loss = 0.25883487\n",
      "Iteration 81, loss = 0.26201291\n",
      "Iteration 96, loss = 0.25830517\n",
      "Iteration 82, loss = 0.26179566\n",
      "Iteration 97, loss = 0.25841593\n",
      "Iteration 83, loss = 0.26179701\n",
      "Iteration 98, loss = 0.25781797\n",
      "Iteration 84, loss = 0.26186120\n",
      "Iteration 99, loss = 0.25811447\n",
      "Iteration 85, loss = 0.26121322\n",
      "Iteration 86, loss = 0.26131465\n",
      "Iteration 100, loss = 0.25743665\n",
      "Iteration 87, loss = 0.26136009\n",
      "Iteration 101, loss = 0.25744913\n",
      "Iteration 88, loss = 0.26065410\n",
      "Iteration 102, loss = 0.25709766\n",
      "Iteration 89, loss = 0.26098390\n",
      "Iteration 103, loss = 0.25750741\n",
      "Iteration 90, loss = 0.25997358\n",
      "Iteration 104, loss = 0.25681937\n",
      "Iteration 91, loss = 0.26021946\n",
      "Iteration 105, loss = 0.25720692\n",
      "Iteration 92, loss = 0.26015814\n",
      "Iteration 106, loss = 0.25680634\n",
      "Iteration 93, loss = 0.25993511\n",
      "Iteration 107, loss = 0.25650488\n",
      "Iteration 94, loss = 0.25935763\n",
      "Iteration 95, loss = 0.26023815\n",
      "Iteration 108, loss = 0.25631750\n",
      "Iteration 96, loss = 0.25922908\n",
      "Iteration 109, loss = 0.25645323\n",
      "Iteration 97, loss = 0.25974511\n",
      "Iteration 110, loss = 0.25605604\n",
      "Iteration 98, loss = 0.25918017\n",
      "Iteration 111, loss = 0.25588136\n",
      "Iteration 99, loss = 0.25875216\n",
      "Iteration 112, loss = 0.25609208\n",
      "Iteration 100, loss = 0.25895117\n",
      "Iteration 113, loss = 0.25620795\n",
      "Iteration 101, loss = 0.26045765\n",
      "Iteration 114, loss = 0.25656492Iteration 102, loss = 0.25833821\n",
      "\n",
      "Iteration 103, loss = 0.25828128\n",
      "Iteration 115, loss = 0.25605603\n",
      "Iteration 104, loss = 0.25861726\n",
      "Iteration 116, loss = 0.25530514\n",
      "Iteration 105, loss = 0.25806816\n",
      "Iteration 117, loss = 0.25498580\n",
      "Iteration 106, loss = 0.25780733\n",
      "Iteration 118, loss = 0.25505708\n",
      "Iteration 107, loss = 0.25798865\n",
      "Iteration 119, loss = 0.25541945\n",
      "Iteration 108, loss = 0.25865250\n",
      "Iteration 120, loss = 0.25501691\n",
      "Iteration 109, loss = 0.25768323\n",
      "Iteration 121, loss = 0.25503141\n",
      "Iteration 110, loss = 0.25754913\n",
      "Iteration 122, loss = 0.25416298\n",
      "Iteration 111, loss = 0.25728770\n",
      "Iteration 123, loss = 0.25442479\n",
      "Iteration 112, loss = 0.25718669\n",
      "Iteration 124, loss = 0.25400080\n",
      "Iteration 113, loss = 0.25882308\n",
      "Iteration 125, loss = 0.25419862\n",
      "Iteration 114, loss = 0.25771133\n",
      "Iteration 126, loss = 0.25369131\n",
      "Iteration 115, loss = 0.25831807\n",
      "Iteration 127, loss = 0.25396683\n",
      "Iteration 116, loss = 0.25679628\n",
      "Iteration 128, loss = 0.25373994\n",
      "Iteration 117, loss = 0.25669942\n",
      "Iteration 129, loss = 0.25368222\n",
      "Iteration 118, loss = 0.25646990\n",
      "Iteration 130, loss = 0.25403725\n",
      "Iteration 119, loss = 0.25674995\n",
      "Iteration 120, loss = 0.25618668Iteration 131, loss = 0.25320332\n",
      "\n",
      "Iteration 132, loss = 0.25334433\n",
      "Iteration 121, loss = 0.25627381\n",
      "Iteration 133, loss = 0.25316013\n",
      "Iteration 122, loss = 0.25658346\n",
      "Iteration 134, loss = 0.25360045\n",
      "Iteration 123, loss = 0.25647338\n",
      "Iteration 135, loss = 0.25305744\n",
      "Iteration 124, loss = 0.25601772\n",
      "Iteration 136, loss = 0.25343267\n",
      "Iteration 125, loss = 0.25617051\n",
      "Iteration 126, loss = 0.25616418\n",
      "Iteration 137, loss = 0.25270242\n",
      "Iteration 127, loss = 0.25537591\n",
      "Iteration 138, loss = 0.25265780\n",
      "Iteration 139, loss = 0.25251354\n",
      "Iteration 128, loss = 0.25525649\n",
      "Iteration 140, loss = 0.25253661Iteration 129, loss = 0.25518109\n",
      "\n",
      "Iteration 130, loss = 0.25487600\n",
      "Iteration 141, loss = 0.25250650\n",
      "Iteration 131, loss = 0.25538656\n",
      "Iteration 142, loss = 0.25210993\n",
      "Iteration 132, loss = 0.25510036\n",
      "Iteration 143, loss = 0.25171412\n",
      "Iteration 133, loss = 0.25457858\n",
      "Iteration 144, loss = 0.25207723\n",
      "Iteration 134, loss = 0.25476456\n",
      "Iteration 145, loss = 0.25230308\n",
      "Iteration 135, loss = 0.25430468\n",
      "Iteration 146, loss = 0.25259968\n",
      "Iteration 136, loss = 0.25435991\n",
      "Iteration 147, loss = 0.25127170\n",
      "Iteration 137, loss = 0.25408303\n",
      "Iteration 138, loss = 0.25456283\n",
      "Iteration 148, loss = 0.25142063\n",
      "Iteration 149, loss = 0.25143932\n",
      "Iteration 139, loss = 0.25464296\n",
      "Iteration 150, loss = 0.25124954\n",
      "Iteration 140, loss = 0.25377403\n",
      "Iteration 141, loss = 0.25393017\n",
      "Iteration 151, loss = 0.25093506\n",
      "Iteration 142, loss = 0.25327247\n",
      "Iteration 143, loss = 0.25353495\n",
      "Iteration 152, loss = 0.25092496\n",
      "Iteration 144, loss = 0.25349836\n",
      "Iteration 145, loss = 0.25369015\n",
      "Iteration 146, loss = 0.25334738\n",
      "Iteration 153, loss = 0.25078371\n",
      "Iteration 147, loss = 0.25327793\n",
      "Iteration 148, loss = 0.25342887\n",
      "Iteration 154, loss = 0.25119444\n",
      "Iteration 155, loss = 0.25093428\n",
      "Iteration 149, loss = 0.25333720\n",
      "Iteration 156, loss = 0.25064677\n",
      "Iteration 150, loss = 0.25271612\n",
      "Iteration 157, loss = 0.25074736\n",
      "Iteration 158, loss = 0.25046159\n",
      "Iteration 151, loss = 0.25297551\n",
      "Iteration 159, loss = 0.25116888\n",
      "Iteration 152, loss = 0.25291785\n",
      "Iteration 160, loss = 0.25013727\n",
      "Iteration 153, loss = 0.25235485\n",
      "Iteration 161, loss = 0.25044833\n",
      "Iteration 154, loss = 0.25247313\n",
      "Iteration 162, loss = 0.25027603\n",
      "Iteration 155, loss = 0.25196630\n",
      "Iteration 156, loss = 0.25272384\n",
      "Iteration 157, loss = 0.25259566\n",
      "Iteration 163, loss = 0.25010591\n",
      "Iteration 158, loss = 0.25241731\n",
      "Iteration 164, loss = 0.25007902\n",
      "Iteration 165, loss = 0.24975650\n",
      "Iteration 159, loss = 0.25230949\n",
      "Iteration 166, loss = 0.24978903\n",
      "Iteration 160, loss = 0.25182682\n",
      "Iteration 167, loss = 0.24902657\n",
      "Iteration 168, loss = 0.25011203\n",
      "Iteration 161, loss = 0.25166566\n",
      "Iteration 169, loss = 0.24958100\n",
      "Iteration 162, loss = 0.25183862\n",
      "Iteration 163, loss = 0.25148165\n",
      "Iteration 164, loss = 0.25186717\n",
      "Iteration 165, loss = 0.25104610\n",
      "Iteration 170, loss = 0.25081419\n",
      "Iteration 166, loss = 0.25128418\n",
      "Iteration 171, loss = 0.24920006\n",
      "Iteration 172, loss = 0.24976895\n",
      "Iteration 173, loss = 0.24963581\n",
      "Iteration 167, loss = 0.25131574\n",
      "Iteration 174, loss = 0.24947677\n",
      "Iteration 168, loss = 0.25119022\n",
      "Iteration 169, loss = 0.25066671\n",
      "Iteration 170, loss = 0.25088615\n",
      "Iteration 175, loss = 0.24967514\n",
      "Iteration 171, loss = 0.25087348\n",
      "Iteration 176, loss = 0.24944395\n",
      "Iteration 172, loss = 0.25093054\n",
      "Iteration 177, loss = 0.24900954\n",
      "Iteration 173, loss = 0.25062891\n",
      "Iteration 178, loss = 0.24921226\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 174, loss = 0.25084042\n",
      "Iteration 175, loss = 0.25033495\n",
      "Iteration 176, loss = 0.24999696\n",
      "Iteration 177, loss = 0.25000145\n",
      "Iteration 178, loss = 0.25023527\n",
      "Iteration 179, loss = 0.24989241\n",
      "Iteration 180, loss = 0.25015624\n",
      "Iteration 181, loss = 0.25052822\n",
      "Iteration 182, loss = 0.25045417\n",
      "Iteration 183, loss = 0.25009460\n",
      "Iteration 184, loss = 0.25001166\n",
      "Iteration 185, loss = 0.25033310\n",
      "Iteration 186, loss = 0.24957693\n",
      "Iteration 187, loss = 0.24924283\n",
      "Iteration 188, loss = 0.24913835\n",
      "Iteration 189, loss = 0.24913252\n",
      "Iteration 190, loss = 0.24883756\n",
      "Iteration 191, loss = 0.24957899\n",
      "Iteration 192, loss = 0.24914097\n",
      "Iteration 193, loss = 0.24878649\n",
      "Iteration 194, loss = 0.24908803\n",
      "Iteration 195, loss = 0.24867207\n",
      "Iteration 196, loss = 0.24885239\n",
      "Iteration 197, loss = 0.24894528\n",
      "Iteration 198, loss = 0.24855684\n",
      "Iteration 199, loss = 0.24880520\n",
      "Iteration 200, loss = 0.24854114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.37136941\n",
      "Iteration 1, loss = 0.37195291\n",
      "Iteration 2, loss = 0.33432819\n",
      "Iteration 3, loss = 0.31376969\n",
      "Iteration 2, loss = 0.33445471\n",
      "Iteration 4, loss = 0.30196535\n",
      "Iteration 3, loss = 0.31278524\n",
      "Iteration 5, loss = 0.29561224\n",
      "Iteration 4, loss = 0.30024108\n",
      "Iteration 6, loss = 0.29148539\n",
      "Iteration 5, loss = 0.29301827\n",
      "Iteration 7, loss = 0.28986558\n",
      "Iteration 6, loss = 0.29000015\n",
      "Iteration 8, loss = 0.28855298\n",
      "Iteration 7, loss = 0.28699711\n",
      "Iteration 9, loss = 0.28681137\n",
      "Iteration 8, loss = 0.28593514\n",
      "Iteration 10, loss = 0.28671653\n",
      "Iteration 9, loss = 0.28502284\n",
      "Iteration 11, loss = 0.28618797\n",
      "Iteration 10, loss = 0.28407555\n",
      "Iteration 12, loss = 0.28590156\n",
      "Iteration 11, loss = 0.28357526\n",
      "Iteration 12, loss = 0.28320408\n",
      "Iteration 13, loss = 0.28588643\n",
      "Iteration 14, loss = 0.28531618\n",
      "Iteration 15, loss = 0.28454651\n",
      "Iteration 13, loss = 0.28350384Iteration 16, loss = 0.28413208\n",
      "\n",
      "Iteration 17, loss = 0.28404133Iteration 14, loss = 0.28275652\n",
      "\n",
      "Iteration 15, loss = 0.28194796\n",
      "Iteration 18, loss = 0.28413309\n",
      "Iteration 16, loss = 0.28154930\n",
      "Iteration 19, loss = 0.28378252\n",
      "Iteration 17, loss = 0.28141064Iteration 20, loss = 0.28309439\n",
      "\n",
      "Iteration 18, loss = 0.28067258Iteration 21, loss = 0.28248657\n",
      "\n",
      "Iteration 22, loss = 0.28249931Iteration 19, loss = 0.28067495\n",
      "\n",
      "Iteration 23, loss = 0.28211623\n",
      "Iteration 20, loss = 0.28056074\n",
      "Iteration 24, loss = 0.28167203\n",
      "Iteration 21, loss = 0.27963786\n",
      "Iteration 22, loss = 0.28006291\n",
      "Iteration 25, loss = 0.28113968\n",
      "Iteration 23, loss = 0.27913188\n",
      "Iteration 26, loss = 0.28095322\n",
      "Iteration 24, loss = 0.27958530\n",
      "Iteration 27, loss = 0.28113383\n",
      "Iteration 25, loss = 0.27876711\n",
      "Iteration 28, loss = 0.28074212\n",
      "Iteration 26, loss = 0.27830001\n",
      "Iteration 29, loss = 0.28080795\n",
      "Iteration 27, loss = 0.27774074\n",
      "Iteration 30, loss = 0.27981002\n",
      "Iteration 28, loss = 0.27798874\n",
      "Iteration 31, loss = 0.27944190\n",
      "Iteration 29, loss = 0.27802168\n",
      "Iteration 32, loss = 0.27904989\n",
      "Iteration 30, loss = 0.27726400\n",
      "Iteration 33, loss = 0.27883629\n",
      "Iteration 31, loss = 0.27649097\n",
      "Iteration 34, loss = 0.27841812\n",
      "Iteration 32, loss = 0.27688242\n",
      "Iteration 35, loss = 0.27792815\n",
      "Iteration 33, loss = 0.27558010\n",
      "Iteration 36, loss = 0.27801148\n",
      "Iteration 34, loss = 0.27547020\n",
      "Iteration 37, loss = 0.27699706\n",
      "Iteration 35, loss = 0.27515164\n",
      "Iteration 38, loss = 0.27724462\n",
      "Iteration 36, loss = 0.27510587\n",
      "Iteration 39, loss = 0.27674467\n",
      "Iteration 37, loss = 0.27434275\n",
      "Iteration 40, loss = 0.27675650\n",
      "Iteration 38, loss = 0.27428465\n",
      "Iteration 41, loss = 0.27661242\n",
      "Iteration 39, loss = 0.27455366\n",
      "Iteration 40, loss = 0.27341512\n",
      "Iteration 42, loss = 0.27643449\n",
      "Iteration 41, loss = 0.27359791\n",
      "Iteration 43, loss = 0.27541553\n",
      "Iteration 42, loss = 0.27315516\n",
      "Iteration 44, loss = 0.27636886\n",
      "Iteration 45, loss = 0.27541952\n",
      "Iteration 46, loss = 0.27471470\n",
      "Iteration 43, loss = 0.27322573\n",
      "Iteration 47, loss = 0.27483580\n",
      "Iteration 48, loss = 0.27448933\n",
      "Iteration 44, loss = 0.27258381\n",
      "Iteration 45, loss = 0.27229015\n",
      "Iteration 49, loss = 0.27446550\n",
      "Iteration 50, loss = 0.27395637\n",
      "Iteration 51, loss = 0.27370840\n",
      "Iteration 46, loss = 0.27192354\n",
      "Iteration 52, loss = 0.27349254\n",
      "Iteration 53, loss = 0.27310838\n",
      "Iteration 47, loss = 0.27162302\n",
      "Iteration 48, loss = 0.27143012\n",
      "Iteration 49, loss = 0.27130167\n",
      "Iteration 54, loss = 0.27275204\n",
      "Iteration 50, loss = 0.27104697\n",
      "Iteration 51, loss = 0.27037931\n",
      "Iteration 55, loss = 0.27290987\n",
      "Iteration 52, loss = 0.27046177\n",
      "Iteration 56, loss = 0.27263132\n",
      "Iteration 53, loss = 0.27029877\n",
      "Iteration 57, loss = 0.27219081\n",
      "Iteration 54, loss = 0.26991894\n",
      "Iteration 58, loss = 0.27196560\n",
      "Iteration 55, loss = 0.26929493\n",
      "Iteration 59, loss = 0.27195318\n",
      "Iteration 56, loss = 0.26896844\n",
      "Iteration 60, loss = 0.27127587\n",
      "Iteration 57, loss = 0.26849165\n",
      "Iteration 61, loss = 0.27105467\n",
      "Iteration 58, loss = 0.26811732\n",
      "Iteration 62, loss = 0.27138687\n",
      "Iteration 59, loss = 0.26828034\n",
      "Iteration 63, loss = 0.27098037Iteration 60, loss = 0.26768791\n",
      "\n",
      "Iteration 61, loss = 0.26756875\n",
      "Iteration 64, loss = 0.27039495\n",
      "Iteration 62, loss = 0.26733941\n",
      "Iteration 65, loss = 0.27035334\n",
      "Iteration 66, loss = 0.26987404\n",
      "Iteration 67, loss = 0.26992608\n",
      "Iteration 63, loss = 0.26663655\n",
      "Iteration 64, loss = 0.26661457\n",
      "Iteration 68, loss = 0.26977680\n",
      "Iteration 65, loss = 0.26621597\n",
      "Iteration 66, loss = 0.26562210\n",
      "Iteration 69, loss = 0.26951128\n",
      "Iteration 70, loss = 0.26922669\n",
      "Iteration 67, loss = 0.26506516\n",
      "Iteration 71, loss = 0.26903744\n",
      "Iteration 68, loss = 0.26437825\n",
      "Iteration 69, loss = 0.26565226\n",
      "Iteration 72, loss = 0.26866181\n",
      "Iteration 70, loss = 0.26416118\n",
      "Iteration 71, loss = 0.26376450\n",
      "Iteration 73, loss = 0.26860398\n",
      "Iteration 72, loss = 0.26393517\n",
      "Iteration 74, loss = 0.26877714\n",
      "Iteration 73, loss = 0.26311255\n",
      "Iteration 75, loss = 0.26806882\n",
      "Iteration 76, loss = 0.26858072\n",
      "Iteration 74, loss = 0.26339445\n",
      "Iteration 77, loss = 0.26804451\n",
      "Iteration 75, loss = 0.26469339\n",
      "Iteration 76, loss = 0.26212633\n",
      "Iteration 78, loss = 0.26784191\n",
      "Iteration 77, loss = 0.26232314\n",
      "Iteration 79, loss = 0.26736347\n",
      "Iteration 78, loss = 0.26189511\n",
      "Iteration 80, loss = 0.26732816\n",
      "Iteration 79, loss = 0.26166452\n",
      "Iteration 81, loss = 0.26797683\n",
      "Iteration 80, loss = 0.26133361\n",
      "Iteration 82, loss = 0.26687887\n",
      "Iteration 81, loss = 0.26130596\n",
      "Iteration 83, loss = 0.26706878\n",
      "Iteration 82, loss = 0.26075080\n",
      "Iteration 84, loss = 0.26688247\n",
      "Iteration 83, loss = 0.26063556\n",
      "Iteration 85, loss = 0.26594102\n",
      "Iteration 84, loss = 0.26007860\n",
      "Iteration 86, loss = 0.26667981\n",
      "Iteration 85, loss = 0.26030210\n",
      "Iteration 87, loss = 0.26531588\n",
      "Iteration 86, loss = 0.25966083\n",
      "Iteration 88, loss = 0.26644299\n",
      "Iteration 87, loss = 0.25932679\n",
      "Iteration 89, loss = 0.26554099\n",
      "Iteration 88, loss = 0.25930531\n",
      "Iteration 90, loss = 0.26508833\n",
      "Iteration 89, loss = 0.25897780\n",
      "Iteration 90, loss = 0.25896130\n",
      "Iteration 91, loss = 0.26554183\n",
      "Iteration 91, loss = 0.25978357\n",
      "Iteration 92, loss = 0.26520933\n",
      "Iteration 92, loss = 0.25847893\n",
      "Iteration 93, loss = 0.26523189\n",
      "Iteration 93, loss = 0.25797523\n",
      "Iteration 94, loss = 0.26442343\n",
      "Iteration 94, loss = 0.25824293\n",
      "Iteration 95, loss = 0.26448462\n",
      "Iteration 95, loss = 0.25849450\n",
      "Iteration 96, loss = 0.26464056\n",
      "Iteration 96, loss = 0.25761870\n",
      "Iteration 97, loss = 0.26444744\n",
      "Iteration 97, loss = 0.25739494\n",
      "Iteration 98, loss = 0.26415560\n",
      "Iteration 98, loss = 0.25736051\n",
      "Iteration 99, loss = 0.26414308\n",
      "Iteration 99, loss = 0.25697085\n",
      "Iteration 100, loss = 0.26389888\n",
      "Iteration 100, loss = 0.25706276\n",
      "Iteration 101, loss = 0.26388344\n",
      "Iteration 101, loss = 0.25695359\n",
      "Iteration 102, loss = 0.26295015\n",
      "Iteration 102, loss = 0.25663404\n",
      "Iteration 103, loss = 0.26407903\n",
      "Iteration 103, loss = 0.25623031\n",
      "Iteration 104, loss = 0.26319900\n",
      "Iteration 104, loss = 0.25615022\n",
      "Iteration 105, loss = 0.26299980\n",
      "Iteration 105, loss = 0.25595519\n",
      "Iteration 106, loss = 0.26282660\n",
      "Iteration 106, loss = 0.25669705\n",
      "Iteration 107, loss = 0.26277350\n",
      "Iteration 107, loss = 0.25591768\n",
      "Iteration 108, loss = 0.26279195\n",
      "Iteration 108, loss = 0.25611647\n",
      "Iteration 109, loss = 0.26262389\n",
      "Iteration 109, loss = 0.25603708\n",
      "Iteration 110, loss = 0.25545698\n",
      "Iteration 110, loss = 0.26320239\n",
      "Iteration 111, loss = 0.25572603\n",
      "Iteration 111, loss = 0.26215029\n",
      "Iteration 112, loss = 0.25523078\n",
      "Iteration 112, loss = 0.26249206\n",
      "Iteration 113, loss = 0.25544234\n",
      "Iteration 113, loss = 0.26198497\n",
      "Iteration 114, loss = 0.25529601\n",
      "Iteration 114, loss = 0.26219272\n",
      "Iteration 115, loss = 0.25486257\n",
      "Iteration 115, loss = 0.26157911\n",
      "Iteration 116, loss = 0.25513765\n",
      "Iteration 116, loss = 0.26192075\n",
      "Iteration 117, loss = 0.25509423\n",
      "Iteration 117, loss = 0.26195043\n",
      "Iteration 118, loss = 0.25450356\n",
      "Iteration 118, loss = 0.26142603\n",
      "Iteration 119, loss = 0.25447629\n",
      "Iteration 119, loss = 0.26121680\n",
      "Iteration 120, loss = 0.25424182\n",
      "Iteration 120, loss = 0.26089239\n",
      "Iteration 121, loss = 0.25482912\n",
      "Iteration 121, loss = 0.26061664\n",
      "Iteration 122, loss = 0.25390081\n",
      "Iteration 122, loss = 0.26092254\n",
      "Iteration 123, loss = 0.25438503\n",
      "Iteration 124, loss = 0.25404023\n",
      "Iteration 123, loss = 0.26077724\n",
      "Iteration 125, loss = 0.25462830\n",
      "Iteration 124, loss = 0.26118179\n",
      "Iteration 126, loss = 0.25381068\n",
      "Iteration 125, loss = 0.26054451\n",
      "Iteration 127, loss = 0.25382498\n",
      "Iteration 126, loss = 0.26073891\n",
      "Iteration 128, loss = 0.25359914\n",
      "Iteration 127, loss = 0.26074783\n",
      "Iteration 129, loss = 0.25417159\n",
      "Iteration 128, loss = 0.26023872\n",
      "Iteration 130, loss = 0.25379709\n",
      "Iteration 129, loss = 0.26104539\n",
      "Iteration 131, loss = 0.25329644\n",
      "Iteration 130, loss = 0.26031817\n",
      "Iteration 132, loss = 0.25333844\n",
      "Iteration 131, loss = 0.25967101\n",
      "Iteration 133, loss = 0.25323905\n",
      "Iteration 132, loss = 0.26067460\n",
      "Iteration 134, loss = 0.25336451\n",
      "Iteration 133, loss = 0.25965680\n",
      "Iteration 135, loss = 0.25313927\n",
      "Iteration 136, loss = 0.25302084\n",
      "Iteration 134, loss = 0.25972516\n",
      "Iteration 137, loss = 0.25310376\n",
      "Iteration 135, loss = 0.25940723\n",
      "Iteration 138, loss = 0.25228324\n",
      "Iteration 136, loss = 0.25942539\n",
      "Iteration 139, loss = 0.25307326\n",
      "Iteration 137, loss = 0.25937875\n",
      "Iteration 140, loss = 0.25281510\n",
      "Iteration 138, loss = 0.25929301\n",
      "Iteration 141, loss = 0.25250589\n",
      "Iteration 139, loss = 0.25881921\n",
      "Iteration 142, loss = 0.25260797\n",
      "Iteration 140, loss = 0.25960311\n",
      "Iteration 143, loss = 0.25222056\n",
      "Iteration 141, loss = 0.25900640\n",
      "Iteration 144, loss = 0.25216242\n",
      "Iteration 142, loss = 0.25900039\n",
      "Iteration 145, loss = 0.25214863\n",
      "Iteration 143, loss = 0.25932929\n",
      "Iteration 146, loss = 0.25206611\n",
      "Iteration 144, loss = 0.25918095\n",
      "Iteration 147, loss = 0.25263565\n",
      "Iteration 145, loss = 0.25888261\n",
      "Iteration 148, loss = 0.25215442\n",
      "Iteration 146, loss = 0.25924916\n",
      "Iteration 149, loss = 0.25191325\n",
      "Iteration 147, loss = 0.25882327\n",
      "Iteration 150, loss = 0.25210816\n",
      "Iteration 148, loss = 0.25823182\n",
      "Iteration 151, loss = 0.25163958\n",
      "Iteration 149, loss = 0.25847483\n",
      "Iteration 152, loss = 0.25198893\n",
      "Iteration 150, loss = 0.25834502\n",
      "Iteration 153, loss = 0.25166530\n",
      "Iteration 151, loss = 0.25864092\n",
      "Iteration 154, loss = 0.25198001\n",
      "Iteration 152, loss = 0.25864822\n",
      "Iteration 155, loss = 0.25128041\n",
      "Iteration 153, loss = 0.25865473\n",
      "Iteration 156, loss = 0.25124498\n",
      "Iteration 154, loss = 0.25829963\n",
      "Iteration 157, loss = 0.25133240\n",
      "Iteration 155, loss = 0.25792511\n",
      "Iteration 158, loss = 0.25132019\n",
      "Iteration 156, loss = 0.25793935\n",
      "Iteration 159, loss = 0.25119524\n",
      "Iteration 157, loss = 0.25801720\n",
      "Iteration 160, loss = 0.25151532\n",
      "Iteration 158, loss = 0.25785830\n",
      "Iteration 161, loss = 0.25127936\n",
      "Iteration 159, loss = 0.25759682\n",
      "Iteration 162, loss = 0.25099798\n",
      "Iteration 160, loss = 0.25799707\n",
      "Iteration 163, loss = 0.25117912\n",
      "Iteration 161, loss = 0.25775448\n",
      "Iteration 164, loss = 0.25175952\n",
      "Iteration 165, loss = 0.25145210\n",
      "Iteration 162, loss = 0.25763357\n",
      "Iteration 166, loss = 0.25139525\n",
      "Iteration 167, loss = 0.25087508\n",
      "Iteration 163, loss = 0.25778493\n",
      "Iteration 168, loss = 0.25079384\n",
      "Iteration 164, loss = 0.25753634\n",
      "Iteration 169, loss = 0.25131525\n",
      "Iteration 165, loss = 0.25803670\n",
      "Iteration 170, loss = 0.25175693\n",
      "Iteration 166, loss = 0.25717643\n",
      "Iteration 171, loss = 0.25113593\n",
      "Iteration 167, loss = 0.25711251\n",
      "Iteration 172, loss = 0.25127125\n",
      "Iteration 173, loss = 0.25169945\n",
      "Iteration 168, loss = 0.25710994\n",
      "Iteration 174, loss = 0.25054299\n",
      "Iteration 169, loss = 0.25697884\n",
      "Iteration 170, loss = 0.25707567\n",
      "Iteration 175, loss = 0.25034761\n",
      "Iteration 171, loss = 0.25705477\n",
      "Iteration 176, loss = 0.25024010\n",
      "Iteration 172, loss = 0.25725596\n",
      "Iteration 177, loss = 0.25014111\n",
      "Iteration 173, loss = 0.25695497\n",
      "Iteration 178, loss = 0.25057401\n",
      "Iteration 174, loss = 0.25669562\n",
      "Iteration 179, loss = 0.25094666\n",
      "Iteration 175, loss = 0.25700956\n",
      "Iteration 180, loss = 0.25025412\n",
      "Iteration 176, loss = 0.25686693\n",
      "Iteration 181, loss = 0.25009432\n",
      "Iteration 177, loss = 0.25671569\n",
      "Iteration 182, loss = 0.25018185\n",
      "Iteration 178, loss = 0.25679619\n",
      "Iteration 183, loss = 0.25006728\n",
      "Iteration 179, loss = 0.25692766\n",
      "Iteration 184, loss = 0.25020286\n",
      "Iteration 180, loss = 0.25700040\n",
      "Iteration 185, loss = 0.25023447\n",
      "Iteration 181, loss = 0.25647665\n",
      "Iteration 186, loss = 0.24994691\n",
      "Iteration 182, loss = 0.25675379\n",
      "Iteration 187, loss = 0.24983620\n",
      "Iteration 183, loss = 0.25683444\n",
      "Iteration 188, loss = 0.25005920\n",
      "Iteration 184, loss = 0.25635773\n",
      "Iteration 185, loss = 0.25610445\n",
      "Iteration 189, loss = 0.25059313Iteration 186, loss = 0.25659637\n",
      "\n",
      "Iteration 190, loss = 0.25018141\n",
      "Iteration 187, loss = 0.25701612\n",
      "Iteration 191, loss = 0.25000731\n",
      "Iteration 192, loss = 0.25013051\n",
      "Iteration 188, loss = 0.25601734\n",
      "Iteration 189, loss = 0.25644494\n",
      "Iteration 190, loss = 0.25588799\n",
      "Iteration 193, loss = 0.25008202\n",
      "Iteration 194, loss = 0.24977938\n",
      "Iteration 191, loss = 0.25652027\n",
      "Iteration 195, loss = 0.24949985\n",
      "Iteration 196, loss = 0.24967335\n",
      "Iteration 192, loss = 0.25602912\n",
      "Iteration 193, loss = 0.25590603\n",
      "Iteration 197, loss = 0.25012168\n",
      "Iteration 198, loss = 0.24970111\n",
      "Iteration 194, loss = 0.25538540\n",
      "Iteration 195, loss = 0.25711311\n",
      "Iteration 196, loss = 0.25572806\n",
      "Iteration 199, loss = 0.24998038\n",
      "Iteration 200, loss = 0.24980773\n",
      "Iteration 197, loss = 0.25551570\n",
      "Iteration 198, loss = 0.25582688\n",
      "Iteration 1, loss = 0.37231634\n",
      "Iteration 199, loss = 0.25628866\n",
      "Iteration 200, loss = 0.25560287\n",
      "Iteration 2, loss = 0.33534263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 0.31471080\n",
      "Iteration 4, loss = 0.30286183\n",
      "Iteration 1, loss = 0.37319083\n",
      "Iteration 5, loss = 0.29596653\n",
      "Iteration 2, loss = 0.33482910\n",
      "Iteration 6, loss = 0.29304128\n",
      "Iteration 3, loss = 0.31374112\n",
      "Iteration 7, loss = 0.29017060\n",
      "Iteration 8, loss = 0.28886690\n",
      "Iteration 4, loss = 0.30153859\n",
      "Iteration 9, loss = 0.28775163\n",
      "Iteration 5, loss = 0.29505992\n",
      "Iteration 10, loss = 0.28694365\n",
      "Iteration 11, loss = 0.28638237\n",
      "Iteration 6, loss = 0.29158911\n",
      "Iteration 7, loss = 0.28897022\n",
      "Iteration 12, loss = 0.28609315\n",
      "Iteration 13, loss = 0.28722563\n",
      "Iteration 8, loss = 0.28793176\n",
      "Iteration 9, loss = 0.28698905\n",
      "Iteration 14, loss = 0.28522065\n",
      "Iteration 10, loss = 0.28562688\n",
      "Iteration 15, loss = 0.28472891\n",
      "Iteration 11, loss = 0.28546375\n",
      "Iteration 16, loss = 0.28404662\n",
      "Iteration 12, loss = 0.28517454\n",
      "Iteration 17, loss = 0.28407279\n",
      "Iteration 13, loss = 0.28447997\n",
      "Iteration 18, loss = 0.28323211\n",
      "Iteration 14, loss = 0.28393293\n",
      "Iteration 19, loss = 0.28342779\n",
      "Iteration 15, loss = 0.28378986\n",
      "Iteration 20, loss = 0.28348622\n",
      "Iteration 16, loss = 0.28275726\n",
      "Iteration 21, loss = 0.28256047\n",
      "Iteration 17, loss = 0.28335196\n",
      "Iteration 22, loss = 0.28255017\n",
      "Iteration 18, loss = 0.28270449\n",
      "Iteration 23, loss = 0.28166738\n",
      "Iteration 19, loss = 0.28298338\n",
      "Iteration 24, loss = 0.28180253\n",
      "Iteration 20, loss = 0.28249114\n",
      "Iteration 25, loss = 0.28118540\n",
      "Iteration 21, loss = 0.28167910\n",
      "Iteration 26, loss = 0.28072075\n",
      "Iteration 22, loss = 0.28127386\n",
      "Iteration 27, loss = 0.28048296\n",
      "Iteration 23, loss = 0.28086178\n",
      "Iteration 28, loss = 0.28132030\n",
      "Iteration 24, loss = 0.28100623\n",
      "Iteration 29, loss = 0.28047763\n",
      "Iteration 25, loss = 0.27977924\n",
      "Iteration 26, loss = 0.27999262\n",
      "Iteration 30, loss = 0.28027479\n",
      "Iteration 27, loss = 0.27924994\n",
      "Iteration 31, loss = 0.27984306\n",
      "Iteration 32, loss = 0.27984591Iteration 28, loss = 0.28022948\n",
      "\n",
      "Iteration 29, loss = 0.27909814\n",
      "Iteration 33, loss = 0.27870639\n",
      "Iteration 34, loss = 0.27850294\n",
      "Iteration 30, loss = 0.27873360\n",
      "Iteration 35, loss = 0.27835309\n",
      "Iteration 31, loss = 0.27838997\n",
      "Iteration 36, loss = 0.27818458\n",
      "Iteration 32, loss = 0.27868749\n",
      "Iteration 37, loss = 0.27787850\n",
      "Iteration 38, loss = 0.27779189\n",
      "Iteration 33, loss = 0.27763911\n",
      "Iteration 39, loss = 0.27829489\n",
      "Iteration 34, loss = 0.27731218\n",
      "Iteration 40, loss = 0.27705292\n",
      "Iteration 35, loss = 0.27731234\n",
      "Iteration 41, loss = 0.27724698\n",
      "Iteration 36, loss = 0.27704582\n",
      "Iteration 42, loss = 0.27651664\n",
      "Iteration 37, loss = 0.27636206\n",
      "Iteration 38, loss = 0.27642704\n",
      "Iteration 43, loss = 0.27668425\n",
      "Iteration 39, loss = 0.27673693\n",
      "Iteration 44, loss = 0.27617780\n",
      "Iteration 40, loss = 0.27566254\n",
      "Iteration 45, loss = 0.27592318\n",
      "Iteration 46, loss = 0.27587195\n",
      "Iteration 41, loss = 0.27577248\n",
      "Iteration 47, loss = 0.27544919\n",
      "Iteration 42, loss = 0.27504573\n",
      "Iteration 48, loss = 0.27553196\n",
      "Iteration 49, loss = 0.27517483\n",
      "Iteration 43, loss = 0.27510568\n",
      "Iteration 44, loss = 0.27476417\n",
      "Iteration 50, loss = 0.27523808\n",
      "Iteration 45, loss = 0.27437807\n",
      "Iteration 51, loss = 0.27502208\n",
      "Iteration 46, loss = 0.27438386\n",
      "Iteration 52, loss = 0.27479500\n",
      "Iteration 47, loss = 0.27407104\n",
      "Iteration 53, loss = 0.27472791\n",
      "Iteration 48, loss = 0.27406700\n",
      "Iteration 54, loss = 0.27433246\n",
      "Iteration 49, loss = 0.27379378\n",
      "Iteration 55, loss = 0.27396362\n",
      "Iteration 50, loss = 0.27381548\n",
      "Iteration 51, loss = 0.27364679\n",
      "Iteration 56, loss = 0.27342627\n",
      "Iteration 52, loss = 0.27313580\n",
      "Iteration 57, loss = 0.27324711\n",
      "Iteration 53, loss = 0.27340049\n",
      "Iteration 58, loss = 0.27292613\n",
      "Iteration 59, loss = 0.27397244\n",
      "Iteration 54, loss = 0.27270583\n",
      "Iteration 60, loss = 0.27317722\n",
      "Iteration 55, loss = 0.27247541\n",
      "Iteration 61, loss = 0.27270543\n",
      "Iteration 56, loss = 0.27218878\n",
      "Iteration 62, loss = 0.27266614\n",
      "Iteration 57, loss = 0.27212303\n",
      "Iteration 63, loss = 0.27231836\n",
      "Iteration 64, loss = 0.27214315\n",
      "Iteration 58, loss = 0.27184354\n",
      "Iteration 65, loss = 0.27183077\n",
      "Iteration 59, loss = 0.27194097\n",
      "Iteration 60, loss = 0.27168325\n",
      "Iteration 66, loss = 0.27139714\n",
      "Iteration 61, loss = 0.27191498\n",
      "Iteration 67, loss = 0.27113029\n",
      "Iteration 68, loss = 0.27075367\n",
      "Iteration 62, loss = 0.27163111\n",
      "Iteration 69, loss = 0.27182486\n",
      "Iteration 63, loss = 0.27079430\n",
      "Iteration 70, loss = 0.27091959\n",
      "Iteration 64, loss = 0.27093689\n",
      "Iteration 71, loss = 0.27066803\n",
      "Iteration 65, loss = 0.27069393\n",
      "Iteration 72, loss = 0.27071846\n",
      "Iteration 66, loss = 0.27034263\n",
      "Iteration 73, loss = 0.27007902\n",
      "Iteration 67, loss = 0.26964796\n",
      "Iteration 74, loss = 0.27010581\n",
      "Iteration 68, loss = 0.26983246\n",
      "Iteration 75, loss = 0.27131031\n",
      "Iteration 69, loss = 0.26998503\n",
      "Iteration 76, loss = 0.26956994\n",
      "Iteration 70, loss = 0.26934754\n",
      "Iteration 77, loss = 0.27000461\n",
      "Iteration 71, loss = 0.26905584\n",
      "Iteration 78, loss = 0.26922457\n",
      "Iteration 72, loss = 0.26850298\n",
      "Iteration 79, loss = 0.26959247\n",
      "Iteration 73, loss = 0.26811177\n",
      "Iteration 80, loss = 0.26955934\n",
      "Iteration 74, loss = 0.26847142\n",
      "Iteration 81, loss = 0.26909647\n",
      "Iteration 75, loss = 0.26811038\n",
      "Iteration 82, loss = 0.26860051\n",
      "Iteration 83, loss = 0.26885694Iteration 76, loss = 0.26716097\n",
      "\n",
      "Iteration 84, loss = 0.26840043Iteration 77, loss = 0.26769113\n",
      "\n",
      "Iteration 78, loss = 0.26649458\n",
      "Iteration 85, loss = 0.26829237\n",
      "Iteration 79, loss = 0.26707408\n",
      "Iteration 86, loss = 0.26876360\n",
      "Iteration 80, loss = 0.26633271\n",
      "Iteration 87, loss = 0.26784784\n",
      "Iteration 88, loss = 0.26807097\n",
      "Iteration 81, loss = 0.26635953\n",
      "Iteration 89, loss = 0.26775045\n",
      "Iteration 82, loss = 0.26592695\n",
      "Iteration 90, loss = 0.26771201\n",
      "Iteration 83, loss = 0.26544511\n",
      "Iteration 91, loss = 0.26815073\n",
      "Iteration 92, loss = 0.26742302\n",
      "Iteration 84, loss = 0.26525441\n",
      "Iteration 85, loss = 0.26496793\n",
      "Iteration 93, loss = 0.26712396\n",
      "Iteration 86, loss = 0.26556921\n",
      "Iteration 94, loss = 0.26760094\n",
      "Iteration 87, loss = 0.26400032\n",
      "Iteration 95, loss = 0.26711841\n",
      "Iteration 88, loss = 0.26473917\n",
      "Iteration 96, loss = 0.26687747\n",
      "Iteration 97, loss = 0.26635929\n",
      "Iteration 89, loss = 0.26372241\n",
      "Iteration 98, loss = 0.26659599\n",
      "Iteration 90, loss = 0.26420875\n",
      "Iteration 91, loss = 0.26411575\n",
      "Iteration 99, loss = 0.26639360\n",
      "Iteration 92, loss = 0.26307535\n",
      "Iteration 93, loss = 0.26318524\n",
      "Iteration 100, loss = 0.26625492\n",
      "Iteration 101, loss = 0.26623849\n",
      "Iteration 94, loss = 0.26357111\n",
      "Iteration 102, loss = 0.26607114\n",
      "Iteration 95, loss = 0.26326995\n",
      "Iteration 103, loss = 0.26586580\n",
      "Iteration 96, loss = 0.26256449\n",
      "Iteration 97, loss = 0.26221339\n",
      "Iteration 104, loss = 0.26596507\n",
      "Iteration 105, loss = 0.26547729\n",
      "Iteration 98, loss = 0.26214921\n",
      "Iteration 106, loss = 0.26594986\n",
      "Iteration 99, loss = 0.26217932\n",
      "Iteration 100, loss = 0.26173949\n",
      "Iteration 107, loss = 0.26523831\n",
      "Iteration 101, loss = 0.26157597\n",
      "Iteration 108, loss = 0.26535988\n",
      "Iteration 102, loss = 0.26156612Iteration 109, loss = 0.26558851\n",
      "\n",
      "Iteration 110, loss = 0.26493450\n",
      "Iteration 103, loss = 0.26158665\n",
      "Iteration 111, loss = 0.26516768\n",
      "Iteration 112, loss = 0.26462517\n",
      "Iteration 104, loss = 0.26151870\n",
      "Iteration 113, loss = 0.26499321\n",
      "Iteration 105, loss = 0.26091457\n",
      "Iteration 114, loss = 0.26471566\n",
      "Iteration 106, loss = 0.26130041\n",
      "Iteration 115, loss = 0.26423123\n",
      "Iteration 107, loss = 0.26081117\n",
      "Iteration 116, loss = 0.26439860\n",
      "Iteration 108, loss = 0.26052751\n",
      "Iteration 117, loss = 0.26432123\n",
      "Iteration 109, loss = 0.26153553\n",
      "Iteration 118, loss = 0.26432633\n",
      "Iteration 110, loss = 0.26048299\n",
      "Iteration 119, loss = 0.26403025\n",
      "Iteration 111, loss = 0.26066200\n",
      "Iteration 120, loss = 0.26386320\n",
      "Iteration 112, loss = 0.25995877\n",
      "Iteration 121, loss = 0.26441890\n",
      "Iteration 113, loss = 0.25987065\n",
      "Iteration 114, loss = 0.25979943\n",
      "Iteration 122, loss = 0.26393492\n",
      "Iteration 115, loss = 0.25974940\n",
      "Iteration 123, loss = 0.26384044\n",
      "Iteration 124, loss = 0.26383247\n",
      "Iteration 125, loss = 0.26433882\n",
      "Iteration 116, loss = 0.25976104\n",
      "Iteration 126, loss = 0.26360864\n",
      "Iteration 117, loss = 0.25947697\n",
      "Iteration 118, loss = 0.25962214\n",
      "Iteration 127, loss = 0.26349812\n",
      "Iteration 119, loss = 0.25922942\n",
      "Iteration 120, loss = 0.25891945\n",
      "Iteration 128, loss = 0.26369848\n",
      "Iteration 129, loss = 0.26359240\n",
      "Iteration 121, loss = 0.25978598\n",
      "Iteration 130, loss = 0.26321265\n",
      "Iteration 122, loss = 0.25937222\n",
      "Iteration 123, loss = 0.25907398\n",
      "Iteration 131, loss = 0.26317464\n",
      "Iteration 124, loss = 0.25879456\n",
      "Iteration 132, loss = 0.26317719\n",
      "Iteration 125, loss = 0.25948942\n",
      "Iteration 133, loss = 0.26298439\n",
      "Iteration 126, loss = 0.25904303\n",
      "Iteration 134, loss = 0.26288024\n",
      "Iteration 127, loss = 0.25860066\n",
      "Iteration 135, loss = 0.26291189\n",
      "Iteration 128, loss = 0.25856597\n",
      "Iteration 136, loss = 0.26240311\n",
      "Iteration 129, loss = 0.25893275\n",
      "Iteration 137, loss = 0.26230289\n",
      "Iteration 130, loss = 0.25848810\n",
      "Iteration 138, loss = 0.26223623\n",
      "Iteration 131, loss = 0.25877381\n",
      "Iteration 139, loss = 0.26259225\n",
      "Iteration 132, loss = 0.25819989\n",
      "Iteration 140, loss = 0.26238354\n",
      "Iteration 133, loss = 0.25818116\n",
      "Iteration 141, loss = 0.26192957\n",
      "Iteration 134, loss = 0.25789546\n",
      "Iteration 142, loss = 0.26207882\n",
      "Iteration 135, loss = 0.25811222\n",
      "Iteration 143, loss = 0.26215083\n",
      "Iteration 136, loss = 0.25761070\n",
      "Iteration 137, loss = 0.25761507\n",
      "Iteration 138, loss = 0.25777224\n",
      "Iteration 144, loss = 0.26213443\n",
      "Iteration 139, loss = 0.25746335\n",
      "Iteration 145, loss = 0.26197742\n",
      "Iteration 140, loss = 0.25723699\n",
      "Iteration 146, loss = 0.26198888\n",
      "Iteration 141, loss = 0.25729944\n",
      "Iteration 147, loss = 0.26203422\n",
      "Iteration 142, loss = 0.25708841\n",
      "Iteration 148, loss = 0.26179524\n",
      "Iteration 143, loss = 0.25701960\n",
      "Iteration 149, loss = 0.26173324\n",
      "Iteration 144, loss = 0.25734555\n",
      "Iteration 150, loss = 0.26160582\n",
      "Iteration 145, loss = 0.25749719\n",
      "Iteration 151, loss = 0.26129217\n",
      "Iteration 146, loss = 0.25722624\n",
      "Iteration 152, loss = 0.26153222\n",
      "Iteration 147, loss = 0.25667976\n",
      "Iteration 153, loss = 0.26156126\n",
      "Iteration 148, loss = 0.25703337\n",
      "Iteration 154, loss = 0.26105850\n",
      "Iteration 149, loss = 0.25684848\n",
      "Iteration 155, loss = 0.26097295\n",
      "Iteration 150, loss = 0.25693544\n",
      "Iteration 156, loss = 0.26122852\n",
      "Iteration 151, loss = 0.25677192\n",
      "Iteration 157, loss = 0.26105573\n",
      "Iteration 152, loss = 0.25684573\n",
      "Iteration 158, loss = 0.26092899\n",
      "Iteration 153, loss = 0.25681250\n",
      "Iteration 159, loss = 0.26102039\n",
      "Iteration 154, loss = 0.25635234\n",
      "Iteration 160, loss = 0.26100663\n",
      "Iteration 155, loss = 0.25635354\n",
      "Iteration 156, loss = 0.25638984\n",
      "Iteration 161, loss = 0.26077362\n",
      "Iteration 157, loss = 0.25633433\n",
      "Iteration 162, loss = 0.26043373\n",
      "Iteration 158, loss = 0.25645574\n",
      "Iteration 163, loss = 0.26086912\n",
      "Iteration 159, loss = 0.25628989\n",
      "Iteration 164, loss = 0.26130592Iteration 160, loss = 0.25607797\n",
      "\n",
      "Iteration 165, loss = 0.26110861\n",
      "Iteration 161, loss = 0.25620287\n",
      "Iteration 166, loss = 0.26121628\n",
      "Iteration 167, loss = 0.26082804\n",
      "Iteration 168, loss = 0.25998240\n",
      "Iteration 162, loss = 0.25580060\n",
      "Iteration 163, loss = 0.25618769\n",
      "Iteration 169, loss = 0.26113536\n",
      "Iteration 164, loss = 0.25669689\n",
      "Iteration 165, loss = 0.25627654\n",
      "Iteration 170, loss = 0.26177655\n",
      "Iteration 166, loss = 0.25672843\n",
      "Iteration 171, loss = 0.26136649\n",
      "Iteration 167, loss = 0.25606588\n",
      "Iteration 172, loss = 0.26020954\n",
      "Iteration 168, loss = 0.25539671\n",
      "Iteration 173, loss = 0.26057864\n",
      "Iteration 169, loss = 0.25646442\n",
      "Iteration 174, loss = 0.25993012\n",
      "Iteration 170, loss = 0.25658564\n",
      "Iteration 175, loss = 0.25977425\n",
      "Iteration 171, loss = 0.25730546\n",
      "Iteration 176, loss = 0.25961594\n",
      "Iteration 172, loss = 0.25557714\n",
      "Iteration 177, loss = 0.25972898\n",
      "Iteration 173, loss = 0.25589428\n",
      "Iteration 178, loss = 0.25987570\n",
      "Iteration 174, loss = 0.25586345\n",
      "Iteration 179, loss = 0.26028497\n",
      "Iteration 175, loss = 0.25535975\n",
      "Iteration 180, loss = 0.25924276Iteration 176, loss = 0.25534046\n",
      "\n",
      "Iteration 177, loss = 0.25496085\n",
      "Iteration 181, loss = 0.25957126\n",
      "Iteration 178, loss = 0.25601811\n",
      "Iteration 182, loss = 0.25934988\n",
      "Iteration 179, loss = 0.25554037\n",
      "Iteration 183, loss = 0.25926771\n",
      "Iteration 180, loss = 0.25520368\n",
      "Iteration 184, loss = 0.25939578\n",
      "Iteration 181, loss = 0.25520362\n",
      "Iteration 185, loss = 0.25954044\n",
      "Iteration 182, loss = 0.25514734\n",
      "Iteration 186, loss = 0.25928671\n",
      "Iteration 183, loss = 0.25494854\n",
      "Iteration 187, loss = 0.25952110\n",
      "Iteration 184, loss = 0.25515360\n",
      "Iteration 188, loss = 0.25895574\n",
      "Iteration 185, loss = 0.25491282\n",
      "Iteration 189, loss = 0.25962380\n",
      "Iteration 186, loss = 0.25467133\n",
      "Iteration 190, loss = 0.25943860\n",
      "Iteration 187, loss = 0.25471563\n",
      "Iteration 191, loss = 0.25918968\n",
      "Iteration 188, loss = 0.25457469\n",
      "Iteration 189, loss = 0.25509128\n",
      "Iteration 192, loss = 0.25954701\n",
      "Iteration 190, loss = 0.25514759\n",
      "Iteration 193, loss = 0.25910749\n",
      "Iteration 191, loss = 0.25459121\n",
      "Iteration 194, loss = 0.25905360\n",
      "Iteration 192, loss = 0.25510689\n",
      "Iteration 193, loss = 0.25454675\n",
      "Iteration 195, loss = 0.25859160\n",
      "Iteration 194, loss = 0.25456439\n",
      "Iteration 196, loss = 0.25890987\n",
      "Iteration 195, loss = 0.25476026\n",
      "Iteration 197, loss = 0.25942555\n",
      "Iteration 196, loss = 0.25430353\n",
      "Iteration 198, loss = 0.25823118\n",
      "Iteration 197, loss = 0.25531391\n",
      "Iteration 199, loss = 0.25873749\n",
      "Iteration 198, loss = 0.25422845\n",
      "Iteration 200, loss = 0.25851182\n",
      "Iteration 199, loss = 0.25457434\n",
      "Iteration 200, loss = 0.25406817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.37372976\n",
      "Iteration 2, loss = 0.33604119\n",
      "Iteration 1, loss = 0.37244053\n",
      "Iteration 3, loss = 0.31536011\n",
      "Iteration 2, loss = 0.33453474\n",
      "Iteration 4, loss = 0.30319822\n",
      "Iteration 3, loss = 0.31424152\n",
      "Iteration 5, loss = 0.29680831\n",
      "Iteration 4, loss = 0.30232483\n",
      "Iteration 5, loss = 0.29648742\n",
      "Iteration 6, loss = 0.29286844\n",
      "Iteration 6, loss = 0.29266883\n",
      "Iteration 7, loss = 0.29044086\n",
      "Iteration 7, loss = 0.29040354\n",
      "Iteration 8, loss = 0.28963882\n",
      "Iteration 8, loss = 0.28947563\n",
      "Iteration 9, loss = 0.28812781\n",
      "Iteration 9, loss = 0.28853107\n",
      "Iteration 10, loss = 0.28705427\n",
      "Iteration 10, loss = 0.28764394\n",
      "Iteration 11, loss = 0.28670678\n",
      "Iteration 11, loss = 0.28648706\n",
      "Iteration 12, loss = 0.28646826\n",
      "Iteration 13, loss = 0.28528819\n",
      "Iteration 12, loss = 0.28612034\n",
      "Iteration 14, loss = 0.28516898\n",
      "Iteration 15, loss = 0.28500965\n",
      "Iteration 13, loss = 0.28531347\n",
      "Iteration 16, loss = 0.28352439\n",
      "Iteration 14, loss = 0.28523418\n",
      "Iteration 17, loss = 0.28457755\n",
      "Iteration 18, loss = 0.28401363\n",
      "Iteration 15, loss = 0.28483223\n",
      "Iteration 19, loss = 0.28369408\n",
      "Iteration 20, loss = 0.28353280\n",
      "Iteration 16, loss = 0.28360483\n",
      "Iteration 21, loss = 0.28217988Iteration 17, loss = 0.28442063\n",
      "\n",
      "Iteration 18, loss = 0.28373483\n",
      "Iteration 22, loss = 0.28207070\n",
      "Iteration 19, loss = 0.28332580\n",
      "Iteration 20, loss = 0.28329208\n",
      "Iteration 23, loss = 0.28171786\n",
      "Iteration 21, loss = 0.28242399\n",
      "Iteration 24, loss = 0.28125412\n",
      "Iteration 22, loss = 0.28218312\n",
      "Iteration 25, loss = 0.28026266\n",
      "Iteration 23, loss = 0.28187969\n",
      "Iteration 26, loss = 0.28034658\n",
      "Iteration 24, loss = 0.28141563\n",
      "Iteration 27, loss = 0.27963672\n",
      "Iteration 25, loss = 0.28068142\n",
      "Iteration 28, loss = 0.28095429\n",
      "Iteration 26, loss = 0.28073207\n",
      "Iteration 27, loss = 0.28022394\n",
      "Iteration 29, loss = 0.27957838\n",
      "Iteration 28, loss = 0.28085303\n",
      "Iteration 30, loss = 0.27883668\n",
      "Iteration 29, loss = 0.28046602\n",
      "Iteration 31, loss = 0.27847998\n",
      "Iteration 30, loss = 0.27971115\n",
      "Iteration 32, loss = 0.27844239\n",
      "Iteration 31, loss = 0.27906146\n",
      "Iteration 33, loss = 0.27808257\n",
      "Iteration 32, loss = 0.27885003\n",
      "Iteration 34, loss = 0.27733174\n",
      "Iteration 33, loss = 0.27868946\n",
      "Iteration 35, loss = 0.27744435\n",
      "Iteration 34, loss = 0.27788901\n",
      "Iteration 36, loss = 0.27696258\n",
      "Iteration 35, loss = 0.27755741\n",
      "Iteration 37, loss = 0.27680205\n",
      "Iteration 36, loss = 0.27734538\n",
      "Iteration 38, loss = 0.27665187\n",
      "Iteration 37, loss = 0.27701418\n",
      "Iteration 39, loss = 0.27649633\n",
      "Iteration 38, loss = 0.27694493\n",
      "Iteration 40, loss = 0.27573255\n",
      "Iteration 39, loss = 0.27668943\n",
      "Iteration 41, loss = 0.27552480\n",
      "Iteration 40, loss = 0.27595347\n",
      "Iteration 42, loss = 0.27519156\n",
      "Iteration 41, loss = 0.27570691\n",
      "Iteration 43, loss = 0.27506124\n",
      "Iteration 42, loss = 0.27530264\n",
      "Iteration 44, loss = 0.27488832\n",
      "Iteration 43, loss = 0.27512466\n",
      "Iteration 45, loss = 0.27442386Iteration 44, loss = 0.27517698\n",
      "\n",
      "Iteration 45, loss = 0.27474614\n",
      "Iteration 46, loss = 0.27380839\n",
      "Iteration 46, loss = 0.27401908\n",
      "Iteration 47, loss = 0.27345600\n",
      "Iteration 47, loss = 0.27364502\n",
      "Iteration 48, loss = 0.27295676\n",
      "Iteration 48, loss = 0.27338468\n",
      "Iteration 49, loss = 0.27269340\n",
      "Iteration 49, loss = 0.27324418\n",
      "Iteration 50, loss = 0.27258394\n",
      "Iteration 50, loss = 0.27327349\n",
      "Iteration 51, loss = 0.27241337\n",
      "Iteration 51, loss = 0.27310915\n",
      "Iteration 52, loss = 0.27227771\n",
      "Iteration 52, loss = 0.27246336\n",
      "Iteration 53, loss = 0.27183736\n",
      "Iteration 53, loss = 0.27202076\n",
      "Iteration 54, loss = 0.27169964\n",
      "Iteration 54, loss = 0.27175289\n",
      "Iteration 55, loss = 0.27113270\n",
      "Iteration 56, loss = 0.27061229\n",
      "Iteration 55, loss = 0.27148600\n",
      "Iteration 57, loss = 0.27063785\n",
      "Iteration 56, loss = 0.27076450\n",
      "Iteration 57, loss = 0.27063174\n",
      "Iteration 58, loss = 0.27043075\n",
      "Iteration 59, loss = 0.27021962\n",
      "Iteration 58, loss = 0.27053910\n",
      "Iteration 60, loss = 0.26971985\n",
      "Iteration 59, loss = 0.27073341\n",
      "Iteration 61, loss = 0.27003510\n",
      "Iteration 60, loss = 0.27006746\n",
      "Iteration 62, loss = 0.26957670\n",
      "Iteration 61, loss = 0.27085930\n",
      "Iteration 63, loss = 0.26933914\n",
      "Iteration 62, loss = 0.26980916\n",
      "Iteration 64, loss = 0.26931796\n",
      "Iteration 63, loss = 0.26940388\n",
      "Iteration 65, loss = 0.26902233\n",
      "Iteration 64, loss = 0.26958853\n",
      "Iteration 66, loss = 0.26848931\n",
      "Iteration 65, loss = 0.26904926\n",
      "Iteration 67, loss = 0.26851429\n",
      "Iteration 66, loss = 0.26847773\n",
      "Iteration 68, loss = 0.26874012\n",
      "Iteration 67, loss = 0.26852330\n",
      "Iteration 69, loss = 0.26894846\n",
      "Iteration 68, loss = 0.26837732\n",
      "Iteration 70, loss = 0.26839726\n",
      "Iteration 69, loss = 0.26862929\n",
      "Iteration 71, loss = 0.26799419\n",
      "Iteration 70, loss = 0.26812419\n",
      "Iteration 72, loss = 0.26730284\n",
      "Iteration 71, loss = 0.26765709\n",
      "Iteration 73, loss = 0.26709822\n",
      "Iteration 72, loss = 0.26717945\n",
      "Iteration 74, loss = 0.26737467\n",
      "Iteration 73, loss = 0.26679422\n",
      "Iteration 74, loss = 0.26728789\n",
      "Iteration 75, loss = 0.26774203\n",
      "Iteration 76, loss = 0.26722092\n",
      "Iteration 75, loss = 0.26697965\n",
      "Iteration 77, loss = 0.26724140\n",
      "Iteration 76, loss = 0.26610742\n",
      "Iteration 77, loss = 0.26745089\n",
      "Iteration 78, loss = 0.26628545\n",
      "Iteration 79, loss = 0.26656409\n",
      "Iteration 78, loss = 0.26571080\n",
      "Iteration 80, loss = 0.26656906\n",
      "Iteration 81, loss = 0.26660632\n",
      "Iteration 79, loss = 0.26585100\n",
      "Iteration 82, loss = 0.26634870\n",
      "Iteration 80, loss = 0.26559845\n",
      "Iteration 83, loss = 0.26622087\n",
      "Iteration 84, loss = 0.26583512\n",
      "Iteration 81, loss = 0.26533640\n",
      "Iteration 82, loss = 0.26527440\n",
      "Iteration 85, loss = 0.26574802\n",
      "Iteration 83, loss = 0.26503869\n",
      "Iteration 86, loss = 0.26652112\n",
      "Iteration 84, loss = 0.26494136\n",
      "Iteration 87, loss = 0.26539645\n",
      "Iteration 85, loss = 0.26487698\n",
      "Iteration 88, loss = 0.26572051\n",
      "Iteration 89, loss = 0.26554776\n",
      "Iteration 86, loss = 0.26547157\n",
      "Iteration 90, loss = 0.26571550\n",
      "Iteration 87, loss = 0.26440687\n",
      "Iteration 91, loss = 0.26518962\n",
      "Iteration 88, loss = 0.26448811\n",
      "Iteration 92, loss = 0.26488090\n",
      "Iteration 89, loss = 0.26425340\n",
      "Iteration 93, loss = 0.26537339\n",
      "Iteration 90, loss = 0.26429213\n",
      "Iteration 94, loss = 0.26554876\n",
      "Iteration 91, loss = 0.26500883\n",
      "Iteration 95, loss = 0.26439493\n",
      "Iteration 92, loss = 0.26342403\n",
      "Iteration 96, loss = 0.26437999\n",
      "Iteration 93, loss = 0.26378921\n",
      "Iteration 97, loss = 0.26424210\n",
      "Iteration 94, loss = 0.26360552\n",
      "Iteration 98, loss = 0.26417401\n",
      "Iteration 95, loss = 0.26298211\n",
      "Iteration 99, loss = 0.26434198\n",
      "Iteration 100, loss = 0.26426951\n",
      "Iteration 96, loss = 0.26303305\n",
      "Iteration 101, loss = 0.26418835\n",
      "Iteration 97, loss = 0.26267228\n",
      "Iteration 102, loss = 0.26394822\n",
      "Iteration 98, loss = 0.26289446\n",
      "Iteration 103, loss = 0.26373039\n",
      "Iteration 99, loss = 0.26316228\n",
      "Iteration 104, loss = 0.26350485\n",
      "Iteration 100, loss = 0.26291119\n",
      "Iteration 105, loss = 0.26354303\n",
      "Iteration 106, loss = 0.26360090\n",
      "Iteration 101, loss = 0.26274061\n",
      "Iteration 107, loss = 0.26328099\n",
      "Iteration 108, loss = 0.26305103\n",
      "Iteration 102, loss = 0.26233056\n",
      "Iteration 109, loss = 0.26352571\n",
      "Iteration 103, loss = 0.26249010\n",
      "Iteration 110, loss = 0.26271540\n",
      "Iteration 104, loss = 0.26188478\n",
      "Iteration 111, loss = 0.26337719\n",
      "Iteration 105, loss = 0.26146449\n",
      "Iteration 112, loss = 0.26245357\n",
      "Iteration 106, loss = 0.26229977\n",
      "Iteration 113, loss = 0.26245349\n",
      "Iteration 107, loss = 0.26174741\n",
      "Iteration 114, loss = 0.26284326\n",
      "Iteration 108, loss = 0.26143545\n",
      "Iteration 115, loss = 0.26228998\n",
      "Iteration 109, loss = 0.26139775\n",
      "Iteration 110, loss = 0.26128508\n",
      "Iteration 116, loss = 0.26205645\n",
      "Iteration 117, loss = 0.26193078Iteration 111, loss = 0.26211517\n",
      "\n",
      "Iteration 118, loss = 0.26212241\n",
      "Iteration 112, loss = 0.26096179\n",
      "Iteration 113, loss = 0.26086688\n",
      "Iteration 119, loss = 0.26197808\n",
      "Iteration 114, loss = 0.26105145\n",
      "Iteration 120, loss = 0.26177817\n",
      "Iteration 115, loss = 0.26057595\n",
      "Iteration 121, loss = 0.26245251\n",
      "Iteration 116, loss = 0.26060958\n",
      "Iteration 122, loss = 0.26153418\n",
      "Iteration 117, loss = 0.26028562\n",
      "Iteration 123, loss = 0.26201507\n",
      "Iteration 118, loss = 0.26039882\n",
      "Iteration 124, loss = 0.26124887\n",
      "Iteration 119, loss = 0.26042637\n",
      "Iteration 125, loss = 0.26177437\n",
      "Iteration 120, loss = 0.26005082\n",
      "Iteration 126, loss = 0.26160717\n",
      "Iteration 121, loss = 0.26051147\n",
      "Iteration 127, loss = 0.26189877\n",
      "Iteration 122, loss = 0.25995013\n",
      "Iteration 123, loss = 0.26030744\n",
      "Iteration 124, loss = 0.25997016\n",
      "Iteration 128, loss = 0.26084761\n",
      "Iteration 129, loss = 0.26162459\n",
      "Iteration 125, loss = 0.26088860\n",
      "Iteration 126, loss = 0.26025486\n",
      "Iteration 130, loss = 0.26106671\n",
      "Iteration 127, loss = 0.25999426\n",
      "Iteration 131, loss = 0.26055089\n",
      "Iteration 128, loss = 0.25959371\n",
      "Iteration 132, loss = 0.26071678\n",
      "Iteration 129, loss = 0.25989875\n",
      "Iteration 130, loss = 0.25922561\n",
      "Iteration 133, loss = 0.26045810\n",
      "Iteration 131, loss = 0.25915281\n",
      "Iteration 132, loss = 0.25886590\n",
      "Iteration 133, loss = 0.25928441\n",
      "Iteration 134, loss = 0.26047163\n",
      "Iteration 134, loss = 0.25901294\n",
      "Iteration 135, loss = 0.25916744Iteration 135, loss = 0.26045360\n",
      "\n",
      "Iteration 136, loss = 0.26016804\n",
      "Iteration 137, loss = 0.26017027\n",
      "Iteration 136, loss = 0.25869546\n",
      "Iteration 138, loss = 0.26004096\n",
      "Iteration 137, loss = 0.25864632\n",
      "Iteration 139, loss = 0.25964694\n",
      "Iteration 138, loss = 0.25888873\n",
      "Iteration 140, loss = 0.25949485\n",
      "Iteration 139, loss = 0.25827236\n",
      "Iteration 141, loss = 0.25964310\n",
      "Iteration 140, loss = 0.25837767\n",
      "Iteration 142, loss = 0.25994285\n",
      "Iteration 141, loss = 0.25829121\n",
      "Iteration 143, loss = 0.25960409\n",
      "Iteration 142, loss = 0.25847183\n",
      "Iteration 144, loss = 0.25988831\n",
      "Iteration 143, loss = 0.25840234\n",
      "Iteration 145, loss = 0.25966475\n",
      "Iteration 144, loss = 0.25828915\n",
      "Iteration 146, loss = 0.25963022\n",
      "Iteration 145, loss = 0.25815294\n",
      "Iteration 147, loss = 0.25918076\n",
      "Iteration 146, loss = 0.25849899\n",
      "Iteration 148, loss = 0.25928219\n",
      "Iteration 147, loss = 0.25781786\n",
      "Iteration 149, loss = 0.25882693\n",
      "Iteration 150, loss = 0.25907644\n",
      "Iteration 148, loss = 0.25799908\n",
      "Iteration 149, loss = 0.25761778\n",
      "Iteration 151, loss = 0.25883684\n",
      "Iteration 152, loss = 0.25886238\n",
      "Iteration 150, loss = 0.25756317\n",
      "Iteration 151, loss = 0.25738623\n",
      "Iteration 152, loss = 0.25776253\n",
      "Iteration 153, loss = 0.25879600\n",
      "Iteration 154, loss = 0.25894221\n",
      "Iteration 153, loss = 0.25743885\n",
      "Iteration 155, loss = 0.25846694\n",
      "Iteration 154, loss = 0.25747965\n",
      "Iteration 156, loss = 0.25896272\n",
      "Iteration 155, loss = 0.25727273\n",
      "Iteration 156, loss = 0.25727273\n",
      "Iteration 157, loss = 0.25815193\n",
      "Iteration 157, loss = 0.25712135\n",
      "Iteration 158, loss = 0.25849072\n",
      "Iteration 158, loss = 0.25706287\n",
      "Iteration 159, loss = 0.25848378\n",
      "Iteration 159, loss = 0.25710193\n",
      "Iteration 160, loss = 0.25725479\n",
      "Iteration 160, loss = 0.25829360\n",
      "Iteration 161, loss = 0.25692180\n",
      "Iteration 161, loss = 0.25801867\n",
      "Iteration 162, loss = 0.25661605\n",
      "Iteration 162, loss = 0.25808837\n",
      "Iteration 163, loss = 0.25686057\n",
      "Iteration 163, loss = 0.25804544\n",
      "Iteration 164, loss = 0.25691421\n",
      "Iteration 164, loss = 0.25794609\n",
      "Iteration 165, loss = 0.25699740\n",
      "Iteration 165, loss = 0.25788959\n",
      "Iteration 166, loss = 0.25688432\n",
      "Iteration 166, loss = 0.25809510\n",
      "Iteration 167, loss = 0.25676619\n",
      "Iteration 167, loss = 0.25766722\n",
      "Iteration 168, loss = 0.25650739\n",
      "Iteration 168, loss = 0.25780960\n",
      "Iteration 169, loss = 0.25658227\n",
      "Iteration 169, loss = 0.25750690\n",
      "Iteration 170, loss = 0.25721744\n",
      "Iteration 170, loss = 0.25817588\n",
      "Iteration 171, loss = 0.25697868\n",
      "Iteration 171, loss = 0.25796792\n",
      "Iteration 172, loss = 0.25628852\n",
      "Iteration 172, loss = 0.25728521\n",
      "Iteration 173, loss = 0.25738997\n",
      "Iteration 173, loss = 0.25641239\n",
      "Iteration 174, loss = 0.25742630\n",
      "Iteration 174, loss = 0.25653374\n",
      "Iteration 175, loss = 0.25604535\n",
      "Iteration 175, loss = 0.25713773\n",
      "Iteration 176, loss = 0.25570229\n",
      "Iteration 176, loss = 0.25764876\n",
      "Iteration 177, loss = 0.25729274\n",
      "Iteration 177, loss = 0.25594319\n",
      "Iteration 178, loss = 0.25725239\n",
      "Iteration 179, loss = 0.25704999\n",
      "Iteration 180, loss = 0.25735886\n",
      "Iteration 178, loss = 0.25604379\n",
      "Iteration 181, loss = 0.25703252\n",
      "Iteration 179, loss = 0.25646954\n",
      "Iteration 182, loss = 0.25737221\n",
      "Iteration 183, loss = 0.25688566\n",
      "Iteration 180, loss = 0.25686883\n",
      "Iteration 184, loss = 0.25661758\n",
      "Iteration 181, loss = 0.25629930\n",
      "Iteration 185, loss = 0.25653101\n",
      "Iteration 182, loss = 0.25594237\n",
      "Iteration 186, loss = 0.25654843\n",
      "Iteration 183, loss = 0.25577424\n",
      "Iteration 187, loss = 0.25647043\n",
      "Iteration 184, loss = 0.25564060\n",
      "Iteration 188, loss = 0.25680993\n",
      "Iteration 185, loss = 0.25558588\n",
      "Iteration 189, loss = 0.25636304\n",
      "Iteration 186, loss = 0.25541392\n",
      "Iteration 190, loss = 0.25657784\n",
      "Iteration 187, loss = 0.25547171\n",
      "Iteration 191, loss = 0.25633248\n",
      "Iteration 188, loss = 0.25529543\n",
      "Iteration 192, loss = 0.25648424\n",
      "Iteration 189, loss = 0.25532536\n",
      "Iteration 193, loss = 0.25612018\n",
      "Iteration 190, loss = 0.25562594\n",
      "Iteration 194, loss = 0.25624107\n",
      "Iteration 191, loss = 0.25539308\n",
      "Iteration 192, loss = 0.25504661\n",
      "Iteration 195, loss = 0.25601120\n",
      "Iteration 196, loss = 0.25652273\n",
      "Iteration 193, loss = 0.25516138\n",
      "Iteration 197, loss = 0.25636820\n",
      "Iteration 194, loss = 0.25511481\n",
      "Iteration 198, loss = 0.25612022\n",
      "Iteration 195, loss = 0.25524015\n",
      "Iteration 199, loss = 0.25629249\n",
      "Iteration 196, loss = 0.25549814\n",
      "Iteration 200, loss = 0.25537436\n",
      "Iteration 197, loss = 0.25540658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 198, loss = 0.25501312\n",
      "Iteration 199, loss = 0.25526945\n",
      "Iteration 1, loss = 0.37184189\n",
      "Iteration 200, loss = 0.25448890\n",
      "Iteration 2, loss = 0.33445959\n",
      "Iteration 3, loss = 0.31402745\n",
      "Iteration 4, loss = 0.30195230\n",
      "Iteration 1, loss = 0.37096339\n",
      "Iteration 5, loss = 0.29610719\n",
      "Iteration 2, loss = 0.33453903\n",
      "Iteration 6, loss = 0.29197879\n",
      "Iteration 3, loss = 0.31468833\n",
      "Iteration 7, loss = 0.28967990\n",
      "Iteration 4, loss = 0.30272830\n",
      "Iteration 8, loss = 0.28865259\n",
      "Iteration 5, loss = 0.29653723\n",
      "Iteration 9, loss = 0.28720904\n",
      "Iteration 6, loss = 0.29270179\n",
      "Iteration 10, loss = 0.28652218\n",
      "Iteration 7, loss = 0.29066393\n",
      "Iteration 11, loss = 0.28575774\n",
      "Iteration 8, loss = 0.28935348\n",
      "Iteration 12, loss = 0.28587909\n",
      "Iteration 9, loss = 0.28765382\n",
      "Iteration 13, loss = 0.28498669\n",
      "Iteration 10, loss = 0.28748680\n",
      "Iteration 14, loss = 0.28469471\n",
      "Iteration 11, loss = 0.28634159\n",
      "Iteration 15, loss = 0.28419361\n",
      "Iteration 12, loss = 0.28629895\n",
      "Iteration 16, loss = 0.28323520\n",
      "Iteration 13, loss = 0.28574406\n",
      "Iteration 17, loss = 0.28369859\n",
      "Iteration 14, loss = 0.28478234\n",
      "Iteration 18, loss = 0.28259003\n",
      "Iteration 15, loss = 0.28508345\n",
      "Iteration 19, loss = 0.28244952\n",
      "Iteration 16, loss = 0.28445727\n",
      "Iteration 20, loss = 0.28269231\n",
      "Iteration 17, loss = 0.28406266\n",
      "Iteration 21, loss = 0.28169542\n",
      "Iteration 18, loss = 0.28335824\n",
      "Iteration 22, loss = 0.28146528\n",
      "Iteration 19, loss = 0.28318910\n",
      "Iteration 23, loss = 0.28102516\n",
      "Iteration 20, loss = 0.28297608\n",
      "Iteration 24, loss = 0.28086025\n",
      "Iteration 21, loss = 0.28216181\n",
      "Iteration 25, loss = 0.28052660\n",
      "Iteration 22, loss = 0.28199765\n",
      "Iteration 26, loss = 0.28036898\n",
      "Iteration 23, loss = 0.28150492\n",
      "Iteration 27, loss = 0.27958510\n",
      "Iteration 24, loss = 0.28100952\n",
      "Iteration 28, loss = 0.27972995\n",
      "Iteration 25, loss = 0.28077953\n",
      "Iteration 29, loss = 0.27923424\n",
      "Iteration 26, loss = 0.28086257\n",
      "Iteration 30, loss = 0.27911973\n",
      "Iteration 27, loss = 0.28028241\n",
      "Iteration 31, loss = 0.27841348\n",
      "Iteration 28, loss = 0.28015035\n",
      "Iteration 32, loss = 0.27819589\n",
      "Iteration 29, loss = 0.27959644\n",
      "Iteration 33, loss = 0.27802892\n",
      "Iteration 30, loss = 0.27911142\n",
      "Iteration 34, loss = 0.27744613\n",
      "Iteration 31, loss = 0.27886223\n",
      "Iteration 35, loss = 0.27724298\n",
      "Iteration 32, loss = 0.27839823\n",
      "Iteration 36, loss = 0.27731363\n",
      "Iteration 33, loss = 0.27910744\n",
      "Iteration 37, loss = 0.27711410\n",
      "Iteration 34, loss = 0.27771133\n",
      "Iteration 38, loss = 0.27697701\n",
      "Iteration 35, loss = 0.27775616\n",
      "Iteration 39, loss = 0.27660279\n",
      "Iteration 36, loss = 0.27740104\n",
      "Iteration 37, loss = 0.27720167\n",
      "Iteration 38, loss = 0.27677017\n",
      "Iteration 40, loss = 0.27593782\n",
      "Iteration 39, loss = 0.27672468\n",
      "Iteration 41, loss = 0.27551991\n",
      "Iteration 42, loss = 0.27528478\n",
      "Iteration 40, loss = 0.27663954\n",
      "Iteration 43, loss = 0.27535303\n",
      "Iteration 41, loss = 0.27564721\n",
      "Iteration 42, loss = 0.27550867\n",
      "Iteration 44, loss = 0.27553693\n",
      "Iteration 43, loss = 0.27532252\n",
      "Iteration 45, loss = 0.27565219\n",
      "Iteration 44, loss = 0.27504870\n",
      "Iteration 45, loss = 0.27539396\n",
      "Iteration 46, loss = 0.27454138\n",
      "Iteration 46, loss = 0.27424053\n",
      "Iteration 47, loss = 0.27398286\n",
      "Iteration 47, loss = 0.27381204\n",
      "Iteration 48, loss = 0.27392270\n",
      "Iteration 48, loss = 0.27381407Iteration 49, loss = 0.27356871\n",
      "\n",
      "Iteration 50, loss = 0.27343256\n",
      "Iteration 49, loss = 0.27393264\n",
      "Iteration 51, loss = 0.27362160\n",
      "Iteration 50, loss = 0.27323363\n",
      "Iteration 52, loss = 0.27310111\n",
      "Iteration 51, loss = 0.27361062Iteration 53, loss = 0.27285441\n",
      "\n",
      "Iteration 54, loss = 0.27270759\n",
      "Iteration 52, loss = 0.27305147\n",
      "Iteration 55, loss = 0.27238102\n",
      "Iteration 53, loss = 0.27277155\n",
      "Iteration 54, loss = 0.27272297\n",
      "Iteration 56, loss = 0.27207037\n",
      "Iteration 57, loss = 0.27193698\n",
      "Iteration 55, loss = 0.27245234\n",
      "Iteration 58, loss = 0.27162677\n",
      "Iteration 56, loss = 0.27204140\n",
      "Iteration 59, loss = 0.27149825\n",
      "Iteration 57, loss = 0.27220363\n",
      "Iteration 60, loss = 0.27069556\n",
      "Iteration 58, loss = 0.27175208\n",
      "Iteration 61, loss = 0.27090991\n",
      "Iteration 59, loss = 0.27159705\n",
      "Iteration 62, loss = 0.27052866\n",
      "Iteration 60, loss = 0.27121781\n",
      "Iteration 63, loss = 0.27024015\n",
      "Iteration 61, loss = 0.27105828\n",
      "Iteration 64, loss = 0.27014401\n",
      "Iteration 62, loss = 0.27063682\n",
      "Iteration 65, loss = 0.27001471\n",
      "Iteration 63, loss = 0.27037980\n",
      "Iteration 66, loss = 0.26968827\n",
      "Iteration 64, loss = 0.27032078\n",
      "Iteration 67, loss = 0.26921799\n",
      "Iteration 65, loss = 0.26984148\n",
      "Iteration 68, loss = 0.26921882\n",
      "Iteration 66, loss = 0.26981505\n",
      "Iteration 69, loss = 0.26961104\n",
      "Iteration 67, loss = 0.26933971\n",
      "Iteration 70, loss = 0.26856164\n",
      "Iteration 68, loss = 0.26958927\n",
      "Iteration 71, loss = 0.26886155\n",
      "Iteration 69, loss = 0.26984148\n",
      "Iteration 72, loss = 0.26775350\n",
      "Iteration 70, loss = 0.26887564\n",
      "Iteration 73, loss = 0.26739911\n",
      "Iteration 71, loss = 0.26919748\n",
      "Iteration 74, loss = 0.26761400\n",
      "Iteration 72, loss = 0.26816544\n",
      "Iteration 75, loss = 0.26709882\n",
      "Iteration 76, loss = 0.26714386\n",
      "Iteration 73, loss = 0.26845071\n",
      "Iteration 77, loss = 0.26674720\n",
      "Iteration 74, loss = 0.26833486\n",
      "Iteration 78, loss = 0.26609001\n",
      "Iteration 75, loss = 0.26784866\n",
      "Iteration 79, loss = 0.26635819\n",
      "Iteration 76, loss = 0.26817791\n",
      "Iteration 80, loss = 0.26628045\n",
      "Iteration 77, loss = 0.26846294\n",
      "Iteration 81, loss = 0.26599707\n",
      "Iteration 78, loss = 0.26706643\n",
      "Iteration 82, loss = 0.26569553\n",
      "Iteration 79, loss = 0.26736569\n",
      "Iteration 83, loss = 0.26522379\n",
      "Iteration 80, loss = 0.26714841\n",
      "Iteration 84, loss = 0.26501017\n",
      "Iteration 81, loss = 0.26694832\n",
      "Iteration 85, loss = 0.26452916\n",
      "Iteration 82, loss = 0.26671747\n",
      "Iteration 86, loss = 0.26493995\n",
      "Iteration 83, loss = 0.26655431\n",
      "Iteration 87, loss = 0.26392788\n",
      "Iteration 84, loss = 0.26604928\n",
      "Iteration 88, loss = 0.26406008\n",
      "Iteration 89, loss = 0.26442766\n",
      "Iteration 85, loss = 0.26562599\n",
      "Iteration 90, loss = 0.26412605\n",
      "Iteration 86, loss = 0.26627550Iteration 91, loss = 0.26329924\n",
      "\n",
      "Iteration 92, loss = 0.26291523\n",
      "Iteration 87, loss = 0.26549283\n",
      "Iteration 93, loss = 0.26317612\n",
      "Iteration 94, loss = 0.26359472\n",
      "Iteration 88, loss = 0.26557569\n",
      "Iteration 95, loss = 0.26213456\n",
      "Iteration 89, loss = 0.26557439Iteration 96, loss = 0.26193772\n",
      "\n",
      "Iteration 97, loss = 0.26249461\n",
      "Iteration 90, loss = 0.26537568\n",
      "Iteration 98, loss = 0.26182798\n",
      "Iteration 99, loss = 0.26166368\n",
      "Iteration 91, loss = 0.26494416\n",
      "Iteration 100, loss = 0.26171451\n",
      "Iteration 92, loss = 0.26476568\n",
      "Iteration 101, loss = 0.26140456\n",
      "Iteration 93, loss = 0.26480349\n",
      "Iteration 102, loss = 0.26114504\n",
      "Iteration 94, loss = 0.26489879\n",
      "Iteration 103, loss = 0.26102776\n",
      "Iteration 95, loss = 0.26437998\n",
      "Iteration 104, loss = 0.26086107\n",
      "Iteration 96, loss = 0.26408025\n",
      "Iteration 105, loss = 0.26061818\n",
      "Iteration 97, loss = 0.26464197\n",
      "Iteration 106, loss = 0.26062618\n",
      "Iteration 98, loss = 0.26416471\n",
      "Iteration 107, loss = 0.26005192\n",
      "Iteration 99, loss = 0.26390430\n",
      "Iteration 108, loss = 0.26066141\n",
      "Iteration 100, loss = 0.26379285\n",
      "Iteration 109, loss = 0.26003148\n",
      "Iteration 101, loss = 0.26377426\n",
      "Iteration 110, loss = 0.26002452\n",
      "Iteration 111, loss = 0.26084862\n",
      "Iteration 102, loss = 0.26323727\n",
      "Iteration 112, loss = 0.25948921\n",
      "Iteration 103, loss = 0.26337223\n",
      "Iteration 113, loss = 0.25930180\n",
      "Iteration 104, loss = 0.26364955\n",
      "Iteration 114, loss = 0.25989680\n",
      "Iteration 105, loss = 0.26344170\n",
      "Iteration 115, loss = 0.25906347\n",
      "Iteration 106, loss = 0.26309589\n",
      "Iteration 116, loss = 0.25915085\n",
      "Iteration 107, loss = 0.26271819\n",
      "Iteration 117, loss = 0.25871043\n",
      "Iteration 108, loss = 0.26297467\n",
      "Iteration 118, loss = 0.25962720\n",
      "Iteration 109, loss = 0.26298449\n",
      "Iteration 119, loss = 0.25887024Iteration 110, loss = 0.26244159\n",
      "\n",
      "Iteration 111, loss = 0.26304128\n",
      "Iteration 120, loss = 0.25865423\n",
      "Iteration 112, loss = 0.26217097\n",
      "Iteration 121, loss = 0.25881219\n",
      "Iteration 113, loss = 0.26225811\n",
      "Iteration 122, loss = 0.25842079\n",
      "Iteration 114, loss = 0.26223068\n",
      "Iteration 123, loss = 0.25840736\n",
      "Iteration 124, loss = 0.25803622\n",
      "Iteration 115, loss = 0.26184307\n",
      "Iteration 125, loss = 0.25908004\n",
      "Iteration 116, loss = 0.26179810\n",
      "Iteration 126, loss = 0.25841140\n",
      "Iteration 127, loss = 0.25868730\n",
      "Iteration 117, loss = 0.26169731\n",
      "Iteration 118, loss = 0.26253801\n",
      "Iteration 128, loss = 0.25770979\n",
      "Iteration 119, loss = 0.26154965\n",
      "Iteration 120, loss = 0.26181825\n",
      "Iteration 129, loss = 0.25821946\n",
      "Iteration 121, loss = 0.26262870\n",
      "Iteration 130, loss = 0.25785855\n",
      "Iteration 122, loss = 0.26165899\n",
      "Iteration 123, loss = 0.26112403\n",
      "Iteration 131, loss = 0.25734017\n",
      "Iteration 124, loss = 0.26109910\n",
      "Iteration 132, loss = 0.25746347\n",
      "Iteration 125, loss = 0.26173099\n",
      "Iteration 133, loss = 0.25750794\n",
      "Iteration 134, loss = 0.25731831\n",
      "Iteration 126, loss = 0.26132222\n",
      "Iteration 135, loss = 0.25703311\n",
      "Iteration 127, loss = 0.26109320\n",
      "Iteration 136, loss = 0.25713625\n",
      "Iteration 128, loss = 0.26132147\n",
      "Iteration 137, loss = 0.25742343\n",
      "Iteration 129, loss = 0.26158730\n",
      "Iteration 138, loss = 0.25706709\n",
      "Iteration 130, loss = 0.26076968\n",
      "Iteration 139, loss = 0.25690613\n",
      "Iteration 131, loss = 0.26016417\n",
      "Iteration 140, loss = 0.25676306\n",
      "Iteration 132, loss = 0.26050899\n",
      "Iteration 141, loss = 0.25680656\n",
      "Iteration 133, loss = 0.26049506\n",
      "Iteration 142, loss = 0.25688571\n",
      "Iteration 134, loss = 0.26034891\n",
      "Iteration 143, loss = 0.25679519\n",
      "Iteration 135, loss = 0.26052108\n",
      "Iteration 144, loss = 0.25667439\n",
      "Iteration 136, loss = 0.26005538\n",
      "Iteration 137, loss = 0.26003444\n",
      "Iteration 145, loss = 0.25692527\n",
      "Iteration 138, loss = 0.25975088\n",
      "Iteration 146, loss = 0.25724079\n",
      "Iteration 139, loss = 0.25957758\n",
      "Iteration 147, loss = 0.25619458\n",
      "Iteration 140, loss = 0.25933820\n",
      "Iteration 148, loss = 0.25645916\n",
      "Iteration 149, loss = 0.25629782\n",
      "Iteration 141, loss = 0.25956539\n",
      "Iteration 150, loss = 0.25638544\n",
      "Iteration 142, loss = 0.25920520\n",
      "Iteration 151, loss = 0.25616591\n",
      "Iteration 152, loss = 0.25582862\n",
      "Iteration 143, loss = 0.25958214\n",
      "Iteration 153, loss = 0.25605101\n",
      "Iteration 144, loss = 0.25933830\n",
      "Iteration 154, loss = 0.25594069\n",
      "Iteration 145, loss = 0.25892519\n",
      "Iteration 155, loss = 0.25550817\n",
      "Iteration 146, loss = 0.25916364\n",
      "Iteration 156, loss = 0.25628456\n",
      "Iteration 147, loss = 0.25870317\n",
      "Iteration 157, loss = 0.25575315\n",
      "Iteration 148, loss = 0.25936647\n",
      "Iteration 158, loss = 0.25561760\n",
      "Iteration 149, loss = 0.25868453\n",
      "Iteration 159, loss = 0.25577308\n",
      "Iteration 160, loss = 0.25567296\n",
      "Iteration 161, loss = 0.25552908\n",
      "Iteration 150, loss = 0.25866874\n",
      "Iteration 151, loss = 0.25829694\n",
      "Iteration 152, loss = 0.25817824\n",
      "Iteration 162, loss = 0.25531015\n",
      "Iteration 163, loss = 0.25538575\n",
      "Iteration 153, loss = 0.25873446\n",
      "Iteration 164, loss = 0.25543551\n",
      "Iteration 165, loss = 0.25547316\n",
      "Iteration 154, loss = 0.25817443\n",
      "Iteration 166, loss = 0.25562271\n",
      "Iteration 155, loss = 0.25850892\n",
      "Iteration 156, loss = 0.25830658\n",
      "Iteration 167, loss = 0.25515104\n",
      "Iteration 157, loss = 0.25776107\n",
      "Iteration 158, loss = 0.25785604\n",
      "Iteration 168, loss = 0.25531840\n",
      "Iteration 159, loss = 0.25807924\n",
      "Iteration 160, loss = 0.25771887\n",
      "Iteration 169, loss = 0.25511672\n",
      "Iteration 161, loss = 0.25766967\n",
      "Iteration 170, loss = 0.25532953\n",
      "Iteration 162, loss = 0.25740926\n",
      "Iteration 171, loss = 0.25521066\n",
      "Iteration 163, loss = 0.25811047\n",
      "Iteration 172, loss = 0.25525625\n",
      "Iteration 164, loss = 0.25777933\n",
      "Iteration 173, loss = 0.25480132\n",
      "Iteration 165, loss = 0.25721681\n",
      "Iteration 174, loss = 0.25460062\n",
      "Iteration 166, loss = 0.25753876\n",
      "Iteration 175, loss = 0.25478102\n",
      "Iteration 167, loss = 0.25737890\n",
      "Iteration 176, loss = 0.25524691\n",
      "Iteration 168, loss = 0.25728110\n",
      "Iteration 177, loss = 0.25479363\n",
      "Iteration 169, loss = 0.25706285\n",
      "Iteration 178, loss = 0.25468227\n",
      "Iteration 170, loss = 0.25758305\n",
      "Iteration 179, loss = 0.25459920\n",
      "Iteration 171, loss = 0.25726261\n",
      "Iteration 180, loss = 0.25561581\n",
      "Iteration 172, loss = 0.25696129\n",
      "Iteration 181, loss = 0.25461639\n",
      "Iteration 173, loss = 0.25699753\n",
      "Iteration 182, loss = 0.25452724\n",
      "Iteration 174, loss = 0.25688113\n",
      "Iteration 183, loss = 0.25440679\n",
      "Iteration 175, loss = 0.25713759\n",
      "Iteration 176, loss = 0.25719728\n",
      "Iteration 184, loss = 0.25426743\n",
      "Iteration 177, loss = 0.25674899\n",
      "Iteration 185, loss = 0.25446119\n",
      "Iteration 178, loss = 0.25655853\n",
      "Iteration 186, loss = 0.25426374\n",
      "Iteration 179, loss = 0.25650114\n",
      "Iteration 187, loss = 0.25427229\n",
      "Iteration 180, loss = 0.25708465\n",
      "Iteration 181, loss = 0.25629638\n",
      "Iteration 188, loss = 0.25428341\n",
      "Iteration 182, loss = 0.25719806\n",
      "Iteration 183, loss = 0.25622996\n",
      "Iteration 189, loss = 0.25446109\n",
      "Iteration 184, loss = 0.25612866\n",
      "Iteration 190, loss = 0.25423993\n",
      "Iteration 191, loss = 0.25448630\n",
      "Iteration 185, loss = 0.25591938\n",
      "Iteration 192, loss = 0.25398617\n",
      "Iteration 186, loss = 0.25599204\n",
      "Iteration 193, loss = 0.25375303\n",
      "Iteration 194, loss = 0.25414633\n",
      "Iteration 187, loss = 0.25582162\n",
      "Iteration 195, loss = 0.25430960\n",
      "Iteration 188, loss = 0.25588044\n",
      "Iteration 196, loss = 0.25371641\n",
      "Iteration 197, loss = 0.25372632\n",
      "Iteration 189, loss = 0.25646119\n",
      "Iteration 190, loss = 0.25593615\n",
      "Iteration 198, loss = 0.25372230\n",
      "Iteration 191, loss = 0.25606409\n",
      "Iteration 192, loss = 0.25603703\n",
      "Iteration 199, loss = 0.25389840\n",
      "Iteration 193, loss = 0.25556236\n",
      "Iteration 200, loss = 0.25333329\n",
      "Iteration 194, loss = 0.25564243\n",
      "Iteration 195, loss = 0.25595850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 196, loss = 0.25547584\n",
      "Iteration 1, loss = 0.37130243\n",
      "Iteration 197, loss = 0.25584833\n",
      "Iteration 2, loss = 0.33450152\n",
      "Iteration 198, loss = 0.25577366\n",
      "Iteration 3, loss = 0.31367828\n",
      "Iteration 199, loss = 0.25580100\n",
      "Iteration 4, loss = 0.30133212\n",
      "Iteration 200, loss = 0.25562083\n",
      "Iteration 5, loss = 0.29528095\n",
      "Iteration 6, loss = 0.29141874\n",
      "Iteration 7, loss = 0.28912793\n",
      "Iteration 1, loss = 0.37103264\n",
      "Iteration 8, loss = 0.28783750\n",
      "Iteration 2, loss = 0.33467516\n",
      "Iteration 9, loss = 0.28615337\n",
      "Iteration 3, loss = 0.31349552\n",
      "Iteration 10, loss = 0.28657065\n",
      "Iteration 4, loss = 0.30110777\n",
      "Iteration 11, loss = 0.28547811\n",
      "Iteration 5, loss = 0.29517677\n",
      "Iteration 12, loss = 0.28496350\n",
      "Iteration 6, loss = 0.29139673\n",
      "Iteration 13, loss = 0.28459240\n",
      "Iteration 7, loss = 0.28921277\n",
      "Iteration 14, loss = 0.28402808\n",
      "Iteration 8, loss = 0.28778134\n",
      "Iteration 15, loss = 0.28413101\n",
      "Iteration 9, loss = 0.28630683\n",
      "Iteration 16, loss = 0.28415155\n",
      "Iteration 10, loss = 0.28630051\n",
      "Iteration 17, loss = 0.28360707\n",
      "Iteration 11, loss = 0.28522824\n",
      "Iteration 18, loss = 0.28236366\n",
      "Iteration 12, loss = 0.28462871\n",
      "Iteration 19, loss = 0.28219479\n",
      "Iteration 13, loss = 0.28403605\n",
      "Iteration 20, loss = 0.28190167\n",
      "Iteration 14, loss = 0.28400217\n",
      "Iteration 21, loss = 0.28152744\n",
      "Iteration 15, loss = 0.28420408\n",
      "Iteration 22, loss = 0.28137443\n",
      "Iteration 16, loss = 0.28344690\n",
      "Iteration 23, loss = 0.28052217\n",
      "Iteration 17, loss = 0.28305041\n",
      "Iteration 24, loss = 0.28008059\n",
      "Iteration 18, loss = 0.28206498\n",
      "Iteration 25, loss = 0.28036970\n",
      "Iteration 19, loss = 0.28176946\n",
      "Iteration 26, loss = 0.27963145\n",
      "Iteration 20, loss = 0.28190484\n",
      "Iteration 27, loss = 0.27911032\n",
      "Iteration 21, loss = 0.28122975\n",
      "Iteration 28, loss = 0.27926967\n",
      "Iteration 22, loss = 0.28153074\n",
      "Iteration 29, loss = 0.27857651\n",
      "Iteration 23, loss = 0.28021176\n",
      "Iteration 30, loss = 0.27819297\n",
      "Iteration 24, loss = 0.27991367\n",
      "Iteration 31, loss = 0.27805234\n",
      "Iteration 25, loss = 0.28010654\n",
      "Iteration 32, loss = 0.27790657\n",
      "Iteration 26, loss = 0.27928646\n",
      "Iteration 33, loss = 0.27851568\n",
      "Iteration 27, loss = 0.27873673\n",
      "Iteration 34, loss = 0.27747107\n",
      "Iteration 28, loss = 0.27888557\n",
      "Iteration 35, loss = 0.27711321\n",
      "Iteration 29, loss = 0.27794657\n",
      "Iteration 36, loss = 0.27663950\n",
      "Iteration 30, loss = 0.27747305\n",
      "Iteration 37, loss = 0.27687046\n",
      "Iteration 31, loss = 0.27734476\n",
      "Iteration 38, loss = 0.27669681\n",
      "Iteration 32, loss = 0.27691000\n",
      "Iteration 39, loss = 0.27649507\n",
      "Iteration 33, loss = 0.27755771\n",
      "Iteration 40, loss = 0.27580244\n",
      "Iteration 34, loss = 0.27678415\n",
      "Iteration 41, loss = 0.27540520\n",
      "Iteration 35, loss = 0.27627932\n",
      "Iteration 42, loss = 0.27527874\n",
      "Iteration 36, loss = 0.27534579\n",
      "Iteration 43, loss = 0.27494593\n",
      "Iteration 37, loss = 0.27551778\n",
      "Iteration 44, loss = 0.27476998\n",
      "Iteration 38, loss = 0.27500896\n",
      "Iteration 45, loss = 0.27554735\n",
      "Iteration 39, loss = 0.27505915\n",
      "Iteration 46, loss = 0.27456250\n",
      "Iteration 40, loss = 0.27425210\n",
      "Iteration 47, loss = 0.27408144\n",
      "Iteration 41, loss = 0.27378294\n",
      "Iteration 48, loss = 0.27411697\n",
      "Iteration 42, loss = 0.27317874\n",
      "Iteration 49, loss = 0.27397315\n",
      "Iteration 43, loss = 0.27300181\n",
      "Iteration 50, loss = 0.27340971\n",
      "Iteration 44, loss = 0.27290984\n",
      "Iteration 51, loss = 0.27359461\n",
      "Iteration 45, loss = 0.27391157\n",
      "Iteration 52, loss = 0.27311075\n",
      "Iteration 46, loss = 0.27181679\n",
      "Iteration 53, loss = 0.27318890\n",
      "Iteration 47, loss = 0.27129354\n",
      "Iteration 54, loss = 0.27337725\n",
      "Iteration 48, loss = 0.27160483\n",
      "Iteration 55, loss = 0.27322667\n",
      "Iteration 49, loss = 0.27153614\n",
      "Iteration 56, loss = 0.27259659\n",
      "Iteration 50, loss = 0.27073880\n",
      "Iteration 57, loss = 0.27304403\n",
      "Iteration 51, loss = 0.27047046\n",
      "Iteration 58, loss = 0.27273363\n",
      "Iteration 59, loss = 0.27236650\n",
      "Iteration 52, loss = 0.26980599\n",
      "Iteration 53, loss = 0.26962667\n",
      "Iteration 60, loss = 0.27161656\n",
      "Iteration 54, loss = 0.26982835Iteration 61, loss = 0.27179789\n",
      "\n",
      "Iteration 62, loss = 0.27142964\n",
      "Iteration 55, loss = 0.26911034\n",
      "Iteration 63, loss = 0.27137308\n",
      "Iteration 56, loss = 0.26905174\n",
      "Iteration 64, loss = 0.27120048\n",
      "Iteration 57, loss = 0.26888488\n",
      "Iteration 65, loss = 0.27104024\n",
      "Iteration 58, loss = 0.26811603\n",
      "Iteration 59, loss = 0.26795318\n",
      "Iteration 66, loss = 0.27053364\n",
      "Iteration 67, loss = 0.27075990\n",
      "Iteration 60, loss = 0.26728147\n",
      "Iteration 68, loss = 0.27053701\n",
      "Iteration 61, loss = 0.26741231\n",
      "Iteration 62, loss = 0.26705054\n",
      "Iteration 69, loss = 0.27136178\n",
      "Iteration 63, loss = 0.26701246\n",
      "Iteration 70, loss = 0.27034051\n",
      "Iteration 64, loss = 0.26682124\n",
      "Iteration 71, loss = 0.27045506\n",
      "Iteration 72, loss = 0.26961680\n",
      "Iteration 65, loss = 0.26609687\n",
      "Iteration 73, loss = 0.26981574\n",
      "Iteration 74, loss = 0.26992036\n",
      "Iteration 66, loss = 0.26585751\n",
      "Iteration 75, loss = 0.26951628\n",
      "Iteration 67, loss = 0.26577139\n",
      "Iteration 68, loss = 0.26566219\n",
      "Iteration 69, loss = 0.26608205\n",
      "Iteration 76, loss = 0.26969236\n",
      "Iteration 70, loss = 0.26540950\n",
      "Iteration 77, loss = 0.26978219\n",
      "Iteration 78, loss = 0.26888294\n",
      "Iteration 71, loss = 0.26626982\n",
      "Iteration 79, loss = 0.26906846\n",
      "Iteration 72, loss = 0.26463233\n",
      "Iteration 80, loss = 0.26931537\n",
      "Iteration 73, loss = 0.26479654\n",
      "Iteration 81, loss = 0.26887824\n",
      "Iteration 74, loss = 0.26466543\n",
      "Iteration 82, loss = 0.26895425\n",
      "Iteration 75, loss = 0.26438601\n",
      "Iteration 83, loss = 0.26886072\n",
      "Iteration 76, loss = 0.26454690\n",
      "Iteration 84, loss = 0.26806095\n",
      "Iteration 77, loss = 0.26406850\n",
      "Iteration 85, loss = 0.26818463\n",
      "Iteration 86, loss = 0.26853159\n",
      "Iteration 78, loss = 0.26366948\n",
      "Iteration 87, loss = 0.26797043\n",
      "Iteration 79, loss = 0.26380567\n",
      "Iteration 88, loss = 0.26780587\n",
      "Iteration 80, loss = 0.26401429\n",
      "Iteration 89, loss = 0.26833224\n",
      "Iteration 90, loss = 0.26752777\n",
      "Iteration 81, loss = 0.26310905\n",
      "Iteration 91, loss = 0.26763071\n",
      "Iteration 82, loss = 0.26335497\n",
      "Iteration 92, loss = 0.26753117\n",
      "Iteration 83, loss = 0.26307309\n",
      "Iteration 93, loss = 0.26744316\n",
      "Iteration 84, loss = 0.26255711\n",
      "Iteration 94, loss = 0.26722571\n",
      "Iteration 85, loss = 0.26244735\n",
      "Iteration 95, loss = 0.26715153\n",
      "Iteration 86, loss = 0.26276313\n",
      "Iteration 96, loss = 0.26691523\n",
      "Iteration 87, loss = 0.26211154\n",
      "Iteration 97, loss = 0.26751800\n",
      "Iteration 88, loss = 0.26177515\n",
      "Iteration 98, loss = 0.26687727\n",
      "Iteration 89, loss = 0.26205432\n",
      "Iteration 99, loss = 0.26707261\n",
      "Iteration 90, loss = 0.26170043\n",
      "Iteration 91, loss = 0.26160088\n",
      "Iteration 100, loss = 0.26634904\n",
      "Iteration 92, loss = 0.26169452\n",
      "Iteration 101, loss = 0.26657435\n",
      "Iteration 93, loss = 0.26149611\n",
      "Iteration 102, loss = 0.26651087\n",
      "Iteration 94, loss = 0.26143089\n",
      "Iteration 103, loss = 0.26655286\n",
      "Iteration 95, loss = 0.26099796\n",
      "Iteration 104, loss = 0.26621281\n",
      "Iteration 96, loss = 0.26047983\n",
      "Iteration 105, loss = 0.26647038\n",
      "Iteration 97, loss = 0.26121751\n",
      "Iteration 106, loss = 0.26630466\n",
      "Iteration 107, loss = 0.26607109\n",
      "Iteration 98, loss = 0.26027274\n",
      "Iteration 108, loss = 0.26605154\n",
      "Iteration 99, loss = 0.26061975\n",
      "Iteration 109, loss = 0.26624335\n",
      "Iteration 100, loss = 0.26025838\n",
      "Iteration 110, loss = 0.26545488\n",
      "Iteration 101, loss = 0.26038713\n",
      "Iteration 111, loss = 0.26572349\n",
      "Iteration 102, loss = 0.26004272\n",
      "Iteration 112, loss = 0.26532377\n",
      "Iteration 103, loss = 0.25999862\n",
      "Iteration 113, loss = 0.26529716\n",
      "Iteration 104, loss = 0.26006645\n",
      "Iteration 114, loss = 0.26599668\n",
      "Iteration 105, loss = 0.26017126\n",
      "Iteration 115, loss = 0.26531771\n",
      "Iteration 106, loss = 0.25956815\n",
      "Iteration 116, loss = 0.26484330\n",
      "Iteration 107, loss = 0.25958023\n",
      "Iteration 117, loss = 0.26519480\n",
      "Iteration 108, loss = 0.26009156\n",
      "Iteration 118, loss = 0.26514917\n",
      "Iteration 109, loss = 0.25923843\n",
      "Iteration 119, loss = 0.26492049\n",
      "Iteration 110, loss = 0.25896182\n",
      "Iteration 120, loss = 0.26492298\n",
      "Iteration 111, loss = 0.25940628\n",
      "Iteration 121, loss = 0.26534641\n",
      "Iteration 112, loss = 0.25884126Iteration 122, loss = 0.26458866\n",
      "\n",
      "Iteration 123, loss = 0.26445793Iteration 113, loss = 0.25853163\n",
      "\n",
      "Iteration 124, loss = 0.26416565Iteration 114, loss = 0.25927279\n",
      "\n",
      "Iteration 125, loss = 0.26434743\n",
      "Iteration 115, loss = 0.25810749\n",
      "Iteration 126, loss = 0.26435565\n",
      "Iteration 116, loss = 0.25839950\n",
      "Iteration 127, loss = 0.26407292\n",
      "Iteration 117, loss = 0.25887606\n",
      "Iteration 128, loss = 0.26385823\n",
      "Iteration 118, loss = 0.25905961\n",
      "Iteration 129, loss = 0.26384818\n",
      "Iteration 119, loss = 0.25805719\n",
      "Iteration 130, loss = 0.26387232\n",
      "Iteration 120, loss = 0.25821768\n",
      "Iteration 131, loss = 0.26355660\n",
      "Iteration 132, loss = 0.26389984\n",
      "Iteration 121, loss = 0.25774529\n",
      "Iteration 133, loss = 0.26364909\n",
      "Iteration 122, loss = 0.25753676\n",
      "Iteration 134, loss = 0.26349107\n",
      "Iteration 123, loss = 0.25840454\n",
      "Iteration 135, loss = 0.26355658\n",
      "Iteration 124, loss = 0.25749479\n",
      "Iteration 136, loss = 0.26314038\n",
      "Iteration 125, loss = 0.25740650\n",
      "Iteration 137, loss = 0.26406567\n",
      "Iteration 126, loss = 0.25726896\n",
      "Iteration 138, loss = 0.26316842\n",
      "Iteration 127, loss = 0.25727080Iteration 139, loss = 0.26296335\n",
      "\n",
      "Iteration 128, loss = 0.25713696Iteration 140, loss = 0.26286353\n",
      "\n",
      "Iteration 129, loss = 0.25737110\n",
      "Iteration 141, loss = 0.26301833\n",
      "Iteration 130, loss = 0.25681915\n",
      "Iteration 142, loss = 0.26273129\n",
      "Iteration 131, loss = 0.25650256\n",
      "Iteration 143, loss = 0.26276531\n",
      "Iteration 144, loss = 0.26326372\n",
      "Iteration 132, loss = 0.25714062\n",
      "Iteration 145, loss = 0.26282021\n",
      "Iteration 133, loss = 0.25701037\n",
      "Iteration 134, loss = 0.25662614Iteration 146, loss = 0.26268229\n",
      "\n",
      "Iteration 135, loss = 0.25616205\n",
      "Iteration 147, loss = 0.26236554\n",
      "Iteration 136, loss = 0.25611902\n",
      "Iteration 148, loss = 0.26234458\n",
      "Iteration 149, loss = 0.26287739\n",
      "Iteration 137, loss = 0.25647516\n",
      "Iteration 150, loss = 0.26228353\n",
      "Iteration 138, loss = 0.25649732\n",
      "Iteration 151, loss = 0.26233560Iteration 139, loss = 0.25610766\n",
      "\n",
      "Iteration 140, loss = 0.25601120\n",
      "Iteration 152, loss = 0.26189557\n",
      "Iteration 141, loss = 0.25605226\n",
      "Iteration 153, loss = 0.26227310\n",
      "Iteration 142, loss = 0.25595430\n",
      "Iteration 154, loss = 0.26227485\n",
      "Iteration 143, loss = 0.25591296\n",
      "Iteration 155, loss = 0.26193252\n",
      "Iteration 144, loss = 0.25596653\n",
      "Iteration 156, loss = 0.26204634\n",
      "Iteration 145, loss = 0.25571561\n",
      "Iteration 157, loss = 0.26160823\n",
      "Iteration 146, loss = 0.25563950\n",
      "Iteration 158, loss = 0.26167597\n",
      "Iteration 147, loss = 0.25564037\n",
      "Iteration 159, loss = 0.26181308\n",
      "Iteration 148, loss = 0.25564468\n",
      "Iteration 160, loss = 0.26177261\n",
      "Iteration 149, loss = 0.25525588\n",
      "Iteration 161, loss = 0.26146355\n",
      "Iteration 150, loss = 0.25533836\n",
      "Iteration 162, loss = 0.26150641\n",
      "Iteration 151, loss = 0.25545773\n",
      "Iteration 163, loss = 0.26168686\n",
      "Iteration 152, loss = 0.25520727\n",
      "Iteration 164, loss = 0.26151147\n",
      "Iteration 153, loss = 0.25545983\n",
      "Iteration 165, loss = 0.26103366\n",
      "Iteration 154, loss = 0.25480141\n",
      "Iteration 166, loss = 0.26145381\n",
      "Iteration 155, loss = 0.25520192\n",
      "Iteration 167, loss = 0.26094484\n",
      "Iteration 156, loss = 0.25525085\n",
      "Iteration 168, loss = 0.26110527\n",
      "Iteration 157, loss = 0.25448967\n",
      "Iteration 169, loss = 0.26070724\n",
      "Iteration 158, loss = 0.25510436\n",
      "Iteration 170, loss = 0.26091163\n",
      "Iteration 159, loss = 0.25452659\n",
      "Iteration 171, loss = 0.26084206\n",
      "Iteration 160, loss = 0.25447271\n",
      "Iteration 172, loss = 0.26106870\n",
      "Iteration 161, loss = 0.25451488\n",
      "Iteration 173, loss = 0.26074907\n",
      "Iteration 162, loss = 0.25445972\n",
      "Iteration 174, loss = 0.26059952\n",
      "Iteration 163, loss = 0.25446579\n",
      "Iteration 175, loss = 0.26064802\n",
      "Iteration 164, loss = 0.25443846\n",
      "Iteration 176, loss = 0.26089065\n",
      "Iteration 165, loss = 0.25455453\n",
      "Iteration 177, loss = 0.26076426\n",
      "Iteration 166, loss = 0.25429908\n",
      "Iteration 178, loss = 0.26038645\n",
      "Iteration 167, loss = 0.25382678\n",
      "Iteration 179, loss = 0.26018909\n",
      "Iteration 168, loss = 0.25413764\n",
      "Iteration 169, loss = 0.25384676\n",
      "Iteration 180, loss = 0.26044316\n",
      "Iteration 170, loss = 0.25407744\n",
      "Iteration 181, loss = 0.26045741\n",
      "Iteration 171, loss = 0.25404315\n",
      "Iteration 182, loss = 0.26028979\n",
      "Iteration 183, loss = 0.26013116\n",
      "Iteration 172, loss = 0.25393382\n",
      "Iteration 184, loss = 0.25984993\n",
      "Iteration 185, loss = 0.25986869\n",
      "Iteration 173, loss = 0.25386188\n",
      "Iteration 174, loss = 0.25361686\n",
      "Iteration 175, loss = 0.25337539\n",
      "Iteration 186, loss = 0.26005927\n",
      "Iteration 176, loss = 0.25376951\n",
      "Iteration 177, loss = 0.25385538\n",
      "Iteration 187, loss = 0.25974143\n",
      "Iteration 178, loss = 0.25343714\n",
      "Iteration 179, loss = 0.25332034\n",
      "Iteration 188, loss = 0.25944847\n",
      "Iteration 189, loss = 0.26011194\n",
      "Iteration 180, loss = 0.25353078\n",
      "Iteration 190, loss = 0.25972109\n",
      "Iteration 181, loss = 0.25330745\n",
      "Iteration 191, loss = 0.25964725\n",
      "Iteration 182, loss = 0.25338115\n",
      "Iteration 192, loss = 0.25959015\n",
      "Iteration 183, loss = 0.25305824\n",
      "Iteration 193, loss = 0.25957776\n",
      "Iteration 184, loss = 0.25270063\n",
      "Iteration 194, loss = 0.25956107\n",
      "Iteration 185, loss = 0.25305420\n",
      "Iteration 195, loss = 0.25994743\n",
      "Iteration 186, loss = 0.25265691\n",
      "Iteration 196, loss = 0.25959140\n",
      "Iteration 187, loss = 0.25248903\n",
      "Iteration 197, loss = 0.25933978\n",
      "Iteration 188, loss = 0.25230719\n",
      "Iteration 198, loss = 0.25969316\n",
      "Iteration 189, loss = 0.25316483\n",
      "Iteration 199, loss = 0.25938670\n",
      "Iteration 190, loss = 0.25297552\n",
      "Iteration 200, loss = 0.25972838\n",
      "Iteration 191, loss = 0.25252485\n",
      "Iteration 192, loss = 0.25246153\n",
      "Iteration 193, loss = 0.25238150\n",
      "Iteration 194, loss = 0.25248059\n",
      "Iteration 195, loss = 0.25287181\n",
      "Iteration 196, loss = 0.25212735\n",
      "Iteration 197, loss = 0.25221661\n",
      "Iteration 198, loss = 0.25310188\n",
      "Iteration 199, loss = 0.25210708\n",
      "Iteration 200, loss = 0.25217707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.35749962\n",
      "Iteration 1, loss = 0.35792620\n",
      "Iteration 2, loss = 0.31917920Iteration 2, loss = 0.32007474\n",
      "\n",
      "Iteration 3, loss = 0.29924824\n",
      "Iteration 3, loss = 0.29785349\n",
      "Iteration 4, loss = 0.28790163\n",
      "Iteration 4, loss = 0.28907466\n",
      "Iteration 5, loss = 0.28328871\n",
      "Iteration 5, loss = 0.28442640\n",
      "Iteration 6, loss = 0.28079670\n",
      "Iteration 6, loss = 0.28193515\n",
      "Iteration 7, loss = 0.27904633\n",
      "Iteration 7, loss = 0.28047259\n",
      "Iteration 8, loss = 0.27795456\n",
      "Iteration 8, loss = 0.27933375\n",
      "Iteration 9, loss = 0.27719850\n",
      "Iteration 9, loss = 0.27861237\n",
      "Iteration 10, loss = 0.27719143\n",
      "Iteration 10, loss = 0.27826352\n",
      "Iteration 11, loss = 0.27679053\n",
      "Iteration 11, loss = 0.27798500\n",
      "Iteration 12, loss = 0.27649391\n",
      "Iteration 12, loss = 0.27787677\n",
      "Iteration 13, loss = 0.27541564\n",
      "Iteration 13, loss = 0.27669580\n",
      "Iteration 14, loss = 0.27592040\n",
      "Iteration 14, loss = 0.27726627\n",
      "Iteration 15, loss = 0.27469381\n",
      "Iteration 15, loss = 0.27615210\n",
      "Iteration 16, loss = 0.27446081\n",
      "Iteration 16, loss = 0.27563407\n",
      "Iteration 17, loss = 0.27459755\n",
      "Iteration 17, loss = 0.27570140\n",
      "Iteration 18, loss = 0.27366046\n",
      "Iteration 19, loss = 0.27338506Iteration 18, loss = 0.27551177\n",
      "\n",
      "Iteration 20, loss = 0.27287266\n",
      "Iteration 19, loss = 0.27501249\n",
      "Iteration 21, loss = 0.27319296\n",
      "Iteration 20, loss = 0.27465208\n",
      "Iteration 22, loss = 0.27289203\n",
      "Iteration 23, loss = 0.27293911\n",
      "Iteration 21, loss = 0.27487369\n",
      "Iteration 24, loss = 0.27194536\n",
      "Iteration 22, loss = 0.27425967\n",
      "Iteration 25, loss = 0.27187594\n",
      "Iteration 23, loss = 0.27418609\n",
      "Iteration 26, loss = 0.27146286\n",
      "Iteration 24, loss = 0.27347324\n",
      "Iteration 27, loss = 0.27117304\n",
      "Iteration 25, loss = 0.27337537\n",
      "Iteration 28, loss = 0.27115718\n",
      "Iteration 26, loss = 0.27286421\n",
      "Iteration 29, loss = 0.27112447\n",
      "Iteration 27, loss = 0.27258778\n",
      "Iteration 30, loss = 0.27053907\n",
      "Iteration 28, loss = 0.27243066\n",
      "Iteration 31, loss = 0.27044888\n",
      "Iteration 29, loss = 0.27220076\n",
      "Iteration 32, loss = 0.27044710\n",
      "Iteration 30, loss = 0.27194903\n",
      "Iteration 33, loss = 0.27031266\n",
      "Iteration 31, loss = 0.27190649\n",
      "Iteration 34, loss = 0.26993150\n",
      "Iteration 32, loss = 0.27208533\n",
      "Iteration 35, loss = 0.26986927\n",
      "Iteration 33, loss = 0.27150433\n",
      "Iteration 36, loss = 0.26930955\n",
      "Iteration 34, loss = 0.27089696\n",
      "Iteration 37, loss = 0.26896919\n",
      "Iteration 35, loss = 0.27096711\n",
      "Iteration 38, loss = 0.26901336\n",
      "Iteration 36, loss = 0.27032004\n",
      "Iteration 39, loss = 0.26882967\n",
      "Iteration 37, loss = 0.27026055\n",
      "Iteration 40, loss = 0.26872868\n",
      "Iteration 38, loss = 0.27019821\n",
      "Iteration 41, loss = 0.26827402\n",
      "Iteration 39, loss = 0.26994781\n",
      "Iteration 42, loss = 0.26833017\n",
      "Iteration 40, loss = 0.26965077\n",
      "Iteration 43, loss = 0.26797242\n",
      "Iteration 41, loss = 0.26927955\n",
      "Iteration 44, loss = 0.26774735\n",
      "Iteration 42, loss = 0.26932684\n",
      "Iteration 45, loss = 0.26755948\n",
      "Iteration 43, loss = 0.26938484\n",
      "Iteration 46, loss = 0.26698314\n",
      "Iteration 47, loss = 0.26680514\n",
      "Iteration 44, loss = 0.26855322\n",
      "Iteration 48, loss = 0.26641401\n",
      "Iteration 49, loss = 0.26594550\n",
      "Iteration 45, loss = 0.26849068\n",
      "Iteration 50, loss = 0.26588262\n",
      "Iteration 46, loss = 0.26810885\n",
      "Iteration 51, loss = 0.26550924\n",
      "Iteration 47, loss = 0.26773987\n",
      "Iteration 52, loss = 0.26503943Iteration 48, loss = 0.26760893\n",
      "\n",
      "Iteration 53, loss = 0.26565494\n",
      "Iteration 49, loss = 0.26737475\n",
      "Iteration 54, loss = 0.26484638\n",
      "Iteration 50, loss = 0.26751823\n",
      "Iteration 55, loss = 0.26410788\n",
      "Iteration 51, loss = 0.26697228\n",
      "Iteration 56, loss = 0.26402770\n",
      "Iteration 52, loss = 0.26672255\n",
      "Iteration 53, loss = 0.26725346\n",
      "Iteration 57, loss = 0.26435399\n",
      "Iteration 54, loss = 0.26691867\n",
      "Iteration 58, loss = 0.26339134\n",
      "Iteration 55, loss = 0.26560484\n",
      "Iteration 56, loss = 0.26556058\n",
      "Iteration 59, loss = 0.26360740\n",
      "Iteration 57, loss = 0.26601229\n",
      "Iteration 60, loss = 0.26355891\n",
      "Iteration 58, loss = 0.26493815\n",
      "Iteration 61, loss = 0.26300006\n",
      "Iteration 59, loss = 0.26526837\n",
      "Iteration 62, loss = 0.26300040\n",
      "Iteration 60, loss = 0.26510902\n",
      "Iteration 63, loss = 0.26257736\n",
      "Iteration 61, loss = 0.26452552\n",
      "Iteration 64, loss = 0.26238391\n",
      "Iteration 62, loss = 0.26432334\n",
      "Iteration 65, loss = 0.26270090\n",
      "Iteration 63, loss = 0.26381342\n",
      "Iteration 66, loss = 0.26180822\n",
      "Iteration 64, loss = 0.26429221\n",
      "Iteration 65, loss = 0.26352065\n",
      "Iteration 67, loss = 0.26146054\n",
      "Iteration 68, loss = 0.26152230\n",
      "Iteration 66, loss = 0.26314975\n",
      "Iteration 69, loss = 0.26149720\n",
      "Iteration 67, loss = 0.26275966\n",
      "Iteration 70, loss = 0.26095766\n",
      "Iteration 71, loss = 0.26107062\n",
      "Iteration 68, loss = 0.26257248\n",
      "Iteration 69, loss = 0.26243247\n",
      "Iteration 72, loss = 0.26054673\n",
      "Iteration 70, loss = 0.26234761\n",
      "Iteration 71, loss = 0.26192393Iteration 73, loss = 0.26084279\n",
      "\n",
      "Iteration 72, loss = 0.26154903\n",
      "Iteration 73, loss = 0.26161733\n",
      "Iteration 74, loss = 0.26079883\n",
      "Iteration 75, loss = 0.26040317\n",
      "Iteration 74, loss = 0.26189051\n",
      "Iteration 76, loss = 0.26014931\n",
      "Iteration 75, loss = 0.26120954\n",
      "Iteration 77, loss = 0.26019293\n",
      "Iteration 76, loss = 0.26083726\n",
      "Iteration 78, loss = 0.26021296\n",
      "Iteration 77, loss = 0.26077082\n",
      "Iteration 79, loss = 0.25964949\n",
      "Iteration 78, loss = 0.26063798\n",
      "Iteration 80, loss = 0.25964923\n",
      "Iteration 79, loss = 0.26048017\n",
      "Iteration 81, loss = 0.25929688\n",
      "Iteration 80, loss = 0.26045103\n",
      "Iteration 82, loss = 0.25905252\n",
      "Iteration 81, loss = 0.26001443\n",
      "Iteration 83, loss = 0.25923685\n",
      "Iteration 82, loss = 0.25975993\n",
      "Iteration 84, loss = 0.25874020\n",
      "Iteration 83, loss = 0.25945154\n",
      "Iteration 85, loss = 0.25890545\n",
      "Iteration 86, loss = 0.25863533\n",
      "Iteration 84, loss = 0.25931900\n",
      "Iteration 87, loss = 0.25818296\n",
      "Iteration 85, loss = 0.25914234\n",
      "Iteration 88, loss = 0.25821909\n",
      "Iteration 86, loss = 0.25882081\n",
      "Iteration 89, loss = 0.25797986\n",
      "Iteration 87, loss = 0.25870756\n",
      "Iteration 90, loss = 0.25783952\n",
      "Iteration 88, loss = 0.25846393\n",
      "Iteration 89, loss = 0.25831288\n",
      "Iteration 91, loss = 0.25774855\n",
      "Iteration 90, loss = 0.25788844\n",
      "Iteration 92, loss = 0.25772376\n",
      "Iteration 91, loss = 0.25765541\n",
      "Iteration 93, loss = 0.25755610\n",
      "Iteration 92, loss = 0.25769522\n",
      "Iteration 94, loss = 0.25853155\n",
      "Iteration 93, loss = 0.25736704\n",
      "Iteration 95, loss = 0.25749589\n",
      "Iteration 94, loss = 0.25851933\n",
      "Iteration 96, loss = 0.25710033\n",
      "Iteration 95, loss = 0.25724816\n",
      "Iteration 97, loss = 0.25764188\n",
      "Iteration 98, loss = 0.25703134\n",
      "Iteration 96, loss = 0.25696530\n",
      "Iteration 99, loss = 0.25713892\n",
      "Iteration 97, loss = 0.25736047\n",
      "Iteration 98, loss = 0.25654916\n",
      "Iteration 100, loss = 0.25672980\n",
      "Iteration 99, loss = 0.25662140\n",
      "Iteration 101, loss = 0.25653609\n",
      "Iteration 100, loss = 0.25632886\n",
      "Iteration 102, loss = 0.25693345\n",
      "Iteration 101, loss = 0.25593907\n",
      "Iteration 103, loss = 0.25707214\n",
      "Iteration 102, loss = 0.25609669\n",
      "Iteration 104, loss = 0.25669207\n",
      "Iteration 103, loss = 0.25656017\n",
      "Iteration 105, loss = 0.25634303\n",
      "Iteration 104, loss = 0.25607563\n",
      "Iteration 106, loss = 0.25584982\n",
      "Iteration 105, loss = 0.25582607\n",
      "Iteration 107, loss = 0.25590535\n",
      "Iteration 106, loss = 0.25521278\n",
      "Iteration 108, loss = 0.25570358\n",
      "Iteration 107, loss = 0.25521345\n",
      "Iteration 109, loss = 0.25597394\n",
      "Iteration 108, loss = 0.25531482\n",
      "Iteration 110, loss = 0.25595084\n",
      "Iteration 109, loss = 0.25518229\n",
      "Iteration 111, loss = 0.25593764\n",
      "Iteration 110, loss = 0.25493409\n",
      "Iteration 112, loss = 0.25568220\n",
      "Iteration 111, loss = 0.25525099\n",
      "Iteration 113, loss = 0.25572424\n",
      "Iteration 112, loss = 0.25477757\n",
      "Iteration 114, loss = 0.25546105\n",
      "Iteration 113, loss = 0.25483688\n",
      "Iteration 115, loss = 0.25495750\n",
      "Iteration 114, loss = 0.25440882\n",
      "Iteration 116, loss = 0.25542439\n",
      "Iteration 115, loss = 0.25416615\n",
      "Iteration 117, loss = 0.25499479\n",
      "Iteration 116, loss = 0.25406266\n",
      "Iteration 118, loss = 0.25517056\n",
      "Iteration 117, loss = 0.25434635\n",
      "Iteration 119, loss = 0.25492256\n",
      "Iteration 118, loss = 0.25412389\n",
      "Iteration 120, loss = 0.25498288\n",
      "Iteration 119, loss = 0.25380694\n",
      "Iteration 121, loss = 0.25498008\n",
      "Iteration 120, loss = 0.25405981\n",
      "Iteration 122, loss = 0.25460426\n",
      "Iteration 121, loss = 0.25363271\n",
      "Iteration 123, loss = 0.25497120\n",
      "Iteration 122, loss = 0.25339401\n",
      "Iteration 124, loss = 0.25469234\n",
      "Iteration 123, loss = 0.25352278\n",
      "Iteration 125, loss = 0.25455700\n",
      "Iteration 124, loss = 0.25353814\n",
      "Iteration 126, loss = 0.25466621\n",
      "Iteration 125, loss = 0.25315826\n",
      "Iteration 127, loss = 0.25483509\n",
      "Iteration 126, loss = 0.25313621\n",
      "Iteration 128, loss = 0.25463380\n",
      "Iteration 127, loss = 0.25349505\n",
      "Iteration 129, loss = 0.25391268\n",
      "Iteration 128, loss = 0.25332129\n",
      "Iteration 129, loss = 0.25342690\n",
      "Iteration 130, loss = 0.25424311\n",
      "Iteration 130, loss = 0.25311433\n",
      "Iteration 131, loss = 0.25419355\n",
      "Iteration 131, loss = 0.25289968\n",
      "Iteration 132, loss = 0.25385058\n",
      "Iteration 132, loss = 0.25262210\n",
      "Iteration 133, loss = 0.25399420\n",
      "Iteration 133, loss = 0.25254301\n",
      "Iteration 134, loss = 0.25389738\n",
      "Iteration 134, loss = 0.25240464\n",
      "Iteration 135, loss = 0.25361901\n",
      "Iteration 135, loss = 0.25223166\n",
      "Iteration 136, loss = 0.25381591\n",
      "Iteration 136, loss = 0.25243693\n",
      "Iteration 137, loss = 0.25394400\n",
      "Iteration 137, loss = 0.25275943\n",
      "Iteration 138, loss = 0.25368554\n",
      "Iteration 138, loss = 0.25203855\n",
      "Iteration 139, loss = 0.25375342\n",
      "Iteration 139, loss = 0.25206396\n",
      "Iteration 140, loss = 0.25360942\n",
      "Iteration 140, loss = 0.25240792\n",
      "Iteration 141, loss = 0.25331345\n",
      "Iteration 141, loss = 0.25206865\n",
      "Iteration 142, loss = 0.25329428\n",
      "Iteration 142, loss = 0.25180436\n",
      "Iteration 143, loss = 0.25337702\n",
      "Iteration 143, loss = 0.25169491\n",
      "Iteration 144, loss = 0.25378287\n",
      "Iteration 144, loss = 0.25219752\n",
      "Iteration 145, loss = 0.25368460\n",
      "Iteration 146, loss = 0.25318359\n",
      "Iteration 145, loss = 0.25202692\n",
      "Iteration 147, loss = 0.25304891\n",
      "Iteration 146, loss = 0.25136697\n",
      "Iteration 148, loss = 0.25324424\n",
      "Iteration 147, loss = 0.25129665\n",
      "Iteration 149, loss = 0.25334621\n",
      "Iteration 148, loss = 0.25166275\n",
      "Iteration 150, loss = 0.25283949\n",
      "Iteration 149, loss = 0.25127810\n",
      "Iteration 150, loss = 0.25116357\n",
      "Iteration 151, loss = 0.25289942\n",
      "Iteration 151, loss = 0.25101309\n",
      "Iteration 152, loss = 0.25289932\n",
      "Iteration 152, loss = 0.25143898\n",
      "Iteration 153, loss = 0.25294333\n",
      "Iteration 153, loss = 0.25089157\n",
      "Iteration 154, loss = 0.25260835\n",
      "Iteration 154, loss = 0.25107396\n",
      "Iteration 155, loss = 0.25257171\n",
      "Iteration 155, loss = 0.25104137\n",
      "Iteration 156, loss = 0.25281495\n",
      "Iteration 156, loss = 0.25070304\n",
      "Iteration 157, loss = 0.25276954\n",
      "Iteration 157, loss = 0.25083455\n",
      "Iteration 158, loss = 0.25270745\n",
      "Iteration 158, loss = 0.25052373\n",
      "Iteration 159, loss = 0.25249148\n",
      "Iteration 159, loss = 0.25023002\n",
      "Iteration 160, loss = 0.25256815\n",
      "Iteration 160, loss = 0.25046364\n",
      "Iteration 161, loss = 0.25237216\n",
      "Iteration 161, loss = 0.25035624\n",
      "Iteration 162, loss = 0.25257677\n",
      "Iteration 162, loss = 0.25029675\n",
      "Iteration 163, loss = 0.25254080\n",
      "Iteration 163, loss = 0.25050769\n",
      "Iteration 164, loss = 0.25240252\n",
      "Iteration 164, loss = 0.25088108\n",
      "Iteration 165, loss = 0.25248417\n",
      "Iteration 165, loss = 0.25039360\n",
      "Iteration 166, loss = 0.25224109\n",
      "Iteration 166, loss = 0.25051900\n",
      "Iteration 167, loss = 0.24991557\n",
      "Iteration 167, loss = 0.25210801\n",
      "Iteration 168, loss = 0.24984404\n",
      "Iteration 168, loss = 0.25219101\n",
      "Iteration 169, loss = 0.25224334\n",
      "Iteration 169, loss = 0.25042395\n",
      "Iteration 170, loss = 0.25225968\n",
      "Iteration 170, loss = 0.25045967\n",
      "Iteration 171, loss = 0.25195588\n",
      "Iteration 172, loss = 0.25204773\n",
      "Iteration 171, loss = 0.25002227\n",
      "Iteration 173, loss = 0.25196646\n",
      "Iteration 172, loss = 0.24992755\n",
      "Iteration 174, loss = 0.25214425\n",
      "Iteration 173, loss = 0.24983573\n",
      "Iteration 175, loss = 0.25207403\n",
      "Iteration 174, loss = 0.24993814\n",
      "Iteration 176, loss = 0.25224264\n",
      "Iteration 175, loss = 0.25013695\n",
      "Iteration 176, loss = 0.24988939\n",
      "Iteration 177, loss = 0.25013666\n",
      "Iteration 177, loss = 0.25266312\n",
      "Iteration 178, loss = 0.25005170\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 178, loss = 0.25196041\n",
      "Iteration 179, loss = 0.25172695\n",
      "Iteration 180, loss = 0.25143503\n",
      "Iteration 181, loss = 0.25149610\n",
      "Iteration 182, loss = 0.25203073\n",
      "Iteration 1, loss = 0.35701099\n",
      "Iteration 183, loss = 0.25163186\n",
      "Iteration 2, loss = 0.32019884\n",
      "Iteration 184, loss = 0.25163408\n",
      "Iteration 185, loss = 0.25166034\n",
      "Iteration 3, loss = 0.29944477\n",
      "Iteration 186, loss = 0.25174251\n",
      "Iteration 4, loss = 0.28999499\n",
      "Iteration 5, loss = 0.28512249\n",
      "Iteration 187, loss = 0.25153008\n",
      "Iteration 6, loss = 0.28255308\n",
      "Iteration 7, loss = 0.28127745\n",
      "Iteration 188, loss = 0.25160060\n",
      "Iteration 8, loss = 0.28013152\n",
      "Iteration 189, loss = 0.25122872\n",
      "Iteration 9, loss = 0.27928225\n",
      "Iteration 190, loss = 0.25114659\n",
      "Iteration 10, loss = 0.27921312\n",
      "Iteration 191, loss = 0.25104978\n",
      "Iteration 11, loss = 0.27909874\n",
      "Iteration 192, loss = 0.25133306\n",
      "Iteration 12, loss = 0.27827879\n",
      "Iteration 193, loss = 0.25153519\n",
      "Iteration 13, loss = 0.27760875\n",
      "Iteration 194, loss = 0.25134002\n",
      "Iteration 14, loss = 0.27836254\n",
      "Iteration 195, loss = 0.25124561\n",
      "Iteration 15, loss = 0.27691697\n",
      "Iteration 196, loss = 0.25144729\n",
      "Iteration 16, loss = 0.27662814\n",
      "Iteration 197, loss = 0.25106659\n",
      "Iteration 17, loss = 0.27663535\n",
      "Iteration 198, loss = 0.25090721\n",
      "Iteration 18, loss = 0.27587895\n",
      "Iteration 199, loss = 0.25096135\n",
      "Iteration 19, loss = 0.27555714\n",
      "Iteration 200, loss = 0.25084255\n",
      "Iteration 20, loss = 0.27513449\n",
      "Iteration 21, loss = 0.27534915\n",
      "Iteration 1, loss = 0.35727359\n",
      "Iteration 22, loss = 0.27515171\n",
      "Iteration 2, loss = 0.32006681\n",
      "Iteration 23, loss = 0.27535241\n",
      "Iteration 3, loss = 0.29908260\n",
      "Iteration 24, loss = 0.27427810\n",
      "Iteration 4, loss = 0.28941380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25, loss = 0.27403677\n",
      "Iteration 5, loss = 0.28413655\n",
      "Iteration 26, loss = 0.27380558\n",
      "Iteration 6, loss = 0.28185102\n",
      "Iteration 27, loss = 0.27350866\n",
      "Iteration 7, loss = 0.28005024\n",
      "Iteration 28, loss = 0.27371319\n",
      "Iteration 8, loss = 0.27919491\n",
      "Iteration 29, loss = 0.27358109\n",
      "Iteration 9, loss = 0.27834405\n",
      "Iteration 30, loss = 0.27291223\n",
      "Iteration 10, loss = 0.27796626\n",
      "Iteration 31, loss = 0.27288688\n",
      "Iteration 11, loss = 0.27760877\n",
      "Iteration 32, loss = 0.27277219\n",
      "Iteration 12, loss = 0.27725336\n",
      "Iteration 33, loss = 0.27250973\n",
      "Iteration 13, loss = 0.27683256\n",
      "Iteration 34, loss = 0.27214703\n",
      "Iteration 14, loss = 0.27654244\n",
      "Iteration 35, loss = 0.27208026\n",
      "Iteration 15, loss = 0.27646325\n",
      "Iteration 36, loss = 0.27153211\n",
      "Iteration 16, loss = 0.27620555\n",
      "Iteration 37, loss = 0.27115817\n",
      "Iteration 17, loss = 0.27547746\n",
      "Iteration 38, loss = 0.27113902\n",
      "Iteration 18, loss = 0.27523192\n",
      "Iteration 39, loss = 0.27071174\n",
      "Iteration 19, loss = 0.27539734\n",
      "Iteration 40, loss = 0.27038464\n",
      "Iteration 20, loss = 0.27473874\n",
      "Iteration 41, loss = 0.27028230\n",
      "Iteration 21, loss = 0.27449993\n",
      "Iteration 42, loss = 0.27011452\n",
      "Iteration 22, loss = 0.27430412\n",
      "Iteration 43, loss = 0.26957944\n",
      "Iteration 23, loss = 0.27431398\n",
      "Iteration 44, loss = 0.26945824\n",
      "Iteration 24, loss = 0.27393413\n",
      "Iteration 45, loss = 0.26935328\n",
      "Iteration 25, loss = 0.27451317\n",
      "Iteration 46, loss = 0.26844016\n",
      "Iteration 26, loss = 0.27304960\n",
      "Iteration 47, loss = 0.26852540\n",
      "Iteration 48, loss = 0.26817822\n",
      "Iteration 27, loss = 0.27281131\n",
      "Iteration 49, loss = 0.26764206\n",
      "Iteration 28, loss = 0.27318391\n",
      "Iteration 50, loss = 0.26814278\n",
      "Iteration 29, loss = 0.27260027\n",
      "Iteration 51, loss = 0.26730291\n",
      "Iteration 30, loss = 0.27225205\n",
      "Iteration 52, loss = 0.26681522\n",
      "Iteration 31, loss = 0.27184764\n",
      "Iteration 53, loss = 0.26734727\n",
      "Iteration 54, loss = 0.26673593\n",
      "Iteration 32, loss = 0.27186186\n",
      "Iteration 33, loss = 0.27173439\n",
      "Iteration 55, loss = 0.26583082\n",
      "Iteration 56, loss = 0.26584311\n",
      "Iteration 34, loss = 0.27229199\n",
      "Iteration 57, loss = 0.26601397\n",
      "Iteration 58, loss = 0.26525166\n",
      "Iteration 35, loss = 0.27090752\n",
      "Iteration 59, loss = 0.26541490\n",
      "Iteration 36, loss = 0.27076039\n",
      "Iteration 60, loss = 0.26528849\n",
      "Iteration 37, loss = 0.27036832\n",
      "Iteration 61, loss = 0.26484388\n",
      "Iteration 38, loss = 0.27041823\n",
      "Iteration 62, loss = 0.26462692\n",
      "Iteration 39, loss = 0.26996484\n",
      "Iteration 63, loss = 0.26425846\n",
      "Iteration 40, loss = 0.26994184\n",
      "Iteration 64, loss = 0.26439080\n",
      "Iteration 41, loss = 0.26967799\n",
      "Iteration 65, loss = 0.26401004\n",
      "Iteration 42, loss = 0.26936348\n",
      "Iteration 66, loss = 0.26382748\n",
      "Iteration 43, loss = 0.26972784\n",
      "Iteration 67, loss = 0.26331731\n",
      "Iteration 44, loss = 0.26949111\n",
      "Iteration 68, loss = 0.26336181\n",
      "Iteration 45, loss = 0.26881360\n",
      "Iteration 69, loss = 0.26358371\n",
      "Iteration 46, loss = 0.26808998\n",
      "Iteration 70, loss = 0.26277142\n",
      "Iteration 47, loss = 0.26762482\n",
      "Iteration 71, loss = 0.26290607\n",
      "Iteration 48, loss = 0.26764333\n",
      "Iteration 72, loss = 0.26238168\n",
      "Iteration 49, loss = 0.26766580\n",
      "Iteration 73, loss = 0.26228884\n",
      "Iteration 50, loss = 0.26692540\n",
      "Iteration 74, loss = 0.26278757\n",
      "Iteration 51, loss = 0.26701232\n",
      "Iteration 75, loss = 0.26197354\n",
      "Iteration 52, loss = 0.26666808\n",
      "Iteration 76, loss = 0.26194123\n",
      "Iteration 53, loss = 0.26599408\n",
      "Iteration 77, loss = 0.26186933\n",
      "Iteration 54, loss = 0.26573536\n",
      "Iteration 78, loss = 0.26221824\n",
      "Iteration 55, loss = 0.26578121\n",
      "Iteration 79, loss = 0.26114081\n",
      "Iteration 56, loss = 0.26528208\n",
      "Iteration 80, loss = 0.26107848\n",
      "Iteration 81, loss = 0.26072668\n",
      "Iteration 57, loss = 0.26534495\n",
      "Iteration 82, loss = 0.26039114\n",
      "Iteration 58, loss = 0.26458760\n",
      "Iteration 83, loss = 0.26070388\n",
      "Iteration 59, loss = 0.26450026\n",
      "Iteration 84, loss = 0.26037292\n",
      "Iteration 60, loss = 0.26403588\n",
      "Iteration 61, loss = 0.26466156\n",
      "Iteration 85, loss = 0.26043645\n",
      "Iteration 62, loss = 0.26419975\n",
      "Iteration 63, loss = 0.26397495\n",
      "Iteration 86, loss = 0.26008864\n",
      "Iteration 87, loss = 0.25976813\n",
      "Iteration 88, loss = 0.25965817\n",
      "Iteration 64, loss = 0.26355144\n",
      "Iteration 89, loss = 0.25977834\n",
      "Iteration 65, loss = 0.26348324\n",
      "Iteration 66, loss = 0.26434133\n",
      "Iteration 90, loss = 0.25921929\n",
      "Iteration 67, loss = 0.26271259\n",
      "Iteration 91, loss = 0.25929155\n",
      "Iteration 68, loss = 0.26304489\n",
      "Iteration 69, loss = 0.26246385\n",
      "Iteration 92, loss = 0.25902668\n",
      "Iteration 70, loss = 0.26221378\n",
      "Iteration 71, loss = 0.26211400\n",
      "Iteration 93, loss = 0.25891883\n",
      "Iteration 72, loss = 0.26190338\n",
      "Iteration 73, loss = 0.26152431\n",
      "Iteration 94, loss = 0.25954368\n",
      "Iteration 74, loss = 0.26161796\n",
      "Iteration 95, loss = 0.25865545\n",
      "Iteration 75, loss = 0.26144055\n",
      "Iteration 96, loss = 0.25851688\n",
      "Iteration 76, loss = 0.26093878\n",
      "Iteration 97, loss = 0.25861625\n",
      "Iteration 77, loss = 0.26100668\n",
      "Iteration 98, loss = 0.25816338\n",
      "Iteration 78, loss = 0.26088883\n",
      "Iteration 99, loss = 0.25822216\n",
      "Iteration 79, loss = 0.26093930\n",
      "Iteration 100, loss = 0.25797816\n",
      "Iteration 80, loss = 0.26069409\n",
      "Iteration 101, loss = 0.25779464\n",
      "Iteration 81, loss = 0.26050376\n",
      "Iteration 102, loss = 0.25790627\n",
      "Iteration 82, loss = 0.26017604\n",
      "Iteration 83, loss = 0.26019520\n",
      "Iteration 103, loss = 0.25779600\n",
      "Iteration 84, loss = 0.25971186\n",
      "Iteration 104, loss = 0.25776975\n",
      "Iteration 85, loss = 0.25987384\n",
      "Iteration 105, loss = 0.25743982\n",
      "Iteration 86, loss = 0.25960633\n",
      "Iteration 106, loss = 0.25718892\n",
      "Iteration 87, loss = 0.25969359\n",
      "Iteration 107, loss = 0.25708922\n",
      "Iteration 88, loss = 0.25931993\n",
      "Iteration 108, loss = 0.25670293\n",
      "Iteration 89, loss = 0.25910892\n",
      "Iteration 109, loss = 0.25712398\n",
      "Iteration 90, loss = 0.25938312\n",
      "Iteration 110, loss = 0.25699405\n",
      "Iteration 91, loss = 0.25930714\n",
      "Iteration 111, loss = 0.25665761\n",
      "Iteration 92, loss = 0.25904559\n",
      "Iteration 112, loss = 0.25668176\n",
      "Iteration 93, loss = 0.25865248\n",
      "Iteration 113, loss = 0.25677603\n",
      "Iteration 94, loss = 0.25903973\n",
      "Iteration 114, loss = 0.25625282\n",
      "Iteration 95, loss = 0.25851801\n",
      "Iteration 115, loss = 0.25625462\n",
      "Iteration 96, loss = 0.25841723\n",
      "Iteration 116, loss = 0.25597071\n",
      "Iteration 97, loss = 0.25809514\n",
      "Iteration 117, loss = 0.25589117\n",
      "Iteration 98, loss = 0.25817582\n",
      "Iteration 118, loss = 0.25607032\n",
      "Iteration 99, loss = 0.25782849\n",
      "Iteration 119, loss = 0.25557062\n",
      "Iteration 100, loss = 0.25866989\n",
      "Iteration 120, loss = 0.25579965\n",
      "Iteration 101, loss = 0.25774879\n",
      "Iteration 102, loss = 0.25785530\n",
      "Iteration 121, loss = 0.25547212\n",
      "Iteration 103, loss = 0.25790109\n",
      "Iteration 122, loss = 0.25509168\n",
      "Iteration 104, loss = 0.25771408\n",
      "Iteration 123, loss = 0.25571796\n",
      "Iteration 105, loss = 0.25756986\n",
      "Iteration 124, loss = 0.25541290\n",
      "Iteration 106, loss = 0.25732770\n",
      "Iteration 125, loss = 0.25493332\n",
      "Iteration 107, loss = 0.25701887\n",
      "Iteration 126, loss = 0.25486019\n",
      "Iteration 108, loss = 0.25719221\n",
      "Iteration 127, loss = 0.25474896\n",
      "Iteration 109, loss = 0.25702302\n",
      "Iteration 110, loss = 0.25712846\n",
      "Iteration 128, loss = 0.25479295\n",
      "Iteration 111, loss = 0.25726820\n",
      "Iteration 129, loss = 0.25445868\n",
      "Iteration 112, loss = 0.25705381\n",
      "Iteration 130, loss = 0.25467939\n",
      "Iteration 113, loss = 0.25635559\n",
      "Iteration 131, loss = 0.25422615\n",
      "Iteration 114, loss = 0.25706786\n",
      "Iteration 132, loss = 0.25423069\n",
      "Iteration 115, loss = 0.25633390\n",
      "Iteration 133, loss = 0.25403389\n",
      "Iteration 116, loss = 0.25647375\n",
      "Iteration 134, loss = 0.25388587\n",
      "Iteration 117, loss = 0.25648104\n",
      "Iteration 135, loss = 0.25377300\n",
      "Iteration 118, loss = 0.25649355\n",
      "Iteration 136, loss = 0.25379709\n",
      "Iteration 119, loss = 0.25611529\n",
      "Iteration 137, loss = 0.25429198\n",
      "Iteration 120, loss = 0.25595604\n",
      "Iteration 138, loss = 0.25348464\n",
      "Iteration 121, loss = 0.25695311\n",
      "Iteration 139, loss = 0.25352371\n",
      "Iteration 122, loss = 0.25597895\n",
      "Iteration 140, loss = 0.25335493\n",
      "Iteration 123, loss = 0.25607036\n",
      "Iteration 141, loss = 0.25320312\n",
      "Iteration 124, loss = 0.25588832\n",
      "Iteration 142, loss = 0.25287219\n",
      "Iteration 125, loss = 0.25571096\n",
      "Iteration 126, loss = 0.25561286Iteration 143, loss = 0.25308480\n",
      "\n",
      "Iteration 127, loss = 0.25554631Iteration 144, loss = 0.25339296\n",
      "\n",
      "Iteration 145, loss = 0.25347709\n",
      "Iteration 128, loss = 0.25536869\n",
      "Iteration 146, loss = 0.25283291\n",
      "Iteration 129, loss = 0.25567726\n",
      "Iteration 147, loss = 0.25279873\n",
      "Iteration 130, loss = 0.25532824\n",
      "Iteration 148, loss = 0.25253374\n",
      "Iteration 131, loss = 0.25544549\n",
      "Iteration 149, loss = 0.25226965\n",
      "Iteration 132, loss = 0.25517547\n",
      "Iteration 150, loss = 0.25218217\n",
      "Iteration 133, loss = 0.25531961\n",
      "Iteration 151, loss = 0.25216170\n",
      "Iteration 134, loss = 0.25545976\n",
      "Iteration 152, loss = 0.25221568\n",
      "Iteration 135, loss = 0.25497568\n",
      "Iteration 153, loss = 0.25214666\n",
      "Iteration 136, loss = 0.25536034\n",
      "Iteration 154, loss = 0.25198414\n",
      "Iteration 137, loss = 0.25513799\n",
      "Iteration 155, loss = 0.25186690\n",
      "Iteration 138, loss = 0.25499354\n",
      "Iteration 156, loss = 0.25193023\n",
      "Iteration 139, loss = 0.25449427\n",
      "Iteration 157, loss = 0.25149255\n",
      "Iteration 140, loss = 0.25475086\n",
      "Iteration 158, loss = 0.25182720\n",
      "Iteration 141, loss = 0.25474892\n",
      "Iteration 159, loss = 0.25150565\n",
      "Iteration 142, loss = 0.25442602\n",
      "Iteration 160, loss = 0.25169626\n",
      "Iteration 143, loss = 0.25436681\n",
      "Iteration 161, loss = 0.25106475\n",
      "Iteration 144, loss = 0.25471875\n",
      "Iteration 162, loss = 0.25109793\n",
      "Iteration 145, loss = 0.25437422\n",
      "Iteration 163, loss = 0.25099099\n",
      "Iteration 146, loss = 0.25402860\n",
      "Iteration 164, loss = 0.25140498\n",
      "Iteration 147, loss = 0.25478069\n",
      "Iteration 165, loss = 0.25153319\n",
      "Iteration 148, loss = 0.25428009\n",
      "Iteration 149, loss = 0.25420491\n",
      "Iteration 166, loss = 0.25086795\n",
      "Iteration 150, loss = 0.25394687\n",
      "Iteration 167, loss = 0.25076089\n",
      "Iteration 168, loss = 0.25061051Iteration 151, loss = 0.25391933\n",
      "\n",
      "Iteration 152, loss = 0.25398445\n",
      "Iteration 169, loss = 0.25086102\n",
      "Iteration 170, loss = 0.25092700\n",
      "Iteration 153, loss = 0.25413692\n",
      "Iteration 154, loss = 0.25400814\n",
      "Iteration 171, loss = 0.25057606\n",
      "Iteration 155, loss = 0.25390786\n",
      "Iteration 172, loss = 0.25057764\n",
      "Iteration 156, loss = 0.25366049\n",
      "Iteration 173, loss = 0.25035686\n",
      "Iteration 174, loss = 0.25019735\n",
      "Iteration 157, loss = 0.25348920\n",
      "Iteration 175, loss = 0.25056447\n",
      "Iteration 158, loss = 0.25372646\n",
      "Iteration 176, loss = 0.25070368\n",
      "Iteration 159, loss = 0.25340153\n",
      "Iteration 177, loss = 0.25133795\n",
      "Iteration 160, loss = 0.25352675\n",
      "Iteration 178, loss = 0.25055850\n",
      "Iteration 161, loss = 0.25325061\n",
      "Iteration 179, loss = 0.25030660\n",
      "Iteration 162, loss = 0.25317866\n",
      "Iteration 163, loss = 0.25313317\n",
      "Iteration 180, loss = 0.25027834\n",
      "Iteration 164, loss = 0.25323462\n",
      "Iteration 181, loss = 0.25007231\n",
      "Iteration 182, loss = 0.25041439\n",
      "Iteration 165, loss = 0.25338998\n",
      "Iteration 166, loss = 0.25336112\n",
      "Iteration 183, loss = 0.25005504\n",
      "Iteration 167, loss = 0.25324638\n",
      "Iteration 184, loss = 0.24990943\n",
      "Iteration 185, loss = 0.25038084\n",
      "Iteration 186, loss = 0.25014005\n",
      "Iteration 168, loss = 0.25317139\n",
      "Iteration 187, loss = 0.24992390\n",
      "Iteration 169, loss = 0.25335065\n",
      "Iteration 188, loss = 0.25013952\n",
      "Iteration 170, loss = 0.25322826\n",
      "Iteration 171, loss = 0.25259758\n",
      "Iteration 189, loss = 0.24956787\n",
      "Iteration 172, loss = 0.25304585\n",
      "Iteration 190, loss = 0.24957206\n",
      "Iteration 173, loss = 0.25338715\n",
      "Iteration 191, loss = 0.24943319\n",
      "Iteration 174, loss = 0.25302016\n",
      "Iteration 192, loss = 0.24952647\n",
      "Iteration 175, loss = 0.25258837\n",
      "Iteration 193, loss = 0.24985009\n",
      "Iteration 176, loss = 0.25256915\n",
      "Iteration 194, loss = 0.24951358\n",
      "Iteration 177, loss = 0.25287558\n",
      "Iteration 195, loss = 0.24974373\n",
      "Iteration 178, loss = 0.25259695\n",
      "Iteration 196, loss = 0.24948219\n",
      "Iteration 179, loss = 0.25244798\n",
      "Iteration 197, loss = 0.24932799\n",
      "Iteration 180, loss = 0.25224697\n",
      "Iteration 198, loss = 0.24943115\n",
      "Iteration 181, loss = 0.25298788\n",
      "Iteration 199, loss = 0.24925041\n",
      "Iteration 182, loss = 0.25281006\n",
      "Iteration 183, loss = 0.25237754\n",
      "Iteration 200, loss = 0.24912189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 184, loss = 0.25209612\n",
      "Iteration 185, loss = 0.25192012\n",
      "Iteration 1, loss = 0.35821156\n",
      "Iteration 186, loss = 0.25277352\n",
      "Iteration 2, loss = 0.31960829\n",
      "Iteration 187, loss = 0.25199080\n",
      "Iteration 3, loss = 0.29773423\n",
      "Iteration 188, loss = 0.25216559\n",
      "Iteration 4, loss = 0.28834435\n",
      "Iteration 189, loss = 0.25165730\n",
      "Iteration 5, loss = 0.28341446\n",
      "Iteration 190, loss = 0.25218635\n",
      "Iteration 6, loss = 0.28142084\n",
      "Iteration 191, loss = 0.25261249\n",
      "Iteration 7, loss = 0.27951323\n",
      "Iteration 192, loss = 0.25192572\n",
      "Iteration 8, loss = 0.27858679\n",
      "Iteration 193, loss = 0.25190279\n",
      "Iteration 9, loss = 0.27786350\n",
      "Iteration 194, loss = 0.25189142\n",
      "Iteration 10, loss = 0.27702122\n",
      "Iteration 195, loss = 0.25191180\n",
      "Iteration 11, loss = 0.27676199\n",
      "Iteration 196, loss = 0.25183052\n",
      "Iteration 197, loss = 0.25163059\n",
      "Iteration 12, loss = 0.27671769\n",
      "Iteration 198, loss = 0.25156231\n",
      "Iteration 13, loss = 0.27658069\n",
      "Iteration 199, loss = 0.25136975\n",
      "Iteration 14, loss = 0.27591275\n",
      "Iteration 200, loss = 0.25173772\n",
      "Iteration 15, loss = 0.27556161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.35743353\n",
      "Iteration 16, loss = 0.27532308\n",
      "Iteration 2, loss = 0.31923760\n",
      "Iteration 17, loss = 0.27451322\n",
      "Iteration 3, loss = 0.29693817\n",
      "Iteration 18, loss = 0.27410597\n",
      "Iteration 4, loss = 0.28680436\n",
      "Iteration 19, loss = 0.27410544\n",
      "Iteration 5, loss = 0.28199875\n",
      "Iteration 20, loss = 0.27345099\n",
      "Iteration 6, loss = 0.27960695\n",
      "Iteration 21, loss = 0.27373442\n",
      "Iteration 7, loss = 0.27781180\n",
      "Iteration 22, loss = 0.27301895\n",
      "Iteration 8, loss = 0.27690851\n",
      "Iteration 23, loss = 0.27291962\n",
      "Iteration 9, loss = 0.27621392\n",
      "Iteration 24, loss = 0.27256020\n",
      "Iteration 10, loss = 0.27517797\n",
      "Iteration 25, loss = 0.27352644\n",
      "Iteration 11, loss = 0.27512996\n",
      "Iteration 26, loss = 0.27197391\n",
      "Iteration 12, loss = 0.27516826\n",
      "Iteration 27, loss = 0.27149917\n",
      "Iteration 13, loss = 0.27481735\n",
      "Iteration 28, loss = 0.27196095\n",
      "Iteration 14, loss = 0.27434195\n",
      "Iteration 29, loss = 0.27115920\n",
      "Iteration 15, loss = 0.27383759\n",
      "Iteration 30, loss = 0.27107941\n",
      "Iteration 16, loss = 0.27344213\n",
      "Iteration 31, loss = 0.27065067\n",
      "Iteration 17, loss = 0.27301959\n",
      "Iteration 32, loss = 0.27059882\n",
      "Iteration 18, loss = 0.27240101\n",
      "Iteration 33, loss = 0.27067044\n",
      "Iteration 19, loss = 0.27243914\n",
      "Iteration 34, loss = 0.27100973\n",
      "Iteration 20, loss = 0.27183523\n",
      "Iteration 35, loss = 0.26975980\n",
      "Iteration 21, loss = 0.27155267\n",
      "Iteration 36, loss = 0.26995527\n",
      "Iteration 22, loss = 0.27111727\n",
      "Iteration 37, loss = 0.26915494\n",
      "Iteration 23, loss = 0.27087271\n",
      "Iteration 38, loss = 0.26923329\n",
      "Iteration 24, loss = 0.27084200\n",
      "Iteration 39, loss = 0.26865883\n",
      "Iteration 25, loss = 0.27150658\n",
      "Iteration 40, loss = 0.26847474\n",
      "Iteration 26, loss = 0.27007263\n",
      "Iteration 41, loss = 0.26789374\n",
      "Iteration 42, loss = 0.26794892\n",
      "Iteration 27, loss = 0.26948711\n",
      "Iteration 28, loss = 0.26986128Iteration 43, loss = 0.26803880\n",
      "\n",
      "Iteration 29, loss = 0.26935354\n",
      "Iteration 44, loss = 0.26832538\n",
      "Iteration 45, loss = 0.26736964\n",
      "Iteration 30, loss = 0.26902581\n",
      "Iteration 46, loss = 0.26662674\n",
      "Iteration 31, loss = 0.26850699\n",
      "Iteration 47, loss = 0.26678777\n",
      "Iteration 32, loss = 0.26831499\n",
      "Iteration 48, loss = 0.26642326\n",
      "Iteration 33, loss = 0.26851495\n",
      "Iteration 49, loss = 0.26606779\n",
      "Iteration 34, loss = 0.26827317\n",
      "Iteration 50, loss = 0.26522519\n",
      "Iteration 35, loss = 0.26707161\n",
      "Iteration 51, loss = 0.26537072\n",
      "Iteration 36, loss = 0.26758090\n",
      "Iteration 52, loss = 0.26512042\n",
      "Iteration 37, loss = 0.26650039\n",
      "Iteration 53, loss = 0.26450003\n",
      "Iteration 38, loss = 0.26642390\n",
      "Iteration 54, loss = 0.26447690\n",
      "Iteration 39, loss = 0.26572602\n",
      "Iteration 55, loss = 0.26416077\n",
      "Iteration 40, loss = 0.26531855\n",
      "Iteration 56, loss = 0.26407942\n",
      "Iteration 41, loss = 0.26473252\n",
      "Iteration 42, loss = 0.26451216\n",
      "Iteration 57, loss = 0.26402816\n",
      "Iteration 43, loss = 0.26470524\n",
      "Iteration 58, loss = 0.26316779\n",
      "Iteration 44, loss = 0.26390675\n",
      "Iteration 59, loss = 0.26329128\n",
      "Iteration 45, loss = 0.26302971\n",
      "Iteration 60, loss = 0.26287285\n",
      "Iteration 46, loss = 0.26282621\n",
      "Iteration 61, loss = 0.26289555\n",
      "Iteration 47, loss = 0.26233010\n",
      "Iteration 62, loss = 0.26280137\n",
      "Iteration 48, loss = 0.26211417\n",
      "Iteration 63, loss = 0.26267903\n",
      "Iteration 64, loss = 0.26200967\n",
      "Iteration 49, loss = 0.26133413\n",
      "Iteration 65, loss = 0.26235380\n",
      "Iteration 50, loss = 0.26047159\n",
      "Iteration 66, loss = 0.26254510\n",
      "Iteration 51, loss = 0.26062651\n",
      "Iteration 67, loss = 0.26137154\n",
      "Iteration 52, loss = 0.25986865Iteration 68, loss = 0.26139622\n",
      "\n",
      "Iteration 53, loss = 0.25906974\n",
      "Iteration 69, loss = 0.26144614\n",
      "Iteration 70, loss = 0.26104382\n",
      "Iteration 71, loss = 0.26098643\n",
      "Iteration 54, loss = 0.25899889\n",
      "Iteration 72, loss = 0.26084618\n",
      "Iteration 55, loss = 0.25828374\n",
      "Iteration 73, loss = 0.26046155\n",
      "Iteration 56, loss = 0.25796831\n",
      "Iteration 74, loss = 0.26056980\n",
      "Iteration 57, loss = 0.25820728\n",
      "Iteration 75, loss = 0.26026111\n",
      "Iteration 58, loss = 0.25753495\n",
      "Iteration 59, loss = 0.25695878\n",
      "Iteration 76, loss = 0.25987039\n",
      "Iteration 60, loss = 0.25652118Iteration 77, loss = 0.25993447\n",
      "\n",
      "Iteration 61, loss = 0.25628932\n",
      "Iteration 78, loss = 0.25983153\n",
      "Iteration 62, loss = 0.25597336\n",
      "Iteration 79, loss = 0.25996877\n",
      "Iteration 63, loss = 0.25659886\n",
      "Iteration 80, loss = 0.25963778\n",
      "Iteration 64, loss = 0.25557841\n",
      "Iteration 81, loss = 0.25944616\n",
      "Iteration 65, loss = 0.25546230\n",
      "Iteration 82, loss = 0.25956380\n",
      "Iteration 83, loss = 0.25950351\n",
      "Iteration 66, loss = 0.25492195\n",
      "Iteration 84, loss = 0.25922689\n",
      "Iteration 67, loss = 0.25458796\n",
      "Iteration 85, loss = 0.25927665\n",
      "Iteration 68, loss = 0.25452610\n",
      "Iteration 69, loss = 0.25422679\n",
      "Iteration 86, loss = 0.25892582\n",
      "Iteration 70, loss = 0.25395773\n",
      "Iteration 87, loss = 0.25875955\n",
      "Iteration 71, loss = 0.25429505\n",
      "Iteration 88, loss = 0.25917499\n",
      "Iteration 72, loss = 0.25345847\n",
      "Iteration 89, loss = 0.25873999\n",
      "Iteration 73, loss = 0.25311313\n",
      "Iteration 90, loss = 0.25904291\n",
      "Iteration 74, loss = 0.25319174\n",
      "Iteration 91, loss = 0.25826421\n",
      "Iteration 75, loss = 0.25329513\n",
      "Iteration 92, loss = 0.25864792\n",
      "Iteration 76, loss = 0.25258434\n",
      "Iteration 93, loss = 0.25831427\n",
      "Iteration 77, loss = 0.25286402\n",
      "Iteration 94, loss = 0.25838711\n",
      "Iteration 78, loss = 0.25245564\n",
      "Iteration 95, loss = 0.25840788\n",
      "Iteration 79, loss = 0.25234397\n",
      "Iteration 96, loss = 0.25816069\n",
      "Iteration 80, loss = 0.25212843\n",
      "Iteration 81, loss = 0.25235334\n",
      "Iteration 97, loss = 0.25800511\n",
      "Iteration 82, loss = 0.25177815\n",
      "Iteration 98, loss = 0.25770877\n",
      "Iteration 83, loss = 0.25221517\n",
      "Iteration 99, loss = 0.25783033\n",
      "Iteration 84, loss = 0.25172184Iteration 100, loss = 0.25865541\n",
      "\n",
      "Iteration 101, loss = 0.25753144Iteration 85, loss = 0.25135431\n",
      "\n",
      "Iteration 102, loss = 0.25735615\n",
      "Iteration 86, loss = 0.25139062\n",
      "Iteration 103, loss = 0.25841238\n",
      "Iteration 87, loss = 0.25122404\n",
      "Iteration 88, loss = 0.25107601\n",
      "Iteration 104, loss = 0.25774545\n",
      "Iteration 89, loss = 0.25099098\n",
      "Iteration 105, loss = 0.25748767\n",
      "Iteration 90, loss = 0.25089784\n",
      "Iteration 106, loss = 0.25719901\n",
      "Iteration 107, loss = 0.25704706\n",
      "Iteration 91, loss = 0.25077050\n",
      "Iteration 108, loss = 0.25692824\n",
      "Iteration 92, loss = 0.25101962\n",
      "Iteration 109, loss = 0.25684911\n",
      "Iteration 93, loss = 0.25066623\n",
      "Iteration 110, loss = 0.25700614\n",
      "Iteration 94, loss = 0.25047088\n",
      "Iteration 111, loss = 0.25697631\n",
      "Iteration 95, loss = 0.25039700\n",
      "Iteration 112, loss = 0.25692912\n",
      "Iteration 96, loss = 0.25027744\n",
      "Iteration 113, loss = 0.25640875\n",
      "Iteration 97, loss = 0.24997706\n",
      "Iteration 114, loss = 0.25689088\n",
      "Iteration 98, loss = 0.24998279\n",
      "Iteration 115, loss = 0.25640479\n",
      "Iteration 99, loss = 0.24982868\n",
      "Iteration 116, loss = 0.25644241\n",
      "Iteration 100, loss = 0.24982840\n",
      "Iteration 117, loss = 0.25653976Iteration 101, loss = 0.24954530\n",
      "\n",
      "Iteration 118, loss = 0.25648393\n",
      "Iteration 102, loss = 0.24984479\n",
      "Iteration 119, loss = 0.25597765\n",
      "Iteration 103, loss = 0.25029091\n",
      "Iteration 120, loss = 0.25617773\n",
      "Iteration 104, loss = 0.25026239\n",
      "Iteration 121, loss = 0.25781367\n",
      "Iteration 105, loss = 0.25007157\n",
      "Iteration 122, loss = 0.25680400\n",
      "Iteration 106, loss = 0.24965385\n",
      "Iteration 123, loss = 0.25690405\n",
      "Iteration 107, loss = 0.24923927\n",
      "Iteration 108, loss = 0.24955346\n",
      "Iteration 124, loss = 0.25619251\n",
      "Iteration 109, loss = 0.24921560\n",
      "Iteration 125, loss = 0.25605311\n",
      "Iteration 110, loss = 0.24891777\n",
      "Iteration 111, loss = 0.24910984\n",
      "Iteration 126, loss = 0.25571598\n",
      "Iteration 112, loss = 0.24894567\n",
      "Iteration 113, loss = 0.24880482\n",
      "Iteration 114, loss = 0.24870565\n",
      "Iteration 127, loss = 0.25581354\n",
      "Iteration 115, loss = 0.24840881\n",
      "Iteration 128, loss = 0.25572795\n",
      "Iteration 116, loss = 0.24885010\n",
      "Iteration 129, loss = 0.25566465\n",
      "Iteration 117, loss = 0.24859414\n",
      "Iteration 130, loss = 0.25533466\n",
      "Iteration 118, loss = 0.24858858\n",
      "Iteration 131, loss = 0.25564905\n",
      "Iteration 119, loss = 0.24818414\n",
      "Iteration 132, loss = 0.25533953\n",
      "Iteration 120, loss = 0.24820264\n",
      "Iteration 133, loss = 0.25529833\n",
      "Iteration 121, loss = 0.24836099\n",
      "Iteration 122, loss = 0.24818370\n",
      "Iteration 134, loss = 0.25530645\n",
      "Iteration 123, loss = 0.24927880\n",
      "Iteration 135, loss = 0.25523044\n",
      "Iteration 124, loss = 0.24822040\n",
      "Iteration 125, loss = 0.24794251\n",
      "Iteration 136, loss = 0.25515394\n",
      "Iteration 126, loss = 0.24790919\n",
      "Iteration 137, loss = 0.25526059\n",
      "Iteration 127, loss = 0.24772020\n",
      "Iteration 138, loss = 0.25526372\n",
      "Iteration 128, loss = 0.24748919\n",
      "Iteration 139, loss = 0.25483476\n",
      "Iteration 129, loss = 0.24814767\n",
      "Iteration 140, loss = 0.25496774\n",
      "Iteration 130, loss = 0.24810026\n",
      "Iteration 141, loss = 0.25496582\n",
      "Iteration 131, loss = 0.24770123\n",
      "Iteration 142, loss = 0.25437706\n",
      "Iteration 132, loss = 0.24739170\n",
      "Iteration 143, loss = 0.25451133\n",
      "Iteration 133, loss = 0.24752951\n",
      "Iteration 144, loss = 0.25441247\n",
      "Iteration 134, loss = 0.24775618\n",
      "Iteration 145, loss = 0.25426575\n",
      "Iteration 135, loss = 0.24748854\n",
      "Iteration 146, loss = 0.25420357\n",
      "Iteration 136, loss = 0.24735492\n",
      "Iteration 147, loss = 0.25526755\n",
      "Iteration 137, loss = 0.24783269\n",
      "Iteration 148, loss = 0.25436961\n",
      "Iteration 138, loss = 0.24748026\n",
      "Iteration 149, loss = 0.25450573\n",
      "Iteration 139, loss = 0.24722521\n",
      "Iteration 150, loss = 0.25419257\n",
      "Iteration 140, loss = 0.24727212\n",
      "Iteration 141, loss = 0.24709534\n",
      "Iteration 151, loss = 0.25422820\n",
      "Iteration 142, loss = 0.24684033\n",
      "Iteration 152, loss = 0.25446103\n",
      "Iteration 143, loss = 0.24716456\n",
      "Iteration 153, loss = 0.25490669\n",
      "Iteration 144, loss = 0.24697896\n",
      "Iteration 154, loss = 0.25404043\n",
      "Iteration 145, loss = 0.24712435\n",
      "Iteration 155, loss = 0.25387866\n",
      "Iteration 146, loss = 0.24683151\n",
      "Iteration 156, loss = 0.25400825\n",
      "Iteration 147, loss = 0.24701648\n",
      "Iteration 157, loss = 0.25390059\n",
      "Iteration 148, loss = 0.24671786\n",
      "Iteration 158, loss = 0.25408843\n",
      "Iteration 149, loss = 0.24658954\n",
      "Iteration 159, loss = 0.25373074\n",
      "Iteration 150, loss = 0.24683792\n",
      "Iteration 160, loss = 0.25429447\n",
      "Iteration 151, loss = 0.24694608\n",
      "Iteration 161, loss = 0.25375284\n",
      "Iteration 152, loss = 0.24717608\n",
      "Iteration 162, loss = 0.25344443\n",
      "Iteration 153, loss = 0.24678003\n",
      "Iteration 163, loss = 0.25346707\n",
      "Iteration 154, loss = 0.24661630\n",
      "Iteration 164, loss = 0.25361966\n",
      "Iteration 155, loss = 0.24662443Iteration 165, loss = 0.25392963\n",
      "\n",
      "Iteration 166, loss = 0.25363741\n",
      "Iteration 156, loss = 0.24647743\n",
      "Iteration 157, loss = 0.24681847\n",
      "Iteration 167, loss = 0.25432653\n",
      "Iteration 158, loss = 0.24634052\n",
      "Iteration 168, loss = 0.25335664\n",
      "Iteration 159, loss = 0.24653414\n",
      "Iteration 169, loss = 0.25351371\n",
      "Iteration 170, loss = 0.25335215\n",
      "Iteration 160, loss = 0.24652952\n",
      "Iteration 171, loss = 0.25312711\n",
      "Iteration 161, loss = 0.24645626\n",
      "Iteration 162, loss = 0.24618641Iteration 172, loss = 0.25335561\n",
      "\n",
      "Iteration 173, loss = 0.25370170\n",
      "Iteration 163, loss = 0.24598848\n",
      "Iteration 174, loss = 0.25350241\n",
      "Iteration 175, loss = 0.25317707\n",
      "Iteration 164, loss = 0.24610564\n",
      "Iteration 176, loss = 0.25304190\n",
      "Iteration 165, loss = 0.24670736\n",
      "Iteration 166, loss = 0.24658509\n",
      "Iteration 177, loss = 0.25307617\n",
      "Iteration 167, loss = 0.24618957\n",
      "Iteration 178, loss = 0.25312290\n",
      "Iteration 168, loss = 0.24616635\n",
      "Iteration 179, loss = 0.25285094\n",
      "Iteration 169, loss = 0.24623803\n",
      "Iteration 180, loss = 0.25286363\n",
      "Iteration 170, loss = 0.24593772\n",
      "Iteration 181, loss = 0.25324562\n",
      "Iteration 171, loss = 0.24580138\n",
      "Iteration 182, loss = 0.25369238\n",
      "Iteration 172, loss = 0.24633360\n",
      "Iteration 183, loss = 0.25311247\n",
      "Iteration 173, loss = 0.24601825\n",
      "Iteration 184, loss = 0.25261981\n",
      "Iteration 174, loss = 0.24619502\n",
      "Iteration 185, loss = 0.25289764\n",
      "Iteration 175, loss = 0.24583786\n",
      "Iteration 186, loss = 0.25293106\n",
      "Iteration 176, loss = 0.24563339\n",
      "Iteration 187, loss = 0.25242335\n",
      "Iteration 177, loss = 0.24581515\n",
      "Iteration 188, loss = 0.25267761\n",
      "Iteration 178, loss = 0.24552209\n",
      "Iteration 189, loss = 0.25241728\n",
      "Iteration 179, loss = 0.24563457\n",
      "Iteration 190, loss = 0.25279715\n",
      "Iteration 180, loss = 0.24552106\n",
      "Iteration 191, loss = 0.25303267\n",
      "Iteration 181, loss = 0.24559507\n",
      "Iteration 192, loss = 0.25246039\n",
      "Iteration 182, loss = 0.24613381\n",
      "Iteration 193, loss = 0.25228021\n",
      "Iteration 183, loss = 0.24555120\n",
      "Iteration 194, loss = 0.25250473\n",
      "Iteration 184, loss = 0.24520938\n",
      "Iteration 195, loss = 0.25253338\n",
      "Iteration 185, loss = 0.24522663\n",
      "Iteration 196, loss = 0.25245238\n",
      "Iteration 186, loss = 0.24551107\n",
      "Iteration 197, loss = 0.25223225\n",
      "Iteration 187, loss = 0.24507275\n",
      "Iteration 198, loss = 0.25211495\n",
      "Iteration 188, loss = 0.24526977\n",
      "Iteration 199, loss = 0.25207756\n",
      "Iteration 189, loss = 0.24515216\n",
      "Iteration 200, loss = 0.25241339\n",
      "Iteration 190, loss = 0.24496039\n",
      "Iteration 191, loss = 0.24514622\n",
      "Iteration 192, loss = 0.24498820\n",
      "Iteration 1, loss = 0.35554294\n",
      "Iteration 193, loss = 0.24495845\n",
      "Iteration 2, loss = 0.31761215\n",
      "Iteration 194, loss = 0.24504108\n",
      "Iteration 3, loss = 0.29544168\n",
      "Iteration 195, loss = 0.24511381\n",
      "Iteration 4, loss = 0.28623905\n",
      "Iteration 5, loss = 0.28199955Iteration 196, loss = 0.24482142\n",
      "\n",
      "Iteration 6, loss = 0.27971378\n",
      "Iteration 197, loss = 0.24489583\n",
      "Iteration 7, loss = 0.27774985\n",
      "Iteration 198, loss = 0.24487629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 199, loss = 0.24495849\n",
      "Iteration 8, loss = 0.27704702\n",
      "Iteration 200, loss = 0.24506547\n",
      "Iteration 9, loss = 0.27635824\n",
      "Iteration 10, loss = 0.27531580\n",
      "Iteration 11, loss = 0.27531816\n",
      "Iteration 1, loss = 0.35588916\n",
      "Iteration 12, loss = 0.27509335\n",
      "Iteration 2, loss = 0.31937639\n",
      "Iteration 13, loss = 0.27474426\n",
      "Iteration 3, loss = 0.29825787\n",
      "Iteration 14, loss = 0.27439864\n",
      "Iteration 4, loss = 0.28906388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, loss = 0.27404050\n",
      "Iteration 5, loss = 0.28562338\n",
      "Iteration 6, loss = 0.28309061\n",
      "Iteration 16, loss = 0.27370042\n",
      "Iteration 7, loss = 0.28072137\n",
      "Iteration 17, loss = 0.27339169\n",
      "Iteration 18, loss = 0.27251277\n",
      "Iteration 19, loss = 0.27262537\n",
      "Iteration 8, loss = 0.28012347\n",
      "Iteration 20, loss = 0.27233121\n",
      "Iteration 9, loss = 0.27969713\n",
      "Iteration 21, loss = 0.27232686\n",
      "Iteration 22, loss = 0.27201928\n",
      "Iteration 10, loss = 0.27859086\n",
      "Iteration 23, loss = 0.27152866\n",
      "Iteration 24, loss = 0.27144581\n",
      "Iteration 11, loss = 0.27882340\n",
      "Iteration 25, loss = 0.27135306\n",
      "Iteration 12, loss = 0.27872318\n",
      "Iteration 26, loss = 0.27090121\n",
      "Iteration 13, loss = 0.27790242\n",
      "Iteration 27, loss = 0.27048038\n",
      "Iteration 14, loss = 0.27794852\n",
      "Iteration 28, loss = 0.27048104\n",
      "Iteration 15, loss = 0.27765418\n",
      "Iteration 29, loss = 0.27048255\n",
      "Iteration 16, loss = 0.27708449\n",
      "Iteration 30, loss = 0.27037212\n",
      "Iteration 17, loss = 0.27685441\n",
      "Iteration 31, loss = 0.26955569\n",
      "Iteration 18, loss = 0.27604519\n",
      "Iteration 32, loss = 0.26928325\n",
      "Iteration 19, loss = 0.27603123\n",
      "Iteration 33, loss = 0.26947487\n",
      "Iteration 20, loss = 0.27568499\n",
      "Iteration 34, loss = 0.26896357\n",
      "Iteration 21, loss = 0.27579929\n",
      "Iteration 35, loss = 0.26811079\n",
      "Iteration 22, loss = 0.27563508\n",
      "Iteration 36, loss = 0.26830396\n",
      "Iteration 23, loss = 0.27487508\n",
      "Iteration 37, loss = 0.26767752\n",
      "Iteration 24, loss = 0.27487699\n",
      "Iteration 38, loss = 0.26761880\n",
      "Iteration 25, loss = 0.27509257\n",
      "Iteration 39, loss = 0.26706351\n",
      "Iteration 26, loss = 0.27418767\n",
      "Iteration 40, loss = 0.26677188\n",
      "Iteration 27, loss = 0.27402563\n",
      "Iteration 41, loss = 0.26644908\n",
      "Iteration 28, loss = 0.27411308\n",
      "Iteration 42, loss = 0.26637162\n",
      "Iteration 29, loss = 0.27390890\n",
      "Iteration 43, loss = 0.26624883\n",
      "Iteration 30, loss = 0.27350902\n",
      "Iteration 44, loss = 0.26583660\n",
      "Iteration 31, loss = 0.27328244\n",
      "Iteration 45, loss = 0.26553191\n",
      "Iteration 32, loss = 0.27270383\n",
      "Iteration 46, loss = 0.26609156\n",
      "Iteration 33, loss = 0.27339583\n",
      "Iteration 47, loss = 0.26527363\n",
      "Iteration 34, loss = 0.27271972\n",
      "Iteration 48, loss = 0.26507990\n",
      "Iteration 35, loss = 0.27221880\n",
      "Iteration 49, loss = 0.26470716\n",
      "Iteration 36, loss = 0.27252603\n",
      "Iteration 50, loss = 0.26409623\n",
      "Iteration 37, loss = 0.27151104\n",
      "Iteration 51, loss = 0.26434147\n",
      "Iteration 38, loss = 0.27144485\n",
      "Iteration 52, loss = 0.26388159\n",
      "Iteration 39, loss = 0.27092308\n",
      "Iteration 53, loss = 0.26346072\n",
      "Iteration 40, loss = 0.27098741\n",
      "Iteration 54, loss = 0.26338586\n",
      "Iteration 41, loss = 0.27067416\n",
      "Iteration 55, loss = 0.26275445\n",
      "Iteration 42, loss = 0.27000135\n",
      "Iteration 56, loss = 0.26282541\n",
      "Iteration 43, loss = 0.27008559\n",
      "Iteration 57, loss = 0.26285325\n",
      "Iteration 58, loss = 0.26233650\n",
      "Iteration 44, loss = 0.26960636\n",
      "Iteration 45, loss = 0.26934142\n",
      "Iteration 59, loss = 0.26223522\n",
      "Iteration 46, loss = 0.26951373\n",
      "Iteration 60, loss = 0.26202560\n",
      "Iteration 47, loss = 0.26891718\n",
      "Iteration 61, loss = 0.26201583\n",
      "Iteration 48, loss = 0.26911829\n",
      "Iteration 62, loss = 0.26156723\n",
      "Iteration 63, loss = 0.26195556\n",
      "Iteration 49, loss = 0.26803651\n",
      "Iteration 64, loss = 0.26100645\n",
      "Iteration 65, loss = 0.26134631\n",
      "Iteration 50, loss = 0.26747868\n",
      "Iteration 66, loss = 0.26089195\n",
      "Iteration 51, loss = 0.26781679\n",
      "Iteration 67, loss = 0.26045405\n",
      "Iteration 52, loss = 0.26717615\n",
      "Iteration 68, loss = 0.25992375\n",
      "Iteration 53, loss = 0.26672269\n",
      "Iteration 69, loss = 0.26003929\n",
      "Iteration 54, loss = 0.26649551\n",
      "Iteration 70, loss = 0.25961740\n",
      "Iteration 55, loss = 0.26584992\n",
      "Iteration 71, loss = 0.26002292\n",
      "Iteration 56, loss = 0.26633729\n",
      "Iteration 72, loss = 0.25963746\n",
      "Iteration 57, loss = 0.26577510\n",
      "Iteration 73, loss = 0.25938255\n",
      "Iteration 58, loss = 0.26549028\n",
      "Iteration 74, loss = 0.25896693\n",
      "Iteration 75, loss = 0.25897614\n",
      "Iteration 59, loss = 0.26556995\n",
      "Iteration 76, loss = 0.25831967\n",
      "Iteration 77, loss = 0.25833524\n",
      "Iteration 60, loss = 0.26480115Iteration 78, loss = 0.25822963\n",
      "\n",
      "Iteration 79, loss = 0.25814744\n",
      "Iteration 61, loss = 0.26446032\n",
      "Iteration 80, loss = 0.25768286\n",
      "Iteration 62, loss = 0.26426065\n",
      "Iteration 81, loss = 0.25779394\n",
      "Iteration 63, loss = 0.26440838\n",
      "Iteration 64, loss = 0.26390868\n",
      "Iteration 82, loss = 0.25765274\n",
      "Iteration 65, loss = 0.26414960\n",
      "Iteration 83, loss = 0.25784673\n",
      "Iteration 66, loss = 0.26386760\n",
      "Iteration 84, loss = 0.25718199\n",
      "Iteration 67, loss = 0.26336684\n",
      "Iteration 85, loss = 0.25694959\n",
      "Iteration 68, loss = 0.26277379\n",
      "Iteration 69, loss = 0.26224053\n",
      "Iteration 86, loss = 0.25690693\n",
      "Iteration 70, loss = 0.26196584\n",
      "Iteration 87, loss = 0.25625762\n",
      "Iteration 88, loss = 0.25613218\n",
      "Iteration 71, loss = 0.26259321\n",
      "Iteration 89, loss = 0.25612508\n",
      "Iteration 72, loss = 0.26200914\n",
      "Iteration 90, loss = 0.25595173\n",
      "Iteration 73, loss = 0.26181836\n",
      "Iteration 74, loss = 0.26133784\n",
      "Iteration 91, loss = 0.25573936\n",
      "Iteration 75, loss = 0.26152036\n",
      "Iteration 92, loss = 0.25577467\n",
      "Iteration 76, loss = 0.26096169\n",
      "Iteration 93, loss = 0.25551384\n",
      "Iteration 77, loss = 0.26091159\n",
      "Iteration 94, loss = 0.25570751\n",
      "Iteration 78, loss = 0.26089788\n",
      "Iteration 95, loss = 0.25526949\n",
      "Iteration 79, loss = 0.26077222\n",
      "Iteration 96, loss = 0.25497365\n",
      "Iteration 80, loss = 0.26032340\n",
      "Iteration 97, loss = 0.25492487\n",
      "Iteration 81, loss = 0.26063148\n",
      "Iteration 98, loss = 0.25487378\n",
      "Iteration 82, loss = 0.26064118\n",
      "Iteration 99, loss = 0.25447929\n",
      "Iteration 83, loss = 0.26069852\n",
      "Iteration 100, loss = 0.25471640\n",
      "Iteration 84, loss = 0.26000664\n",
      "Iteration 101, loss = 0.25434937\n",
      "Iteration 85, loss = 0.25993948\n",
      "Iteration 102, loss = 0.25461189\n",
      "Iteration 86, loss = 0.25995795\n",
      "Iteration 103, loss = 0.25451331\n",
      "Iteration 87, loss = 0.25933772\n",
      "Iteration 104, loss = 0.25505134\n",
      "Iteration 105, loss = 0.25455884\n",
      "Iteration 88, loss = 0.25875715\n",
      "Iteration 89, loss = 0.25906586\n",
      "Iteration 106, loss = 0.25425171\n",
      "Iteration 107, loss = 0.25390608\n",
      "Iteration 90, loss = 0.25967908\n",
      "Iteration 108, loss = 0.25390501\n",
      "Iteration 91, loss = 0.25883786\n",
      "Iteration 92, loss = 0.25900874\n",
      "Iteration 109, loss = 0.25360703\n",
      "Iteration 93, loss = 0.25878309\n",
      "Iteration 110, loss = 0.25337774\n",
      "Iteration 94, loss = 0.25838538\n",
      "Iteration 111, loss = 0.25424641\n",
      "Iteration 112, loss = 0.25350278\n",
      "Iteration 95, loss = 0.25870591\n",
      "Iteration 96, loss = 0.25832256\n",
      "Iteration 113, loss = 0.25309390\n",
      "Iteration 97, loss = 0.25834561\n",
      "Iteration 114, loss = 0.25316904\n",
      "Iteration 98, loss = 0.25828312\n",
      "Iteration 115, loss = 0.25305059\n",
      "Iteration 99, loss = 0.25814013\n",
      "Iteration 116, loss = 0.25300650\n",
      "Iteration 100, loss = 0.25808834\n",
      "Iteration 117, loss = 0.25279175\n",
      "Iteration 101, loss = 0.25789864\n",
      "Iteration 118, loss = 0.25279934\n",
      "Iteration 102, loss = 0.25798837\n",
      "Iteration 119, loss = 0.25285787\n",
      "Iteration 103, loss = 0.25770692\n",
      "Iteration 120, loss = 0.25286310\n",
      "Iteration 104, loss = 0.25858002\n",
      "Iteration 121, loss = 0.25261342\n",
      "Iteration 105, loss = 0.25815160\n",
      "Iteration 122, loss = 0.25358105\n",
      "Iteration 106, loss = 0.25764610\n",
      "Iteration 123, loss = 0.25382259\n",
      "Iteration 107, loss = 0.25767077\n",
      "Iteration 124, loss = 0.25289333\n",
      "Iteration 108, loss = 0.25736826\n",
      "Iteration 125, loss = 0.25226266\n",
      "Iteration 109, loss = 0.25751272\n",
      "Iteration 126, loss = 0.25260445\n",
      "Iteration 110, loss = 0.25685774\n",
      "Iteration 127, loss = 0.25207778\n",
      "Iteration 111, loss = 0.25794232\n",
      "Iteration 128, loss = 0.25192016\n",
      "Iteration 112, loss = 0.25703598\n",
      "Iteration 129, loss = 0.25213727\n",
      "Iteration 113, loss = 0.25687230\n",
      "Iteration 130, loss = 0.25248522\n",
      "Iteration 114, loss = 0.25656059\n",
      "Iteration 131, loss = 0.25219504\n",
      "Iteration 115, loss = 0.25686450\n",
      "Iteration 132, loss = 0.25207639\n",
      "Iteration 116, loss = 0.25658202\n",
      "Iteration 133, loss = 0.25209274\n",
      "Iteration 117, loss = 0.25647819\n",
      "Iteration 134, loss = 0.25181037\n",
      "Iteration 118, loss = 0.25664060\n",
      "Iteration 135, loss = 0.25184219\n",
      "Iteration 119, loss = 0.25635746\n",
      "Iteration 136, loss = 0.25190546\n",
      "Iteration 120, loss = 0.25664698\n",
      "Iteration 137, loss = 0.25139127\n",
      "Iteration 121, loss = 0.25667992\n",
      "Iteration 138, loss = 0.25209967\n",
      "Iteration 122, loss = 0.25741808\n",
      "Iteration 139, loss = 0.25145013\n",
      "Iteration 123, loss = 0.25732023\n",
      "Iteration 140, loss = 0.25136947\n",
      "Iteration 124, loss = 0.25612971\n",
      "Iteration 125, loss = 0.25625217\n",
      "Iteration 141, loss = 0.25130687\n",
      "Iteration 126, loss = 0.25638755\n",
      "Iteration 142, loss = 0.25122354\n",
      "Iteration 127, loss = 0.25589283\n",
      "Iteration 143, loss = 0.25147717\n",
      "Iteration 128, loss = 0.25577045\n",
      "Iteration 144, loss = 0.25118230\n",
      "Iteration 129, loss = 0.25602565\n",
      "Iteration 145, loss = 0.25134770\n",
      "Iteration 130, loss = 0.25605971\n",
      "Iteration 146, loss = 0.25086002\n",
      "Iteration 131, loss = 0.25586233\n",
      "Iteration 147, loss = 0.25185011\n",
      "Iteration 132, loss = 0.25580602\n",
      "Iteration 148, loss = 0.25101079\n",
      "Iteration 149, loss = 0.25087732\n",
      "Iteration 133, loss = 0.25566121\n",
      "Iteration 150, loss = 0.25115438\n",
      "Iteration 134, loss = 0.25553870\n",
      "Iteration 151, loss = 0.25070634\n",
      "Iteration 135, loss = 0.25537611\n",
      "Iteration 152, loss = 0.25127241\n",
      "Iteration 136, loss = 0.25527917\n",
      "Iteration 153, loss = 0.25119564\n",
      "Iteration 137, loss = 0.25532811\n",
      "Iteration 154, loss = 0.25066879\n",
      "Iteration 138, loss = 0.25539978\n",
      "Iteration 155, loss = 0.25079488\n",
      "Iteration 156, loss = 0.25102092\n",
      "Iteration 139, loss = 0.25582498\n",
      "Iteration 157, loss = 0.25047664\n",
      "Iteration 140, loss = 0.25489793\n",
      "Iteration 158, loss = 0.25076394\n",
      "Iteration 141, loss = 0.25550096\n",
      "Iteration 159, loss = 0.25076007\n",
      "Iteration 142, loss = 0.25489506\n",
      "Iteration 160, loss = 0.25128940\n",
      "Iteration 143, loss = 0.25517198\n",
      "Iteration 161, loss = 0.25060253\n",
      "Iteration 144, loss = 0.25469251\n",
      "Iteration 162, loss = 0.25061872\n",
      "Iteration 145, loss = 0.25476581\n",
      "Iteration 163, loss = 0.25022599\n",
      "Iteration 146, loss = 0.25440271\n",
      "Iteration 164, loss = 0.25029073\n",
      "Iteration 147, loss = 0.25556148\n",
      "Iteration 165, loss = 0.25086534\n",
      "Iteration 148, loss = 0.25480646\n",
      "Iteration 166, loss = 0.25077106\n",
      "Iteration 149, loss = 0.25463047\n",
      "Iteration 167, loss = 0.25038243\n",
      "Iteration 150, loss = 0.25463364\n",
      "Iteration 168, loss = 0.25058328\n",
      "Iteration 151, loss = 0.25458021\n",
      "Iteration 169, loss = 0.25118137\n",
      "Iteration 152, loss = 0.25450457\n",
      "Iteration 170, loss = 0.25061526\n",
      "Iteration 153, loss = 0.25458059\n",
      "Iteration 171, loss = 0.25016161\n",
      "Iteration 154, loss = 0.25466063\n",
      "Iteration 172, loss = 0.25062075\n",
      "Iteration 155, loss = 0.25439509\n",
      "Iteration 173, loss = 0.25123893\n",
      "Iteration 156, loss = 0.25503705\n",
      "Iteration 174, loss = 0.25026806\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 157, loss = 0.25420785\n",
      "Iteration 158, loss = 0.25422983\n",
      "Iteration 159, loss = 0.25431657\n",
      "Iteration 1, loss = 0.35678826\n",
      "Iteration 160, loss = 0.25431677\n",
      "Iteration 2, loss = 0.32110778\n",
      "Iteration 161, loss = 0.25393563\n",
      "Iteration 3, loss = 0.30028476\n",
      "Iteration 162, loss = 0.25385043\n",
      "Iteration 4, loss = 0.29087432\n",
      "Iteration 163, loss = 0.25392587\n",
      "Iteration 5, loss = 0.28716011\n",
      "Iteration 164, loss = 0.25357872\n",
      "Iteration 6, loss = 0.28403341\n",
      "Iteration 165, loss = 0.25373052\n",
      "Iteration 7, loss = 0.28177149\n",
      "Iteration 166, loss = 0.25364658\n",
      "Iteration 8, loss = 0.28137633\n",
      "Iteration 167, loss = 0.25363367\n",
      "Iteration 9, loss = 0.28047644\n",
      "Iteration 168, loss = 0.25391369\n",
      "Iteration 169, loss = 0.25421260\n",
      "Iteration 10, loss = 0.27961335\n",
      "Iteration 170, loss = 0.25347034\n",
      "Iteration 11, loss = 0.27963167\n",
      "Iteration 171, loss = 0.25357057\n",
      "Iteration 12, loss = 0.27953968\n",
      "Iteration 172, loss = 0.25428442\n",
      "Iteration 13, loss = 0.27869700\n",
      "Iteration 173, loss = 0.25386558\n",
      "Iteration 14, loss = 0.27839426\n",
      "Iteration 15, loss = 0.27814341\n",
      "Iteration 174, loss = 0.25379659\n",
      "Iteration 16, loss = 0.27753592\n",
      "Iteration 175, loss = 0.25307074\n",
      "Iteration 17, loss = 0.27749562\n",
      "Iteration 176, loss = 0.25315392\n",
      "Iteration 18, loss = 0.27675973\n",
      "Iteration 177, loss = 0.25318282\n",
      "Iteration 19, loss = 0.27674422\n",
      "Iteration 178, loss = 0.25303079\n",
      "Iteration 20, loss = 0.27652751\n",
      "Iteration 179, loss = 0.25297982\n",
      "Iteration 21, loss = 0.27632429\n",
      "Iteration 180, loss = 0.25307455\n",
      "Iteration 22, loss = 0.27610214\n",
      "Iteration 181, loss = 0.25339378\n",
      "Iteration 23, loss = 0.27529161\n",
      "Iteration 182, loss = 0.25366813\n",
      "Iteration 183, loss = 0.25245715\n",
      "Iteration 24, loss = 0.27538914\n",
      "Iteration 184, loss = 0.25255162\n",
      "Iteration 25, loss = 0.27531741\n",
      "Iteration 185, loss = 0.25258190\n",
      "Iteration 26, loss = 0.27476962\n",
      "Iteration 186, loss = 0.25323762\n",
      "Iteration 27, loss = 0.27433192\n",
      "Iteration 187, loss = 0.25255113\n",
      "Iteration 28, loss = 0.27414978\n",
      "Iteration 188, loss = 0.25240422\n",
      "Iteration 29, loss = 0.27374990\n",
      "Iteration 189, loss = 0.25227353\n",
      "Iteration 30, loss = 0.27375575\n",
      "Iteration 190, loss = 0.25254180\n",
      "Iteration 31, loss = 0.27343604\n",
      "Iteration 191, loss = 0.25233212\n",
      "Iteration 32, loss = 0.27281875\n",
      "Iteration 192, loss = 0.25241304\n",
      "Iteration 33, loss = 0.27300568\n",
      "Iteration 193, loss = 0.25223540\n",
      "Iteration 34, loss = 0.27239130\n",
      "Iteration 194, loss = 0.25254599\n",
      "Iteration 35, loss = 0.27187775\n",
      "Iteration 195, loss = 0.25260985\n",
      "Iteration 36, loss = 0.27217104\n",
      "Iteration 196, loss = 0.25269795\n",
      "Iteration 197, loss = 0.25190031\n",
      "Iteration 37, loss = 0.27126114\n",
      "Iteration 198, loss = 0.25180822\n",
      "Iteration 38, loss = 0.27067961\n",
      "Iteration 199, loss = 0.25190639\n",
      "Iteration 39, loss = 0.27040992\n",
      "Iteration 200, loss = 0.25224926\n",
      "Iteration 40, loss = 0.27026444\n",
      "Iteration 41, loss = 0.27002917\n",
      "Iteration 42, loss = 0.26923539\n",
      "Iteration 1, loss = 0.35614925\n",
      "Iteration 43, loss = 0.26943283\n",
      "Iteration 2, loss = 0.31828930\n",
      "Iteration 44, loss = 0.26874249\n",
      "Iteration 3, loss = 0.29655188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 45, loss = 0.26849064\n",
      "Iteration 4, loss = 0.28716068\n",
      "Iteration 5, loss = 0.28319989\n",
      "Iteration 46, loss = 0.26904823\n",
      "Iteration 6, loss = 0.28036408\n",
      "Iteration 47, loss = 0.26865709\n",
      "Iteration 7, loss = 0.27854228\n",
      "Iteration 48, loss = 0.26902853\n",
      "Iteration 8, loss = 0.27783825\n",
      "Iteration 49, loss = 0.26774048\n",
      "Iteration 9, loss = 0.27721145\n",
      "Iteration 50, loss = 0.26716021\n",
      "Iteration 10, loss = 0.27644718\n",
      "Iteration 51, loss = 0.26747709\n",
      "Iteration 11, loss = 0.27637766\n",
      "Iteration 52, loss = 0.26674931\n",
      "Iteration 12, loss = 0.27653208\n",
      "Iteration 53, loss = 0.26631920\n",
      "Iteration 13, loss = 0.27618066\n",
      "Iteration 54, loss = 0.26634548\n",
      "Iteration 14, loss = 0.27590276\n",
      "Iteration 55, loss = 0.26581331\n",
      "Iteration 15, loss = 0.27541127\n",
      "Iteration 56, loss = 0.26585492\n",
      "Iteration 16, loss = 0.27514144\n",
      "Iteration 57, loss = 0.26545844\n",
      "Iteration 17, loss = 0.27484592\n",
      "Iteration 58, loss = 0.26535860\n",
      "Iteration 18, loss = 0.27437894\n",
      "Iteration 59, loss = 0.26576145\n",
      "Iteration 19, loss = 0.27431347\n",
      "Iteration 60, loss = 0.26485456\n",
      "Iteration 20, loss = 0.27387914\n",
      "Iteration 61, loss = 0.26469959\n",
      "Iteration 62, loss = 0.26446563\n",
      "Iteration 21, loss = 0.27365309\n",
      "Iteration 63, loss = 0.26424066\n",
      "Iteration 64, loss = 0.26395689\n",
      "Iteration 22, loss = 0.27368434\n",
      "Iteration 65, loss = 0.26412157\n",
      "Iteration 23, loss = 0.27275309\n",
      "Iteration 66, loss = 0.26385128\n",
      "Iteration 24, loss = 0.27263521\n",
      "Iteration 67, loss = 0.26354706\n",
      "Iteration 25, loss = 0.27229710\n",
      "Iteration 68, loss = 0.26321869\n",
      "Iteration 26, loss = 0.27209718\n",
      "Iteration 69, loss = 0.26257758\n",
      "Iteration 27, loss = 0.27189464\n",
      "Iteration 70, loss = 0.26234271\n",
      "Iteration 28, loss = 0.27126047\n",
      "Iteration 71, loss = 0.26267281\n",
      "Iteration 29, loss = 0.27122244\n",
      "Iteration 72, loss = 0.26208973\n",
      "Iteration 30, loss = 0.27122093\n",
      "Iteration 31, loss = 0.27111872\n",
      "Iteration 73, loss = 0.26205397\n",
      "Iteration 32, loss = 0.27019180\n",
      "Iteration 74, loss = 0.26154121\n",
      "Iteration 75, loss = 0.26203867\n",
      "Iteration 33, loss = 0.26999301\n",
      "Iteration 34, loss = 0.26999567\n",
      "Iteration 76, loss = 0.26094644\n",
      "Iteration 35, loss = 0.26950710\n",
      "Iteration 77, loss = 0.26084704\n",
      "Iteration 36, loss = 0.27015695\n",
      "Iteration 78, loss = 0.26057365\n",
      "Iteration 37, loss = 0.26897074\n",
      "Iteration 38, loss = 0.26854937\n",
      "Iteration 39, loss = 0.26840692\n",
      "Iteration 79, loss = 0.26081360\n",
      "Iteration 40, loss = 0.26780266\n",
      "Iteration 41, loss = 0.26754485\n",
      "Iteration 80, loss = 0.26053367\n",
      "Iteration 81, loss = 0.26102204\n",
      "Iteration 82, loss = 0.26002561\n",
      "Iteration 42, loss = 0.26707097\n",
      "Iteration 83, loss = 0.26033371\n",
      "Iteration 43, loss = 0.26722685\n",
      "Iteration 84, loss = 0.25974124\n",
      "Iteration 44, loss = 0.26650641\n",
      "Iteration 85, loss = 0.25981856\n",
      "Iteration 45, loss = 0.26613133\n",
      "Iteration 46, loss = 0.26620768\n",
      "Iteration 86, loss = 0.25999858\n",
      "Iteration 47, loss = 0.26611634\n",
      "Iteration 87, loss = 0.25918052\n",
      "Iteration 88, loss = 0.25875049\n",
      "Iteration 48, loss = 0.26706453\n",
      "Iteration 49, loss = 0.26513822Iteration 89, loss = 0.25889260\n",
      "\n",
      "Iteration 90, loss = 0.25907242\n",
      "Iteration 50, loss = 0.26464828\n",
      "Iteration 91, loss = 0.25869394\n",
      "Iteration 92, loss = 0.25852588\n",
      "Iteration 51, loss = 0.26502861\n",
      "Iteration 93, loss = 0.25827208\n",
      "Iteration 52, loss = 0.26429950\n",
      "Iteration 53, loss = 0.26362459\n",
      "Iteration 94, loss = 0.25823900\n",
      "Iteration 54, loss = 0.26345918\n",
      "Iteration 55, loss = 0.26311074\n",
      "Iteration 95, loss = 0.25810558\n",
      "Iteration 56, loss = 0.26327158\n",
      "Iteration 96, loss = 0.25806609\n",
      "Iteration 57, loss = 0.26277587\n",
      "Iteration 97, loss = 0.25808362\n",
      "Iteration 58, loss = 0.26238226\n",
      "Iteration 98, loss = 0.25823544\n",
      "Iteration 99, loss = 0.25757290\n",
      "Iteration 59, loss = 0.26292765\n",
      "Iteration 60, loss = 0.26199231\n",
      "Iteration 100, loss = 0.25749672\n",
      "Iteration 61, loss = 0.26189538\n",
      "Iteration 62, loss = 0.26160936\n",
      "Iteration 101, loss = 0.25711482\n",
      "Iteration 63, loss = 0.26107051\n",
      "Iteration 102, loss = 0.25738602\n",
      "Iteration 64, loss = 0.26080528\n",
      "Iteration 103, loss = 0.25739948\n",
      "Iteration 65, loss = 0.26138958\n",
      "Iteration 104, loss = 0.25758727\n",
      "Iteration 66, loss = 0.26076307\n",
      "Iteration 105, loss = 0.25774278\n",
      "Iteration 67, loss = 0.26048950\n",
      "Iteration 106, loss = 0.25709280\n",
      "Iteration 107, loss = 0.25682786\n",
      "Iteration 68, loss = 0.26024620\n",
      "Iteration 69, loss = 0.25980255\n",
      "Iteration 108, loss = 0.25673643\n",
      "Iteration 70, loss = 0.25970223\n",
      "Iteration 109, loss = 0.25659155\n",
      "Iteration 71, loss = 0.25999492\n",
      "Iteration 110, loss = 0.25630764\n",
      "Iteration 72, loss = 0.25964110\n",
      "Iteration 111, loss = 0.25684047\n",
      "Iteration 112, loss = 0.25609572\n",
      "Iteration 73, loss = 0.25933224\n",
      "Iteration 113, loss = 0.25608170\n",
      "Iteration 74, loss = 0.25916080\n",
      "Iteration 114, loss = 0.25610872\n",
      "Iteration 75, loss = 0.25873990\n",
      "Iteration 115, loss = 0.25602082\n",
      "Iteration 76, loss = 0.25868999\n",
      "Iteration 116, loss = 0.25558590\n",
      "Iteration 77, loss = 0.25830300\n",
      "Iteration 78, loss = 0.25822508\n",
      "Iteration 117, loss = 0.25551359\n",
      "Iteration 79, loss = 0.25833602Iteration 118, loss = 0.25551612\n",
      "\n",
      "Iteration 80, loss = 0.25787965\n",
      "Iteration 81, loss = 0.25833819\n",
      "Iteration 119, loss = 0.25526961\n",
      "Iteration 82, loss = 0.25768466\n",
      "Iteration 83, loss = 0.25802888\n",
      "Iteration 120, loss = 0.25537510\n",
      "Iteration 84, loss = 0.25751447\n",
      "Iteration 121, loss = 0.25537086\n",
      "Iteration 85, loss = 0.25757873\n",
      "Iteration 122, loss = 0.25560499\n",
      "Iteration 86, loss = 0.25745818\n",
      "Iteration 123, loss = 0.25536042\n",
      "Iteration 87, loss = 0.25703280\n",
      "Iteration 124, loss = 0.25490863\n",
      "Iteration 88, loss = 0.25677373\n",
      "Iteration 125, loss = 0.25476526\n",
      "Iteration 89, loss = 0.25678564\n",
      "Iteration 126, loss = 0.25512726\n",
      "Iteration 90, loss = 0.25689913\n",
      "Iteration 127, loss = 0.25473256\n",
      "Iteration 91, loss = 0.25641818\n",
      "Iteration 128, loss = 0.25463433\n",
      "Iteration 92, loss = 0.25647761\n",
      "Iteration 129, loss = 0.25465190\n",
      "Iteration 93, loss = 0.25646437\n",
      "Iteration 130, loss = 0.25453604\n",
      "Iteration 94, loss = 0.25584516\n",
      "Iteration 131, loss = 0.25459364\n",
      "Iteration 95, loss = 0.25610987\n",
      "Iteration 132, loss = 0.25449695\n",
      "Iteration 96, loss = 0.25563804\n",
      "Iteration 133, loss = 0.25457452\n",
      "Iteration 97, loss = 0.25577629\n",
      "Iteration 134, loss = 0.25421580\n",
      "Iteration 98, loss = 0.25579172\n",
      "Iteration 135, loss = 0.25432910\n",
      "Iteration 136, loss = 0.25420798\n",
      "Iteration 99, loss = 0.25606875\n",
      "Iteration 137, loss = 0.25392303\n",
      "Iteration 100, loss = 0.25576438\n",
      "Iteration 138, loss = 0.25424444\n",
      "Iteration 101, loss = 0.25551027\n",
      "Iteration 139, loss = 0.25453849\n",
      "Iteration 102, loss = 0.25526427\n",
      "Iteration 140, loss = 0.25322841\n",
      "Iteration 103, loss = 0.25573488\n",
      "Iteration 141, loss = 0.25423854\n",
      "Iteration 104, loss = 0.25627597\n",
      "Iteration 142, loss = 0.25351090\n",
      "Iteration 105, loss = 0.25570573\n",
      "Iteration 143, loss = 0.25400879\n",
      "Iteration 144, loss = 0.25340798\n",
      "Iteration 106, loss = 0.25561362\n",
      "Iteration 145, loss = 0.25334248\n",
      "Iteration 107, loss = 0.25507032\n",
      "Iteration 146, loss = 0.25315860\n",
      "Iteration 108, loss = 0.25527705\n",
      "Iteration 147, loss = 0.25353804\n",
      "Iteration 109, loss = 0.25498758\n",
      "Iteration 148, loss = 0.25319190\n",
      "Iteration 110, loss = 0.25512252\n",
      "Iteration 149, loss = 0.25311675\n",
      "Iteration 111, loss = 0.25566077\n",
      "Iteration 150, loss = 0.25326836\n",
      "Iteration 112, loss = 0.25485833\n",
      "Iteration 151, loss = 0.25300466\n",
      "Iteration 113, loss = 0.25470416\n",
      "Iteration 152, loss = 0.25297749\n",
      "Iteration 114, loss = 0.25468873\n",
      "Iteration 153, loss = 0.25312932\n",
      "Iteration 115, loss = 0.25458433\n",
      "Iteration 154, loss = 0.25316532\n",
      "Iteration 116, loss = 0.25411676\n",
      "Iteration 155, loss = 0.25273236\n",
      "Iteration 117, loss = 0.25403408\n",
      "Iteration 156, loss = 0.25318554\n",
      "Iteration 118, loss = 0.25422303\n",
      "Iteration 157, loss = 0.25286311\n",
      "Iteration 119, loss = 0.25401571\n",
      "Iteration 158, loss = 0.25270537\n",
      "Iteration 120, loss = 0.25407557\n",
      "Iteration 159, loss = 0.25333384\n",
      "Iteration 121, loss = 0.25458552\n",
      "Iteration 160, loss = 0.25316237\n",
      "Iteration 122, loss = 0.25479579\n",
      "Iteration 161, loss = 0.25266657\n",
      "Iteration 123, loss = 0.25446360\n",
      "Iteration 162, loss = 0.25225435\n",
      "Iteration 163, loss = 0.25262080\n",
      "Iteration 124, loss = 0.25390594\n",
      "Iteration 125, loss = 0.25391661\n",
      "Iteration 164, loss = 0.25247286\n",
      "Iteration 126, loss = 0.25413615\n",
      "Iteration 165, loss = 0.25229988Iteration 127, loss = 0.25378091\n",
      "\n",
      "Iteration 166, loss = 0.25229798\n",
      "Iteration 128, loss = 0.25362916\n",
      "Iteration 167, loss = 0.25263273\n",
      "Iteration 168, loss = 0.25220645\n",
      "Iteration 129, loss = 0.25341137\n",
      "Iteration 169, loss = 0.25274772\n",
      "Iteration 130, loss = 0.25377700\n",
      "Iteration 131, loss = 0.25356486\n",
      "Iteration 170, loss = 0.25214538\n",
      "Iteration 171, loss = 0.25207301\n",
      "Iteration 132, loss = 0.25386615\n",
      "Iteration 172, loss = 0.25242992\n",
      "Iteration 133, loss = 0.25333002\n",
      "Iteration 173, loss = 0.25243528Iteration 134, loss = 0.25330222\n",
      "\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 135, loss = 0.25358378\n",
      "Iteration 136, loss = 0.25289164\n",
      "Iteration 137, loss = 0.25327113\n",
      "Iteration 138, loss = 0.25337701\n",
      "Iteration 139, loss = 0.25428586\n",
      "Iteration 140, loss = 0.25250260\n",
      "Iteration 141, loss = 0.25281406\n",
      "Iteration 142, loss = 0.25275713\n",
      "Iteration 143, loss = 0.25288828\n",
      "Iteration 144, loss = 0.25268104\n",
      "Iteration 145, loss = 0.25296536\n",
      "Iteration 146, loss = 0.25221860\n",
      "Iteration 147, loss = 0.25302010\n",
      "Iteration 148, loss = 0.25237120\n",
      "Iteration 149, loss = 0.25261521\n",
      "Iteration 150, loss = 0.25254551\n",
      "Iteration 151, loss = 0.25234973\n",
      "Iteration 152, loss = 0.25222136\n",
      "Iteration 153, loss = 0.25196380\n",
      "Iteration 154, loss = 0.25206517\n",
      "Iteration 155, loss = 0.25227135\n",
      "Iteration 156, loss = 0.25207062\n",
      "Iteration 157, loss = 0.25198947\n",
      "Iteration 158, loss = 0.25198548\n",
      "Iteration 159, loss = 0.25186391\n",
      "Iteration 160, loss = 0.25210179\n",
      "Iteration 161, loss = 0.25180191\n",
      "Iteration 162, loss = 0.25132298\n",
      "Iteration 163, loss = 0.25146182\n",
      "Iteration 164, loss = 0.25150202\n",
      "Iteration 165, loss = 0.25154515\n",
      "Iteration 166, loss = 0.25137026\n",
      "Iteration 167, loss = 0.25134272\n",
      "Iteration 168, loss = 0.25138291\n",
      "Iteration 169, loss = 0.25161604\n",
      "Iteration 170, loss = 0.25128678\n",
      "Iteration 171, loss = 0.25123555\n",
      "Iteration 172, loss = 0.25184683\n",
      "Iteration 173, loss = 0.25141892\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.36094402\n",
      "Iteration 1, loss = 0.36097323\n",
      "Iteration 2, loss = 0.31932341\n",
      "Iteration 2, loss = 0.32008403\n",
      "Iteration 3, loss = 0.29960825\n",
      "Iteration 3, loss = 0.30068010\n",
      "Iteration 4, loss = 0.29176116\n",
      "Iteration 4, loss = 0.29317651\n",
      "Iteration 5, loss = 0.28921819\n",
      "Iteration 5, loss = 0.29049484\n",
      "Iteration 6, loss = 0.28804542\n",
      "Iteration 6, loss = 0.28671214\n",
      "Iteration 7, loss = 0.28708039\n",
      "Iteration 7, loss = 0.28563129\n",
      "Iteration 8, loss = 0.28704091\n",
      "Iteration 8, loss = 0.28574433\n",
      "Iteration 9, loss = 0.28678121\n",
      "Iteration 9, loss = 0.28591188\n",
      "Iteration 10, loss = 0.28607708\n",
      "Iteration 10, loss = 0.28426486\n",
      "Iteration 11, loss = 0.28523813\n",
      "Iteration 11, loss = 0.28393947\n",
      "Iteration 12, loss = 0.28486448\n",
      "Iteration 12, loss = 0.28321496\n",
      "Iteration 13, loss = 0.28491283\n",
      "Iteration 13, loss = 0.28300318\n",
      "Iteration 14, loss = 0.28462687\n",
      "Iteration 14, loss = 0.28293267\n",
      "Iteration 15, loss = 0.28269608\n",
      "Iteration 15, loss = 0.28378684\n",
      "Iteration 16, loss = 0.28162391\n",
      "Iteration 16, loss = 0.28301731\n",
      "Iteration 17, loss = 0.28185935\n",
      "Iteration 17, loss = 0.28322375\n",
      "Iteration 18, loss = 0.28177956\n",
      "Iteration 18, loss = 0.28258792\n",
      "Iteration 19, loss = 0.28071666\n",
      "Iteration 19, loss = 0.28207042\n",
      "Iteration 20, loss = 0.28059701\n",
      "Iteration 20, loss = 0.28199891\n",
      "Iteration 21, loss = 0.28213562\n",
      "Iteration 21, loss = 0.28067997\n",
      "Iteration 22, loss = 0.28142494\n",
      "Iteration 22, loss = 0.28008477\n",
      "Iteration 23, loss = 0.28092033\n",
      "Iteration 23, loss = 0.27969555\n",
      "Iteration 24, loss = 0.28094276\n",
      "Iteration 24, loss = 0.28000045\n",
      "Iteration 25, loss = 0.28019688\n",
      "Iteration 25, loss = 0.27933124\n",
      "Iteration 26, loss = 0.27996960\n",
      "Iteration 26, loss = 0.27907543\n",
      "Iteration 27, loss = 0.27989157\n",
      "Iteration 27, loss = 0.27908499\n",
      "Iteration 28, loss = 0.27996193\n",
      "Iteration 28, loss = 0.27867708\n",
      "Iteration 29, loss = 0.27928106\n",
      "Iteration 29, loss = 0.27808411\n",
      "Iteration 30, loss = 0.27937589\n",
      "Iteration 30, loss = 0.27799446\n",
      "Iteration 31, loss = 0.27847896\n",
      "Iteration 31, loss = 0.27714966\n",
      "Iteration 32, loss = 0.27803851\n",
      "Iteration 32, loss = 0.27699767\n",
      "Iteration 33, loss = 0.27804110\n",
      "Iteration 33, loss = 0.27709735\n",
      "Iteration 34, loss = 0.27827144\n",
      "Iteration 34, loss = 0.27675795\n",
      "Iteration 35, loss = 0.27762927\n",
      "Iteration 35, loss = 0.27630965\n",
      "Iteration 36, loss = 0.27709469\n",
      "Iteration 36, loss = 0.27593564\n",
      "Iteration 37, loss = 0.27731639\n",
      "Iteration 37, loss = 0.27627069\n",
      "Iteration 38, loss = 0.27714486\n",
      "Iteration 39, loss = 0.27625990\n",
      "Iteration 38, loss = 0.27617258\n",
      "Iteration 39, loss = 0.27515538\n",
      "Iteration 40, loss = 0.27638165\n",
      "Iteration 40, loss = 0.27502551\n",
      "Iteration 41, loss = 0.27459423\n",
      "Iteration 41, loss = 0.27594521\n",
      "Iteration 42, loss = 0.27627790\n",
      "Iteration 42, loss = 0.27499360\n",
      "Iteration 43, loss = 0.27527484\n",
      "Iteration 43, loss = 0.27598174\n",
      "Iteration 44, loss = 0.27528154\n",
      "Iteration 44, loss = 0.27416277\n",
      "Iteration 45, loss = 0.27508312\n",
      "Iteration 45, loss = 0.27367637\n",
      "Iteration 46, loss = 0.27459475\n",
      "Iteration 46, loss = 0.27338053\n",
      "Iteration 47, loss = 0.27464009\n",
      "Iteration 48, loss = 0.27426523\n",
      "Iteration 47, loss = 0.27367021\n",
      "Iteration 49, loss = 0.27396367\n",
      "Iteration 48, loss = 0.27294468\n",
      "Iteration 50, loss = 0.27360704\n",
      "Iteration 49, loss = 0.27255841\n",
      "Iteration 51, loss = 0.27330062\n",
      "Iteration 50, loss = 0.27225317\n",
      "Iteration 51, loss = 0.27191508\n",
      "Iteration 52, loss = 0.27345956\n",
      "Iteration 53, loss = 0.27354975\n",
      "Iteration 52, loss = 0.27190835\n",
      "Iteration 54, loss = 0.27297114\n",
      "Iteration 53, loss = 0.27244085\n",
      "Iteration 55, loss = 0.27285402\n",
      "Iteration 54, loss = 0.27159645\n",
      "Iteration 56, loss = 0.27288104\n",
      "Iteration 57, loss = 0.27245476\n",
      "Iteration 55, loss = 0.27133177\n",
      "Iteration 56, loss = 0.27124527\n",
      "Iteration 58, loss = 0.27227808\n",
      "Iteration 59, loss = 0.27139343\n",
      "Iteration 57, loss = 0.27120905\n",
      "Iteration 58, loss = 0.27081350\n",
      "Iteration 60, loss = 0.27119377\n",
      "Iteration 59, loss = 0.27020048\n",
      "Iteration 61, loss = 0.27109466\n",
      "Iteration 60, loss = 0.26992542\n",
      "Iteration 62, loss = 0.27060700\n",
      "Iteration 61, loss = 0.26999194\n",
      "Iteration 62, loss = 0.26969442\n",
      "Iteration 63, loss = 0.27021454\n",
      "Iteration 63, loss = 0.26932561\n",
      "Iteration 64, loss = 0.27048347\n",
      "Iteration 64, loss = 0.26962505\n",
      "Iteration 65, loss = 0.27008849\n",
      "Iteration 65, loss = 0.26915408\n",
      "Iteration 66, loss = 0.26966874\n",
      "Iteration 67, loss = 0.26951544\n",
      "Iteration 66, loss = 0.26913513\n",
      "Iteration 68, loss = 0.26986266\n",
      "Iteration 67, loss = 0.26870557\n",
      "Iteration 69, loss = 0.26958761\n",
      "Iteration 68, loss = 0.26875649\n",
      "Iteration 70, loss = 0.26904166\n",
      "Iteration 69, loss = 0.26869359\n",
      "Iteration 71, loss = 0.26895751\n",
      "Iteration 70, loss = 0.26822886\n",
      "Iteration 72, loss = 0.26880321\n",
      "Iteration 71, loss = 0.26795944\n",
      "Iteration 73, loss = 0.26836908\n",
      "Iteration 72, loss = 0.26853456\n",
      "Iteration 74, loss = 0.26848753\n",
      "Iteration 73, loss = 0.26771460\n",
      "Iteration 75, loss = 0.26793374\n",
      "Iteration 74, loss = 0.26781330\n",
      "Iteration 76, loss = 0.26836701\n",
      "Iteration 75, loss = 0.26751679\n",
      "Iteration 77, loss = 0.26813334\n",
      "Iteration 76, loss = 0.26741620\n",
      "Iteration 78, loss = 0.26834046\n",
      "Iteration 77, loss = 0.26755106\n",
      "Iteration 79, loss = 0.26776381\n",
      "Iteration 78, loss = 0.26749410\n",
      "Iteration 80, loss = 0.26773231\n",
      "Iteration 79, loss = 0.26701910\n",
      "Iteration 81, loss = 0.26808355\n",
      "Iteration 80, loss = 0.26712360\n",
      "Iteration 82, loss = 0.26755445\n",
      "Iteration 81, loss = 0.26731768\n",
      "Iteration 83, loss = 0.26748293\n",
      "Iteration 82, loss = 0.26706533\n",
      "Iteration 84, loss = 0.26706094\n",
      "Iteration 85, loss = 0.26714852\n",
      "Iteration 83, loss = 0.26649687\n",
      "Iteration 86, loss = 0.26741327\n",
      "Iteration 84, loss = 0.26636254\n",
      "Iteration 87, loss = 0.26694008\n",
      "Iteration 85, loss = 0.26661447\n",
      "Iteration 88, loss = 0.26726322\n",
      "Iteration 86, loss = 0.26650843\n",
      "Iteration 89, loss = 0.26652947\n",
      "Iteration 87, loss = 0.26637035\n",
      "Iteration 90, loss = 0.26656025\n",
      "Iteration 88, loss = 0.26623465\n",
      "Iteration 91, loss = 0.26656591\n",
      "Iteration 89, loss = 0.26565250\n",
      "Iteration 92, loss = 0.26691763\n",
      "Iteration 90, loss = 0.26572040\n",
      "Iteration 93, loss = 0.26633458\n",
      "Iteration 91, loss = 0.26560880\n",
      "Iteration 94, loss = 0.26615338\n",
      "Iteration 92, loss = 0.26569309\n",
      "Iteration 95, loss = 0.26612365\n",
      "Iteration 93, loss = 0.26569698\n",
      "Iteration 96, loss = 0.26677459\n",
      "Iteration 94, loss = 0.26546525\n",
      "Iteration 97, loss = 0.26548535\n",
      "Iteration 95, loss = 0.26524341\n",
      "Iteration 98, loss = 0.26571936\n",
      "Iteration 96, loss = 0.26589674\n",
      "Iteration 99, loss = 0.26616797\n",
      "Iteration 97, loss = 0.26487467\n",
      "Iteration 100, loss = 0.26570048\n",
      "Iteration 98, loss = 0.26509947\n",
      "Iteration 101, loss = 0.26588218\n",
      "Iteration 102, loss = 0.26528929\n",
      "Iteration 99, loss = 0.26505844\n",
      "Iteration 103, loss = 0.26493843\n",
      "Iteration 100, loss = 0.26496065Iteration 104, loss = 0.26534385\n",
      "\n",
      "Iteration 105, loss = 0.26550513\n",
      "Iteration 106, loss = 0.26545947\n",
      "Iteration 101, loss = 0.26534599\n",
      "Iteration 107, loss = 0.26517935\n",
      "Iteration 102, loss = 0.26483095\n",
      "Iteration 108, loss = 0.26509826\n",
      "Iteration 103, loss = 0.26439861\n",
      "Iteration 109, loss = 0.26468411\n",
      "Iteration 104, loss = 0.26503404\n",
      "Iteration 110, loss = 0.26529568\n",
      "Iteration 105, loss = 0.26508680\n",
      "Iteration 111, loss = 0.26475858\n",
      "Iteration 106, loss = 0.26478087\n",
      "Iteration 112, loss = 0.26470839\n",
      "Iteration 107, loss = 0.26435771\n",
      "Iteration 113, loss = 0.26496391\n",
      "Iteration 108, loss = 0.26414178\n",
      "Iteration 114, loss = 0.26471855\n",
      "Iteration 109, loss = 0.26433259\n",
      "Iteration 115, loss = 0.26452512\n",
      "Iteration 110, loss = 0.26457560\n",
      "Iteration 116, loss = 0.26433257Iteration 111, loss = 0.26354565\n",
      "\n",
      "Iteration 112, loss = 0.26405040\n",
      "Iteration 117, loss = 0.26464210\n",
      "Iteration 113, loss = 0.26421569\n",
      "Iteration 114, loss = 0.26364487\n",
      "Iteration 118, loss = 0.26463840\n",
      "Iteration 115, loss = 0.26377089\n",
      "Iteration 119, loss = 0.26440262\n",
      "Iteration 116, loss = 0.26357712\n",
      "Iteration 120, loss = 0.26525144\n",
      "Iteration 117, loss = 0.26372636\n",
      "Iteration 121, loss = 0.26469260\n",
      "Iteration 118, loss = 0.26368412\n",
      "Iteration 122, loss = 0.26415874\n",
      "Iteration 119, loss = 0.26336434\n",
      "Iteration 123, loss = 0.26393402\n",
      "Iteration 120, loss = 0.26346013\n",
      "Iteration 124, loss = 0.26410009\n",
      "Iteration 121, loss = 0.26328985\n",
      "Iteration 125, loss = 0.26403413\n",
      "Iteration 122, loss = 0.26328738\n",
      "Iteration 126, loss = 0.26408949\n",
      "Iteration 127, loss = 0.26397023\n",
      "Iteration 123, loss = 0.26311476\n",
      "Iteration 128, loss = 0.26469187\n",
      "Iteration 129, loss = 0.26501845\n",
      "Iteration 124, loss = 0.26322296\n",
      "Iteration 130, loss = 0.26431328\n",
      "Iteration 125, loss = 0.26293273\n",
      "Iteration 131, loss = 0.26385854\n",
      "Iteration 126, loss = 0.26296240\n",
      "Iteration 132, loss = 0.26391262\n",
      "Iteration 127, loss = 0.26306024\n",
      "Iteration 133, loss = 0.26417670\n",
      "Iteration 128, loss = 0.26370971\n",
      "Iteration 134, loss = 0.26374100\n",
      "Iteration 129, loss = 0.26364914\n",
      "Iteration 135, loss = 0.26396424\n",
      "Iteration 136, loss = 0.26448219\n",
      "Iteration 130, loss = 0.26309707\n",
      "Iteration 137, loss = 0.26410793\n",
      "Iteration 131, loss = 0.26284094\n",
      "Iteration 132, loss = 0.26291687\n",
      "Iteration 138, loss = 0.26417359\n",
      "Iteration 133, loss = 0.26288592\n",
      "Iteration 139, loss = 0.26364893\n",
      "Iteration 134, loss = 0.26256711\n",
      "Iteration 140, loss = 0.26344238\n",
      "Iteration 141, loss = 0.26385320Iteration 135, loss = 0.26270594\n",
      "\n",
      "Iteration 142, loss = 0.26413817\n",
      "Iteration 136, loss = 0.26312002\n",
      "Iteration 143, loss = 0.26390321\n",
      "Iteration 137, loss = 0.26264008\n",
      "Iteration 144, loss = 0.26369370\n",
      "Iteration 138, loss = 0.26256971\n",
      "Iteration 145, loss = 0.26315391\n",
      "Iteration 139, loss = 0.26232451\n",
      "Iteration 146, loss = 0.26368433\n",
      "Iteration 140, loss = 0.26226447\n",
      "Iteration 141, loss = 0.26241590\n",
      "Iteration 147, loss = 0.26372963\n",
      "Iteration 142, loss = 0.26274486\n",
      "Iteration 143, loss = 0.26268487\n",
      "Iteration 148, loss = 0.26384422\n",
      "Iteration 149, loss = 0.26333541\n",
      "Iteration 144, loss = 0.26225886\n",
      "Iteration 150, loss = 0.26310001\n",
      "Iteration 145, loss = 0.26193941\n",
      "Iteration 151, loss = 0.26370154\n",
      "Iteration 146, loss = 0.26197264\n",
      "Iteration 152, loss = 0.26300100\n",
      "Iteration 147, loss = 0.26185524\n",
      "Iteration 153, loss = 0.26297551\n",
      "Iteration 148, loss = 0.26219439\n",
      "Iteration 154, loss = 0.26330807\n",
      "Iteration 149, loss = 0.26181097\n",
      "Iteration 155, loss = 0.26303868\n",
      "Iteration 150, loss = 0.26157509\n",
      "Iteration 156, loss = 0.26339687\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 151, loss = 0.26192577\n",
      "Iteration 152, loss = 0.26155919\n",
      "Iteration 153, loss = 0.26158857\n",
      "Iteration 1, loss = 0.35932450\n",
      "Iteration 154, loss = 0.26151762\n",
      "Iteration 2, loss = 0.31795135\n",
      "Iteration 155, loss = 0.26140323\n",
      "Iteration 3, loss = 0.29853936\n",
      "Iteration 156, loss = 0.26176953\n",
      "Iteration 4, loss = 0.29117059\n",
      "Iteration 157, loss = 0.26123177\n",
      "Iteration 5, loss = 0.28872996\n",
      "Iteration 158, loss = 0.26125570\n",
      "Iteration 6, loss = 0.28669822\n",
      "Iteration 159, loss = 0.26148425\n",
      "Iteration 7, loss = 0.28551685\n",
      "Iteration 160, loss = 0.26125375\n",
      "Iteration 8, loss = 0.28540119\n",
      "Iteration 161, loss = 0.26120130\n",
      "Iteration 9, loss = 0.28521309\n",
      "Iteration 10, loss = 0.28468961\n",
      "Iteration 162, loss = 0.26135292\n",
      "Iteration 163, loss = 0.26141604\n",
      "Iteration 11, loss = 0.28364178\n",
      "Iteration 12, loss = 0.28319059\n",
      "Iteration 164, loss = 0.26118951\n",
      "Iteration 13, loss = 0.28298690\n",
      "Iteration 165, loss = 0.26115329\n",
      "Iteration 14, loss = 0.28324484\n",
      "Iteration 166, loss = 0.26138744\n",
      "Iteration 15, loss = 0.28231855\n",
      "Iteration 167, loss = 0.26116641\n",
      "Iteration 16, loss = 0.28188748\n",
      "Iteration 168, loss = 0.26095935\n",
      "Iteration 17, loss = 0.28159938\n",
      "Iteration 169, loss = 0.26104166\n",
      "Iteration 18, loss = 0.28144030\n",
      "Iteration 170, loss = 0.26123605\n",
      "Iteration 19, loss = 0.28069188\n",
      "Iteration 171, loss = 0.26111487\n",
      "Iteration 20, loss = 0.28070573\n",
      "Iteration 172, loss = 0.26069268\n",
      "Iteration 21, loss = 0.28093000\n",
      "Iteration 173, loss = 0.26072130\n",
      "Iteration 22, loss = 0.28015883\n",
      "Iteration 174, loss = 0.26066824\n",
      "Iteration 23, loss = 0.27990164\n",
      "Iteration 175, loss = 0.26073617\n",
      "Iteration 24, loss = 0.27964277\n",
      "Iteration 176, loss = 0.26086791\n",
      "Iteration 25, loss = 0.27910899\n",
      "Iteration 177, loss = 0.26094817\n",
      "Iteration 26, loss = 0.27896518\n",
      "Iteration 178, loss = 0.26050795\n",
      "Iteration 27, loss = 0.27874656\n",
      "Iteration 28, loss = 0.27925505\n",
      "Iteration 179, loss = 0.26077842\n",
      "Iteration 29, loss = 0.27844339Iteration 180, loss = 0.26044093\n",
      "\n",
      "Iteration 181, loss = 0.26049213\n",
      "Iteration 30, loss = 0.27888962\n",
      "Iteration 182, loss = 0.26064967\n",
      "Iteration 31, loss = 0.27767487\n",
      "Iteration 183, loss = 0.26060564\n",
      "Iteration 32, loss = 0.27759957\n",
      "Iteration 184, loss = 0.26015497\n",
      "Iteration 33, loss = 0.27709644\n",
      "Iteration 185, loss = 0.26033610\n",
      "Iteration 34, loss = 0.27775482\n",
      "Iteration 186, loss = 0.26049822\n",
      "Iteration 35, loss = 0.27714565\n",
      "Iteration 187, loss = 0.26042551\n",
      "Iteration 36, loss = 0.27641128\n",
      "Iteration 188, loss = 0.26040797\n",
      "Iteration 37, loss = 0.27649028\n",
      "Iteration 189, loss = 0.26035554\n",
      "Iteration 190, loss = 0.26038502\n",
      "Iteration 38, loss = 0.27643306\n",
      "Iteration 191, loss = 0.26017432\n",
      "Iteration 192, loss = 0.26017425\n",
      "Iteration 193, loss = 0.25990762\n",
      "Iteration 39, loss = 0.27609613\n",
      "Iteration 194, loss = 0.26013581\n",
      "Iteration 40, loss = 0.27576942\n",
      "Iteration 195, loss = 0.26023614\n",
      "Iteration 41, loss = 0.27509993\n",
      "Iteration 196, loss = 0.25986688\n",
      "Iteration 42, loss = 0.27568131\n",
      "Iteration 197, loss = 0.26040069\n",
      "Iteration 43, loss = 0.27552964Iteration 198, loss = 0.26001149\n",
      "\n",
      "Iteration 44, loss = 0.27448981\n",
      "Iteration 199, loss = 0.25984472\n",
      "Iteration 200, loss = 0.26001335\n",
      "Iteration 45, loss = 0.27476948\n",
      "Iteration 46, loss = 0.27389648\n",
      "Iteration 47, loss = 0.27423942\n",
      "Iteration 1, loss = 0.35927119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 48, loss = 0.27362689\n",
      "Iteration 2, loss = 0.31743387\n",
      "Iteration 49, loss = 0.27342672\n",
      "Iteration 3, loss = 0.29848900\n",
      "Iteration 50, loss = 0.27298763\n",
      "Iteration 4, loss = 0.29062903\n",
      "Iteration 51, loss = 0.27277144\n",
      "Iteration 5, loss = 0.28811068\n",
      "Iteration 52, loss = 0.27228700\n",
      "Iteration 6, loss = 0.28612194\n",
      "Iteration 7, loss = 0.28474300\n",
      "Iteration 53, loss = 0.27281159\n",
      "Iteration 54, loss = 0.27239078\n",
      "Iteration 8, loss = 0.28476246\n",
      "Iteration 55, loss = 0.27238600\n",
      "Iteration 9, loss = 0.28427510\n",
      "Iteration 56, loss = 0.27188984\n",
      "Iteration 10, loss = 0.28392268\n",
      "Iteration 57, loss = 0.27160558\n",
      "Iteration 11, loss = 0.28266376\n",
      "Iteration 58, loss = 0.27152118\n",
      "Iteration 12, loss = 0.28220526\n",
      "Iteration 59, loss = 0.27085102\n",
      "Iteration 13, loss = 0.28172823\n",
      "Iteration 60, loss = 0.27075305\n",
      "Iteration 14, loss = 0.28160240\n",
      "Iteration 61, loss = 0.27079427\n",
      "Iteration 15, loss = 0.28099062\n",
      "Iteration 62, loss = 0.27048115\n",
      "Iteration 16, loss = 0.28041852\n",
      "Iteration 63, loss = 0.26989784\n",
      "Iteration 64, loss = 0.26992637\n",
      "Iteration 17, loss = 0.28026363\n",
      "Iteration 18, loss = 0.27997219\n",
      "Iteration 65, loss = 0.26991730\n",
      "Iteration 19, loss = 0.27920527\n",
      "Iteration 20, loss = 0.27916253\n",
      "Iteration 66, loss = 0.26937608\n",
      "Iteration 67, loss = 0.26943503\n",
      "Iteration 21, loss = 0.27966479\n",
      "Iteration 68, loss = 0.26946143\n",
      "Iteration 69, loss = 0.26934039\n",
      "Iteration 22, loss = 0.27867907\n",
      "Iteration 23, loss = 0.27854443Iteration 70, loss = 0.26900207\n",
      "\n",
      "Iteration 71, loss = 0.26866785\n",
      "Iteration 24, loss = 0.27802928\n",
      "Iteration 72, loss = 0.26852111\n",
      "Iteration 25, loss = 0.27765445\n",
      "Iteration 73, loss = 0.26834678\n",
      "Iteration 26, loss = 0.27725381\n",
      "Iteration 74, loss = 0.26837736\n",
      "Iteration 27, loss = 0.27708724\n",
      "Iteration 75, loss = 0.26799782\n",
      "Iteration 28, loss = 0.27752600\n",
      "Iteration 76, loss = 0.26815281\n",
      "Iteration 29, loss = 0.27661290\n",
      "Iteration 77, loss = 0.26837565\n",
      "Iteration 30, loss = 0.27656828\n",
      "Iteration 78, loss = 0.26818660\n",
      "Iteration 31, loss = 0.27612754\n",
      "Iteration 79, loss = 0.26784260\n",
      "Iteration 32, loss = 0.27576464\n",
      "Iteration 80, loss = 0.26766294\n",
      "Iteration 33, loss = 0.27524108\n",
      "Iteration 81, loss = 0.26774923\n",
      "Iteration 34, loss = 0.27551571\n",
      "Iteration 82, loss = 0.26745203\n",
      "Iteration 35, loss = 0.27504701\n",
      "Iteration 83, loss = 0.26743385\n",
      "Iteration 36, loss = 0.27453483\n",
      "Iteration 84, loss = 0.26721369\n",
      "Iteration 37, loss = 0.27495322\n",
      "Iteration 85, loss = 0.26723123\n",
      "Iteration 38, loss = 0.27478529\n",
      "Iteration 86, loss = 0.26742334\n",
      "Iteration 39, loss = 0.27396262\n",
      "Iteration 87, loss = 0.26708720\n",
      "Iteration 40, loss = 0.27345408\n",
      "Iteration 88, loss = 0.26732222\n",
      "Iteration 41, loss = 0.27337300\n",
      "Iteration 89, loss = 0.26661376\n",
      "Iteration 42, loss = 0.27354639\n",
      "Iteration 90, loss = 0.26676703\n",
      "Iteration 43, loss = 0.27340281\n",
      "Iteration 91, loss = 0.26670095\n",
      "Iteration 44, loss = 0.27257616\n",
      "Iteration 92, loss = 0.26691560\n",
      "Iteration 45, loss = 0.27277864\n",
      "Iteration 93, loss = 0.26630190\n",
      "Iteration 46, loss = 0.27195092\n",
      "Iteration 94, loss = 0.26620729\n",
      "Iteration 47, loss = 0.27233599\n",
      "Iteration 95, loss = 0.26620274\n",
      "Iteration 48, loss = 0.27159238\n",
      "Iteration 96, loss = 0.26636415\n",
      "Iteration 49, loss = 0.27118221\n",
      "Iteration 97, loss = 0.26582147\n",
      "Iteration 50, loss = 0.27097593\n",
      "Iteration 98, loss = 0.26603354\n",
      "Iteration 51, loss = 0.27095103\n",
      "Iteration 99, loss = 0.26605209\n",
      "Iteration 52, loss = 0.27040339\n",
      "Iteration 100, loss = 0.26608504\n",
      "Iteration 53, loss = 0.27091772\n",
      "Iteration 101, loss = 0.26666038\n",
      "Iteration 54, loss = 0.27019892\n",
      "Iteration 102, loss = 0.26556111\n",
      "Iteration 55, loss = 0.26996124\n",
      "Iteration 103, loss = 0.26545966\n",
      "Iteration 56, loss = 0.26989783\n",
      "Iteration 104, loss = 0.26584830\n",
      "Iteration 57, loss = 0.26955945\n",
      "Iteration 105, loss = 0.26589551\n",
      "Iteration 58, loss = 0.26947611\n",
      "Iteration 106, loss = 0.26624774\n",
      "Iteration 59, loss = 0.26904756\n",
      "Iteration 107, loss = 0.26550106\n",
      "Iteration 60, loss = 0.26895233\n",
      "Iteration 61, loss = 0.26950204\n",
      "Iteration 108, loss = 0.26529573\n",
      "Iteration 62, loss = 0.26849295\n",
      "Iteration 109, loss = 0.26495492\n",
      "Iteration 63, loss = 0.26830111\n",
      "Iteration 110, loss = 0.26514711\n",
      "Iteration 64, loss = 0.26816852\n",
      "Iteration 111, loss = 0.26512284\n",
      "Iteration 65, loss = 0.26804190\n",
      "Iteration 112, loss = 0.26527235\n",
      "Iteration 66, loss = 0.26772491\n",
      "Iteration 113, loss = 0.26503345\n",
      "Iteration 67, loss = 0.26769271\n",
      "Iteration 114, loss = 0.26485866\n",
      "Iteration 68, loss = 0.26768213\n",
      "Iteration 115, loss = 0.26478205\n",
      "Iteration 69, loss = 0.26778274\n",
      "Iteration 116, loss = 0.26476420\n",
      "Iteration 70, loss = 0.26741145\n",
      "Iteration 117, loss = 0.26554612\n",
      "Iteration 118, loss = 0.26475148\n",
      "Iteration 71, loss = 0.26682047\n",
      "Iteration 119, loss = 0.26479589\n",
      "Iteration 120, loss = 0.26479922\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 72, loss = 0.26704809\n",
      "Iteration 73, loss = 0.26700249\n",
      "Iteration 74, loss = 0.26659742\n",
      "Iteration 1, loss = 0.35860421\n",
      "Iteration 75, loss = 0.26632464\n",
      "Iteration 2, loss = 0.31688041\n",
      "Iteration 76, loss = 0.26626123\n",
      "Iteration 3, loss = 0.29797247\n",
      "Iteration 4, loss = 0.29019416\n",
      "Iteration 77, loss = 0.26650592\n",
      "Iteration 5, loss = 0.28743367\n",
      "Iteration 78, loss = 0.26654564\n",
      "Iteration 6, loss = 0.28584139\n",
      "Iteration 7, loss = 0.28466807\n",
      "Iteration 79, loss = 0.26630940\n",
      "Iteration 8, loss = 0.28410044\n",
      "Iteration 80, loss = 0.26591689\n",
      "Iteration 9, loss = 0.28384790\n",
      "Iteration 81, loss = 0.26634589\n",
      "Iteration 10, loss = 0.28344877\n",
      "Iteration 82, loss = 0.26546029\n",
      "Iteration 11, loss = 0.28277631\n",
      "Iteration 83, loss = 0.26543028\n",
      "Iteration 12, loss = 0.28210126\n",
      "Iteration 84, loss = 0.26523752\n",
      "Iteration 13, loss = 0.28216703\n",
      "Iteration 85, loss = 0.26542363\n",
      "Iteration 14, loss = 0.28160769\n",
      "Iteration 86, loss = 0.26560938\n",
      "Iteration 15, loss = 0.28160227\n",
      "Iteration 87, loss = 0.26508167\n",
      "Iteration 16, loss = 0.28058692\n",
      "Iteration 88, loss = 0.26499773\n",
      "Iteration 17, loss = 0.28075325\n",
      "Iteration 89, loss = 0.26494073\n",
      "Iteration 18, loss = 0.28015910\n",
      "Iteration 90, loss = 0.26517121\n",
      "Iteration 19, loss = 0.27966379\n",
      "Iteration 91, loss = 0.26490289\n",
      "Iteration 20, loss = 0.27945095\n",
      "Iteration 92, loss = 0.26492902\n",
      "Iteration 21, loss = 0.27993872\n",
      "Iteration 93, loss = 0.26475101\n",
      "Iteration 22, loss = 0.27885520\n",
      "Iteration 94, loss = 0.26418384\n",
      "Iteration 23, loss = 0.27842824\n",
      "Iteration 95, loss = 0.26471048\n",
      "Iteration 24, loss = 0.27799856\n",
      "Iteration 96, loss = 0.26420945Iteration 25, loss = 0.27752733\n",
      "\n",
      "Iteration 26, loss = 0.27752969\n",
      "Iteration 97, loss = 0.26422158\n",
      "Iteration 27, loss = 0.27712553\n",
      "Iteration 98, loss = 0.26416551\n",
      "Iteration 28, loss = 0.27730418\n",
      "Iteration 99, loss = 0.26391588\n",
      "Iteration 29, loss = 0.27664843\n",
      "Iteration 100, loss = 0.26435500\n",
      "Iteration 30, loss = 0.27653309\n",
      "Iteration 31, loss = 0.27617056\n",
      "Iteration 101, loss = 0.26446551\n",
      "Iteration 102, loss = 0.26383485\n",
      "Iteration 32, loss = 0.27529563\n",
      "Iteration 103, loss = 0.26367175\n",
      "Iteration 104, loss = 0.26390358\n",
      "Iteration 105, loss = 0.26420696\n",
      "Iteration 33, loss = 0.27528913\n",
      "Iteration 106, loss = 0.26408130Iteration 34, loss = 0.27508428\n",
      "\n",
      "Iteration 35, loss = 0.27467512\n",
      "Iteration 107, loss = 0.26386474\n",
      "Iteration 36, loss = 0.27443481\n",
      "Iteration 108, loss = 0.26357150\n",
      "Iteration 37, loss = 0.27423601\n",
      "Iteration 109, loss = 0.26324295\n",
      "Iteration 38, loss = 0.27412680\n",
      "Iteration 110, loss = 0.26321239\n",
      "Iteration 111, loss = 0.26309516\n",
      "Iteration 112, loss = 0.26316223\n",
      "Iteration 39, loss = 0.27370407\n",
      "Iteration 113, loss = 0.26329068\n",
      "Iteration 40, loss = 0.27334576\n",
      "Iteration 114, loss = 0.26298824\n",
      "Iteration 41, loss = 0.27318054\n",
      "Iteration 115, loss = 0.26278873\n",
      "Iteration 42, loss = 0.27309628\n",
      "Iteration 116, loss = 0.26311775\n",
      "Iteration 43, loss = 0.27272900\n",
      "Iteration 117, loss = 0.26311904\n",
      "Iteration 44, loss = 0.27243971\n",
      "Iteration 118, loss = 0.26290637\n",
      "Iteration 45, loss = 0.27206961\n",
      "Iteration 119, loss = 0.26309995\n",
      "Iteration 46, loss = 0.27176194\n",
      "Iteration 120, loss = 0.26294041\n",
      "Iteration 47, loss = 0.27209181\n",
      "Iteration 121, loss = 0.26278809\n",
      "Iteration 48, loss = 0.27135170\n",
      "Iteration 122, loss = 0.26266592\n",
      "Iteration 49, loss = 0.27142872\n",
      "Iteration 123, loss = 0.26306164\n",
      "Iteration 50, loss = 0.27103665\n",
      "Iteration 124, loss = 0.26268784\n",
      "Iteration 51, loss = 0.27086366\n",
      "Iteration 125, loss = 0.26245906\n",
      "Iteration 52, loss = 0.27055326\n",
      "Iteration 126, loss = 0.26238616\n",
      "Iteration 53, loss = 0.27064491\n",
      "Iteration 54, loss = 0.27041957Iteration 127, loss = 0.26257546\n",
      "\n",
      "Iteration 128, loss = 0.26286466\n",
      "Iteration 55, loss = 0.27028309\n",
      "Iteration 129, loss = 0.26230081\n",
      "Iteration 56, loss = 0.27021808\n",
      "Iteration 130, loss = 0.26256686\n",
      "Iteration 57, loss = 0.26972166\n",
      "Iteration 131, loss = 0.26219563\n",
      "Iteration 58, loss = 0.26959896\n",
      "Iteration 132, loss = 0.26199253\n",
      "Iteration 59, loss = 0.26968815\n",
      "Iteration 133, loss = 0.26244554\n",
      "Iteration 60, loss = 0.26935237\n",
      "Iteration 134, loss = 0.26186491\n",
      "Iteration 61, loss = 0.26936328\n",
      "Iteration 135, loss = 0.26204396\n",
      "Iteration 62, loss = 0.26912669\n",
      "Iteration 136, loss = 0.26329199\n",
      "Iteration 63, loss = 0.26902620\n",
      "Iteration 137, loss = 0.26211517\n",
      "Iteration 64, loss = 0.26876053\n",
      "Iteration 138, loss = 0.26197760\n",
      "Iteration 65, loss = 0.26870713\n",
      "Iteration 139, loss = 0.26196908\n",
      "Iteration 66, loss = 0.26856483\n",
      "Iteration 140, loss = 0.26171903\n",
      "Iteration 67, loss = 0.26846141\n",
      "Iteration 141, loss = 0.26194496\n",
      "Iteration 68, loss = 0.26869522\n",
      "Iteration 142, loss = 0.26290863\n",
      "Iteration 69, loss = 0.26852459\n",
      "Iteration 143, loss = 0.26207823\n",
      "Iteration 70, loss = 0.26838611\n",
      "Iteration 144, loss = 0.26207887\n",
      "Iteration 145, loss = 0.26152117\n",
      "Iteration 71, loss = 0.26789467\n",
      "Iteration 72, loss = 0.26793174Iteration 146, loss = 0.26150139\n",
      "\n",
      "Iteration 73, loss = 0.26789287\n",
      "Iteration 147, loss = 0.26148671\n",
      "Iteration 74, loss = 0.26788296\n",
      "Iteration 148, loss = 0.26212292\n",
      "Iteration 75, loss = 0.26760456\n",
      "Iteration 149, loss = 0.26160758\n",
      "Iteration 76, loss = 0.26741615\n",
      "Iteration 77, loss = 0.26764434\n",
      "Iteration 150, loss = 0.26127804\n",
      "Iteration 78, loss = 0.26747474\n",
      "Iteration 151, loss = 0.26176944\n",
      "Iteration 79, loss = 0.26741151\n",
      "Iteration 152, loss = 0.26157464\n",
      "Iteration 80, loss = 0.26714836\n",
      "Iteration 153, loss = 0.26149248\n",
      "Iteration 81, loss = 0.26717404\n",
      "Iteration 154, loss = 0.26113384\n",
      "Iteration 82, loss = 0.26712235\n",
      "Iteration 155, loss = 0.26141969\n",
      "Iteration 83, loss = 0.26690996\n",
      "Iteration 156, loss = 0.26165411\n",
      "Iteration 84, loss = 0.26700920\n",
      "Iteration 157, loss = 0.26101326\n",
      "Iteration 85, loss = 0.26683317\n",
      "Iteration 158, loss = 0.26104923\n",
      "Iteration 86, loss = 0.26686403\n",
      "Iteration 159, loss = 0.26094856\n",
      "Iteration 87, loss = 0.26652746\n",
      "Iteration 160, loss = 0.26114525\n",
      "Iteration 88, loss = 0.26641493\n",
      "Iteration 161, loss = 0.26106729\n",
      "Iteration 89, loss = 0.26626886\n",
      "Iteration 162, loss = 0.26122488\n",
      "Iteration 90, loss = 0.26735438\n",
      "Iteration 163, loss = 0.26107465\n",
      "Iteration 91, loss = 0.26646025\n",
      "Iteration 164, loss = 0.26079146\n",
      "Iteration 92, loss = 0.26661545\n",
      "Iteration 165, loss = 0.26082371\n",
      "Iteration 93, loss = 0.26595033\n",
      "Iteration 166, loss = 0.26070749\n",
      "Iteration 94, loss = 0.26565880\n",
      "Iteration 167, loss = 0.26041242\n",
      "Iteration 95, loss = 0.26584505\n",
      "Iteration 168, loss = 0.26069162\n",
      "Iteration 169, loss = 0.26056696\n",
      "Iteration 96, loss = 0.26550648\n",
      "Iteration 97, loss = 0.26573020\n",
      "Iteration 170, loss = 0.26078280\n",
      "Iteration 98, loss = 0.26554389\n",
      "Iteration 171, loss = 0.26036218\n",
      "Iteration 99, loss = 0.26534484\n",
      "Iteration 172, loss = 0.26060207\n",
      "Iteration 100, loss = 0.26542853\n",
      "Iteration 173, loss = 0.26053681\n",
      "Iteration 101, loss = 0.26581778\n",
      "Iteration 174, loss = 0.25995280\n",
      "Iteration 102, loss = 0.26513911\n",
      "Iteration 175, loss = 0.26041571\n",
      "Iteration 176, loss = 0.26087319\n",
      "Iteration 103, loss = 0.26512841\n",
      "Iteration 177, loss = 0.26035213\n",
      "Iteration 104, loss = 0.26543165\n",
      "Iteration 178, loss = 0.26010354\n",
      "Iteration 105, loss = 0.26529292\n",
      "Iteration 179, loss = 0.26014246\n",
      "Iteration 106, loss = 0.26524192\n",
      "Iteration 180, loss = 0.26013038\n",
      "Iteration 107, loss = 0.26480593Iteration 181, loss = 0.26032254\n",
      "\n",
      "Iteration 182, loss = 0.26006477\n",
      "Iteration 108, loss = 0.26467566\n",
      "Iteration 109, loss = 0.26444848\n",
      "Iteration 183, loss = 0.25986198\n",
      "Iteration 184, loss = 0.26000663\n",
      "Iteration 110, loss = 0.26430501\n",
      "Iteration 185, loss = 0.26005353\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 111, loss = 0.26420835\n",
      "Iteration 1, loss = 0.35932061\n",
      "Iteration 112, loss = 0.26446515\n",
      "Iteration 2, loss = 0.31803542\n",
      "Iteration 113, loss = 0.26451587\n",
      "Iteration 3, loss = 0.29919603\n",
      "Iteration 114, loss = 0.26407587\n",
      "Iteration 4, loss = 0.29137304\n",
      "Iteration 115, loss = 0.26404365\n",
      "Iteration 5, loss = 0.28811450\n",
      "Iteration 116, loss = 0.26399687\n",
      "Iteration 6, loss = 0.28623246\n",
      "Iteration 117, loss = 0.26371945\n",
      "Iteration 7, loss = 0.28540607\n",
      "Iteration 118, loss = 0.26354163\n",
      "Iteration 8, loss = 0.28462734\n",
      "Iteration 119, loss = 0.26397539\n",
      "Iteration 9, loss = 0.28440124\n",
      "Iteration 120, loss = 0.26363645\n",
      "Iteration 10, loss = 0.28366577\n",
      "Iteration 121, loss = 0.26412072\n",
      "Iteration 122, loss = 0.26341133\n",
      "Iteration 11, loss = 0.28370234\n",
      "Iteration 123, loss = 0.26341168\n",
      "Iteration 12, loss = 0.28258063\n",
      "Iteration 124, loss = 0.26315494\n",
      "Iteration 125, loss = 0.26311379\n",
      "Iteration 13, loss = 0.28308567\n",
      "Iteration 126, loss = 0.26312498\n",
      "Iteration 127, loss = 0.26315159\n",
      "Iteration 14, loss = 0.28285631\n",
      "Iteration 128, loss = 0.26312935\n",
      "Iteration 15, loss = 0.28288304\n",
      "Iteration 129, loss = 0.26300811\n",
      "Iteration 130, loss = 0.26290119\n",
      "Iteration 16, loss = 0.28180097\n",
      "Iteration 131, loss = 0.26286857\n",
      "Iteration 17, loss = 0.28112307\n",
      "Iteration 18, loss = 0.28049570\n",
      "Iteration 132, loss = 0.26250214\n",
      "Iteration 19, loss = 0.28008752\n",
      "Iteration 133, loss = 0.26288407\n",
      "Iteration 20, loss = 0.28009235\n",
      "Iteration 134, loss = 0.26247813\n",
      "Iteration 21, loss = 0.27967534\n",
      "Iteration 135, loss = 0.26240484\n",
      "Iteration 22, loss = 0.27918625\n",
      "Iteration 136, loss = 0.26311940\n",
      "Iteration 23, loss = 0.27910753\n",
      "Iteration 137, loss = 0.26259910\n",
      "Iteration 24, loss = 0.27877790\n",
      "Iteration 138, loss = 0.26255097\n",
      "Iteration 25, loss = 0.27858266\n",
      "Iteration 139, loss = 0.26230857\n",
      "Iteration 26, loss = 0.27853594\n",
      "Iteration 140, loss = 0.26207990\n",
      "Iteration 27, loss = 0.27790668\n",
      "Iteration 28, loss = 0.27749838\n",
      "Iteration 141, loss = 0.26223488\n",
      "Iteration 142, loss = 0.26264580\n",
      "Iteration 29, loss = 0.27703243\n",
      "Iteration 143, loss = 0.26254109\n",
      "Iteration 30, loss = 0.27689535\n",
      "Iteration 144, loss = 0.26239695\n",
      "Iteration 31, loss = 0.27651144\n",
      "Iteration 145, loss = 0.26180284\n",
      "Iteration 32, loss = 0.27618235\n",
      "Iteration 33, loss = 0.27566290\n",
      "Iteration 146, loss = 0.26183783\n",
      "Iteration 34, loss = 0.27642710\n",
      "Iteration 147, loss = 0.26182574\n",
      "Iteration 35, loss = 0.27555012\n",
      "Iteration 148, loss = 0.26188442\n",
      "Iteration 149, loss = 0.26185528\n",
      "Iteration 36, loss = 0.27541984\n",
      "Iteration 150, loss = 0.26165412\n",
      "Iteration 37, loss = 0.27502533\n",
      "Iteration 38, loss = 0.27475739\n",
      "Iteration 151, loss = 0.26199198\n",
      "Iteration 39, loss = 0.27473659\n",
      "Iteration 152, loss = 0.26154543\n",
      "Iteration 40, loss = 0.27322974\n",
      "Iteration 153, loss = 0.26146575\n",
      "Iteration 154, loss = 0.26174579\n",
      "Iteration 41, loss = 0.27381524\n",
      "Iteration 155, loss = 0.26199784\n",
      "Iteration 42, loss = 0.27325988\n",
      "Iteration 156, loss = 0.26181425\n",
      "Iteration 43, loss = 0.27333047\n",
      "Iteration 157, loss = 0.26144275\n",
      "Iteration 44, loss = 0.27334445\n",
      "Iteration 158, loss = 0.26141908\n",
      "Iteration 45, loss = 0.27247858\n",
      "Iteration 159, loss = 0.26129823\n",
      "Iteration 46, loss = 0.27258039\n",
      "Iteration 160, loss = 0.26114137\n",
      "Iteration 47, loss = 0.27216652\n",
      "Iteration 161, loss = 0.26124023\n",
      "Iteration 48, loss = 0.27249063\n",
      "Iteration 162, loss = 0.26107473\n",
      "Iteration 49, loss = 0.27191396\n",
      "Iteration 163, loss = 0.26141512\n",
      "Iteration 50, loss = 0.27184279\n",
      "Iteration 164, loss = 0.26110803\n",
      "Iteration 51, loss = 0.27147998\n",
      "Iteration 165, loss = 0.26093449\n",
      "Iteration 52, loss = 0.27135080\n",
      "Iteration 166, loss = 0.26094616\n",
      "Iteration 53, loss = 0.27131709\n",
      "Iteration 167, loss = 0.26081139\n",
      "Iteration 54, loss = 0.27107848\n",
      "Iteration 168, loss = 0.26107469\n",
      "Iteration 169, loss = 0.26086672\n",
      "Iteration 55, loss = 0.27098418\n",
      "Iteration 170, loss = 0.26066843\n",
      "Iteration 56, loss = 0.27141318\n",
      "Iteration 171, loss = 0.26059802\n",
      "Iteration 57, loss = 0.27005485\n",
      "Iteration 172, loss = 0.26093672\n",
      "Iteration 58, loss = 0.27014911\n",
      "Iteration 173, loss = 0.26075505\n",
      "Iteration 59, loss = 0.27017851\n",
      "Iteration 174, loss = 0.26035840\n",
      "Iteration 60, loss = 0.26979863\n",
      "Iteration 175, loss = 0.26054852\n",
      "Iteration 61, loss = 0.26980155\n",
      "Iteration 176, loss = 0.26064086\n",
      "Iteration 62, loss = 0.26998479\n",
      "Iteration 177, loss = 0.26038867\n",
      "Iteration 63, loss = 0.26954560\n",
      "Iteration 178, loss = 0.26040741\n",
      "Iteration 64, loss = 0.26971231\n",
      "Iteration 179, loss = 0.26047883\n",
      "Iteration 65, loss = 0.26914021\n",
      "Iteration 180, loss = 0.26066415\n",
      "Iteration 66, loss = 0.26922389\n",
      "Iteration 181, loss = 0.26031335\n",
      "Iteration 182, loss = 0.26055488\n",
      "Iteration 67, loss = 0.26897138\n",
      "Iteration 68, loss = 0.26901544\n",
      "Iteration 183, loss = 0.26009928\n",
      "Iteration 69, loss = 0.26894942\n",
      "Iteration 184, loss = 0.26023382\n",
      "Iteration 70, loss = 0.26853651\n",
      "Iteration 185, loss = 0.26044153\n",
      "Iteration 71, loss = 0.26846116\n",
      "Iteration 186, loss = 0.26043532\n",
      "Iteration 72, loss = 0.26828873\n",
      "Iteration 187, loss = 0.26016045\n",
      "Iteration 73, loss = 0.26834610\n",
      "Iteration 188, loss = 0.26053480Iteration 74, loss = 0.26808725\n",
      "\n",
      "Iteration 75, loss = 0.26790445\n",
      "Iteration 189, loss = 0.26014836\n",
      "Iteration 190, loss = 0.26041287\n",
      "Iteration 76, loss = 0.26793126\n",
      "Iteration 191, loss = 0.26006086\n",
      "Iteration 77, loss = 0.26813178\n",
      "Iteration 78, loss = 0.26729504Iteration 192, loss = 0.26016620\n",
      "\n",
      "Iteration 193, loss = 0.26016884\n",
      "Iteration 79, loss = 0.26834976\n",
      "Iteration 80, loss = 0.26789708\n",
      "Iteration 194, loss = 0.26027417\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 81, loss = 0.26708162\n",
      "Iteration 82, loss = 0.26720370\n",
      "Iteration 1, loss = 0.35875046\n",
      "Iteration 83, loss = 0.26747602\n",
      "Iteration 2, loss = 0.31916257\n",
      "Iteration 84, loss = 0.26746668\n",
      "Iteration 3, loss = 0.30035100\n",
      "Iteration 85, loss = 0.26721962\n",
      "Iteration 4, loss = 0.29250110\n",
      "Iteration 86, loss = 0.26636917\n",
      "Iteration 5, loss = 0.28911926\n",
      "Iteration 87, loss = 0.26637438\n",
      "Iteration 6, loss = 0.28745603\n",
      "Iteration 88, loss = 0.26656852\n",
      "Iteration 89, loss = 0.26648055\n",
      "Iteration 7, loss = 0.28696797\n",
      "Iteration 90, loss = 0.26608465\n",
      "Iteration 8, loss = 0.28591906\n",
      "Iteration 91, loss = 0.26604028\n",
      "Iteration 9, loss = 0.28517179\n",
      "Iteration 92, loss = 0.26624158\n",
      "Iteration 10, loss = 0.28514885\n",
      "Iteration 93, loss = 0.26588330\n",
      "Iteration 11, loss = 0.28463652\n",
      "Iteration 94, loss = 0.26604899\n",
      "Iteration 12, loss = 0.28370383\n",
      "Iteration 95, loss = 0.26552708\n",
      "Iteration 13, loss = 0.28388743\n",
      "Iteration 96, loss = 0.26556236\n",
      "Iteration 14, loss = 0.28397276\n",
      "Iteration 97, loss = 0.26538328\n",
      "Iteration 15, loss = 0.28344718\n",
      "Iteration 98, loss = 0.26551600\n",
      "Iteration 16, loss = 0.28223610\n",
      "Iteration 99, loss = 0.26574455\n",
      "Iteration 100, loss = 0.26517893\n",
      "Iteration 17, loss = 0.28203457\n",
      "Iteration 101, loss = 0.26554683\n",
      "Iteration 18, loss = 0.28121838\n",
      "Iteration 19, loss = 0.28069403\n",
      "Iteration 102, loss = 0.26533335\n",
      "Iteration 103, loss = 0.26502709\n",
      "Iteration 20, loss = 0.28071640\n",
      "Iteration 21, loss = 0.28072688Iteration 104, loss = 0.26482550\n",
      "\n",
      "Iteration 105, loss = 0.26504445\n",
      "Iteration 22, loss = 0.28034397\n",
      "Iteration 106, loss = 0.26502299\n",
      "Iteration 23, loss = 0.27991919\n",
      "Iteration 107, loss = 0.26505940\n",
      "Iteration 24, loss = 0.27975136\n",
      "Iteration 108, loss = 0.26463839\n",
      "Iteration 25, loss = 0.27959068\n",
      "Iteration 109, loss = 0.26471026\n",
      "Iteration 26, loss = 0.27948474\n",
      "Iteration 110, loss = 0.26464601\n",
      "Iteration 27, loss = 0.27910519\n",
      "Iteration 111, loss = 0.26407244\n",
      "Iteration 28, loss = 0.27892287\n",
      "Iteration 112, loss = 0.26422939\n",
      "Iteration 29, loss = 0.27810901\n",
      "Iteration 113, loss = 0.26382238\n",
      "Iteration 30, loss = 0.27801891\n",
      "Iteration 114, loss = 0.26498530\n",
      "Iteration 31, loss = 0.27760191\n",
      "Iteration 115, loss = 0.26405816\n",
      "Iteration 32, loss = 0.27727069\n",
      "Iteration 116, loss = 0.26444417\n",
      "Iteration 33, loss = 0.27677679\n",
      "Iteration 117, loss = 0.26460024\n",
      "Iteration 34, loss = 0.27708895\n",
      "Iteration 118, loss = 0.26394171\n",
      "Iteration 119, loss = 0.26348472\n",
      "Iteration 35, loss = 0.27695000\n",
      "Iteration 120, loss = 0.26350311\n",
      "Iteration 36, loss = 0.27604889\n",
      "Iteration 121, loss = 0.26335605\n",
      "Iteration 37, loss = 0.27611154\n",
      "Iteration 122, loss = 0.26411770\n",
      "Iteration 38, loss = 0.27606730\n",
      "Iteration 123, loss = 0.26313559\n",
      "Iteration 39, loss = 0.27584184\n",
      "Iteration 124, loss = 0.26340896\n",
      "Iteration 40, loss = 0.27429074\n",
      "Iteration 125, loss = 0.26333228\n",
      "Iteration 41, loss = 0.27449008\n",
      "Iteration 126, loss = 0.26307646\n",
      "Iteration 42, loss = 0.27407168\n",
      "Iteration 127, loss = 0.26285145\n",
      "Iteration 43, loss = 0.27380318\n",
      "Iteration 128, loss = 0.26288473\n",
      "Iteration 44, loss = 0.27395496\n",
      "Iteration 129, loss = 0.26318112\n",
      "Iteration 45, loss = 0.27312498\n",
      "Iteration 130, loss = 0.26327094\n",
      "Iteration 46, loss = 0.27293327\n",
      "Iteration 131, loss = 0.26329463\n",
      "Iteration 132, loss = 0.26265282\n",
      "Iteration 47, loss = 0.27235503\n",
      "Iteration 133, loss = 0.26257417\n",
      "Iteration 48, loss = 0.27257150\n",
      "Iteration 49, loss = 0.27194786\n",
      "Iteration 134, loss = 0.26290838\n",
      "Iteration 135, loss = 0.26287985\n",
      "Iteration 50, loss = 0.27176503\n",
      "Iteration 136, loss = 0.26243434\n",
      "Iteration 51, loss = 0.27136509\n",
      "Iteration 137, loss = 0.26226664\n",
      "Iteration 52, loss = 0.27087421\n",
      "Iteration 138, loss = 0.26308001\n",
      "Iteration 53, loss = 0.27100710\n",
      "Iteration 139, loss = 0.26244955\n",
      "Iteration 54, loss = 0.27071938\n",
      "Iteration 140, loss = 0.26211895\n",
      "Iteration 55, loss = 0.27086049\n",
      "Iteration 141, loss = 0.26220680\n",
      "Iteration 56, loss = 0.27159081\n",
      "Iteration 142, loss = 0.26242892\n",
      "Iteration 57, loss = 0.26967873\n",
      "Iteration 143, loss = 0.26189262\n",
      "Iteration 58, loss = 0.26966054\n",
      "Iteration 144, loss = 0.26198123\n",
      "Iteration 59, loss = 0.26963985\n",
      "Iteration 145, loss = 0.26214675\n",
      "Iteration 60, loss = 0.26928188Iteration 146, loss = 0.26221472\n",
      "\n",
      "Iteration 61, loss = 0.26912641\n",
      "Iteration 147, loss = 0.26210767\n",
      "Iteration 148, loss = 0.26194694\n",
      "Iteration 62, loss = 0.26948147\n",
      "Iteration 63, loss = 0.26878807\n",
      "Iteration 149, loss = 0.26191917\n",
      "Iteration 64, loss = 0.26844523\n",
      "Iteration 150, loss = 0.26204785\n",
      "Iteration 65, loss = 0.26841025\n",
      "Iteration 151, loss = 0.26180916\n",
      "Iteration 66, loss = 0.26841849\n",
      "Iteration 152, loss = 0.26299295\n",
      "Iteration 67, loss = 0.26816970\n",
      "Iteration 153, loss = 0.26174473\n",
      "Iteration 154, loss = 0.26159677\n",
      "Iteration 68, loss = 0.26820561\n",
      "Iteration 155, loss = 0.26170923\n",
      "Iteration 69, loss = 0.26828585\n",
      "Iteration 156, loss = 0.26106865\n",
      "Iteration 70, loss = 0.26764007\n",
      "Iteration 157, loss = 0.26157777\n",
      "Iteration 71, loss = 0.26759020\n",
      "Iteration 158, loss = 0.26161419\n",
      "Iteration 72, loss = 0.26743506\n",
      "Iteration 159, loss = 0.26120482\n",
      "Iteration 73, loss = 0.26730943\n",
      "Iteration 160, loss = 0.26128477\n",
      "Iteration 74, loss = 0.26725124\n",
      "Iteration 161, loss = 0.26177996\n",
      "Iteration 75, loss = 0.26719603\n",
      "Iteration 162, loss = 0.26118438\n",
      "Iteration 76, loss = 0.26705131\n",
      "Iteration 163, loss = 0.26113492\n",
      "Iteration 77, loss = 0.26726239\n",
      "Iteration 164, loss = 0.26137915\n",
      "Iteration 78, loss = 0.26649239\n",
      "Iteration 165, loss = 0.26100398\n",
      "Iteration 79, loss = 0.26767862\n",
      "Iteration 166, loss = 0.26137751\n",
      "Iteration 80, loss = 0.26684521\n",
      "Iteration 167, loss = 0.26085353\n",
      "Iteration 81, loss = 0.26596281\n",
      "Iteration 168, loss = 0.26115851\n",
      "Iteration 82, loss = 0.26681220\n",
      "Iteration 169, loss = 0.26107977\n",
      "Iteration 83, loss = 0.26655920\n",
      "Iteration 170, loss = 0.26140707\n",
      "Iteration 84, loss = 0.26651718\n",
      "Iteration 171, loss = 0.26153480\n",
      "Iteration 85, loss = 0.26605454\n",
      "Iteration 172, loss = 0.26141631\n",
      "Iteration 86, loss = 0.26602716\n",
      "Iteration 173, loss = 0.26070963\n",
      "Iteration 87, loss = 0.26561956\n",
      "Iteration 174, loss = 0.26111568\n",
      "Iteration 175, loss = 0.26152016\n",
      "Iteration 88, loss = 0.26566644\n",
      "Iteration 176, loss = 0.26049153\n",
      "Iteration 89, loss = 0.26581031\n",
      "Iteration 177, loss = 0.26086623\n",
      "Iteration 90, loss = 0.26548121\n",
      "Iteration 178, loss = 0.26068889\n",
      "Iteration 91, loss = 0.26538961\n",
      "Iteration 179, loss = 0.26069754\n",
      "Iteration 92, loss = 0.26566349\n",
      "Iteration 93, loss = 0.26569419\n",
      "Iteration 94, loss = 0.26530113\n",
      "Iteration 95, loss = 0.26487955\n",
      "Iteration 180, loss = 0.26075210\n",
      "Iteration 181, loss = 0.26049909\n",
      "Iteration 96, loss = 0.26506561\n",
      "Iteration 182, loss = 0.26040092Iteration 97, loss = 0.26506180\n",
      "\n",
      "Iteration 183, loss = 0.26036414\n",
      "Iteration 98, loss = 0.26490112\n",
      "Iteration 184, loss = 0.26036842\n",
      "Iteration 185, loss = 0.26051625\n",
      "Iteration 99, loss = 0.26465417\n",
      "Iteration 186, loss = 0.26054440\n",
      "Iteration 100, loss = 0.26457160\n",
      "Iteration 187, loss = 0.26024675\n",
      "Iteration 101, loss = 0.26461298\n",
      "Iteration 188, loss = 0.26040664\n",
      "Iteration 189, loss = 0.26037439\n",
      "Iteration 102, loss = 0.26442934\n",
      "Iteration 103, loss = 0.26460538\n",
      "Iteration 190, loss = 0.26019630\n",
      "Iteration 104, loss = 0.26418967\n",
      "Iteration 191, loss = 0.26000370\n",
      "Iteration 105, loss = 0.26432072\n",
      "Iteration 192, loss = 0.26002389\n",
      "Iteration 106, loss = 0.26447564\n",
      "Iteration 193, loss = 0.25994738\n",
      "Iteration 107, loss = 0.26454215\n",
      "Iteration 194, loss = 0.26085811\n",
      "Iteration 108, loss = 0.26453604\n",
      "Iteration 195, loss = 0.26030065\n",
      "Iteration 109, loss = 0.26407353\n",
      "Iteration 196, loss = 0.26020087\n",
      "Iteration 110, loss = 0.26467707\n",
      "Iteration 197, loss = 0.26063117\n",
      "Iteration 111, loss = 0.26400543\n",
      "Iteration 198, loss = 0.26004651\n",
      "Iteration 112, loss = 0.26375923\n",
      "Iteration 199, loss = 0.25975273\n",
      "Iteration 113, loss = 0.26358941\n",
      "Iteration 200, loss = 0.25971387\n",
      "Iteration 114, loss = 0.26398259\n",
      "Iteration 115, loss = 0.26387771\n",
      "Iteration 116, loss = 0.26357547\n",
      "Iteration 1, loss = 0.35828595\n",
      "Iteration 117, loss = 0.26388619\n",
      "Iteration 2, loss = 0.31758040\n",
      "Iteration 118, loss = 0.26382236\n",
      "Iteration 3, loss = 0.29776491\n",
      "Iteration 119, loss = 0.26312248\n",
      "Iteration 4, loss = 0.28975476\n",
      "Iteration 120, loss = 0.26299685\n",
      "Iteration 5, loss = 0.28621156\n",
      "Iteration 121, loss = 0.26298756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 0.28479087\n",
      "Iteration 122, loss = 0.26339423\n",
      "Iteration 7, loss = 0.28436739\n",
      "Iteration 123, loss = 0.26278313\n",
      "Iteration 124, loss = 0.26301732\n",
      "Iteration 8, loss = 0.28387082\n",
      "Iteration 125, loss = 0.26301252\n",
      "Iteration 126, loss = 0.26276999\n",
      "Iteration 9, loss = 0.28270173\n",
      "Iteration 127, loss = 0.26281913\n",
      "Iteration 128, loss = 0.26265734Iteration 10, loss = 0.28269090\n",
      "\n",
      "Iteration 11, loss = 0.28250339\n",
      "Iteration 129, loss = 0.26269845\n",
      "Iteration 12, loss = 0.28150865\n",
      "Iteration 13, loss = 0.28163058\n",
      "Iteration 130, loss = 0.26300527\n",
      "Iteration 14, loss = 0.28229335\n",
      "Iteration 131, loss = 0.26271569\n",
      "Iteration 15, loss = 0.28167950\n",
      "Iteration 132, loss = 0.26246488\n",
      "Iteration 16, loss = 0.28089342\n",
      "Iteration 133, loss = 0.26242395\n",
      "Iteration 17, loss = 0.27992420\n",
      "Iteration 134, loss = 0.26279164\n",
      "Iteration 18, loss = 0.27985636\n",
      "Iteration 135, loss = 0.26226559\n",
      "Iteration 19, loss = 0.27889726\n",
      "Iteration 136, loss = 0.26217243\n",
      "Iteration 20, loss = 0.27873619\n",
      "Iteration 137, loss = 0.26206495\n",
      "Iteration 21, loss = 0.27880068\n",
      "Iteration 138, loss = 0.26233976\n",
      "Iteration 22, loss = 0.27903441\n",
      "Iteration 139, loss = 0.26205069\n",
      "Iteration 23, loss = 0.27787486\n",
      "Iteration 140, loss = 0.26188348\n",
      "Iteration 24, loss = 0.27749385\n",
      "Iteration 141, loss = 0.26199531\n",
      "Iteration 25, loss = 0.27739395\n",
      "Iteration 142, loss = 0.26263952\n",
      "Iteration 26, loss = 0.27732339\n",
      "Iteration 143, loss = 0.26183677\n",
      "Iteration 27, loss = 0.27686455\n",
      "Iteration 144, loss = 0.26180827\n",
      "Iteration 28, loss = 0.27706839\n",
      "Iteration 145, loss = 0.26217632\n",
      "Iteration 29, loss = 0.27576890\n",
      "Iteration 146, loss = 0.26217489\n",
      "Iteration 30, loss = 0.27557221\n",
      "Iteration 147, loss = 0.26153281\n",
      "Iteration 31, loss = 0.27542875\n",
      "Iteration 148, loss = 0.26220141\n",
      "Iteration 32, loss = 0.27516219\n",
      "Iteration 149, loss = 0.26160069\n",
      "Iteration 33, loss = 0.27471144\n",
      "Iteration 150, loss = 0.26173498\n",
      "Iteration 34, loss = 0.27546143\n",
      "Iteration 151, loss = 0.26160262\n",
      "Iteration 35, loss = 0.27466792\n",
      "Iteration 36, loss = 0.27369506\n",
      "Iteration 152, loss = 0.26213894\n",
      "Iteration 37, loss = 0.27379549\n",
      "Iteration 153, loss = 0.26143417\n",
      "Iteration 38, loss = 0.27381846\n",
      "Iteration 154, loss = 0.26143577\n",
      "Iteration 39, loss = 0.27326277\n",
      "Iteration 155, loss = 0.26149036\n",
      "Iteration 40, loss = 0.27229951\n",
      "Iteration 156, loss = 0.26103063\n",
      "Iteration 41, loss = 0.27258995\n",
      "Iteration 157, loss = 0.26151600\n",
      "Iteration 42, loss = 0.27200013\n",
      "Iteration 43, loss = 0.27184550\n",
      "Iteration 158, loss = 0.26182343\n",
      "Iteration 44, loss = 0.27216276\n",
      "Iteration 159, loss = 0.26097471\n",
      "Iteration 160, loss = 0.26098460\n",
      "Iteration 45, loss = 0.27130937\n",
      "Iteration 161, loss = 0.26145527Iteration 46, loss = 0.27138284\n",
      "\n",
      "Iteration 47, loss = 0.27088955\n",
      "Iteration 162, loss = 0.26117433\n",
      "Iteration 163, loss = 0.26100967\n",
      "Iteration 48, loss = 0.27083987\n",
      "Iteration 164, loss = 0.26080899\n",
      "Iteration 49, loss = 0.27020189\n",
      "Iteration 165, loss = 0.26099775\n",
      "Iteration 50, loss = 0.27013140\n",
      "Iteration 51, loss = 0.26987467\n",
      "Iteration 166, loss = 0.26123677\n",
      "Iteration 52, loss = 0.26928310\n",
      "Iteration 53, loss = 0.26956864\n",
      "Iteration 167, loss = 0.26067468\n",
      "Iteration 54, loss = 0.26924525\n",
      "Iteration 55, loss = 0.26895608\n",
      "Iteration 168, loss = 0.26110006\n",
      "Iteration 56, loss = 0.26986573\n",
      "Iteration 57, loss = 0.26858946\n",
      "Iteration 169, loss = 0.26065409\n",
      "Iteration 58, loss = 0.26847944\n",
      "Iteration 59, loss = 0.26867740\n",
      "Iteration 170, loss = 0.26091063\n",
      "Iteration 171, loss = 0.26106121\n",
      "Iteration 60, loss = 0.26822392\n",
      "Iteration 172, loss = 0.26090230\n",
      "Iteration 61, loss = 0.26813247\n",
      "Iteration 173, loss = 0.26073253\n",
      "Iteration 62, loss = 0.26850529\n",
      "Iteration 174, loss = 0.26077001\n",
      "Iteration 63, loss = 0.26762795\n",
      "Iteration 175, loss = 0.26111952\n",
      "Iteration 64, loss = 0.26786352\n",
      "Iteration 176, loss = 0.26058856\n",
      "Iteration 65, loss = 0.26786149\n",
      "Iteration 66, loss = 0.26737888\n",
      "Iteration 177, loss = 0.26075483\n",
      "Iteration 67, loss = 0.26744606\n",
      "Iteration 178, loss = 0.26052572\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 68, loss = 0.26717579\n",
      "Iteration 69, loss = 0.26763605\n",
      "Iteration 1, loss = 0.35889183\n",
      "Iteration 70, loss = 0.26674851\n",
      "Iteration 2, loss = 0.31936100\n",
      "Iteration 71, loss = 0.26664606\n",
      "Iteration 3, loss = 0.30009623\n",
      "Iteration 72, loss = 0.26680588\n",
      "Iteration 4, loss = 0.29247453\n",
      "Iteration 73, loss = 0.26668302\n",
      "Iteration 5, loss = 0.28920762\n",
      "Iteration 6, loss = 0.28779086\n",
      "Iteration 74, loss = 0.26620887\n",
      "Iteration 7, loss = 0.28721835\n",
      "Iteration 75, loss = 0.26646417\n",
      "Iteration 8, loss = 0.28648555\n",
      "Iteration 76, loss = 0.26591604\n",
      "Iteration 77, loss = 0.26584835\n",
      "Iteration 9, loss = 0.28597018\n",
      "Iteration 78, loss = 0.26533088\n",
      "Iteration 10, loss = 0.28516264\n",
      "Iteration 79, loss = 0.26596746\n",
      "Iteration 11, loss = 0.28485778\n",
      "Iteration 80, loss = 0.26556453\n",
      "Iteration 12, loss = 0.28382687\n",
      "Iteration 81, loss = 0.26490521\n",
      "Iteration 13, loss = 0.28356555\n",
      "Iteration 14, loss = 0.28403784\n",
      "Iteration 82, loss = 0.26547686\n",
      "Iteration 15, loss = 0.28365231\n",
      "Iteration 83, loss = 0.26531292\n",
      "Iteration 16, loss = 0.28250865\n",
      "Iteration 17, loss = 0.28212666\n",
      "Iteration 18, loss = 0.28160173\n",
      "Iteration 84, loss = 0.26482069\n",
      "Iteration 19, loss = 0.28086370\n",
      "Iteration 85, loss = 0.26474374\n",
      "Iteration 20, loss = 0.28086449\n",
      "Iteration 86, loss = 0.26444897\n",
      "Iteration 21, loss = 0.28075744\n",
      "Iteration 87, loss = 0.26431031\n",
      "Iteration 22, loss = 0.28102764\n",
      "Iteration 88, loss = 0.26428714\n",
      "Iteration 23, loss = 0.28003163\n",
      "Iteration 89, loss = 0.26426999\n",
      "Iteration 24, loss = 0.27989298\n",
      "Iteration 90, loss = 0.26436631\n",
      "Iteration 25, loss = 0.27983832\n",
      "Iteration 91, loss = 0.26390497\n",
      "Iteration 26, loss = 0.27966357\n",
      "Iteration 92, loss = 0.26482675\n",
      "Iteration 27, loss = 0.27928964\n",
      "Iteration 28, loss = 0.27947520\n",
      "Iteration 93, loss = 0.26374676\n",
      "Iteration 29, loss = 0.27825358\n",
      "Iteration 94, loss = 0.26347162\n",
      "Iteration 30, loss = 0.27801795\n",
      "Iteration 95, loss = 0.26322956\n",
      "Iteration 31, loss = 0.27817907\n",
      "Iteration 96, loss = 0.26348227\n",
      "Iteration 32, loss = 0.27785292\n",
      "Iteration 97, loss = 0.26349772\n",
      "Iteration 33, loss = 0.27722469\n",
      "Iteration 98, loss = 0.26320536\n",
      "Iteration 34, loss = 0.27767059\n",
      "Iteration 99, loss = 0.26344999\n",
      "Iteration 35, loss = 0.27694133\n",
      "Iteration 100, loss = 0.26296420\n",
      "Iteration 36, loss = 0.27613034\n",
      "Iteration 101, loss = 0.26307777\n",
      "Iteration 37, loss = 0.27640634\n",
      "Iteration 102, loss = 0.26304359\n",
      "Iteration 38, loss = 0.27641778\n",
      "Iteration 39, loss = 0.27617358\n",
      "Iteration 103, loss = 0.26282995\n",
      "Iteration 40, loss = 0.27533717\n",
      "Iteration 104, loss = 0.26250607\n",
      "Iteration 41, loss = 0.27532174\n",
      "Iteration 105, loss = 0.26253487\n",
      "Iteration 42, loss = 0.27523992\n",
      "Iteration 106, loss = 0.26274965\n",
      "Iteration 43, loss = 0.27488004\n",
      "Iteration 107, loss = 0.26287450\n",
      "Iteration 44, loss = 0.27514627\n",
      "Iteration 108, loss = 0.26262604\n",
      "Iteration 109, loss = 0.26274952\n",
      "Iteration 110, loss = 0.26277238\n",
      "Iteration 45, loss = 0.27443590\n",
      "Iteration 111, loss = 0.26233972\n",
      "Iteration 46, loss = 0.27426255\n",
      "Iteration 112, loss = 0.26211655\n",
      "Iteration 47, loss = 0.27387196\n",
      "Iteration 113, loss = 0.26216763\n",
      "Iteration 48, loss = 0.27411704\n",
      "Iteration 114, loss = 0.26207961\n",
      "Iteration 49, loss = 0.27300267\n",
      "Iteration 115, loss = 0.26224572\n",
      "Iteration 116, loss = 0.26225089\n",
      "Iteration 50, loss = 0.27300617\n",
      "Iteration 51, loss = 0.27245731\n",
      "Iteration 117, loss = 0.26198013\n",
      "Iteration 118, loss = 0.26295506\n",
      "Iteration 52, loss = 0.27204878\n",
      "Iteration 119, loss = 0.26161708\n",
      "Iteration 53, loss = 0.27210959\n",
      "Iteration 120, loss = 0.26182499\n",
      "Iteration 54, loss = 0.27193665\n",
      "Iteration 121, loss = 0.26171968\n",
      "Iteration 55, loss = 0.27150798\n",
      "Iteration 122, loss = 0.26179695\n",
      "Iteration 123, loss = 0.26154311\n",
      "Iteration 56, loss = 0.27187956\n",
      "Iteration 124, loss = 0.26182046\n",
      "Iteration 57, loss = 0.27144284\n",
      "Iteration 125, loss = 0.26161344\n",
      "Iteration 58, loss = 0.27127318\n",
      "Iteration 126, loss = 0.26177739\n",
      "Iteration 59, loss = 0.27113086\n",
      "Iteration 127, loss = 0.26158962\n",
      "Iteration 60, loss = 0.27076686\n",
      "Iteration 128, loss = 0.26123062\n",
      "Iteration 61, loss = 0.27074009\n",
      "Iteration 129, loss = 0.26153884\n",
      "Iteration 62, loss = 0.27122808\n",
      "Iteration 130, loss = 0.26107331\n",
      "Iteration 131, loss = 0.26117717\n",
      "Iteration 63, loss = 0.27006859\n",
      "Iteration 132, loss = 0.26108403\n",
      "Iteration 64, loss = 0.27021132\n",
      "Iteration 133, loss = 0.26160525\n",
      "Iteration 65, loss = 0.26996796\n",
      "Iteration 66, loss = 0.26949111\n",
      "Iteration 134, loss = 0.26132036\n",
      "Iteration 135, loss = 0.26100727\n",
      "Iteration 67, loss = 0.27004855\n",
      "Iteration 68, loss = 0.26935008\n",
      "Iteration 136, loss = 0.26111332\n",
      "Iteration 69, loss = 0.26969513\n",
      "Iteration 137, loss = 0.26110037\n",
      "Iteration 70, loss = 0.26944279\n",
      "Iteration 138, loss = 0.26114570\n",
      "Iteration 71, loss = 0.26924196\n",
      "Iteration 139, loss = 0.26099247\n",
      "Iteration 72, loss = 0.26910908\n",
      "Iteration 140, loss = 0.26092888\n",
      "Iteration 73, loss = 0.26897397\n",
      "Iteration 141, loss = 0.26090529\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 74, loss = 0.26858823\n",
      "Iteration 75, loss = 0.26845267\n",
      "Iteration 1, loss = 0.35839349\n",
      "Iteration 76, loss = 0.26836568\n",
      "Iteration 77, loss = 0.26816706\n",
      "Iteration 2, loss = 0.31804719\n",
      "Iteration 78, loss = 0.26805323\n",
      "Iteration 3, loss = 0.29910566\n",
      "Iteration 79, loss = 0.26832105\n",
      "Iteration 4, loss = 0.29153921\n",
      "Iteration 80, loss = 0.26866466\n",
      "Iteration 5, loss = 0.28824561\n",
      "Iteration 81, loss = 0.26762603\n",
      "Iteration 6, loss = 0.28658175\n",
      "Iteration 82, loss = 0.26798918\n",
      "Iteration 7, loss = 0.28590952\n",
      "Iteration 83, loss = 0.26809834\n",
      "Iteration 8, loss = 0.28570213\n",
      "Iteration 84, loss = 0.26773107\n",
      "Iteration 9, loss = 0.28458030\n",
      "Iteration 85, loss = 0.26735351\n",
      "Iteration 86, loss = 0.26697508\n",
      "Iteration 10, loss = 0.28397121\n",
      "Iteration 87, loss = 0.26686155\n",
      "Iteration 11, loss = 0.28358735\n",
      "Iteration 88, loss = 0.26677379\n",
      "Iteration 89, loss = 0.26655783\n",
      "Iteration 12, loss = 0.28285707\n",
      "Iteration 90, loss = 0.26684606\n",
      "Iteration 13, loss = 0.28252519\n",
      "Iteration 91, loss = 0.26652076\n",
      "Iteration 14, loss = 0.28289385\n",
      "Iteration 92, loss = 0.26721466\n",
      "Iteration 15, loss = 0.28181010\n",
      "Iteration 93, loss = 0.26675415\n",
      "Iteration 94, loss = 0.26621188Iteration 16, loss = 0.28123507\n",
      "\n",
      "Iteration 17, loss = 0.28115503\n",
      "Iteration 95, loss = 0.26646813\n",
      "Iteration 18, loss = 0.28072128\n",
      "Iteration 96, loss = 0.26632139\n",
      "Iteration 19, loss = 0.27985084\n",
      "Iteration 20, loss = 0.27952358\n",
      "Iteration 97, loss = 0.26638800\n",
      "Iteration 21, loss = 0.27935236\n",
      "Iteration 98, loss = 0.26598693\n",
      "Iteration 22, loss = 0.27945524\n",
      "Iteration 99, loss = 0.26634217\n",
      "Iteration 23, loss = 0.27910657\n",
      "Iteration 100, loss = 0.26592330\n",
      "Iteration 24, loss = 0.27880167\n",
      "Iteration 101, loss = 0.26564610\n",
      "Iteration 102, loss = 0.26573761Iteration 25, loss = 0.27822551\n",
      "\n",
      "Iteration 103, loss = 0.26575513\n",
      "Iteration 26, loss = 0.27824255\n",
      "Iteration 104, loss = 0.26551268\n",
      "Iteration 27, loss = 0.27763621\n",
      "Iteration 105, loss = 0.26559668\n",
      "Iteration 28, loss = 0.27821077\n",
      "Iteration 106, loss = 0.26585812\n",
      "Iteration 29, loss = 0.27689582\n",
      "Iteration 107, loss = 0.26625158\n",
      "Iteration 30, loss = 0.27635660\n",
      "Iteration 108, loss = 0.26571192\n",
      "Iteration 31, loss = 0.27630099\n",
      "Iteration 109, loss = 0.26562452\n",
      "Iteration 32, loss = 0.27611202\n",
      "Iteration 110, loss = 0.26576704\n",
      "Iteration 33, loss = 0.27550983\n",
      "Iteration 111, loss = 0.26548407\n",
      "Iteration 34, loss = 0.27587903\n",
      "Iteration 112, loss = 0.26504859\n",
      "Iteration 35, loss = 0.27494036\n",
      "Iteration 36, loss = 0.27443032\n",
      "Iteration 113, loss = 0.26506540\n",
      "Iteration 114, loss = 0.26485765\n",
      "Iteration 37, loss = 0.27424807\n",
      "Iteration 38, loss = 0.27418722\n",
      "Iteration 115, loss = 0.26500016\n",
      "Iteration 39, loss = 0.27367139\n",
      "Iteration 116, loss = 0.26501606\n",
      "Iteration 40, loss = 0.27353804\n",
      "Iteration 117, loss = 0.26493843\n",
      "Iteration 118, loss = 0.26548505\n",
      "Iteration 41, loss = 0.27308563\n",
      "Iteration 119, loss = 0.26494440\n",
      "Iteration 42, loss = 0.27282625\n",
      "Iteration 120, loss = 0.26466318\n",
      "Iteration 43, loss = 0.27255871\n",
      "Iteration 121, loss = 0.26444937\n",
      "Iteration 44, loss = 0.27274548\n",
      "Iteration 122, loss = 0.26473314\n",
      "Iteration 45, loss = 0.27223923\n",
      "Iteration 123, loss = 0.26452343\n",
      "Iteration 46, loss = 0.27190732\n",
      "Iteration 124, loss = 0.26463433\n",
      "Iteration 47, loss = 0.27151084\n",
      "Iteration 125, loss = 0.26448988\n",
      "Iteration 48, loss = 0.27142367\n",
      "Iteration 126, loss = 0.26465353\n",
      "Iteration 49, loss = 0.27110461\n",
      "Iteration 127, loss = 0.26531127\n",
      "Iteration 128, loss = 0.26409494\n",
      "Iteration 50, loss = 0.27101221\n",
      "Iteration 51, loss = 0.27076767\n",
      "Iteration 129, loss = 0.26455113\n",
      "Iteration 52, loss = 0.27014739\n",
      "Iteration 130, loss = 0.26397789\n",
      "Iteration 53, loss = 0.27001122\n",
      "Iteration 131, loss = 0.26408131\n",
      "Iteration 54, loss = 0.27006960\n",
      "Iteration 132, loss = 0.26400099\n",
      "Iteration 55, loss = 0.26977413\n",
      "Iteration 56, loss = 0.26986367\n",
      "Iteration 133, loss = 0.26432218\n",
      "Iteration 134, loss = 0.26427603\n",
      "Iteration 57, loss = 0.26919561\n",
      "Iteration 135, loss = 0.26404632\n",
      "Iteration 136, loss = 0.26397018\n",
      "Iteration 58, loss = 0.26951153\n",
      "Iteration 137, loss = 0.26425151\n",
      "Iteration 59, loss = 0.26928846Iteration 138, loss = 0.26394111\n",
      "\n",
      "Iteration 139, loss = 0.26395410\n",
      "Iteration 60, loss = 0.26875143\n",
      "Iteration 140, loss = 0.26386589\n",
      "Iteration 61, loss = 0.26869106\n",
      "Iteration 141, loss = 0.26364306\n",
      "Iteration 62, loss = 0.26882572\n",
      "Iteration 142, loss = 0.26362985\n",
      "Iteration 63, loss = 0.26822702\n",
      "Iteration 143, loss = 0.26415786\n",
      "Iteration 64, loss = 0.26854496\n",
      "Iteration 144, loss = 0.26361886\n",
      "Iteration 65, loss = 0.26843905\n",
      "Iteration 145, loss = 0.26420981\n",
      "Iteration 66, loss = 0.26751985\n",
      "Iteration 146, loss = 0.26338059\n",
      "Iteration 67, loss = 0.26814518\n",
      "Iteration 147, loss = 0.26316424\n",
      "Iteration 68, loss = 0.26749283\n",
      "Iteration 69, loss = 0.26765601\n",
      "Iteration 148, loss = 0.26347219\n",
      "Iteration 70, loss = 0.26736861\n",
      "Iteration 149, loss = 0.26295855\n",
      "Iteration 71, loss = 0.26722052\n",
      "Iteration 150, loss = 0.26330525\n",
      "Iteration 72, loss = 0.26715766\n",
      "Iteration 151, loss = 0.26286395\n",
      "Iteration 73, loss = 0.26745378\n",
      "Iteration 152, loss = 0.26364775\n",
      "Iteration 74, loss = 0.26695394\n",
      "Iteration 153, loss = 0.26305859\n",
      "Iteration 75, loss = 0.26680324\n",
      "Iteration 154, loss = 0.26291311\n",
      "Iteration 76, loss = 0.26660577\n",
      "Iteration 155, loss = 0.26295535\n",
      "Iteration 77, loss = 0.26651550\n",
      "Iteration 156, loss = 0.26332858\n",
      "Iteration 78, loss = 0.26599446\n",
      "Iteration 157, loss = 0.26249939\n",
      "Iteration 79, loss = 0.26656040\n",
      "Iteration 158, loss = 0.26264666\n",
      "Iteration 80, loss = 0.26694378\n",
      "Iteration 159, loss = 0.26282167\n",
      "Iteration 81, loss = 0.26573304\n",
      "Iteration 160, loss = 0.26248794\n",
      "Iteration 82, loss = 0.26628229\n",
      "Iteration 161, loss = 0.26326360\n",
      "Iteration 162, loss = 0.26289238\n",
      "Iteration 83, loss = 0.26620800\n",
      "Iteration 84, loss = 0.26604845\n",
      "Iteration 163, loss = 0.26242721\n",
      "Iteration 85, loss = 0.26563335\n",
      "Iteration 164, loss = 0.26225923\n",
      "Iteration 86, loss = 0.26555869\n",
      "Iteration 165, loss = 0.26240782\n",
      "Iteration 166, loss = 0.26241305\n",
      "Iteration 87, loss = 0.26552168\n",
      "Iteration 167, loss = 0.26205266\n",
      "Iteration 88, loss = 0.26517096\n",
      "Iteration 89, loss = 0.26534050\n",
      "Iteration 168, loss = 0.26229121\n",
      "Iteration 90, loss = 0.26542505\n",
      "Iteration 169, loss = 0.26223099\n",
      "Iteration 91, loss = 0.26504124\n",
      "Iteration 170, loss = 0.26239216\n",
      "Iteration 92, loss = 0.26612482\n",
      "Iteration 171, loss = 0.26231411\n",
      "Iteration 93, loss = 0.26515380\n",
      "Iteration 172, loss = 0.26200260\n",
      "Iteration 94, loss = 0.26501688\n",
      "Iteration 173, loss = 0.26209283\n",
      "Iteration 174, loss = 0.26209605Iteration 95, loss = 0.26497704\n",
      "\n",
      "Iteration 96, loss = 0.26487341\n",
      "Iteration 175, loss = 0.26221256\n",
      "Iteration 97, loss = 0.26489836\n",
      "Iteration 176, loss = 0.26203305\n",
      "Iteration 98, loss = 0.26471427\n",
      "Iteration 177, loss = 0.26180451\n",
      "Iteration 99, loss = 0.26448646\n",
      "Iteration 178, loss = 0.26179313\n",
      "Iteration 100, loss = 0.26454494\n",
      "Iteration 179, loss = 0.26179347\n",
      "Iteration 101, loss = 0.26429181\n",
      "Iteration 180, loss = 0.26236448\n",
      "Iteration 102, loss = 0.26439603\n",
      "Iteration 181, loss = 0.26165164\n",
      "Iteration 103, loss = 0.26424290\n",
      "Iteration 182, loss = 0.26155800\n",
      "Iteration 104, loss = 0.26406262\n",
      "Iteration 183, loss = 0.26158114\n",
      "Iteration 105, loss = 0.26431016\n",
      "Iteration 184, loss = 0.26152147\n",
      "Iteration 106, loss = 0.26423805\n",
      "Iteration 107, loss = 0.26480804\n",
      "Iteration 185, loss = 0.26227057\n",
      "Iteration 108, loss = 0.26408336\n",
      "Iteration 186, loss = 0.26159333\n",
      "Iteration 109, loss = 0.26439178\n",
      "Iteration 110, loss = 0.26406991\n",
      "Iteration 187, loss = 0.26156276\n",
      "Iteration 111, loss = 0.26424912\n",
      "Iteration 188, loss = 0.26141096\n",
      "Iteration 112, loss = 0.26363634\n",
      "Iteration 189, loss = 0.26179355\n",
      "Iteration 113, loss = 0.26348794\n",
      "Iteration 190, loss = 0.26164760\n",
      "Iteration 114, loss = 0.26344353\n",
      "Iteration 191, loss = 0.26132249\n",
      "Iteration 115, loss = 0.26352746\n",
      "Iteration 192, loss = 0.26137834\n",
      "Iteration 116, loss = 0.26344031\n",
      "Iteration 193, loss = 0.26122189\n",
      "Iteration 117, loss = 0.26320746\n",
      "Iteration 194, loss = 0.26126830\n",
      "Iteration 118, loss = 0.26349310\n",
      "Iteration 195, loss = 0.26131849\n",
      "Iteration 119, loss = 0.26335729\n",
      "Iteration 196, loss = 0.26143696\n",
      "Iteration 120, loss = 0.26322369\n",
      "Iteration 121, loss = 0.26307619\n",
      "Iteration 197, loss = 0.26111618\n",
      "Iteration 122, loss = 0.26289682\n",
      "Iteration 198, loss = 0.26123338\n",
      "Iteration 123, loss = 0.26309061\n",
      "Iteration 199, loss = 0.26112690\n",
      "Iteration 124, loss = 0.26319913\n",
      "Iteration 200, loss = 0.26115757\n",
      "Iteration 125, loss = 0.26320584\n",
      "Iteration 126, loss = 0.26297304\n",
      "Iteration 127, loss = 0.26311179\n",
      "Iteration 128, loss = 0.26235095\n",
      "Iteration 129, loss = 0.26280470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 130, loss = 0.26223415\n",
      "Iteration 131, loss = 0.26232625\n",
      "Iteration 132, loss = 0.26247636\n",
      "Iteration 133, loss = 0.26386394\n",
      "Iteration 134, loss = 0.26225316\n",
      "Iteration 135, loss = 0.26239500\n",
      "Iteration 136, loss = 0.26227789\n",
      "Iteration 137, loss = 0.26257201\n",
      "Iteration 138, loss = 0.26230655\n",
      "Iteration 139, loss = 0.26219579\n",
      "Iteration 140, loss = 0.26232820\n",
      "Iteration 141, loss = 0.26172842\n",
      "Iteration 142, loss = 0.26169327\n",
      "Iteration 143, loss = 0.26225612\n",
      "Iteration 144, loss = 0.26211099\n",
      "Iteration 145, loss = 0.26241350\n",
      "Iteration 146, loss = 0.26172585\n",
      "Iteration 147, loss = 0.26168327\n",
      "Iteration 148, loss = 0.26199145\n",
      "Iteration 149, loss = 0.26178150\n",
      "Iteration 150, loss = 0.26154135\n",
      "Iteration 151, loss = 0.26145465\n",
      "Iteration 152, loss = 0.26162195\n",
      "Iteration 153, loss = 0.26166735\n",
      "Iteration 154, loss = 0.26173257\n",
      "Iteration 155, loss = 0.26220765\n",
      "Iteration 156, loss = 0.26156952\n",
      "Iteration 157, loss = 0.26141274\n",
      "Iteration 158, loss = 0.26130790\n",
      "Iteration 159, loss = 0.26133661\n",
      "Iteration 160, loss = 0.26114522\n",
      "Iteration 161, loss = 0.26153027\n",
      "Iteration 162, loss = 0.26165535\n",
      "Iteration 163, loss = 0.26131498\n",
      "Iteration 164, loss = 0.26096846\n",
      "Iteration 165, loss = 0.26119067\n",
      "Iteration 166, loss = 0.26137790\n",
      "Iteration 167, loss = 0.26138748\n",
      "Iteration 168, loss = 0.26139393\n",
      "Iteration 169, loss = 0.26113144\n",
      "Iteration 170, loss = 0.26113867\n",
      "Iteration 171, loss = 0.26088588\n",
      "Iteration 172, loss = 0.26076276\n",
      "Iteration 173, loss = 0.26094575\n",
      "Iteration 174, loss = 0.26107614\n",
      "Iteration 175, loss = 0.26115527\n",
      "Iteration 176, loss = 0.26089144\n",
      "Iteration 177, loss = 0.26087689\n",
      "Iteration 178, loss = 0.26079228\n",
      "Iteration 179, loss = 0.26046875\n",
      "Iteration 180, loss = 0.26076744\n",
      "Iteration 181, loss = 0.26061533\n",
      "Iteration 182, loss = 0.26078419\n",
      "Iteration 183, loss = 0.26066602\n",
      "Iteration 184, loss = 0.26039294\n",
      "Iteration 185, loss = 0.26137509\n",
      "Iteration 186, loss = 0.26051440\n",
      "Iteration 187, loss = 0.26063567\n",
      "Iteration 188, loss = 0.26039337\n",
      "Iteration 189, loss = 0.26074342\n",
      "Iteration 190, loss = 0.26067887\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34626699\n",
      "Iteration 2, loss = 0.30307907\n",
      "Iteration 1, loss = 0.34558556\n",
      "Iteration 3, loss = 0.28669090\n",
      "Iteration 2, loss = 0.30350961\n",
      "Iteration 4, loss = 0.28058603\n",
      "Iteration 5, loss = 0.27818065\n",
      "Iteration 3, loss = 0.28687896\n",
      "Iteration 4, loss = 0.28080629\n",
      "Iteration 6, loss = 0.27676458\n",
      "Iteration 5, loss = 0.27879538\n",
      "Iteration 7, loss = 0.27628134\n",
      "Iteration 6, loss = 0.27760990\n",
      "Iteration 8, loss = 0.27517324\n",
      "Iteration 7, loss = 0.27687929\n",
      "Iteration 9, loss = 0.27458299\n",
      "Iteration 8, loss = 0.27607472\n",
      "Iteration 10, loss = 0.27517792\n",
      "Iteration 9, loss = 0.27554611\n",
      "Iteration 11, loss = 0.27422304\n",
      "Iteration 10, loss = 0.27597166\n",
      "Iteration 12, loss = 0.27456986\n",
      "Iteration 11, loss = 0.27488213\n",
      "Iteration 13, loss = 0.27372290\n",
      "Iteration 12, loss = 0.27480355\n",
      "Iteration 14, loss = 0.27381527\n",
      "Iteration 13, loss = 0.27421178\n",
      "Iteration 15, loss = 0.27325889\n",
      "Iteration 14, loss = 0.27437125\n",
      "Iteration 16, loss = 0.27254099\n",
      "Iteration 15, loss = 0.27399086\n",
      "Iteration 17, loss = 0.27239864\n",
      "Iteration 16, loss = 0.27311147\n",
      "Iteration 18, loss = 0.27254828\n",
      "Iteration 17, loss = 0.27300861\n",
      "Iteration 19, loss = 0.27198504\n",
      "Iteration 18, loss = 0.27340992\n",
      "Iteration 20, loss = 0.27177100\n",
      "Iteration 21, loss = 0.27093048\n",
      "Iteration 19, loss = 0.27279344\n",
      "Iteration 22, loss = 0.27068035\n",
      "Iteration 20, loss = 0.27262811\n",
      "Iteration 23, loss = 0.27045178\n",
      "Iteration 21, loss = 0.27178277\n",
      "Iteration 24, loss = 0.27029284\n",
      "Iteration 22, loss = 0.27179347\n",
      "Iteration 25, loss = 0.27006368\n",
      "Iteration 23, loss = 0.27125842\n",
      "Iteration 26, loss = 0.26957116\n",
      "Iteration 24, loss = 0.27091973\n",
      "Iteration 27, loss = 0.26932326\n",
      "Iteration 25, loss = 0.27102401\n",
      "Iteration 28, loss = 0.26873627\n",
      "Iteration 26, loss = 0.27038213\n",
      "Iteration 29, loss = 0.26840624\n",
      "Iteration 30, loss = 0.26807830\n",
      "Iteration 27, loss = 0.27035698\n",
      "Iteration 28, loss = 0.26991481\n",
      "Iteration 31, loss = 0.26848889\n",
      "Iteration 29, loss = 0.26935908\n",
      "Iteration 32, loss = 0.26753178\n",
      "Iteration 30, loss = 0.26952435\n",
      "Iteration 33, loss = 0.26710573\n",
      "Iteration 31, loss = 0.26949391\n",
      "Iteration 34, loss = 0.26710428\n",
      "Iteration 32, loss = 0.26856515\n",
      "Iteration 35, loss = 0.26639715\n",
      "Iteration 33, loss = 0.26819692\n",
      "Iteration 34, loss = 0.26831092\n",
      "Iteration 35, loss = 0.26777557\n",
      "Iteration 36, loss = 0.26636475\n",
      "Iteration 37, loss = 0.26628698\n",
      "Iteration 36, loss = 0.26768770\n",
      "Iteration 37, loss = 0.26763363\n",
      "Iteration 38, loss = 0.26606882\n",
      "Iteration 39, loss = 0.26559554\n",
      "Iteration 38, loss = 0.26761369\n",
      "Iteration 40, loss = 0.26543262\n",
      "Iteration 39, loss = 0.26713227\n",
      "Iteration 40, loss = 0.26689385\n",
      "Iteration 41, loss = 0.26552843\n",
      "Iteration 41, loss = 0.26670998\n",
      "Iteration 42, loss = 0.26493342\n",
      "Iteration 43, loss = 0.26493730\n",
      "Iteration 42, loss = 0.26624908\n",
      "Iteration 44, loss = 0.26483079\n",
      "Iteration 43, loss = 0.26636851\n",
      "Iteration 45, loss = 0.26476313\n",
      "Iteration 46, loss = 0.26416539\n",
      "Iteration 47, loss = 0.26402361Iteration 44, loss = 0.26635580\n",
      "\n",
      "Iteration 45, loss = 0.26597146\n",
      "Iteration 48, loss = 0.26439370\n",
      "Iteration 46, loss = 0.26549726\n",
      "Iteration 47, loss = 0.26513072\n",
      "Iteration 49, loss = 0.26379599\n",
      "Iteration 48, loss = 0.26567573\n",
      "Iteration 50, loss = 0.26353602\n",
      "Iteration 49, loss = 0.26500526\n",
      "Iteration 51, loss = 0.26338306\n",
      "Iteration 50, loss = 0.26466929\n",
      "Iteration 52, loss = 0.26327479\n",
      "Iteration 53, loss = 0.26319237\n",
      "Iteration 51, loss = 0.26471351\n",
      "Iteration 54, loss = 0.26289514\n",
      "Iteration 52, loss = 0.26425290\n",
      "Iteration 55, loss = 0.26284323\n",
      "Iteration 53, loss = 0.26438414\n",
      "Iteration 56, loss = 0.26291689\n",
      "Iteration 54, loss = 0.26392764\n",
      "Iteration 57, loss = 0.26262263\n",
      "Iteration 55, loss = 0.26390206\n",
      "Iteration 58, loss = 0.26237312\n",
      "Iteration 56, loss = 0.26423121\n",
      "Iteration 59, loss = 0.26237456\n",
      "Iteration 57, loss = 0.26335181\n",
      "Iteration 60, loss = 0.26227200\n",
      "Iteration 58, loss = 0.26339979\n",
      "Iteration 61, loss = 0.26212529\n",
      "Iteration 59, loss = 0.26327056\n",
      "Iteration 62, loss = 0.26198834\n",
      "Iteration 60, loss = 0.26327288\n",
      "Iteration 63, loss = 0.26158791\n",
      "Iteration 61, loss = 0.26306674\n",
      "Iteration 64, loss = 0.26194405\n",
      "Iteration 62, loss = 0.26260922\n",
      "Iteration 65, loss = 0.26197385\n",
      "Iteration 63, loss = 0.26262016\n",
      "Iteration 66, loss = 0.26151863\n",
      "Iteration 64, loss = 0.26260884\n",
      "Iteration 65, loss = 0.26256121\n",
      "Iteration 67, loss = 0.26111449\n",
      "Iteration 66, loss = 0.26222747\n",
      "Iteration 68, loss = 0.26079235\n",
      "Iteration 69, loss = 0.26102465\n",
      "Iteration 67, loss = 0.26230804\n",
      "Iteration 70, loss = 0.26089261\n",
      "Iteration 68, loss = 0.26167936\n",
      "Iteration 71, loss = 0.26048190\n",
      "Iteration 69, loss = 0.26199200\n",
      "Iteration 72, loss = 0.26096205\n",
      "Iteration 70, loss = 0.26178585\n",
      "Iteration 73, loss = 0.26081937\n",
      "Iteration 71, loss = 0.26161735\n",
      "Iteration 74, loss = 0.26026214\n",
      "Iteration 72, loss = 0.26143299\n",
      "Iteration 75, loss = 0.26010033\n",
      "Iteration 73, loss = 0.26143620\n",
      "Iteration 76, loss = 0.26025664\n",
      "Iteration 74, loss = 0.26133002\n",
      "Iteration 77, loss = 0.26089361\n",
      "Iteration 75, loss = 0.26126920\n",
      "Iteration 78, loss = 0.26015950\n",
      "Iteration 76, loss = 0.26090269\n",
      "Iteration 79, loss = 0.25951584\n",
      "Iteration 77, loss = 0.26145921\n",
      "Iteration 80, loss = 0.25947804\n",
      "Iteration 81, loss = 0.25927834\n",
      "Iteration 78, loss = 0.26077609\n",
      "Iteration 82, loss = 0.25945846\n",
      "Iteration 83, loss = 0.25997733\n",
      "Iteration 79, loss = 0.26059775\n",
      "Iteration 84, loss = 0.25913804\n",
      "Iteration 80, loss = 0.26034437\n",
      "Iteration 81, loss = 0.26044371\n",
      "Iteration 82, loss = 0.26054664\n",
      "Iteration 85, loss = 0.25926194\n",
      "Iteration 86, loss = 0.25900859\n",
      "Iteration 83, loss = 0.26077088\n",
      "Iteration 87, loss = 0.25871750\n",
      "Iteration 84, loss = 0.26055866\n",
      "Iteration 88, loss = 0.25861347\n",
      "Iteration 85, loss = 0.26045231\n",
      "Iteration 89, loss = 0.25891913\n",
      "Iteration 86, loss = 0.26000243\n",
      "Iteration 90, loss = 0.25844458\n",
      "Iteration 87, loss = 0.25989582\n",
      "Iteration 91, loss = 0.25852080\n",
      "Iteration 88, loss = 0.25967377\n",
      "Iteration 92, loss = 0.25861333\n",
      "Iteration 89, loss = 0.25975762\n",
      "Iteration 93, loss = 0.25863668\n",
      "Iteration 90, loss = 0.25966990\n",
      "Iteration 94, loss = 0.25836751\n",
      "Iteration 91, loss = 0.25985956\n",
      "Iteration 95, loss = 0.25812114\n",
      "Iteration 92, loss = 0.25967823\n",
      "Iteration 96, loss = 0.25845399\n",
      "Iteration 93, loss = 0.25939630\n",
      "Iteration 97, loss = 0.25857137\n",
      "Iteration 94, loss = 0.25917240\n",
      "Iteration 98, loss = 0.25803599\n",
      "Iteration 95, loss = 0.25924389\n",
      "Iteration 99, loss = 0.25839184\n",
      "Iteration 100, loss = 0.25776889\n",
      "Iteration 96, loss = 0.25942279\n",
      "Iteration 101, loss = 0.25799536\n",
      "Iteration 97, loss = 0.25992236\n",
      "Iteration 98, loss = 0.25907604\n",
      "Iteration 102, loss = 0.25818680\n",
      "Iteration 103, loss = 0.25792508Iteration 99, loss = 0.25932149\n",
      "\n",
      "Iteration 100, loss = 0.25887952\n",
      "Iteration 104, loss = 0.25795797\n",
      "Iteration 101, loss = 0.25896609\n",
      "Iteration 105, loss = 0.25835325\n",
      "Iteration 102, loss = 0.25893177\n",
      "Iteration 106, loss = 0.25798699\n",
      "Iteration 103, loss = 0.25870497\n",
      "Iteration 107, loss = 0.25762308\n",
      "Iteration 104, loss = 0.25884030\n",
      "Iteration 108, loss = 0.25785331\n",
      "Iteration 105, loss = 0.25931474\n",
      "Iteration 109, loss = 0.25786448\n",
      "Iteration 106, loss = 0.25909478\n",
      "Iteration 110, loss = 0.25753279Iteration 107, loss = 0.25851096\n",
      "\n",
      "Iteration 111, loss = 0.25735107\n",
      "Iteration 108, loss = 0.25879346\n",
      "Iteration 112, loss = 0.25719939\n",
      "Iteration 109, loss = 0.25889216\n",
      "Iteration 110, loss = 0.25849368\n",
      "Iteration 113, loss = 0.25724919\n",
      "Iteration 111, loss = 0.25847654\n",
      "Iteration 114, loss = 0.25719099\n",
      "Iteration 115, loss = 0.25735541\n",
      "Iteration 112, loss = 0.25827155\n",
      "Iteration 116, loss = 0.25709905\n",
      "Iteration 113, loss = 0.25825635\n",
      "Iteration 117, loss = 0.25721025\n",
      "Iteration 114, loss = 0.25822276\n",
      "Iteration 118, loss = 0.25707867\n",
      "Iteration 115, loss = 0.25842150\n",
      "Iteration 119, loss = 0.25727315\n",
      "Iteration 116, loss = 0.25811237\n",
      "Iteration 120, loss = 0.25723948\n",
      "Iteration 117, loss = 0.25831174\n",
      "Iteration 121, loss = 0.25708553\n",
      "Iteration 118, loss = 0.25795450\n",
      "Iteration 122, loss = 0.25708449\n",
      "Iteration 119, loss = 0.25802147\n",
      "Iteration 123, loss = 0.25744028\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 120, loss = 0.25824930\n",
      "Iteration 121, loss = 0.25834413\n",
      "Iteration 1, loss = 0.34487404\n",
      "Iteration 122, loss = 0.25806320\n",
      "Iteration 2, loss = 0.30193991\n",
      "Iteration 123, loss = 0.25812025\n",
      "Iteration 3, loss = 0.28508029\n",
      "Iteration 124, loss = 0.25768127\n",
      "Iteration 4, loss = 0.27906967\n",
      "Iteration 125, loss = 0.25814191\n",
      "Iteration 126, loss = 0.25829058\n",
      "Iteration 5, loss = 0.27695481\n",
      "Iteration 127, loss = 0.25775488\n",
      "Iteration 6, loss = 0.27558518\n",
      "Iteration 128, loss = 0.25760905\n",
      "Iteration 7, loss = 0.27496163\n",
      "Iteration 129, loss = 0.25750066\n",
      "Iteration 130, loss = 0.25774958\n",
      "Iteration 8, loss = 0.27424096\n",
      "Iteration 131, loss = 0.25798446\n",
      "Iteration 9, loss = 0.27340556\n",
      "Iteration 132, loss = 0.25744772\n",
      "Iteration 10, loss = 0.27438753\n",
      "Iteration 133, loss = 0.25737686\n",
      "Iteration 11, loss = 0.27279061\n",
      "Iteration 134, loss = 0.25729920\n",
      "Iteration 12, loss = 0.27279272\n",
      "Iteration 135, loss = 0.25727120\n",
      "Iteration 13, loss = 0.27234938\n",
      "Iteration 136, loss = 0.25736093\n",
      "Iteration 14, loss = 0.27210360\n",
      "Iteration 137, loss = 0.25755071\n",
      "Iteration 15, loss = 0.27178381\n",
      "Iteration 138, loss = 0.25736615\n",
      "Iteration 16, loss = 0.27089362\n",
      "Iteration 139, loss = 0.25755227\n",
      "Iteration 17, loss = 0.27080862\n",
      "Iteration 140, loss = 0.25727090\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 18, loss = 0.27084612\n",
      "Iteration 19, loss = 0.27071780\n",
      "Iteration 1, loss = 0.34468834\n",
      "Iteration 20, loss = 0.27035668\n",
      "Iteration 2, loss = 0.30263633\n",
      "Iteration 21, loss = 0.26917375\n",
      "Iteration 3, loss = 0.28525440\n",
      "Iteration 22, loss = 0.26935711\n",
      "Iteration 4, loss = 0.27966410\n",
      "Iteration 23, loss = 0.26905190\n",
      "Iteration 5, loss = 0.27746186\n",
      "Iteration 24, loss = 0.26848985\n",
      "Iteration 6, loss = 0.27626118\n",
      "Iteration 25, loss = 0.26855118\n",
      "Iteration 7, loss = 0.27564543\n",
      "Iteration 26, loss = 0.26800997\n",
      "Iteration 8, loss = 0.27484071\n",
      "Iteration 27, loss = 0.26803767\n",
      "Iteration 9, loss = 0.27391712\n",
      "Iteration 28, loss = 0.26756227\n",
      "Iteration 10, loss = 0.27465230\n",
      "Iteration 29, loss = 0.26704826\n",
      "Iteration 11, loss = 0.27335815\n",
      "Iteration 30, loss = 0.26669915\n",
      "Iteration 12, loss = 0.27312074\n",
      "Iteration 31, loss = 0.26667196\n",
      "Iteration 13, loss = 0.27300771\n",
      "Iteration 32, loss = 0.26591902\n",
      "Iteration 14, loss = 0.27244211\n",
      "Iteration 33, loss = 0.26560676\n",
      "Iteration 15, loss = 0.27274322\n",
      "Iteration 34, loss = 0.26556672\n",
      "Iteration 16, loss = 0.27181702\n",
      "Iteration 35, loss = 0.26508173\n",
      "Iteration 17, loss = 0.27149722\n",
      "Iteration 36, loss = 0.26509180\n",
      "Iteration 18, loss = 0.27163934\n",
      "Iteration 37, loss = 0.26496723\n",
      "Iteration 19, loss = 0.27125636\n",
      "Iteration 38, loss = 0.26504952\n",
      "Iteration 20, loss = 0.27098287\n",
      "Iteration 39, loss = 0.26430097\n",
      "Iteration 21, loss = 0.26995043\n",
      "Iteration 40, loss = 0.26402092\n",
      "Iteration 22, loss = 0.27037779\n",
      "Iteration 41, loss = 0.26374414\n",
      "Iteration 23, loss = 0.26999353\n",
      "Iteration 42, loss = 0.26337814\n",
      "Iteration 24, loss = 0.26962184\n",
      "Iteration 43, loss = 0.26302082\n",
      "Iteration 25, loss = 0.26970584\n",
      "Iteration 44, loss = 0.26328006\n",
      "Iteration 26, loss = 0.26878557\n",
      "Iteration 45, loss = 0.26293376\n",
      "Iteration 27, loss = 0.26885289\n",
      "Iteration 46, loss = 0.26241949\n",
      "Iteration 28, loss = 0.26827124\n",
      "Iteration 47, loss = 0.26208594\n",
      "Iteration 29, loss = 0.26805333\n",
      "Iteration 30, loss = 0.26782010\n",
      "Iteration 48, loss = 0.26241512\n",
      "Iteration 31, loss = 0.26759598\n",
      "Iteration 49, loss = 0.26198601\n",
      "Iteration 32, loss = 0.26681435\n",
      "Iteration 33, loss = 0.26666873\n",
      "Iteration 50, loss = 0.26160063\n",
      "Iteration 34, loss = 0.26673379\n",
      "Iteration 51, loss = 0.26127790\n",
      "Iteration 52, loss = 0.26146023\n",
      "Iteration 35, loss = 0.26651002\n",
      "Iteration 53, loss = 0.26147609\n",
      "Iteration 54, loss = 0.26099582\n",
      "Iteration 36, loss = 0.26621868\n",
      "Iteration 37, loss = 0.26599225\n",
      "Iteration 55, loss = 0.26071181\n",
      "Iteration 38, loss = 0.26647854\n",
      "Iteration 56, loss = 0.26079623\n",
      "Iteration 39, loss = 0.26571807\n",
      "Iteration 57, loss = 0.26055261\n",
      "Iteration 58, loss = 0.26036813\n",
      "Iteration 40, loss = 0.26519126\n",
      "Iteration 41, loss = 0.26491020\n",
      "Iteration 59, loss = 0.26040679\n",
      "Iteration 42, loss = 0.26460444\n",
      "Iteration 43, loss = 0.26434959\n",
      "Iteration 60, loss = 0.26041279\n",
      "Iteration 44, loss = 0.26446956\n",
      "Iteration 61, loss = 0.26001312\n",
      "Iteration 45, loss = 0.26414379\n",
      "Iteration 62, loss = 0.25981244\n",
      "Iteration 46, loss = 0.26375430\n",
      "Iteration 63, loss = 0.25972617\n",
      "Iteration 47, loss = 0.26359084\n",
      "Iteration 64, loss = 0.25988255\n",
      "Iteration 48, loss = 0.26388164\n",
      "Iteration 65, loss = 0.25958884\n",
      "Iteration 49, loss = 0.26364327\n",
      "Iteration 66, loss = 0.25922272\n",
      "Iteration 50, loss = 0.26325479\n",
      "Iteration 51, loss = 0.26293549\n",
      "Iteration 67, loss = 0.25934042\n",
      "Iteration 52, loss = 0.26290769\n",
      "Iteration 68, loss = 0.25907694\n",
      "Iteration 53, loss = 0.26306182\n",
      "Iteration 69, loss = 0.25942845\n",
      "Iteration 54, loss = 0.26270663\n",
      "Iteration 70, loss = 0.25944878\n",
      "Iteration 71, loss = 0.25905117\n",
      "Iteration 55, loss = 0.26273736\n",
      "Iteration 56, loss = 0.26238559\n",
      "Iteration 72, loss = 0.25919948\n",
      "Iteration 57, loss = 0.26234650\n",
      "Iteration 73, loss = 0.25861002\n",
      "Iteration 58, loss = 0.26203256\n",
      "Iteration 74, loss = 0.25869793\n",
      "Iteration 59, loss = 0.26219234\n",
      "Iteration 75, loss = 0.25914019\n",
      "Iteration 60, loss = 0.26211932\n",
      "Iteration 76, loss = 0.25835163\n",
      "Iteration 61, loss = 0.26183846\n",
      "Iteration 77, loss = 0.25875098\n",
      "Iteration 78, loss = 0.25836743\n",
      "Iteration 62, loss = 0.26180332\n",
      "Iteration 79, loss = 0.25810339\n",
      "Iteration 80, loss = 0.25790253\n",
      "Iteration 81, loss = 0.25824118\n",
      "Iteration 63, loss = 0.26142446\n",
      "Iteration 82, loss = 0.25797735\n",
      "Iteration 64, loss = 0.26114080\n",
      "Iteration 83, loss = 0.25837563\n",
      "Iteration 65, loss = 0.26100436\n",
      "Iteration 84, loss = 0.25809523\n",
      "Iteration 66, loss = 0.26116172\n",
      "Iteration 85, loss = 0.25787556\n",
      "Iteration 67, loss = 0.26083599\n",
      "Iteration 86, loss = 0.25761860\n",
      "Iteration 68, loss = 0.26071719\n",
      "Iteration 87, loss = 0.25761803\n",
      "Iteration 69, loss = 0.26062826\n",
      "Iteration 88, loss = 0.25736604\n",
      "Iteration 70, loss = 0.26107258\n",
      "Iteration 89, loss = 0.25748572\n",
      "Iteration 71, loss = 0.26066984\n",
      "Iteration 90, loss = 0.25738797\n",
      "Iteration 72, loss = 0.26018540\n",
      "Iteration 91, loss = 0.25754116\n",
      "Iteration 73, loss = 0.25975590\n",
      "Iteration 92, loss = 0.25769473\n",
      "Iteration 74, loss = 0.25987218\n",
      "Iteration 93, loss = 0.25732781\n",
      "Iteration 75, loss = 0.26047798\n",
      "Iteration 94, loss = 0.25726269\n",
      "Iteration 76, loss = 0.25975854\n",
      "Iteration 77, loss = 0.25993656\n",
      "Iteration 95, loss = 0.25715611\n",
      "Iteration 78, loss = 0.25958555\n",
      "Iteration 96, loss = 0.25705938\n",
      "Iteration 79, loss = 0.25934338\n",
      "Iteration 97, loss = 0.25743508\n",
      "Iteration 80, loss = 0.25893090\n",
      "Iteration 98, loss = 0.25667274\n",
      "Iteration 81, loss = 0.25935799\n",
      "Iteration 99, loss = 0.25709904\n",
      "Iteration 82, loss = 0.25941645\n",
      "Iteration 100, loss = 0.25636682\n",
      "Iteration 83, loss = 0.25888502\n",
      "Iteration 101, loss = 0.25663561\n",
      "Iteration 84, loss = 0.25908029\n",
      "Iteration 102, loss = 0.25659993\n",
      "Iteration 85, loss = 0.25917148\n",
      "Iteration 103, loss = 0.25650259\n",
      "Iteration 86, loss = 0.25829112\n",
      "Iteration 104, loss = 0.25654963\n",
      "Iteration 87, loss = 0.25874964\n",
      "Iteration 105, loss = 0.25682992\n",
      "Iteration 88, loss = 0.25805102\n",
      "Iteration 106, loss = 0.25710334\n",
      "Iteration 89, loss = 0.25792152\n",
      "Iteration 107, loss = 0.25648879\n",
      "Iteration 90, loss = 0.25794894\n",
      "Iteration 108, loss = 0.25655870\n",
      "Iteration 91, loss = 0.25814630\n",
      "Iteration 109, loss = 0.25699856\n",
      "Iteration 92, loss = 0.25791608\n",
      "Iteration 110, loss = 0.25617653\n",
      "Iteration 93, loss = 0.25760218\n",
      "Iteration 111, loss = 0.25603929\n",
      "Iteration 94, loss = 0.25745367\n",
      "Iteration 112, loss = 0.25594738\n",
      "Iteration 95, loss = 0.25760583\n",
      "Iteration 113, loss = 0.25602041\n",
      "Iteration 96, loss = 0.25766548\n",
      "Iteration 114, loss = 0.25578756\n",
      "Iteration 97, loss = 0.25752815\n",
      "Iteration 115, loss = 0.25606522\n",
      "Iteration 116, loss = 0.25577911\n",
      "Iteration 98, loss = 0.25735911\n",
      "Iteration 117, loss = 0.25599016\n",
      "Iteration 118, loss = 0.25579078\n",
      "Iteration 99, loss = 0.25726255\n",
      "Iteration 119, loss = 0.25601771\n",
      "Iteration 100, loss = 0.25664528\n",
      "Iteration 120, loss = 0.25566208\n",
      "Iteration 101, loss = 0.25662581\n",
      "Iteration 121, loss = 0.25575343\n",
      "Iteration 102, loss = 0.25726671\n",
      "Iteration 122, loss = 0.25569105\n",
      "Iteration 103, loss = 0.25715697\n",
      "Iteration 123, loss = 0.25574180\n",
      "Iteration 124, loss = 0.25545655\n",
      "Iteration 104, loss = 0.25674175\n",
      "Iteration 105, loss = 0.25678092\n",
      "Iteration 125, loss = 0.25584999\n",
      "Iteration 126, loss = 0.25590225\n",
      "Iteration 106, loss = 0.25698016\n",
      "Iteration 107, loss = 0.25672201\n",
      "Iteration 127, loss = 0.25547411\n",
      "Iteration 108, loss = 0.25667008\n",
      "Iteration 109, loss = 0.25676698\n",
      "Iteration 128, loss = 0.25523123\n",
      "Iteration 110, loss = 0.25635792\n",
      "Iteration 129, loss = 0.25508140\n",
      "Iteration 111, loss = 0.25628991\n",
      "Iteration 130, loss = 0.25512978\n",
      "Iteration 112, loss = 0.25607377\n",
      "Iteration 131, loss = 0.25583842\n",
      "Iteration 113, loss = 0.25583151\n",
      "Iteration 132, loss = 0.25517331\n",
      "Iteration 114, loss = 0.25605383\n",
      "Iteration 133, loss = 0.25501794\n",
      "Iteration 115, loss = 0.25591966\n",
      "Iteration 116, loss = 0.25580121\n",
      "Iteration 134, loss = 0.25521762\n",
      "Iteration 135, loss = 0.25505688\n",
      "Iteration 117, loss = 0.25572780\n",
      "Iteration 136, loss = 0.25526481\n",
      "Iteration 118, loss = 0.25568719\n",
      "Iteration 137, loss = 0.25493877\n",
      "Iteration 119, loss = 0.25571816\n",
      "Iteration 138, loss = 0.25506023\n",
      "Iteration 120, loss = 0.25566328\n",
      "Iteration 139, loss = 0.25501436\n",
      "Iteration 140, loss = 0.25473407\n",
      "Iteration 121, loss = 0.25545671\n",
      "Iteration 141, loss = 0.25473448\n",
      "Iteration 122, loss = 0.25587327\n",
      "Iteration 142, loss = 0.25495513\n",
      "Iteration 143, loss = 0.25473421\n",
      "Iteration 123, loss = 0.25572977\n",
      "Iteration 144, loss = 0.25470035\n",
      "Iteration 124, loss = 0.25553922\n",
      "Iteration 145, loss = 0.25460740\n",
      "Iteration 125, loss = 0.25639480\n",
      "Iteration 146, loss = 0.25447593\n",
      "Iteration 126, loss = 0.25534740\n",
      "Iteration 147, loss = 0.25504101\n",
      "Iteration 127, loss = 0.25481355\n",
      "Iteration 148, loss = 0.25494483\n",
      "Iteration 128, loss = 0.25497518\n",
      "Iteration 149, loss = 0.25449457\n",
      "Iteration 129, loss = 0.25488371\n",
      "Iteration 150, loss = 0.25431941\n",
      "Iteration 130, loss = 0.25497652\n",
      "Iteration 151, loss = 0.25438670\n",
      "Iteration 131, loss = 0.25491960\n",
      "Iteration 132, loss = 0.25481749\n",
      "Iteration 152, loss = 0.25416067\n",
      "Iteration 133, loss = 0.25449862\n",
      "Iteration 153, loss = 0.25436313\n",
      "Iteration 134, loss = 0.25467300\n",
      "Iteration 154, loss = 0.25480903\n",
      "Iteration 135, loss = 0.25466703\n",
      "Iteration 155, loss = 0.25450861\n",
      "Iteration 156, loss = 0.25453799\n",
      "Iteration 136, loss = 0.25497241\n",
      "Iteration 157, loss = 0.25401683\n",
      "Iteration 137, loss = 0.25453924\n",
      "Iteration 158, loss = 0.25399885\n",
      "Iteration 138, loss = 0.25454881\n",
      "Iteration 159, loss = 0.25407228\n",
      "Iteration 139, loss = 0.25432663\n",
      "Iteration 160, loss = 0.25399532\n",
      "Iteration 140, loss = 0.25422956\n",
      "Iteration 161, loss = 0.25395631\n",
      "Iteration 141, loss = 0.25431657\n",
      "Iteration 162, loss = 0.25415700\n",
      "Iteration 142, loss = 0.25417979\n",
      "Iteration 163, loss = 0.25380819\n",
      "Iteration 143, loss = 0.25408504\n",
      "Iteration 164, loss = 0.25402321\n",
      "Iteration 144, loss = 0.25412087\n",
      "Iteration 165, loss = 0.25388769\n",
      "Iteration 145, loss = 0.25397661\n",
      "Iteration 166, loss = 0.25460633\n",
      "Iteration 146, loss = 0.25409125\n",
      "Iteration 167, loss = 0.25371964\n",
      "Iteration 147, loss = 0.25443169\n",
      "Iteration 168, loss = 0.25381217\n",
      "Iteration 148, loss = 0.25403506\n",
      "Iteration 169, loss = 0.25384965\n",
      "Iteration 149, loss = 0.25378711\n",
      "Iteration 170, loss = 0.25362979\n",
      "Iteration 150, loss = 0.25382228\n",
      "Iteration 171, loss = 0.25364209\n",
      "Iteration 151, loss = 0.25404393\n",
      "Iteration 172, loss = 0.25353760\n",
      "Iteration 152, loss = 0.25374241\n",
      "Iteration 173, loss = 0.25337131\n",
      "Iteration 153, loss = 0.25344660\n",
      "Iteration 174, loss = 0.25396598\n",
      "Iteration 154, loss = 0.25412617\n",
      "Iteration 175, loss = 0.25368958\n",
      "Iteration 155, loss = 0.25318926\n",
      "Iteration 176, loss = 0.25371642\n",
      "Iteration 156, loss = 0.25371129\n",
      "Iteration 177, loss = 0.25337183\n",
      "Iteration 157, loss = 0.25357085\n",
      "Iteration 178, loss = 0.25356638\n",
      "Iteration 158, loss = 0.25364971\n",
      "Iteration 179, loss = 0.25347099\n",
      "Iteration 159, loss = 0.25334811\n",
      "Iteration 180, loss = 0.25336208\n",
      "Iteration 160, loss = 0.25326558\n",
      "Iteration 181, loss = 0.25372569\n",
      "Iteration 161, loss = 0.25307507\n",
      "Iteration 182, loss = 0.25311451\n",
      "Iteration 162, loss = 0.25369774\n",
      "Iteration 183, loss = 0.25337793\n",
      "Iteration 163, loss = 0.25303186\n",
      "Iteration 184, loss = 0.25306657\n",
      "Iteration 164, loss = 0.25324749\n",
      "Iteration 185, loss = 0.25325158\n",
      "Iteration 165, loss = 0.25322184\n",
      "Iteration 166, loss = 0.25367927\n",
      "Iteration 186, loss = 0.25317679\n",
      "Iteration 167, loss = 0.25283682\n",
      "Iteration 187, loss = 0.25308087\n",
      "Iteration 168, loss = 0.25296973\n",
      "Iteration 188, loss = 0.25291057\n",
      "Iteration 169, loss = 0.25311813\n",
      "Iteration 189, loss = 0.25291938\n",
      "Iteration 170, loss = 0.25283720\n",
      "Iteration 171, loss = 0.25287051\n",
      "Iteration 190, loss = 0.25298065\n",
      "Iteration 172, loss = 0.25294611\n",
      "Iteration 191, loss = 0.25327823\n",
      "Iteration 173, loss = 0.25276301\n",
      "Iteration 192, loss = 0.25304393\n",
      "Iteration 174, loss = 0.25316890Iteration 193, loss = 0.25268933\n",
      "\n",
      "Iteration 175, loss = 0.25280889Iteration 194, loss = 0.25295136\n",
      "\n",
      "Iteration 176, loss = 0.25288474\n",
      "Iteration 195, loss = 0.25289604\n",
      "Iteration 177, loss = 0.25255450\n",
      "Iteration 196, loss = 0.25285587\n",
      "Iteration 178, loss = 0.25276756\n",
      "Iteration 197, loss = 0.25287723\n",
      "Iteration 179, loss = 0.25245712\n",
      "Iteration 198, loss = 0.25279438\n",
      "Iteration 180, loss = 0.25234740\n",
      "Iteration 199, loss = 0.25302463\n",
      "Iteration 181, loss = 0.25265939\n",
      "Iteration 200, loss = 0.25259882\n",
      "Iteration 182, loss = 0.25235200\n",
      "Iteration 183, loss = 0.25260855\n",
      "Iteration 184, loss = 0.25243550\n",
      "Iteration 185, loss = 0.25240678\n",
      "Iteration 1, loss = 0.34359442\n",
      "Iteration 186, loss = 0.25267490Iteration 2, loss = 0.30163693\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 0.28442376\n",
      "Iteration 187, loss = 0.25240071\n",
      "Iteration 4, loss = 0.27880211\n",
      "Iteration 188, loss = 0.25224395\n",
      "Iteration 5, loss = 0.27687733\n",
      "Iteration 189, loss = 0.25245287\n",
      "Iteration 6, loss = 0.27594403\n",
      "Iteration 190, loss = 0.25217543\n",
      "Iteration 7, loss = 0.27454385\n",
      "Iteration 191, loss = 0.25228350\n",
      "Iteration 8, loss = 0.27383780\n",
      "Iteration 192, loss = 0.25235913\n",
      "Iteration 9, loss = 0.27299305\n",
      "Iteration 193, loss = 0.25204387\n",
      "Iteration 10, loss = 0.27395897\n",
      "Iteration 194, loss = 0.25206508\n",
      "Iteration 11, loss = 0.27247606\n",
      "Iteration 195, loss = 0.25222496\n",
      "Iteration 12, loss = 0.27230725\n",
      "Iteration 196, loss = 0.25243399\n",
      "Iteration 13, loss = 0.27209881\n",
      "Iteration 197, loss = 0.25207777\n",
      "Iteration 14, loss = 0.27173439\n",
      "Iteration 198, loss = 0.25194960\n",
      "Iteration 15, loss = 0.27176147\n",
      "Iteration 199, loss = 0.25202929\n",
      "Iteration 16, loss = 0.27081752\n",
      "Iteration 200, loss = 0.25196464\n",
      "Iteration 17, loss = 0.27055191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, loss = 0.27043334\n",
      "Iteration 19, loss = 0.27014001\n",
      "Iteration 1, loss = 0.34363771\n",
      "Iteration 20, loss = 0.26994747\n",
      "Iteration 2, loss = 0.30135466\n",
      "Iteration 21, loss = 0.26907408\n",
      "Iteration 3, loss = 0.28422992\n",
      "Iteration 22, loss = 0.26920013\n",
      "Iteration 4, loss = 0.27895689\n",
      "Iteration 23, loss = 0.26874836\n",
      "Iteration 5, loss = 0.27705928\n",
      "Iteration 24, loss = 0.26869725\n",
      "Iteration 6, loss = 0.27548245\n",
      "Iteration 25, loss = 0.26822974\n",
      "Iteration 7, loss = 0.27462905\n",
      "Iteration 26, loss = 0.26719346\n",
      "Iteration 8, loss = 0.27400329\n",
      "Iteration 27, loss = 0.26705058\n",
      "Iteration 9, loss = 0.27325727\n",
      "Iteration 28, loss = 0.26648984\n",
      "Iteration 10, loss = 0.27437773\n",
      "Iteration 29, loss = 0.26631478\n",
      "Iteration 11, loss = 0.27262273\n",
      "Iteration 30, loss = 0.26589192\n",
      "Iteration 12, loss = 0.27249805\n",
      "Iteration 31, loss = 0.26575903\n",
      "Iteration 32, loss = 0.26497173\n",
      "Iteration 13, loss = 0.27202763\n",
      "Iteration 33, loss = 0.26472166\n",
      "Iteration 14, loss = 0.27188248\n",
      "Iteration 34, loss = 0.26473563\n",
      "Iteration 15, loss = 0.27173429\n",
      "Iteration 35, loss = 0.26423434\n",
      "Iteration 16, loss = 0.27099348\n",
      "Iteration 36, loss = 0.26414800\n",
      "Iteration 17, loss = 0.27089315\n",
      "Iteration 18, loss = 0.27056798\n",
      "Iteration 37, loss = 0.26362600\n",
      "Iteration 19, loss = 0.27001123\n",
      "Iteration 38, loss = 0.26390309\n",
      "Iteration 20, loss = 0.27054528\n",
      "Iteration 39, loss = 0.26311464\n",
      "Iteration 21, loss = 0.26956534\n",
      "Iteration 40, loss = 0.26260758\n",
      "Iteration 41, loss = 0.26238960\n",
      "Iteration 22, loss = 0.26984642\n",
      "Iteration 42, loss = 0.26216733\n",
      "Iteration 23, loss = 0.26882247\n",
      "Iteration 43, loss = 0.26209467\n",
      "Iteration 44, loss = 0.26202861\n",
      "Iteration 24, loss = 0.26941956\n",
      "Iteration 45, loss = 0.26144646\n",
      "Iteration 25, loss = 0.26883995\n",
      "Iteration 46, loss = 0.26111422\n",
      "Iteration 26, loss = 0.26782600\n",
      "Iteration 47, loss = 0.26096518\n",
      "Iteration 27, loss = 0.26841825\n",
      "Iteration 48, loss = 0.26111374\n",
      "Iteration 28, loss = 0.26733303\n",
      "Iteration 49, loss = 0.26115049\n",
      "Iteration 29, loss = 0.26685085\n",
      "Iteration 50, loss = 0.26076727\n",
      "Iteration 30, loss = 0.26681305\n",
      "Iteration 51, loss = 0.26045696\n",
      "Iteration 31, loss = 0.26660290\n",
      "Iteration 52, loss = 0.26038716\n",
      "Iteration 32, loss = 0.26631089\n",
      "Iteration 53, loss = 0.26036329\n",
      "Iteration 33, loss = 0.26578626\n",
      "Iteration 54, loss = 0.26023202\n",
      "Iteration 34, loss = 0.26597958\n",
      "Iteration 55, loss = 0.25988568\n",
      "Iteration 35, loss = 0.26533909\n",
      "Iteration 56, loss = 0.25952303\n",
      "Iteration 36, loss = 0.26530119\n",
      "Iteration 57, loss = 0.25930903\n",
      "Iteration 37, loss = 0.26496532\n",
      "Iteration 58, loss = 0.25918094\n",
      "Iteration 38, loss = 0.26474153\n",
      "Iteration 39, loss = 0.26463693\n",
      "Iteration 59, loss = 0.25932790\n",
      "Iteration 60, loss = 0.25902595\n",
      "Iteration 40, loss = 0.26391178\n",
      "Iteration 41, loss = 0.26403446Iteration 61, loss = 0.25914597\n",
      "\n",
      "Iteration 42, loss = 0.26368719\n",
      "Iteration 43, loss = 0.26375000\n",
      "Iteration 62, loss = 0.25960554\n",
      "Iteration 44, loss = 0.26352619\n",
      "Iteration 63, loss = 0.25886599\n",
      "Iteration 64, loss = 0.25843924\n",
      "Iteration 45, loss = 0.26327551\n",
      "Iteration 65, loss = 0.25810645\n",
      "Iteration 46, loss = 0.26291334\n",
      "Iteration 47, loss = 0.26268233\n",
      "Iteration 66, loss = 0.25831375\n",
      "Iteration 67, loss = 0.25816257\n",
      "Iteration 48, loss = 0.26272672\n",
      "Iteration 68, loss = 0.25802430\n",
      "Iteration 49, loss = 0.26276205\n",
      "Iteration 69, loss = 0.25817943\n",
      "Iteration 70, loss = 0.25812177\n",
      "Iteration 50, loss = 0.26223550\n",
      "Iteration 51, loss = 0.26222429\n",
      "Iteration 71, loss = 0.25784039\n",
      "Iteration 52, loss = 0.26208732\n",
      "Iteration 72, loss = 0.25714597\n",
      "Iteration 53, loss = 0.26226123\n",
      "Iteration 73, loss = 0.25718948\n",
      "Iteration 54, loss = 0.26197887\n",
      "Iteration 74, loss = 0.25716652\n",
      "Iteration 55, loss = 0.26162113\n",
      "Iteration 75, loss = 0.25762573\n",
      "Iteration 56, loss = 0.26130730\n",
      "Iteration 76, loss = 0.25700276\n",
      "Iteration 57, loss = 0.26131880\n",
      "Iteration 77, loss = 0.25701241\n",
      "Iteration 58, loss = 0.26094468\n",
      "Iteration 78, loss = 0.25719432\n",
      "Iteration 59, loss = 0.26101911\n",
      "Iteration 79, loss = 0.25682339\n",
      "Iteration 60, loss = 0.26059233\n",
      "Iteration 80, loss = 0.25652921\n",
      "Iteration 61, loss = 0.26091058\n",
      "Iteration 81, loss = 0.25678775\n",
      "Iteration 62, loss = 0.26032535\n",
      "Iteration 82, loss = 0.25683262\n",
      "Iteration 63, loss = 0.26024129\n",
      "Iteration 83, loss = 0.25635423\n",
      "Iteration 64, loss = 0.26002485\n",
      "Iteration 84, loss = 0.25653752\n",
      "Iteration 65, loss = 0.25965628\n",
      "Iteration 85, loss = 0.25684567\n",
      "Iteration 66, loss = 0.25961427\n",
      "Iteration 86, loss = 0.25599331\n",
      "Iteration 67, loss = 0.25964251\n",
      "Iteration 87, loss = 0.25658667\n",
      "Iteration 68, loss = 0.25932764\n",
      "Iteration 88, loss = 0.25588675\n",
      "Iteration 69, loss = 0.25953003\n",
      "Iteration 89, loss = 0.25592817\n",
      "Iteration 70, loss = 0.25932391\n",
      "Iteration 90, loss = 0.25607676\n",
      "Iteration 71, loss = 0.25936777\n",
      "Iteration 91, loss = 0.25632956\n",
      "Iteration 72, loss = 0.25832689\n",
      "Iteration 92, loss = 0.25578593\n",
      "Iteration 73, loss = 0.25860265\n",
      "Iteration 93, loss = 0.25549694\n",
      "Iteration 74, loss = 0.25875514\n",
      "Iteration 94, loss = 0.25603013\n",
      "Iteration 75, loss = 0.25882607\n",
      "Iteration 95, loss = 0.25549354\n",
      "Iteration 76, loss = 0.25833928\n",
      "Iteration 96, loss = 0.25583521\n",
      "Iteration 77, loss = 0.25821434\n",
      "Iteration 97, loss = 0.25539779\n",
      "Iteration 78, loss = 0.25782584\n",
      "Iteration 98, loss = 0.25506017\n",
      "Iteration 79, loss = 0.25813366\n",
      "Iteration 99, loss = 0.25543166\n",
      "Iteration 80, loss = 0.25769169\n",
      "Iteration 100, loss = 0.25494145\n",
      "Iteration 81, loss = 0.25798889\n",
      "Iteration 101, loss = 0.25489767\n",
      "Iteration 82, loss = 0.25802867\n",
      "Iteration 102, loss = 0.25529442\n",
      "Iteration 83, loss = 0.25771326\n",
      "Iteration 103, loss = 0.25547794\n",
      "Iteration 84, loss = 0.25794614\n",
      "Iteration 104, loss = 0.25496858\n",
      "Iteration 85, loss = 0.25746708\n",
      "Iteration 105, loss = 0.25529289\n",
      "Iteration 86, loss = 0.25710936\n",
      "Iteration 106, loss = 0.25534863\n",
      "Iteration 87, loss = 0.25744952\n",
      "Iteration 107, loss = 0.25508196\n",
      "Iteration 88, loss = 0.25711138\n",
      "Iteration 108, loss = 0.25511531\n",
      "Iteration 89, loss = 0.25685007\n",
      "Iteration 109, loss = 0.25560141\n",
      "Iteration 90, loss = 0.25719531\n",
      "Iteration 110, loss = 0.25441487\n",
      "Iteration 91, loss = 0.25715478\n",
      "Iteration 111, loss = 0.25455147\n",
      "Iteration 92, loss = 0.25656251\n",
      "Iteration 112, loss = 0.25438876\n",
      "Iteration 93, loss = 0.25618908\n",
      "Iteration 113, loss = 0.25413695\n",
      "Iteration 94, loss = 0.25693618\n",
      "Iteration 114, loss = 0.25429895\n",
      "Iteration 95, loss = 0.25650571\n",
      "Iteration 115, loss = 0.25412175\n",
      "Iteration 96, loss = 0.25657556\n",
      "Iteration 116, loss = 0.25403232\n",
      "Iteration 97, loss = 0.25652513\n",
      "Iteration 117, loss = 0.25372454\n",
      "Iteration 98, loss = 0.25669579\n",
      "Iteration 118, loss = 0.25446287\n",
      "Iteration 99, loss = 0.25649343\n",
      "Iteration 119, loss = 0.25409257\n",
      "Iteration 100, loss = 0.25623188\n",
      "Iteration 120, loss = 0.25400748\n",
      "Iteration 101, loss = 0.25606087\n",
      "Iteration 121, loss = 0.25431901\n",
      "Iteration 102, loss = 0.25661098\n",
      "Iteration 122, loss = 0.25434393\n",
      "Iteration 103, loss = 0.25628100\n",
      "Iteration 104, loss = 0.25588833Iteration 123, loss = 0.25399276\n",
      "\n",
      "Iteration 105, loss = 0.25584844\n",
      "Iteration 124, loss = 0.25409501\n",
      "Iteration 106, loss = 0.25626075\n",
      "Iteration 125, loss = 0.25557945\n",
      "Iteration 107, loss = 0.25591472\n",
      "Iteration 126, loss = 0.25355283\n",
      "Iteration 108, loss = 0.25588262\n",
      "Iteration 127, loss = 0.25348824\n",
      "Iteration 109, loss = 0.25639330\n",
      "Iteration 128, loss = 0.25382136\n",
      "Iteration 110, loss = 0.25596733\n",
      "Iteration 129, loss = 0.25330591\n",
      "Iteration 111, loss = 0.25551979\n",
      "Iteration 130, loss = 0.25331175\n",
      "Iteration 112, loss = 0.25554501\n",
      "Iteration 131, loss = 0.25340901\n",
      "Iteration 113, loss = 0.25555082\n",
      "Iteration 132, loss = 0.25358609\n",
      "Iteration 114, loss = 0.25519314\n",
      "Iteration 133, loss = 0.25328322\n",
      "Iteration 115, loss = 0.25508129\n",
      "Iteration 134, loss = 0.25334576\n",
      "Iteration 135, loss = 0.25330302\n",
      "Iteration 116, loss = 0.25520816\n",
      "Iteration 117, loss = 0.25515618\n",
      "Iteration 136, loss = 0.25361685\n",
      "Iteration 118, loss = 0.25516485\n",
      "Iteration 137, loss = 0.25362581\n",
      "Iteration 119, loss = 0.25587997\n",
      "Iteration 120, loss = 0.25490496\n",
      "Iteration 138, loss = 0.25375775\n",
      "Iteration 121, loss = 0.25483503\n",
      "Iteration 139, loss = 0.25306635\n",
      "Iteration 140, loss = 0.25339481\n",
      "Iteration 122, loss = 0.25504679\n",
      "Iteration 141, loss = 0.25314617\n",
      "Iteration 142, loss = 0.25291127\n",
      "Iteration 123, loss = 0.25496026\n",
      "Iteration 143, loss = 0.25286423\n",
      "Iteration 124, loss = 0.25495594\n",
      "Iteration 144, loss = 0.25327200\n",
      "Iteration 125, loss = 0.25608587\n",
      "Iteration 126, loss = 0.25473234\n",
      "Iteration 145, loss = 0.25265122\n",
      "Iteration 127, loss = 0.25459347\n",
      "Iteration 146, loss = 0.25295894\n",
      "Iteration 128, loss = 0.25465524\n",
      "Iteration 147, loss = 0.25373207\n",
      "Iteration 129, loss = 0.25431866\n",
      "Iteration 148, loss = 0.25324328\n",
      "Iteration 130, loss = 0.25467634\n",
      "Iteration 149, loss = 0.25276121\n",
      "Iteration 131, loss = 0.25422660\n",
      "Iteration 150, loss = 0.25312364\n",
      "Iteration 132, loss = 0.25453099\n",
      "Iteration 151, loss = 0.25291195\n",
      "Iteration 133, loss = 0.25437512\n",
      "Iteration 152, loss = 0.25299976\n",
      "Iteration 134, loss = 0.25428076\n",
      "Iteration 153, loss = 0.25258515\n",
      "Iteration 135, loss = 0.25430943\n",
      "Iteration 136, loss = 0.25463620\n",
      "Iteration 154, loss = 0.25291764\n",
      "Iteration 155, loss = 0.25244828\n",
      "Iteration 137, loss = 0.25439106\n",
      "Iteration 138, loss = 0.25474910\n",
      "Iteration 156, loss = 0.25276479\n",
      "Iteration 157, loss = 0.25258021\n",
      "Iteration 139, loss = 0.25414400\n",
      "Iteration 140, loss = 0.25432688\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 158, loss = 0.25256868\n",
      "Iteration 159, loss = 0.25250799\n",
      "Iteration 160, loss = 0.25246289\n",
      "Iteration 1, loss = 0.34350775\n",
      "Iteration 161, loss = 0.25241908\n",
      "Iteration 2, loss = 0.30223087\n",
      "Iteration 162, loss = 0.25277920\n",
      "Iteration 3, loss = 0.28565657\n",
      "Iteration 163, loss = 0.25241290\n",
      "Iteration 4, loss = 0.28053599\n",
      "Iteration 164, loss = 0.25245857\n",
      "Iteration 5, loss = 0.27835218\n",
      "Iteration 165, loss = 0.25227325\n",
      "Iteration 6, loss = 0.27649480\n",
      "Iteration 166, loss = 0.25263128\n",
      "Iteration 7, loss = 0.27592238\n",
      "Iteration 167, loss = 0.25236032\n",
      "Iteration 8, loss = 0.27547772\n",
      "Iteration 168, loss = 0.25249873\n",
      "Iteration 9, loss = 0.27461370\n",
      "Iteration 169, loss = 0.25253369\n",
      "Iteration 10, loss = 0.27552648\n",
      "Iteration 170, loss = 0.25211097\n",
      "Iteration 11, loss = 0.27420801\n",
      "Iteration 171, loss = 0.25227696\n",
      "Iteration 12, loss = 0.27365953\n",
      "Iteration 13, loss = 0.27349596\n",
      "Iteration 172, loss = 0.25265965\n",
      "Iteration 14, loss = 0.27348336\n",
      "Iteration 173, loss = 0.25209303\n",
      "Iteration 15, loss = 0.27366611\n",
      "Iteration 174, loss = 0.25270479\n",
      "Iteration 16, loss = 0.27252702\n",
      "Iteration 175, loss = 0.25238231\n",
      "Iteration 17, loss = 0.27204329\n",
      "Iteration 176, loss = 0.25212866\n",
      "Iteration 18, loss = 0.27174589\n",
      "Iteration 177, loss = 0.25202445\n",
      "Iteration 19, loss = 0.27145492\n",
      "Iteration 178, loss = 0.25214539\n",
      "Iteration 20, loss = 0.27145924\n",
      "Iteration 179, loss = 0.25190046\n",
      "Iteration 21, loss = 0.27070892\n",
      "Iteration 180, loss = 0.25192548\n",
      "Iteration 22, loss = 0.27050009\n",
      "Iteration 181, loss = 0.25213372\n",
      "Iteration 23, loss = 0.26994853\n",
      "Iteration 182, loss = 0.25202723\n",
      "Iteration 24, loss = 0.27002976\n",
      "Iteration 183, loss = 0.25204038\n",
      "Iteration 25, loss = 0.26975417\n",
      "Iteration 184, loss = 0.25166780\n",
      "Iteration 26, loss = 0.26882864\n",
      "Iteration 185, loss = 0.25172452\n",
      "Iteration 27, loss = 0.26891732\n",
      "Iteration 186, loss = 0.25199559\n",
      "Iteration 28, loss = 0.26834078\n",
      "Iteration 187, loss = 0.25205002\n",
      "Iteration 29, loss = 0.26783883\n",
      "Iteration 188, loss = 0.25174533\n",
      "Iteration 30, loss = 0.26734626\n",
      "Iteration 31, loss = 0.26725817\n",
      "Iteration 189, loss = 0.25193484\n",
      "Iteration 32, loss = 0.26695925\n",
      "Iteration 190, loss = 0.25167200\n",
      "Iteration 33, loss = 0.26680439\n",
      "Iteration 191, loss = 0.25175501\n",
      "Iteration 34, loss = 0.26634251\n",
      "Iteration 192, loss = 0.25198844\n",
      "Iteration 35, loss = 0.26622788\n",
      "Iteration 193, loss = 0.25159539\n",
      "Iteration 36, loss = 0.26581623\n",
      "Iteration 194, loss = 0.25146366\n",
      "Iteration 37, loss = 0.26537581\n",
      "Iteration 195, loss = 0.25151194\n",
      "Iteration 38, loss = 0.26504417\n",
      "Iteration 196, loss = 0.25154225\n",
      "Iteration 39, loss = 0.26452691\n",
      "Iteration 197, loss = 0.25160578\n",
      "Iteration 40, loss = 0.26427134\n",
      "Iteration 198, loss = 0.25121697\n",
      "Iteration 41, loss = 0.26409656\n",
      "Iteration 199, loss = 0.25154193\n",
      "Iteration 42, loss = 0.26371472\n",
      "Iteration 200, loss = 0.25147398\n",
      "Iteration 43, loss = 0.26389392\n",
      "Iteration 1, loss = 0.34305527\n",
      "Iteration 44, loss = 0.26353998"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 2, loss = 0.30175956\n",
      "Iteration 3, loss = 0.28578884\n",
      "Iteration 45, loss = 0.26337977\n",
      "Iteration 4, loss = 0.28065398\n",
      "Iteration 46, loss = 0.26284395\n",
      "Iteration 5, loss = 0.27824199\n",
      "Iteration 47, loss = 0.26249984\n",
      "Iteration 48, loss = 0.26258133\n",
      "Iteration 49, loss = 0.26271519Iteration 6, loss = 0.27734487\n",
      "\n",
      "Iteration 50, loss = 0.26195648\n",
      "Iteration 7, loss = 0.27613481\n",
      "Iteration 51, loss = 0.26197622\n",
      "Iteration 8, loss = 0.27555803\n",
      "Iteration 9, loss = 0.27550486\n",
      "Iteration 10, loss = 0.27486366Iteration 52, loss = 0.26200497\n",
      "\n",
      "Iteration 11, loss = 0.27459177Iteration 53, loss = 0.26181043\n",
      "\n",
      "Iteration 54, loss = 0.26199667\n",
      "Iteration 12, loss = 0.27412922\n",
      "Iteration 55, loss = 0.26161471\n",
      "Iteration 13, loss = 0.27461253\n",
      "Iteration 56, loss = 0.26097222\n",
      "Iteration 14, loss = 0.27386679\n",
      "Iteration 57, loss = 0.26084688\n",
      "Iteration 15, loss = 0.27371613\n",
      "Iteration 16, loss = 0.27291819\n",
      "Iteration 58, loss = 0.26065429\n",
      "Iteration 17, loss = 0.27268654\n",
      "Iteration 59, loss = 0.26074209\n",
      "Iteration 18, loss = 0.27270736\n",
      "Iteration 60, loss = 0.26064890\n",
      "Iteration 61, loss = 0.26069292\n",
      "Iteration 19, loss = 0.27174020\n",
      "Iteration 20, loss = 0.27148012\n",
      "Iteration 62, loss = 0.26022476\n",
      "Iteration 21, loss = 0.27127277\n",
      "Iteration 63, loss = 0.26020981\n",
      "Iteration 22, loss = 0.27062443\n",
      "Iteration 64, loss = 0.26006989\n",
      "Iteration 23, loss = 0.27089919\n",
      "Iteration 65, loss = 0.25996858\n",
      "Iteration 24, loss = 0.27057780\n",
      "Iteration 66, loss = 0.25961383\n",
      "Iteration 25, loss = 0.26986662\n",
      "Iteration 67, loss = 0.25984099\n",
      "Iteration 68, loss = 0.25935856\n",
      "Iteration 26, loss = 0.26916935\n",
      "Iteration 27, loss = 0.26915667\n",
      "Iteration 69, loss = 0.25983046\n",
      "Iteration 28, loss = 0.26864332\n",
      "Iteration 70, loss = 0.25972698\n",
      "Iteration 71, loss = 0.25945263\n",
      "Iteration 29, loss = 0.26862344\n",
      "Iteration 72, loss = 0.25915790\n",
      "Iteration 30, loss = 0.26852678\n",
      "Iteration 73, loss = 0.25923136\n",
      "Iteration 31, loss = 0.26859913\n",
      "Iteration 74, loss = 0.25942317\n",
      "Iteration 32, loss = 0.26788390\n",
      "Iteration 75, loss = 0.25977113\n",
      "Iteration 33, loss = 0.26722830\n",
      "Iteration 76, loss = 0.25911177\n",
      "Iteration 34, loss = 0.26678540\n",
      "Iteration 77, loss = 0.25870481\n",
      "Iteration 35, loss = 0.26676249\n",
      "Iteration 78, loss = 0.25847970\n",
      "Iteration 36, loss = 0.26680489\n",
      "Iteration 79, loss = 0.25897248\n",
      "Iteration 37, loss = 0.26659056\n",
      "Iteration 80, loss = 0.25873374\n",
      "Iteration 38, loss = 0.26631486\n",
      "Iteration 81, loss = 0.25857511\n",
      "Iteration 39, loss = 0.26683216\n",
      "Iteration 82, loss = 0.25867130\n",
      "Iteration 40, loss = 0.26571021\n",
      "Iteration 83, loss = 0.25811853\n",
      "Iteration 41, loss = 0.26503338\n",
      "Iteration 84, loss = 0.25867901\n",
      "Iteration 42, loss = 0.26513285\n",
      "Iteration 85, loss = 0.25844145\n",
      "Iteration 43, loss = 0.26455979\n",
      "Iteration 86, loss = 0.25813538\n",
      "Iteration 44, loss = 0.26455831\n",
      "Iteration 87, loss = 0.25850171\n",
      "Iteration 45, loss = 0.26410853\n",
      "Iteration 88, loss = 0.25809195\n",
      "Iteration 46, loss = 0.26381440\n",
      "Iteration 89, loss = 0.25794262\n",
      "Iteration 47, loss = 0.26388430\n",
      "Iteration 90, loss = 0.25856537\n",
      "Iteration 48, loss = 0.26347475\n",
      "Iteration 91, loss = 0.25817610\n",
      "Iteration 49, loss = 0.26344113\n",
      "Iteration 92, loss = 0.25787500\n",
      "Iteration 50, loss = 0.26411973\n",
      "Iteration 93, loss = 0.25754066\n",
      "Iteration 51, loss = 0.26313283\n",
      "Iteration 94, loss = 0.25757854\n",
      "Iteration 52, loss = 0.26288993\n",
      "Iteration 95, loss = 0.25747060\n",
      "Iteration 53, loss = 0.26288181\n",
      "Iteration 96, loss = 0.25821395\n",
      "Iteration 54, loss = 0.26233067\n",
      "Iteration 97, loss = 0.25736192\n",
      "Iteration 55, loss = 0.26232289\n",
      "Iteration 98, loss = 0.25793619\n",
      "Iteration 56, loss = 0.26207088\n",
      "Iteration 99, loss = 0.25745353\n",
      "Iteration 57, loss = 0.26185614\n",
      "Iteration 100, loss = 0.25730735\n",
      "Iteration 58, loss = 0.26185671\n",
      "Iteration 101, loss = 0.25714727\n",
      "Iteration 59, loss = 0.26167512\n",
      "Iteration 102, loss = 0.25786267\n",
      "Iteration 60, loss = 0.26160440\n",
      "Iteration 103, loss = 0.25768384\n",
      "Iteration 61, loss = 0.26140570\n",
      "Iteration 104, loss = 0.25694797\n",
      "Iteration 62, loss = 0.26114612\n",
      "Iteration 105, loss = 0.25677815\n",
      "Iteration 63, loss = 0.26104104\n",
      "Iteration 106, loss = 0.25722468\n",
      "Iteration 64, loss = 0.26111502\n",
      "Iteration 65, loss = 0.26054218\n",
      "Iteration 107, loss = 0.25708788\n",
      "Iteration 108, loss = 0.25684793\n",
      "Iteration 66, loss = 0.26056749\n",
      "Iteration 109, loss = 0.25716936\n",
      "Iteration 67, loss = 0.26052095\n",
      "Iteration 110, loss = 0.25669407\n",
      "Iteration 68, loss = 0.26045625\n",
      "Iteration 111, loss = 0.25659980\n",
      "Iteration 69, loss = 0.26054435\n",
      "Iteration 112, loss = 0.25663195\n",
      "Iteration 70, loss = 0.25998525\n",
      "Iteration 113, loss = 0.25670416\n",
      "Iteration 71, loss = 0.26028712\n",
      "Iteration 114, loss = 0.25655104\n",
      "Iteration 72, loss = 0.25985760\n",
      "Iteration 115, loss = 0.25624304\n",
      "Iteration 73, loss = 0.25982999\n",
      "Iteration 116, loss = 0.25644385\n",
      "Iteration 74, loss = 0.25986503\n",
      "Iteration 117, loss = 0.25639579\n",
      "Iteration 118, loss = 0.25603204\n",
      "Iteration 119, loss = 0.25603871\n",
      "Iteration 120, loss = 0.25604605\n",
      "Iteration 75, loss = 0.25948930\n",
      "Iteration 121, loss = 0.25636054\n",
      "Iteration 76, loss = 0.25955543\n",
      "Iteration 77, loss = 0.25911912\n",
      "Iteration 122, loss = 0.25591011\n",
      "Iteration 78, loss = 0.25916091\n",
      "Iteration 123, loss = 0.25588855\n",
      "Iteration 124, loss = 0.25616421\n",
      "Iteration 79, loss = 0.25933326\n",
      "Iteration 80, loss = 0.25976057\n",
      "Iteration 125, loss = 0.25665947\n",
      "Iteration 81, loss = 0.25883661\n",
      "Iteration 126, loss = 0.25589026\n",
      "Iteration 82, loss = 0.25924531\n",
      "Iteration 127, loss = 0.25559014\n",
      "Iteration 83, loss = 0.25872007\n",
      "Iteration 128, loss = 0.25570213\n",
      "Iteration 84, loss = 0.25848722\n",
      "Iteration 129, loss = 0.25544131\n",
      "Iteration 85, loss = 0.25862410\n",
      "Iteration 130, loss = 0.25566911\n",
      "Iteration 86, loss = 0.25859094\n",
      "Iteration 131, loss = 0.25529152\n",
      "Iteration 87, loss = 0.25849646\n",
      "Iteration 132, loss = 0.25551583\n",
      "Iteration 88, loss = 0.25847163\n",
      "Iteration 133, loss = 0.25525061\n",
      "Iteration 89, loss = 0.25818128\n",
      "Iteration 134, loss = 0.25540526\n",
      "Iteration 90, loss = 0.25805031\n",
      "Iteration 135, loss = 0.25514740\n",
      "Iteration 91, loss = 0.25808258\n",
      "Iteration 136, loss = 0.25541565\n",
      "Iteration 92, loss = 0.25808589\n",
      "Iteration 137, loss = 0.25509995\n",
      "Iteration 93, loss = 0.25820618\n",
      "Iteration 138, loss = 0.25570234\n",
      "Iteration 94, loss = 0.25791697\n",
      "Iteration 95, loss = 0.25758412\n",
      "Iteration 139, loss = 0.25496102\n",
      "Iteration 96, loss = 0.25803436\n",
      "Iteration 140, loss = 0.25528899\n",
      "Iteration 97, loss = 0.25744987\n",
      "Iteration 141, loss = 0.25502964\n",
      "Iteration 98, loss = 0.25763923\n",
      "Iteration 142, loss = 0.25492899\n",
      "Iteration 99, loss = 0.25734426\n",
      "Iteration 143, loss = 0.25476419\n",
      "Iteration 100, loss = 0.25723798\n",
      "Iteration 144, loss = 0.25520647\n",
      "Iteration 101, loss = 0.25710784\n",
      "Iteration 145, loss = 0.25473525\n",
      "Iteration 102, loss = 0.25782508\n",
      "Iteration 146, loss = 0.25462079\n",
      "Iteration 103, loss = 0.25699601\n",
      "Iteration 147, loss = 0.25509280\n",
      "Iteration 148, loss = 0.25466915Iteration 104, loss = 0.25732298\n",
      "\n",
      "Iteration 149, loss = 0.25458261\n",
      "Iteration 105, loss = 0.25702274\n",
      "Iteration 150, loss = 0.25469235\n",
      "Iteration 106, loss = 0.25711281\n",
      "Iteration 151, loss = 0.25438508\n",
      "Iteration 107, loss = 0.25661052\n",
      "Iteration 152, loss = 0.25451973\n",
      "Iteration 108, loss = 0.25669045\n",
      "Iteration 153, loss = 0.25418699\n",
      "Iteration 109, loss = 0.25706697\n",
      "Iteration 154, loss = 0.25441721\n",
      "Iteration 110, loss = 0.25644056\n",
      "Iteration 155, loss = 0.25409635\n",
      "Iteration 111, loss = 0.25700851\n",
      "Iteration 156, loss = 0.25442366\n",
      "Iteration 112, loss = 0.25641173\n",
      "Iteration 157, loss = 0.25425679\n",
      "Iteration 113, loss = 0.25629200\n",
      "Iteration 158, loss = 0.25428951\n",
      "Iteration 114, loss = 0.25638347\n",
      "Iteration 159, loss = 0.25407961\n",
      "Iteration 115, loss = 0.25632527\n",
      "Iteration 160, loss = 0.25439749\n",
      "Iteration 116, loss = 0.25634509\n",
      "Iteration 161, loss = 0.25466814\n",
      "Iteration 117, loss = 0.25628061\n",
      "Iteration 162, loss = 0.25424089\n",
      "Iteration 118, loss = 0.25590993\n",
      "Iteration 163, loss = 0.25402668\n",
      "Iteration 119, loss = 0.25591517\n",
      "Iteration 164, loss = 0.25431074\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 120, loss = 0.25614141\n",
      "Iteration 121, loss = 0.25611571\n",
      "Iteration 1, loss = 0.34229164\n",
      "Iteration 122, loss = 0.25584311\n",
      "Iteration 2, loss = 0.30099380\n",
      "Iteration 123, loss = 0.25602542\n",
      "Iteration 3, loss = 0.28475731\n",
      "Iteration 124, loss = 0.25553353\n",
      "Iteration 4, loss = 0.27966540\n",
      "Iteration 125, loss = 0.25619004\n",
      "Iteration 5, loss = 0.27722574\n",
      "Iteration 126, loss = 0.25576870\n",
      "Iteration 6, loss = 0.27612905\n",
      "Iteration 127, loss = 0.25550866\n",
      "Iteration 7, loss = 0.27491415\n",
      "Iteration 128, loss = 0.25556934\n",
      "Iteration 8, loss = 0.27421396\n",
      "Iteration 129, loss = 0.25563356\n",
      "Iteration 9, loss = 0.27398184\n",
      "Iteration 130, loss = 0.25557643\n",
      "Iteration 10, loss = 0.27371977\n",
      "Iteration 131, loss = 0.25524727\n",
      "Iteration 11, loss = 0.27325069\n",
      "Iteration 132, loss = 0.25517833\n",
      "Iteration 12, loss = 0.27284528\n",
      "Iteration 133, loss = 0.25549543\n",
      "Iteration 13, loss = 0.27329623\n",
      "Iteration 134, loss = 0.25551813\n",
      "Iteration 14, loss = 0.27233230\n",
      "Iteration 135, loss = 0.25543680\n",
      "Iteration 15, loss = 0.27251954\n",
      "Iteration 136, loss = 0.25514993\n",
      "Iteration 16, loss = 0.27168817\n",
      "Iteration 137, loss = 0.25498545\n",
      "Iteration 17, loss = 0.27114877\n",
      "Iteration 138, loss = 0.25494813\n",
      "Iteration 18, loss = 0.27158707\n",
      "Iteration 139, loss = 0.25533204\n",
      "Iteration 19, loss = 0.27067971\n",
      "Iteration 140, loss = 0.25524085\n",
      "Iteration 20, loss = 0.27047064\n",
      "Iteration 141, loss = 0.25481497\n",
      "Iteration 21, loss = 0.27014063\n",
      "Iteration 142, loss = 0.25496589\n",
      "Iteration 22, loss = 0.26983989\n",
      "Iteration 143, loss = 0.25477983\n",
      "Iteration 23, loss = 0.26957062\n",
      "Iteration 144, loss = 0.25481574\n",
      "Iteration 24, loss = 0.26963899\n",
      "Iteration 145, loss = 0.25447227\n",
      "Iteration 25, loss = 0.26898149\n",
      "Iteration 146, loss = 0.25449836\n",
      "Iteration 26, loss = 0.26856531\n",
      "Iteration 27, loss = 0.26870330\n",
      "Iteration 147, loss = 0.25474283\n",
      "Iteration 28, loss = 0.26832982\n",
      "Iteration 148, loss = 0.25469463\n",
      "Iteration 29, loss = 0.26816378\n",
      "Iteration 30, loss = 0.26793555\n",
      "Iteration 149, loss = 0.25472979\n",
      "Iteration 150, loss = 0.25447448\n",
      "Iteration 31, loss = 0.26795814\n",
      "Iteration 151, loss = 0.25438623\n",
      "Iteration 32, loss = 0.26763786\n",
      "Iteration 152, loss = 0.25451708\n",
      "Iteration 33, loss = 0.26723556\n",
      "Iteration 153, loss = 0.25424532\n",
      "Iteration 154, loss = 0.25448780\n",
      "Iteration 34, loss = 0.26693969\n",
      "Iteration 35, loss = 0.26691736\n",
      "Iteration 155, loss = 0.25482475\n",
      "Iteration 36, loss = 0.26664373\n",
      "Iteration 156, loss = 0.25400373\n",
      "Iteration 37, loss = 0.26702691\n",
      "Iteration 157, loss = 0.25409087\n",
      "Iteration 38, loss = 0.26631490\n",
      "Iteration 158, loss = 0.25436082\n",
      "Iteration 39, loss = 0.26628075\n",
      "Iteration 159, loss = 0.25416489\n",
      "Iteration 40, loss = 0.26605625\n",
      "Iteration 41, loss = 0.26553022\n",
      "Iteration 160, loss = 0.25382121\n",
      "Iteration 42, loss = 0.26549073\n",
      "Iteration 43, loss = 0.26535782\n",
      "Iteration 44, loss = 0.26506697\n",
      "Iteration 161, loss = 0.25384482\n",
      "Iteration 45, loss = 0.26486252\n",
      "Iteration 162, loss = 0.25374538\n",
      "Iteration 46, loss = 0.26449249\n",
      "Iteration 163, loss = 0.25380163\n",
      "Iteration 47, loss = 0.26438773\n",
      "Iteration 164, loss = 0.25420003\n",
      "Iteration 48, loss = 0.26405687\n",
      "Iteration 165, loss = 0.25369188\n",
      "Iteration 49, loss = 0.26413975\n",
      "Iteration 166, loss = 0.25357177\n",
      "Iteration 50, loss = 0.26416294\n",
      "Iteration 167, loss = 0.25356389\n",
      "Iteration 51, loss = 0.26373926\n",
      "Iteration 168, loss = 0.25348471\n",
      "Iteration 169, loss = 0.25408641\n",
      "Iteration 52, loss = 0.26376289\n",
      "Iteration 170, loss = 0.25339357\n",
      "Iteration 53, loss = 0.26367982\n",
      "Iteration 171, loss = 0.25365138\n",
      "Iteration 54, loss = 0.26307236\n",
      "Iteration 55, loss = 0.26289547\n",
      "Iteration 172, loss = 0.25383898\n",
      "Iteration 56, loss = 0.26279216\n",
      "Iteration 173, loss = 0.25334090\n",
      "Iteration 57, loss = 0.26255076\n",
      "Iteration 174, loss = 0.25318028\n",
      "Iteration 58, loss = 0.26242527\n",
      "Iteration 175, loss = 0.25376415\n",
      "Iteration 59, loss = 0.26226023\n",
      "Iteration 176, loss = 0.25344455\n",
      "Iteration 60, loss = 0.26215920\n",
      "Iteration 177, loss = 0.25306136\n",
      "Iteration 61, loss = 0.26209713\n",
      "Iteration 178, loss = 0.25332708Iteration 62, loss = 0.26206362\n",
      "\n",
      "Iteration 63, loss = 0.26164135\n",
      "Iteration 179, loss = 0.25320625\n",
      "Iteration 64, loss = 0.26166912\n",
      "Iteration 180, loss = 0.25304292\n",
      "Iteration 65, loss = 0.26113733\n",
      "Iteration 66, loss = 0.26125288\n",
      "Iteration 181, loss = 0.25307712\n",
      "Iteration 67, loss = 0.26113509\n",
      "Iteration 182, loss = 0.25309907\n",
      "Iteration 68, loss = 0.26108699\n",
      "Iteration 183, loss = 0.25302056\n",
      "Iteration 69, loss = 0.26092119\n",
      "Iteration 184, loss = 0.25276742\n",
      "Iteration 70, loss = 0.26079905\n",
      "Iteration 185, loss = 0.25312691\n",
      "Iteration 71, loss = 0.26104560\n",
      "Iteration 186, loss = 0.25320550\n",
      "Iteration 187, loss = 0.25310082\n",
      "Iteration 72, loss = 0.26071477\n",
      "Iteration 188, loss = 0.25271824\n",
      "Iteration 73, loss = 0.26068127\n",
      "Iteration 189, loss = 0.25314962\n",
      "Iteration 74, loss = 0.26048386\n",
      "Iteration 190, loss = 0.25300405\n",
      "Iteration 75, loss = 0.26042775\n",
      "Iteration 191, loss = 0.25269104\n",
      "Iteration 76, loss = 0.26045627\n",
      "Iteration 192, loss = 0.25328078\n",
      "Iteration 77, loss = 0.26042136\n",
      "Iteration 193, loss = 0.25293157\n",
      "Iteration 78, loss = 0.26016538\n",
      "Iteration 194, loss = 0.25281064\n",
      "Iteration 79, loss = 0.26015726\n",
      "Iteration 195, loss = 0.25288801\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 80, loss = 0.26045160\n",
      "Iteration 81, loss = 0.25990443\n",
      "Iteration 1, loss = 0.34277331\n",
      "Iteration 82, loss = 0.26003713\n",
      "Iteration 2, loss = 0.30225579\n",
      "Iteration 83, loss = 0.25990188\n",
      "Iteration 3, loss = 0.28628781\n",
      "Iteration 84, loss = 0.25948954\n",
      "Iteration 4, loss = 0.28091954\n",
      "Iteration 85, loss = 0.25958766\n",
      "Iteration 5, loss = 0.27831556\n",
      "Iteration 86, loss = 0.25974658\n",
      "Iteration 6, loss = 0.27730026\n",
      "Iteration 87, loss = 0.25952558\n",
      "Iteration 7, loss = 0.27605345\n",
      "Iteration 88, loss = 0.25992421\n",
      "Iteration 8, loss = 0.27548864\n",
      "Iteration 89, loss = 0.25945324\n",
      "Iteration 9, loss = 0.27510536\n",
      "Iteration 90, loss = 0.25939504\n",
      "Iteration 10, loss = 0.27518371\n",
      "Iteration 91, loss = 0.25926298\n",
      "Iteration 11, loss = 0.27422704\n",
      "Iteration 92, loss = 0.25928335\n",
      "Iteration 12, loss = 0.27408642\n",
      "Iteration 93, loss = 0.25925990\n",
      "Iteration 94, loss = 0.25896895\n",
      "Iteration 13, loss = 0.27461995\n",
      "Iteration 95, loss = 0.25925996\n",
      "Iteration 14, loss = 0.27320826\n",
      "Iteration 96, loss = 0.25973198\n",
      "Iteration 97, loss = 0.25888079\n",
      "Iteration 15, loss = 0.27370558\n",
      "Iteration 98, loss = 0.25916435\n",
      "Iteration 99, loss = 0.25873352Iteration 16, loss = 0.27281005\n",
      "\n",
      "Iteration 17, loss = 0.27230313\n",
      "Iteration 100, loss = 0.25874589\n",
      "Iteration 18, loss = 0.27276120\n",
      "Iteration 101, loss = 0.25861449\n",
      "Iteration 19, loss = 0.27148493\n",
      "Iteration 20, loss = 0.27120369\n",
      "Iteration 102, loss = 0.25887688\n",
      "Iteration 21, loss = 0.27072523\n",
      "Iteration 103, loss = 0.25849996Iteration 22, loss = 0.27068140\n",
      "\n",
      "Iteration 104, loss = 0.25875508\n",
      "Iteration 23, loss = 0.27022125\n",
      "Iteration 105, loss = 0.25841198\n",
      "Iteration 24, loss = 0.26967493\n",
      "Iteration 25, loss = 0.26935795\n",
      "Iteration 106, loss = 0.25838213\n",
      "Iteration 26, loss = 0.26894484\n",
      "Iteration 107, loss = 0.25830155\n",
      "Iteration 27, loss = 0.26874370\n",
      "Iteration 108, loss = 0.25824746\n",
      "Iteration 28, loss = 0.26840836\n",
      "Iteration 109, loss = 0.25858841\n",
      "Iteration 29, loss = 0.26819005\n",
      "Iteration 110, loss = 0.25800742\n",
      "Iteration 30, loss = 0.26778775\n",
      "Iteration 111, loss = 0.25829607\n",
      "Iteration 31, loss = 0.26789122\n",
      "Iteration 112, loss = 0.25797693\n",
      "Iteration 32, loss = 0.26751873\n",
      "Iteration 113, loss = 0.25821826\n",
      "Iteration 33, loss = 0.26688521\n",
      "Iteration 114, loss = 0.25806073\n",
      "Iteration 34, loss = 0.26647469\n",
      "Iteration 115, loss = 0.25785407\n",
      "Iteration 35, loss = 0.26639173\n",
      "Iteration 116, loss = 0.25790502\n",
      "Iteration 36, loss = 0.26623324\n",
      "Iteration 117, loss = 0.25801979\n",
      "Iteration 37, loss = 0.26610006\n",
      "Iteration 118, loss = 0.25797823\n",
      "Iteration 38, loss = 0.26558289\n",
      "Iteration 119, loss = 0.25756541\n",
      "Iteration 39, loss = 0.26594756\n",
      "Iteration 120, loss = 0.25778878\n",
      "Iteration 40, loss = 0.26561600\n",
      "Iteration 121, loss = 0.25795121\n",
      "Iteration 41, loss = 0.26472001\n",
      "Iteration 122, loss = 0.25760324\n",
      "Iteration 42, loss = 0.26483127\n",
      "Iteration 43, loss = 0.26457001\n",
      "Iteration 123, loss = 0.25767627\n",
      "Iteration 44, loss = 0.26450584\n",
      "Iteration 124, loss = 0.25733980\n",
      "Iteration 45, loss = 0.26430303\n",
      "Iteration 125, loss = 0.25801350\n",
      "Iteration 46, loss = 0.26373980\n",
      "Iteration 126, loss = 0.25766080\n",
      "Iteration 47, loss = 0.26402874\n",
      "Iteration 127, loss = 0.25753608\n",
      "Iteration 48, loss = 0.26337582\n",
      "Iteration 128, loss = 0.25734688\n",
      "Iteration 49, loss = 0.26356687\n",
      "Iteration 129, loss = 0.25728761\n",
      "Iteration 50, loss = 0.26365766\n",
      "Iteration 130, loss = 0.25740615\n",
      "Iteration 51, loss = 0.26300313\n",
      "Iteration 131, loss = 0.25714667\n",
      "Iteration 52, loss = 0.26291340\n",
      "Iteration 132, loss = 0.25696694\n",
      "Iteration 53, loss = 0.26253235\n",
      "Iteration 133, loss = 0.25727925\n",
      "Iteration 54, loss = 0.26273214\n",
      "Iteration 134, loss = 0.25731655\n",
      "Iteration 55, loss = 0.26211630\n",
      "Iteration 56, loss = 0.26211607\n",
      "Iteration 135, loss = 0.25736087\n",
      "Iteration 57, loss = 0.26197832\n",
      "Iteration 58, loss = 0.26157976\n",
      "Iteration 136, loss = 0.25704299\n",
      "Iteration 59, loss = 0.26182215\n",
      "Iteration 137, loss = 0.25689835\n",
      "Iteration 60, loss = 0.26128276\n",
      "Iteration 138, loss = 0.25697234\n",
      "Iteration 61, loss = 0.26147855\n",
      "Iteration 139, loss = 0.25670094\n",
      "Iteration 62, loss = 0.26123493\n",
      "Iteration 140, loss = 0.25690414\n",
      "Iteration 63, loss = 0.26113514\n",
      "Iteration 141, loss = 0.25700558\n",
      "Iteration 64, loss = 0.26093736\n",
      "Iteration 142, loss = 0.25651828\n",
      "Iteration 65, loss = 0.26039665\n",
      "Iteration 66, loss = 0.26086658\n",
      "Iteration 143, loss = 0.25663456\n",
      "Iteration 67, loss = 0.26059492\n",
      "Iteration 144, loss = 0.25692226\n",
      "Iteration 68, loss = 0.26025212\n",
      "Iteration 145, loss = 0.25672539\n",
      "Iteration 69, loss = 0.26036933\n",
      "Iteration 146, loss = 0.25682305\n",
      "Iteration 70, loss = 0.25977743\n",
      "Iteration 147, loss = 0.25638327\n",
      "Iteration 71, loss = 0.26019661\n",
      "Iteration 148, loss = 0.25649684\n",
      "Iteration 72, loss = 0.26017774\n",
      "Iteration 149, loss = 0.25678321\n",
      "Iteration 73, loss = 0.25971091\n",
      "Iteration 150, loss = 0.25614584\n",
      "Iteration 74, loss = 0.25985812\n",
      "Iteration 151, loss = 0.25639319\n",
      "Iteration 75, loss = 0.25965402\n",
      "Iteration 152, loss = 0.25636424\n",
      "Iteration 153, loss = 0.25625092\n",
      "Iteration 76, loss = 0.25951053\n",
      "Iteration 77, loss = 0.25961610\n",
      "Iteration 154, loss = 0.25619704\n",
      "Iteration 78, loss = 0.25938687\n",
      "Iteration 155, loss = 0.25649763\n",
      "Iteration 79, loss = 0.25936491\n",
      "Iteration 156, loss = 0.25608013\n",
      "Iteration 157, loss = 0.25591089\n",
      "Iteration 80, loss = 0.25925440\n",
      "Iteration 81, loss = 0.25905260\n",
      "Iteration 158, loss = 0.25638431\n",
      "Iteration 82, loss = 0.25890831\n",
      "Iteration 159, loss = 0.25585487\n",
      "Iteration 160, loss = 0.25602564\n",
      "Iteration 83, loss = 0.25875771\n",
      "Iteration 84, loss = 0.25848568\n",
      "Iteration 161, loss = 0.25582943\n",
      "Iteration 85, loss = 0.25893642\n",
      "Iteration 162, loss = 0.25584625\n",
      "Iteration 163, loss = 0.25579270\n",
      "Iteration 86, loss = 0.25868134\n",
      "Iteration 164, loss = 0.25602524\n",
      "Iteration 87, loss = 0.25869310\n",
      "Iteration 88, loss = 0.25851452\n",
      "Iteration 165, loss = 0.25564605\n",
      "Iteration 89, loss = 0.25836746\n",
      "Iteration 166, loss = 0.25558601\n",
      "Iteration 90, loss = 0.25797690\n",
      "Iteration 167, loss = 0.25569381\n",
      "Iteration 91, loss = 0.25808801\n",
      "Iteration 168, loss = 0.25538783\n",
      "Iteration 92, loss = 0.25825016\n",
      "Iteration 169, loss = 0.25610209\n",
      "Iteration 93, loss = 0.25815365\n",
      "Iteration 170, loss = 0.25533744\n",
      "Iteration 94, loss = 0.25787666\n",
      "Iteration 95, loss = 0.25756704\n",
      "Iteration 171, loss = 0.25545045\n",
      "Iteration 96, loss = 0.25815211\n",
      "Iteration 172, loss = 0.25554209Iteration 97, loss = 0.25766464\n",
      "\n",
      "Iteration 173, loss = 0.25546725\n",
      "Iteration 98, loss = 0.25782910\n",
      "Iteration 174, loss = 0.25519798\n",
      "Iteration 99, loss = 0.25761915\n",
      "Iteration 100, loss = 0.25740777\n",
      "Iteration 175, loss = 0.25535849\n",
      "Iteration 176, loss = 0.25538046\n",
      "Iteration 177, loss = 0.25520235\n",
      "Iteration 101, loss = 0.25775709\n",
      "Iteration 102, loss = 0.25734171\n",
      "Iteration 178, loss = 0.25516102\n",
      "Iteration 103, loss = 0.25712660\n",
      "Iteration 179, loss = 0.25525727\n",
      "Iteration 104, loss = 0.25775525\n",
      "Iteration 180, loss = 0.25498866\n",
      "Iteration 105, loss = 0.25709959\n",
      "Iteration 181, loss = 0.25514469\n",
      "Iteration 106, loss = 0.25705759\n",
      "Iteration 182, loss = 0.25501961\n",
      "Iteration 107, loss = 0.25692257\n",
      "Iteration 183, loss = 0.25507779\n",
      "Iteration 108, loss = 0.25707616\n",
      "Iteration 184, loss = 0.25497843\n",
      "Iteration 109, loss = 0.25712873\n",
      "Iteration 185, loss = 0.25519267\n",
      "Iteration 110, loss = 0.25678556\n",
      "Iteration 186, loss = 0.25505022\n",
      "Iteration 187, loss = 0.25487524\n",
      "Iteration 111, loss = 0.25694054\n",
      "Iteration 188, loss = 0.25474181\n",
      "Iteration 112, loss = 0.25672886\n",
      "Iteration 189, loss = 0.25490036\n",
      "Iteration 113, loss = 0.25689896\n",
      "Iteration 190, loss = 0.25503017\n",
      "Iteration 114, loss = 0.25674014\n",
      "Iteration 191, loss = 0.25478021\n",
      "Iteration 115, loss = 0.25676345\n",
      "Iteration 192, loss = 0.25510303\n",
      "Iteration 116, loss = 0.25660298\n",
      "Iteration 193, loss = 0.25467338\n",
      "Iteration 117, loss = 0.25654311\n",
      "Iteration 194, loss = 0.25517220\n",
      "Iteration 118, loss = 0.25644457\n",
      "Iteration 195, loss = 0.25470329\n",
      "Iteration 119, loss = 0.25650496\n",
      "Iteration 196, loss = 0.25456668\n",
      "Iteration 120, loss = 0.25652942\n",
      "Iteration 197, loss = 0.25481499\n",
      "Iteration 198, loss = 0.25463390\n",
      "Iteration 121, loss = 0.25686749\n",
      "Iteration 199, loss = 0.25450173\n",
      "Iteration 122, loss = 0.25632619\n",
      "Iteration 200, loss = 0.25469167\n",
      "Iteration 123, loss = 0.25626236\n",
      "Iteration 124, loss = 0.25623927\n",
      "Iteration 125, loss = 0.25663352\n",
      "Iteration 126, loss = 0.25611194\n",
      "Iteration 127, loss = 0.25593948\n",
      "Iteration 128, loss = 0.25590759\n",
      "Iteration 129, loss = 0.25592566\n",
      "Iteration 130, loss = 0.25615385\n",
      "Iteration 131, loss = 0.25595427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 132, loss = 0.25577715\n",
      "Iteration 133, loss = 0.25597092\n",
      "Iteration 134, loss = 0.25603750\n",
      "Iteration 135, loss = 0.25588979\n",
      "Iteration 136, loss = 0.25559458\n",
      "Iteration 137, loss = 0.25544651\n",
      "Iteration 138, loss = 0.25577258\n",
      "Iteration 139, loss = 0.25555301\n",
      "Iteration 140, loss = 0.25529924\n",
      "Iteration 141, loss = 0.25557770\n",
      "Iteration 142, loss = 0.25540782\n",
      "Iteration 143, loss = 0.25540953\n",
      "Iteration 144, loss = 0.25539046\n",
      "Iteration 145, loss = 0.25526144\n",
      "Iteration 146, loss = 0.25572321\n",
      "Iteration 147, loss = 0.25540646\n",
      "Iteration 148, loss = 0.25526826\n",
      "Iteration 149, loss = 0.25563208\n",
      "Iteration 150, loss = 0.25492807\n",
      "Iteration 151, loss = 0.25509555\n",
      "Iteration 152, loss = 0.25496269\n",
      "Iteration 153, loss = 0.25523993\n",
      "Iteration 154, loss = 0.25488387\n",
      "Iteration 155, loss = 0.25547556\n",
      "Iteration 156, loss = 0.25491317\n",
      "Iteration 157, loss = 0.25474553\n",
      "Iteration 158, loss = 0.25504331\n",
      "Iteration 159, loss = 0.25518583\n",
      "Iteration 160, loss = 0.25480711\n",
      "Iteration 161, loss = 0.25507048\n",
      "Iteration 162, loss = 0.25480977\n",
      "Iteration 163, loss = 0.25476906\n",
      "Iteration 164, loss = 0.25487632\n",
      "Iteration 165, loss = 0.25473010\n",
      "Iteration 166, loss = 0.25463355\n",
      "Iteration 167, loss = 0.25472699\n",
      "Iteration 168, loss = 0.25450384\n",
      "Iteration 169, loss = 0.25480052\n",
      "Iteration 170, loss = 0.25441606\n",
      "Iteration 171, loss = 0.25421504\n",
      "Iteration 172, loss = 0.25443069\n",
      "Iteration 173, loss = 0.25429950\n",
      "Iteration 174, loss = 0.25395054\n",
      "Iteration 175, loss = 0.25473665\n",
      "Iteration 176, loss = 0.25422174\n",
      "Iteration 177, loss = 0.25416819\n",
      "Iteration 178, loss = 0.25441032\n",
      "Iteration 179, loss = 0.25410065\n",
      "Iteration 180, loss = 0.25411282\n",
      "Iteration 181, loss = 0.25385552\n",
      "Iteration 182, loss = 0.25389709\n",
      "Iteration 183, loss = 0.25387450\n",
      "Iteration 184, loss = 0.25390213\n",
      "Iteration 185, loss = 0.25411526\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.35310636\n",
      "Iteration 1, loss = 0.35280442\n",
      "Iteration 2, loss = 0.30610841\n",
      "Iteration 2, loss = 0.30568947\n",
      "Iteration 3, loss = 0.29099192\n",
      "Iteration 3, loss = 0.28960579\n",
      "Iteration 4, loss = 0.28595438\n",
      "Iteration 4, loss = 0.28417506\n",
      "Iteration 5, loss = 0.28420194\n",
      "Iteration 5, loss = 0.28242862\n",
      "Iteration 6, loss = 0.28364142\n",
      "Iteration 6, loss = 0.28157522\n",
      "Iteration 7, loss = 0.28287638\n",
      "Iteration 7, loss = 0.28072638\n",
      "Iteration 8, loss = 0.28276909\n",
      "Iteration 8, loss = 0.28053606\n",
      "Iteration 9, loss = 0.28216689\n",
      "Iteration 10, loss = 0.28233262\n",
      "Iteration 9, loss = 0.28007330\n",
      "Iteration 11, loss = 0.28120039\n",
      "Iteration 10, loss = 0.27974861\n",
      "Iteration 11, loss = 0.27885757\n",
      "Iteration 12, loss = 0.28096792\n",
      "Iteration 12, loss = 0.27869254\n",
      "Iteration 13, loss = 0.28007923\n",
      "Iteration 14, loss = 0.28086013\n",
      "Iteration 13, loss = 0.27783742\n",
      "Iteration 15, loss = 0.28026248\n",
      "Iteration 14, loss = 0.27849437\n",
      "Iteration 16, loss = 0.27901400\n",
      "Iteration 15, loss = 0.27793662\n",
      "Iteration 17, loss = 0.27885679\n",
      "Iteration 16, loss = 0.27680656\n",
      "Iteration 18, loss = 0.27850747\n",
      "Iteration 17, loss = 0.27639068\n",
      "Iteration 19, loss = 0.27841199\n",
      "Iteration 18, loss = 0.27591852\n",
      "Iteration 20, loss = 0.27751882\n",
      "Iteration 19, loss = 0.27596391\n",
      "Iteration 21, loss = 0.27710712\n",
      "Iteration 22, loss = 0.27687303\n",
      "Iteration 20, loss = 0.27518497\n",
      "Iteration 23, loss = 0.27639442\n",
      "Iteration 21, loss = 0.27504657\n",
      "Iteration 24, loss = 0.27585347\n",
      "Iteration 22, loss = 0.27458544\n",
      "Iteration 25, loss = 0.27548378\n",
      "Iteration 23, loss = 0.27422071\n",
      "Iteration 26, loss = 0.27534969\n",
      "Iteration 24, loss = 0.27359905\n",
      "Iteration 27, loss = 0.27540762\n",
      "Iteration 25, loss = 0.27316201\n",
      "Iteration 28, loss = 0.27506737\n",
      "Iteration 26, loss = 0.27289767\n",
      "Iteration 29, loss = 0.27427238\n",
      "Iteration 27, loss = 0.27326441\n",
      "Iteration 30, loss = 0.27435794\n",
      "Iteration 28, loss = 0.27279189\n",
      "Iteration 31, loss = 0.27381385\n",
      "Iteration 29, loss = 0.27208485\n",
      "Iteration 32, loss = 0.27516127\n",
      "Iteration 30, loss = 0.27205784\n",
      "Iteration 33, loss = 0.27300598\n",
      "Iteration 31, loss = 0.27163358\n",
      "Iteration 34, loss = 0.27323731\n",
      "Iteration 32, loss = 0.27214217\n",
      "Iteration 35, loss = 0.27249526\n",
      "Iteration 33, loss = 0.27110163\n",
      "Iteration 36, loss = 0.27266052\n",
      "Iteration 34, loss = 0.27106298\n",
      "Iteration 37, loss = 0.27233487\n",
      "Iteration 35, loss = 0.27046146\n",
      "Iteration 38, loss = 0.27199441\n",
      "Iteration 36, loss = 0.27080772\n",
      "Iteration 39, loss = 0.27176049\n",
      "Iteration 37, loss = 0.27026525\n",
      "Iteration 40, loss = 0.27163554\n",
      "Iteration 41, loss = 0.27131820\n",
      "Iteration 38, loss = 0.26989315\n",
      "Iteration 42, loss = 0.27121091\n",
      "Iteration 39, loss = 0.26975574\n",
      "Iteration 43, loss = 0.27093604\n",
      "Iteration 40, loss = 0.26965712\n",
      "Iteration 44, loss = 0.27093803\n",
      "Iteration 41, loss = 0.26938774\n",
      "Iteration 45, loss = 0.27078596\n",
      "Iteration 42, loss = 0.26915269\n",
      "Iteration 46, loss = 0.27045447\n",
      "Iteration 43, loss = 0.26899239\n",
      "Iteration 47, loss = 0.27047581\n",
      "Iteration 44, loss = 0.26902041\n",
      "Iteration 48, loss = 0.27027702\n",
      "Iteration 45, loss = 0.26883016\n",
      "Iteration 49, loss = 0.27055214\n",
      "Iteration 46, loss = 0.26845747\n",
      "Iteration 50, loss = 0.27007597\n",
      "Iteration 47, loss = 0.26839568\n",
      "Iteration 51, loss = 0.26969590\n",
      "Iteration 48, loss = 0.26865878\n",
      "Iteration 52, loss = 0.26920569\n",
      "Iteration 49, loss = 0.26820269\n",
      "Iteration 50, loss = 0.26809267\n",
      "Iteration 53, loss = 0.26939548\n",
      "Iteration 51, loss = 0.26777382\n",
      "Iteration 54, loss = 0.26966861\n",
      "Iteration 52, loss = 0.26729120\n",
      "Iteration 53, loss = 0.26759859\n",
      "Iteration 55, loss = 0.26928999\n",
      "Iteration 54, loss = 0.26789743\n",
      "Iteration 56, loss = 0.26901687Iteration 55, loss = 0.26739210\n",
      "\n",
      "Iteration 56, loss = 0.26702494\n",
      "Iteration 57, loss = 0.26682315\n",
      "Iteration 57, loss = 0.26851216\n",
      "Iteration 58, loss = 0.26886281\n",
      "Iteration 58, loss = 0.26684585\n",
      "Iteration 59, loss = 0.26893926\n",
      "Iteration 59, loss = 0.26709591\n",
      "Iteration 60, loss = 0.26845231\n",
      "Iteration 60, loss = 0.26646910\n",
      "Iteration 61, loss = 0.26881761\n",
      "Iteration 61, loss = 0.26722965\n",
      "Iteration 62, loss = 0.26945940Iteration 62, loss = 0.26711403\n",
      "\n",
      "Iteration 63, loss = 0.26611596\n",
      "Iteration 63, loss = 0.26806930\n",
      "Iteration 64, loss = 0.26817352\n",
      "Iteration 64, loss = 0.26624284\n",
      "Iteration 65, loss = 0.26807007\n",
      "Iteration 65, loss = 0.26640876\n",
      "Iteration 66, loss = 0.26767248\n",
      "Iteration 66, loss = 0.26594723\n",
      "Iteration 67, loss = 0.26770082\n",
      "Iteration 67, loss = 0.26589157\n",
      "Iteration 68, loss = 0.26568443\n",
      "Iteration 68, loss = 0.26741615\n",
      "Iteration 69, loss = 0.26600509\n",
      "Iteration 69, loss = 0.26760807\n",
      "Iteration 70, loss = 0.26567871\n",
      "Iteration 70, loss = 0.26759061\n",
      "Iteration 71, loss = 0.26565411\n",
      "Iteration 71, loss = 0.26759101\n",
      "Iteration 72, loss = 0.26521619\n",
      "Iteration 72, loss = 0.26717467\n",
      "Iteration 73, loss = 0.26536441\n",
      "Iteration 73, loss = 0.26727515\n",
      "Iteration 74, loss = 0.26506580\n",
      "Iteration 74, loss = 0.26705245\n",
      "Iteration 75, loss = 0.26535597\n",
      "Iteration 75, loss = 0.26707745\n",
      "Iteration 76, loss = 0.26519575\n",
      "Iteration 76, loss = 0.26697284\n",
      "Iteration 77, loss = 0.26480216\n",
      "Iteration 77, loss = 0.26672925\n",
      "Iteration 78, loss = 0.26681010\n",
      "Iteration 78, loss = 0.26491845\n",
      "Iteration 79, loss = 0.26459993\n",
      "Iteration 80, loss = 0.26463136\n",
      "Iteration 79, loss = 0.26662603\n",
      "Iteration 81, loss = 0.26449665\n",
      "Iteration 80, loss = 0.26652484\n",
      "Iteration 82, loss = 0.26446484\n",
      "Iteration 81, loss = 0.26645310\n",
      "Iteration 83, loss = 0.26415211\n",
      "Iteration 82, loss = 0.26639775\n",
      "Iteration 84, loss = 0.26434726\n",
      "Iteration 83, loss = 0.26618825\n",
      "Iteration 85, loss = 0.26423373\n",
      "Iteration 84, loss = 0.26658897\n",
      "Iteration 86, loss = 0.26399948\n",
      "Iteration 85, loss = 0.26629274\n",
      "Iteration 87, loss = 0.26380459\n",
      "Iteration 86, loss = 0.26622240\n",
      "Iteration 88, loss = 0.26360818\n",
      "Iteration 87, loss = 0.26605554\n",
      "Iteration 89, loss = 0.26376232\n",
      "Iteration 88, loss = 0.26579567\n",
      "Iteration 90, loss = 0.26385479\n",
      "Iteration 89, loss = 0.26591249\n",
      "Iteration 91, loss = 0.26328255\n",
      "Iteration 90, loss = 0.26604042\n",
      "Iteration 92, loss = 0.26338243\n",
      "Iteration 91, loss = 0.26565755\n",
      "Iteration 92, loss = 0.26568493\n",
      "Iteration 93, loss = 0.26329746\n",
      "Iteration 93, loss = 0.26583148\n",
      "Iteration 94, loss = 0.26339397\n",
      "Iteration 94, loss = 0.26581542\n",
      "Iteration 95, loss = 0.26319221\n",
      "Iteration 96, loss = 0.26300120\n",
      "Iteration 95, loss = 0.26561197\n",
      "Iteration 97, loss = 0.26285508\n",
      "Iteration 96, loss = 0.26550962\n",
      "Iteration 98, loss = 0.26304356\n",
      "Iteration 97, loss = 0.26536637\n",
      "Iteration 99, loss = 0.26302510\n",
      "Iteration 98, loss = 0.26574138\n",
      "Iteration 100, loss = 0.26268197\n",
      "Iteration 99, loss = 0.26558579\n",
      "Iteration 101, loss = 0.26293800\n",
      "Iteration 100, loss = 0.26520404\n",
      "Iteration 102, loss = 0.26268520\n",
      "Iteration 101, loss = 0.26529759\n",
      "Iteration 103, loss = 0.26319374\n",
      "Iteration 102, loss = 0.26536263\n",
      "Iteration 104, loss = 0.26244376\n",
      "Iteration 103, loss = 0.26561742\n",
      "Iteration 105, loss = 0.26284576\n",
      "Iteration 104, loss = 0.26499168\n",
      "Iteration 106, loss = 0.26315329\n",
      "Iteration 105, loss = 0.26532848\n",
      "Iteration 107, loss = 0.26252021\n",
      "Iteration 106, loss = 0.26556395\n",
      "Iteration 108, loss = 0.26227997\n",
      "Iteration 107, loss = 0.26497915\n",
      "Iteration 109, loss = 0.26207998\n",
      "Iteration 108, loss = 0.26502998\n",
      "Iteration 110, loss = 0.26211651\n",
      "Iteration 109, loss = 0.26501903\n",
      "Iteration 111, loss = 0.26199485\n",
      "Iteration 110, loss = 0.26470665\n",
      "Iteration 112, loss = 0.26220765\n",
      "Iteration 111, loss = 0.26459978\n",
      "Iteration 113, loss = 0.26222998\n",
      "Iteration 114, loss = 0.26218159\n",
      "Iteration 112, loss = 0.26504594Iteration 115, loss = 0.26192617\n",
      "\n",
      "Iteration 116, loss = 0.26210043\n",
      "Iteration 113, loss = 0.26464990\n",
      "Iteration 114, loss = 0.26467713\n",
      "Iteration 115, loss = 0.26455324\n",
      "Iteration 117, loss = 0.26197478\n",
      "Iteration 118, loss = 0.26195896\n",
      "Iteration 116, loss = 0.26435429\n",
      "Iteration 119, loss = 0.26155555\n",
      "Iteration 117, loss = 0.26425311\n",
      "Iteration 120, loss = 0.26199668\n",
      "Iteration 121, loss = 0.26177467\n",
      "Iteration 118, loss = 0.26445896\n",
      "Iteration 122, loss = 0.26151951\n",
      "Iteration 119, loss = 0.26424804\n",
      "Iteration 120, loss = 0.26425911\n",
      "Iteration 123, loss = 0.26156845\n",
      "Iteration 121, loss = 0.26418219\n",
      "Iteration 124, loss = 0.26211195\n",
      "Iteration 122, loss = 0.26404047\n",
      "Iteration 125, loss = 0.26142667\n",
      "Iteration 126, loss = 0.26140944\n",
      "Iteration 123, loss = 0.26412396\n",
      "Iteration 127, loss = 0.26138224\n",
      "Iteration 124, loss = 0.26420846\n",
      "Iteration 128, loss = 0.26138863\n",
      "Iteration 125, loss = 0.26402331\n",
      "Iteration 129, loss = 0.26122558\n",
      "Iteration 126, loss = 0.26400952\n",
      "Iteration 130, loss = 0.26126928\n",
      "Iteration 127, loss = 0.26395499\n",
      "Iteration 131, loss = 0.26102548\n",
      "Iteration 128, loss = 0.26392870\n",
      "Iteration 132, loss = 0.26117153\n",
      "Iteration 129, loss = 0.26377793\n",
      "Iteration 133, loss = 0.26115049\n",
      "Iteration 130, loss = 0.26410415\n",
      "Iteration 134, loss = 0.26094947\n",
      "Iteration 131, loss = 0.26362987\n",
      "Iteration 135, loss = 0.26090090\n",
      "Iteration 132, loss = 0.26356882\n",
      "Iteration 136, loss = 0.26099502\n",
      "Iteration 133, loss = 0.26384572\n",
      "Iteration 137, loss = 0.26083653\n",
      "Iteration 134, loss = 0.26350765\n",
      "Iteration 138, loss = 0.26080006\n",
      "Iteration 135, loss = 0.26341503\n",
      "Iteration 139, loss = 0.26060458\n",
      "Iteration 136, loss = 0.26355367\n",
      "Iteration 140, loss = 0.26090099\n",
      "Iteration 137, loss = 0.26350087\n",
      "Iteration 138, loss = 0.26346475\n",
      "Iteration 141, loss = 0.26079702\n",
      "Iteration 142, loss = 0.26052025\n",
      "Iteration 139, loss = 0.26321646\n",
      "Iteration 140, loss = 0.26334326\n",
      "Iteration 143, loss = 0.26071271\n",
      "Iteration 144, loss = 0.26065034\n",
      "Iteration 141, loss = 0.26376691\n",
      "Iteration 145, loss = 0.26048911\n",
      "Iteration 142, loss = 0.26315887\n",
      "Iteration 146, loss = 0.26019451\n",
      "Iteration 147, loss = 0.26048043\n",
      "Iteration 143, loss = 0.26339465\n",
      "Iteration 148, loss = 0.26014011\n",
      "Iteration 144, loss = 0.26310824\n",
      "Iteration 149, loss = 0.26046015\n",
      "Iteration 145, loss = 0.26364663\n",
      "Iteration 150, loss = 0.26015834\n",
      "Iteration 146, loss = 0.26296536\n",
      "Iteration 151, loss = 0.26000359\n",
      "Iteration 152, loss = 0.25982486\n",
      "Iteration 147, loss = 0.26289708\n",
      "Iteration 153, loss = 0.26036069\n",
      "Iteration 148, loss = 0.26290579\n",
      "Iteration 154, loss = 0.26013823\n",
      "Iteration 149, loss = 0.26337164\n",
      "Iteration 155, loss = 0.25979124\n",
      "Iteration 150, loss = 0.26276985\n",
      "Iteration 156, loss = 0.25980019\n",
      "Iteration 151, loss = 0.26291773\n",
      "Iteration 157, loss = 0.25970637\n",
      "Iteration 158, loss = 0.25968065\n",
      "Iteration 152, loss = 0.26276665\n",
      "Iteration 153, loss = 0.26313547\n",
      "Iteration 159, loss = 0.25960481\n",
      "Iteration 154, loss = 0.26288204\n",
      "Iteration 155, loss = 0.26292439\n",
      "Iteration 160, loss = 0.25988868\n",
      "Iteration 161, loss = 0.25977846\n",
      "Iteration 156, loss = 0.26273477\n",
      "Iteration 162, loss = 0.25948307\n",
      "Iteration 157, loss = 0.26257780\n",
      "Iteration 163, loss = 0.25939179\n",
      "Iteration 158, loss = 0.26244425\n",
      "Iteration 164, loss = 0.25938469\n",
      "Iteration 159, loss = 0.26255869\n",
      "Iteration 165, loss = 0.25909777\n",
      "Iteration 160, loss = 0.26279608\n",
      "Iteration 166, loss = 0.25930538\n",
      "Iteration 161, loss = 0.26243943\n",
      "Iteration 167, loss = 0.25909139\n",
      "Iteration 162, loss = 0.26214411\n",
      "Iteration 168, loss = 0.25903302\n",
      "Iteration 163, loss = 0.26244056\n",
      "Iteration 169, loss = 0.25917331\n",
      "Iteration 164, loss = 0.26248690\n",
      "Iteration 170, loss = 0.25904067\n",
      "Iteration 165, loss = 0.26240200\n",
      "Iteration 171, loss = 0.25890673\n",
      "Iteration 172, loss = 0.25903080\n",
      "Iteration 166, loss = 0.26239284\n",
      "Iteration 167, loss = 0.26222804\n",
      "Iteration 173, loss = 0.25892069\n",
      "Iteration 174, loss = 0.25878154\n",
      "Iteration 175, loss = 0.25869193\n",
      "Iteration 168, loss = 0.26225507\n",
      "Iteration 169, loss = 0.26221403\n",
      "Iteration 176, loss = 0.25839283\n",
      "Iteration 170, loss = 0.26221780\n",
      "Iteration 177, loss = 0.25918653\n",
      "Iteration 171, loss = 0.26201543\n",
      "Iteration 178, loss = 0.25882422\n",
      "Iteration 172, loss = 0.26207604\n",
      "Iteration 179, loss = 0.25849323\n",
      "Iteration 180, loss = 0.25836749\n",
      "Iteration 173, loss = 0.26191409\n",
      "Iteration 181, loss = 0.25857649\n",
      "Iteration 174, loss = 0.26209461\n",
      "Iteration 182, loss = 0.25863461\n",
      "Iteration 175, loss = 0.26195582\n",
      "Iteration 183, loss = 0.25864313\n",
      "Iteration 184, loss = 0.25847660\n",
      "Iteration 176, loss = 0.26156613\n",
      "Iteration 185, loss = 0.25848056\n",
      "Iteration 177, loss = 0.26235575\n",
      "Iteration 186, loss = 0.25847611\n",
      "Iteration 178, loss = 0.26215768\n",
      "Iteration 187, loss = 0.25841382\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 179, loss = 0.26162866\n",
      "Iteration 180, loss = 0.26174300\n",
      "Iteration 1, loss = 0.35244372\n",
      "Iteration 181, loss = 0.26184503\n",
      "Iteration 2, loss = 0.30422737\n",
      "Iteration 182, loss = 0.26183220\n",
      "Iteration 3, loss = 0.28840077\n",
      "Iteration 183, loss = 0.26202837\n",
      "Iteration 4, loss = 0.28283183\n",
      "Iteration 184, loss = 0.26175085\n",
      "Iteration 5, loss = 0.28082003\n",
      "Iteration 185, loss = 0.26188929\n",
      "Iteration 6, loss = 0.27982708\n",
      "Iteration 7, loss = 0.27930649\n",
      "Iteration 186, loss = 0.26193281\n",
      "Iteration 8, loss = 0.27902532\n",
      "Iteration 187, loss = 0.26158242\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 9, loss = 0.27815439\n",
      "Iteration 10, loss = 0.27815467\n",
      "Iteration 1, loss = 0.35253727\n",
      "Iteration 11, loss = 0.27729400\n",
      "Iteration 2, loss = 0.30426478\n",
      "Iteration 12, loss = 0.27698986\n",
      "Iteration 13, loss = 0.27664345\n",
      "Iteration 3, loss = 0.28886835\n",
      "Iteration 4, loss = 0.28376277\n",
      "Iteration 14, loss = 0.27619246\n",
      "Iteration 5, loss = 0.28178147\n",
      "Iteration 15, loss = 0.27584350\n",
      "Iteration 6, loss = 0.28081238\n",
      "Iteration 7, loss = 0.28042086\n",
      "Iteration 16, loss = 0.27499439\n",
      "Iteration 8, loss = 0.28000774\n",
      "Iteration 17, loss = 0.27486916\n",
      "Iteration 9, loss = 0.27959181\n",
      "Iteration 18, loss = 0.27449605\n",
      "Iteration 10, loss = 0.27938311\n",
      "Iteration 19, loss = 0.27410445\n",
      "Iteration 11, loss = 0.27848948\n",
      "Iteration 20, loss = 0.27342104\n",
      "Iteration 12, loss = 0.27850682\n",
      "Iteration 21, loss = 0.27320774\n",
      "Iteration 13, loss = 0.27804957\n",
      "Iteration 22, loss = 0.27318223\n",
      "Iteration 14, loss = 0.27741807\n",
      "Iteration 23, loss = 0.27252198\n",
      "Iteration 15, loss = 0.27704855\n",
      "Iteration 24, loss = 0.27216335\n",
      "Iteration 16, loss = 0.27639734\n",
      "Iteration 25, loss = 0.27174701\n",
      "Iteration 17, loss = 0.27640041\n",
      "Iteration 26, loss = 0.27213193\n",
      "Iteration 18, loss = 0.27596880\n",
      "Iteration 27, loss = 0.27155601\n",
      "Iteration 19, loss = 0.27536202\n",
      "Iteration 28, loss = 0.27182949\n",
      "Iteration 20, loss = 0.27507548\n",
      "Iteration 29, loss = 0.27129271\n",
      "Iteration 21, loss = 0.27453794\n",
      "Iteration 30, loss = 0.27107627\n",
      "Iteration 22, loss = 0.27418874\n",
      "Iteration 31, loss = 0.27062903\n",
      "Iteration 23, loss = 0.27395110\n",
      "Iteration 32, loss = 0.27186327\n",
      "Iteration 24, loss = 0.27343561\n",
      "Iteration 33, loss = 0.27004561\n",
      "Iteration 25, loss = 0.27319725\n",
      "Iteration 34, loss = 0.26989435\n",
      "Iteration 26, loss = 0.27345075\n",
      "Iteration 35, loss = 0.26966741\n",
      "Iteration 27, loss = 0.27298707\n",
      "Iteration 36, loss = 0.26979224\n",
      "Iteration 28, loss = 0.27269208\n",
      "Iteration 37, loss = 0.26932419\n",
      "Iteration 29, loss = 0.27250619\n",
      "Iteration 38, loss = 0.26891346\n",
      "Iteration 30, loss = 0.27225996\n",
      "Iteration 39, loss = 0.26890904\n",
      "Iteration 31, loss = 0.27156371\n",
      "Iteration 40, loss = 0.26868621\n",
      "Iteration 32, loss = 0.27254493\n",
      "Iteration 41, loss = 0.26830176\n",
      "Iteration 33, loss = 0.27139297\n",
      "Iteration 42, loss = 0.26813926\n",
      "Iteration 34, loss = 0.27115588\n",
      "Iteration 43, loss = 0.26806002\n",
      "Iteration 35, loss = 0.27068990\n",
      "Iteration 44, loss = 0.26783368\n",
      "Iteration 36, loss = 0.27090639\n",
      "Iteration 45, loss = 0.26801863\n",
      "Iteration 46, loss = 0.26791804\n",
      "Iteration 37, loss = 0.27037723\n",
      "Iteration 47, loss = 0.26802474\n",
      "Iteration 48, loss = 0.26724864\n",
      "Iteration 38, loss = 0.26988037\n",
      "Iteration 49, loss = 0.26714047\n",
      "Iteration 50, loss = 0.26688378\n",
      "Iteration 39, loss = 0.27006044\n",
      "Iteration 51, loss = 0.26682373\n",
      "Iteration 40, loss = 0.26956569\n",
      "Iteration 52, loss = 0.26641034\n",
      "Iteration 41, loss = 0.26923231\n",
      "Iteration 53, loss = 0.26655917\n",
      "Iteration 42, loss = 0.26906255\n",
      "Iteration 54, loss = 0.26657614\n",
      "Iteration 43, loss = 0.26900786\n",
      "Iteration 55, loss = 0.26646074\n",
      "Iteration 44, loss = 0.26889352\n",
      "Iteration 56, loss = 0.26619454\n",
      "Iteration 45, loss = 0.26870609\n",
      "Iteration 57, loss = 0.26567092\n",
      "Iteration 46, loss = 0.26854552\n",
      "Iteration 58, loss = 0.26572688\n",
      "Iteration 47, loss = 0.26898562\n",
      "Iteration 59, loss = 0.26574683\n",
      "Iteration 48, loss = 0.26782082\n",
      "Iteration 60, loss = 0.26522900\n",
      "Iteration 49, loss = 0.26780488\n",
      "Iteration 61, loss = 0.26552742\n",
      "Iteration 50, loss = 0.26727477\n",
      "Iteration 62, loss = 0.26641194\n",
      "Iteration 51, loss = 0.26733285\n",
      "Iteration 63, loss = 0.26507775\n",
      "Iteration 52, loss = 0.26713051\n",
      "Iteration 64, loss = 0.26500100\n",
      "Iteration 53, loss = 0.26699877\n",
      "Iteration 65, loss = 0.26506842\n",
      "Iteration 54, loss = 0.26687654\n",
      "Iteration 66, loss = 0.26449341\n",
      "Iteration 55, loss = 0.26667346\n",
      "Iteration 67, loss = 0.26464285\n",
      "Iteration 56, loss = 0.26672941\n",
      "Iteration 68, loss = 0.26435185\n",
      "Iteration 57, loss = 0.26614152\n",
      "Iteration 69, loss = 0.26414792\n",
      "Iteration 58, loss = 0.26622571\n",
      "Iteration 70, loss = 0.26398514\n",
      "Iteration 59, loss = 0.26598491\n",
      "Iteration 71, loss = 0.26429608\n",
      "Iteration 60, loss = 0.26610205\n",
      "Iteration 72, loss = 0.26375544\n",
      "Iteration 61, loss = 0.26613130\n",
      "Iteration 73, loss = 0.26382514\n",
      "Iteration 62, loss = 0.26656250\n",
      "Iteration 74, loss = 0.26361197\n",
      "Iteration 63, loss = 0.26561824\n",
      "Iteration 64, loss = 0.26564064\n",
      "Iteration 75, loss = 0.26394936\n",
      "Iteration 76, loss = 0.26351721\n",
      "Iteration 65, loss = 0.26563460\n",
      "Iteration 77, loss = 0.26331655\n",
      "Iteration 66, loss = 0.26508262\n",
      "Iteration 78, loss = 0.26314435\n",
      "Iteration 67, loss = 0.26501512\n",
      "Iteration 79, loss = 0.26308586\n",
      "Iteration 80, loss = 0.26308313\n",
      "Iteration 68, loss = 0.26498511\n",
      "Iteration 81, loss = 0.26286934\n",
      "Iteration 69, loss = 0.26460521\n",
      "Iteration 70, loss = 0.26436698\n",
      "Iteration 82, loss = 0.26285722\n",
      "Iteration 71, loss = 0.26450954\n",
      "Iteration 83, loss = 0.26258179\n",
      "Iteration 72, loss = 0.26418329\n",
      "Iteration 73, loss = 0.26442919\n",
      "Iteration 84, loss = 0.26284338\n",
      "Iteration 74, loss = 0.26404595\n",
      "Iteration 85, loss = 0.26279175\n",
      "Iteration 75, loss = 0.26394952\n",
      "Iteration 86, loss = 0.26263729\n",
      "Iteration 76, loss = 0.26392077\n",
      "Iteration 87, loss = 0.26215721\n",
      "Iteration 77, loss = 0.26355180\n",
      "Iteration 88, loss = 0.26234724\n",
      "Iteration 78, loss = 0.26359339\n",
      "Iteration 89, loss = 0.26245982\n",
      "Iteration 79, loss = 0.26374533\n",
      "Iteration 90, loss = 0.26226226\n",
      "Iteration 80, loss = 0.26338924\n",
      "Iteration 91, loss = 0.26189538\n",
      "Iteration 81, loss = 0.26317484\n",
      "Iteration 92, loss = 0.26223093\n",
      "Iteration 82, loss = 0.26321658\n",
      "Iteration 93, loss = 0.26181132\n",
      "Iteration 83, loss = 0.26288177\n",
      "Iteration 94, loss = 0.26210778\n",
      "Iteration 84, loss = 0.26321666\n",
      "Iteration 95, loss = 0.26185889\n",
      "Iteration 85, loss = 0.26299635\n",
      "Iteration 96, loss = 0.26163977\n",
      "Iteration 86, loss = 0.26298983\n",
      "Iteration 97, loss = 0.26145269\n",
      "Iteration 87, loss = 0.26257300\n",
      "Iteration 98, loss = 0.26183927\n",
      "Iteration 88, loss = 0.26281703\n",
      "Iteration 99, loss = 0.26160888\n",
      "Iteration 89, loss = 0.26272420\n",
      "Iteration 100, loss = 0.26132004\n",
      "Iteration 90, loss = 0.26257424\n",
      "Iteration 101, loss = 0.26148432\n",
      "Iteration 91, loss = 0.26255551\n",
      "Iteration 92, loss = 0.26246472\n",
      "Iteration 102, loss = 0.26146754\n",
      "Iteration 93, loss = 0.26237077\n",
      "Iteration 103, loss = 0.26166891\n",
      "Iteration 104, loss = 0.26141964\n",
      "Iteration 94, loss = 0.26241103\n",
      "Iteration 105, loss = 0.26142334\n",
      "Iteration 106, loss = 0.26173274\n",
      "Iteration 95, loss = 0.26223025\n",
      "Iteration 96, loss = 0.26189495\n",
      "Iteration 107, loss = 0.26139112\n",
      "Iteration 97, loss = 0.26192628\n",
      "Iteration 108, loss = 0.26106760\n",
      "Iteration 98, loss = 0.26194929\n",
      "Iteration 109, loss = 0.26100886\n",
      "Iteration 99, loss = 0.26251473\n",
      "Iteration 110, loss = 0.26119829\n",
      "Iteration 100, loss = 0.26175526\n",
      "Iteration 111, loss = 0.26095432\n",
      "Iteration 101, loss = 0.26171009\n",
      "Iteration 102, loss = 0.26204173\n",
      "Iteration 112, loss = 0.26102697\n",
      "Iteration 103, loss = 0.26175159\n",
      "Iteration 113, loss = 0.26051872\n",
      "Iteration 104, loss = 0.26146244\n",
      "Iteration 114, loss = 0.26110913\n",
      "Iteration 105, loss = 0.26170848\n",
      "Iteration 115, loss = 0.26074402\n",
      "Iteration 106, loss = 0.26207545\n",
      "Iteration 116, loss = 0.26071528\n",
      "Iteration 107, loss = 0.26179598\n",
      "Iteration 117, loss = 0.26047524\n",
      "Iteration 108, loss = 0.26128655\n",
      "Iteration 118, loss = 0.26067907\n",
      "Iteration 109, loss = 0.26130044\n",
      "Iteration 119, loss = 0.26059636\n",
      "Iteration 110, loss = 0.26140140\n",
      "Iteration 120, loss = 0.26029403\n",
      "Iteration 111, loss = 0.26114749\n",
      "Iteration 121, loss = 0.26054315\n",
      "Iteration 112, loss = 0.26117162\n",
      "Iteration 122, loss = 0.26017749\n",
      "Iteration 113, loss = 0.26089994\n",
      "Iteration 123, loss = 0.26049685\n",
      "Iteration 114, loss = 0.26127126\n",
      "Iteration 124, loss = 0.26056699\n",
      "Iteration 115, loss = 0.26089106\n",
      "Iteration 125, loss = 0.26053271\n",
      "Iteration 116, loss = 0.26098936\n",
      "Iteration 126, loss = 0.26060347\n",
      "Iteration 117, loss = 0.26082574\n",
      "Iteration 127, loss = 0.26046591\n",
      "Iteration 118, loss = 0.26119511\n",
      "Iteration 128, loss = 0.26009181\n",
      "Iteration 119, loss = 0.26089421\n",
      "Iteration 129, loss = 0.26007946\n",
      "Iteration 120, loss = 0.26075515\n",
      "Iteration 130, loss = 0.26006557\n",
      "Iteration 121, loss = 0.26070029\n",
      "Iteration 131, loss = 0.25994116\n",
      "Iteration 122, loss = 0.26075701\n",
      "Iteration 132, loss = 0.25987582\n",
      "Iteration 123, loss = 0.26068172\n",
      "Iteration 133, loss = 0.26011263\n",
      "Iteration 124, loss = 0.26109331\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 134, loss = 0.25985994\n",
      "Iteration 1, loss = 0.35218500\n",
      "Iteration 135, loss = 0.25966114\n",
      "Iteration 136, loss = 0.25970637\n",
      "Iteration 2, loss = 0.30348068\n",
      "Iteration 3, loss = 0.28794583\n",
      "Iteration 137, loss = 0.25967675\n",
      "Iteration 138, loss = 0.25958565\n",
      "Iteration 4, loss = 0.28328659\n",
      "Iteration 5, loss = 0.28099452\n",
      "Iteration 6, loss = 0.28034462\n",
      "Iteration 139, loss = 0.25953553\n",
      "Iteration 7, loss = 0.27969399\n",
      "Iteration 140, loss = 0.25942930\n",
      "Iteration 141, loss = 0.26005486\n",
      "Iteration 8, loss = 0.27943268\n",
      "Iteration 142, loss = 0.25958904\n",
      "Iteration 9, loss = 0.27878577\n",
      "Iteration 143, loss = 0.25980197\n",
      "Iteration 10, loss = 0.27865197\n",
      "Iteration 144, loss = 0.25943867\n",
      "Iteration 11, loss = 0.27783740\n",
      "Iteration 12, loss = 0.27747062\n",
      "Iteration 145, loss = 0.26022494\n",
      "Iteration 13, loss = 0.27722967\n",
      "Iteration 146, loss = 0.25934190\n",
      "Iteration 14, loss = 0.27640053\n",
      "Iteration 147, loss = 0.25942779\n",
      "Iteration 15, loss = 0.27620210\n",
      "Iteration 148, loss = 0.25924249\n",
      "Iteration 16, loss = 0.27557756\n",
      "Iteration 149, loss = 0.25970089\n",
      "Iteration 17, loss = 0.27554598\n",
      "Iteration 150, loss = 0.25905907\n",
      "Iteration 18, loss = 0.27502055\n",
      "Iteration 151, loss = 0.25933287\n",
      "Iteration 19, loss = 0.27423337\n",
      "Iteration 152, loss = 0.25919041\n",
      "Iteration 20, loss = 0.27404128\n",
      "Iteration 153, loss = 0.25943738\n",
      "Iteration 21, loss = 0.27384734\n",
      "Iteration 154, loss = 0.25919248\n",
      "Iteration 22, loss = 0.27298309\n",
      "Iteration 155, loss = 0.25922932\n",
      "Iteration 23, loss = 0.27330284\n",
      "Iteration 156, loss = 0.25888058\n",
      "Iteration 24, loss = 0.27280445\n",
      "Iteration 157, loss = 0.25890727\n",
      "Iteration 25, loss = 0.27215031\n",
      "Iteration 158, loss = 0.25888783\n",
      "Iteration 26, loss = 0.27210199\n",
      "Iteration 159, loss = 0.25911815\n",
      "Iteration 27, loss = 0.27149018\n",
      "Iteration 28, loss = 0.27129950\n",
      "Iteration 160, loss = 0.25930214\n",
      "Iteration 161, loss = 0.25887970\n",
      "Iteration 29, loss = 0.27088580\n",
      "Iteration 162, loss = 0.25876080\n",
      "Iteration 30, loss = 0.27085693\n",
      "Iteration 163, loss = 0.25891819\n",
      "Iteration 31, loss = 0.27009764\n",
      "Iteration 164, loss = 0.25910614\n",
      "Iteration 32, loss = 0.27102759\n",
      "Iteration 165, loss = 0.25882096\n",
      "Iteration 33, loss = 0.27009342\n",
      "Iteration 166, loss = 0.25872606\n",
      "Iteration 34, loss = 0.27003521\n",
      "Iteration 167, loss = 0.25883531\n",
      "Iteration 35, loss = 0.26929932\n",
      "Iteration 168, loss = 0.25873632\n",
      "Iteration 36, loss = 0.26989800\n",
      "Iteration 169, loss = 0.25862089Iteration 37, loss = 0.26927749\n",
      "\n",
      "Iteration 38, loss = 0.26868897\n",
      "Iteration 170, loss = 0.25875550\n",
      "Iteration 39, loss = 0.26925706\n",
      "Iteration 171, loss = 0.25845098\n",
      "Iteration 40, loss = 0.26820870\n",
      "Iteration 172, loss = 0.25841880\n",
      "Iteration 41, loss = 0.26812902\n",
      "Iteration 173, loss = 0.25855151\n",
      "Iteration 42, loss = 0.26799545\n",
      "Iteration 174, loss = 0.25856429\n",
      "Iteration 43, loss = 0.26782850\n",
      "Iteration 175, loss = 0.25838780\n",
      "Iteration 44, loss = 0.26794858\n",
      "Iteration 176, loss = 0.25827758\n",
      "Iteration 45, loss = 0.26769564\n",
      "Iteration 177, loss = 0.25919253\n",
      "Iteration 46, loss = 0.26775382\n",
      "Iteration 178, loss = 0.25854336\n",
      "Iteration 47, loss = 0.26826874\n",
      "Iteration 179, loss = 0.25839642\n",
      "Iteration 48, loss = 0.26685729\n",
      "Iteration 180, loss = 0.25837746\n",
      "Iteration 49, loss = 0.26712653\n",
      "Iteration 50, loss = 0.26667032\n",
      "Iteration 181, loss = 0.25854681\n",
      "Iteration 51, loss = 0.26679793\n",
      "Iteration 182, loss = 0.25842704\n",
      "Iteration 52, loss = 0.26658515\n",
      "Iteration 183, loss = 0.25894325\n",
      "Iteration 53, loss = 0.26647590\n",
      "Iteration 184, loss = 0.25838271\n",
      "Iteration 54, loss = 0.26640669\n",
      "Iteration 185, loss = 0.25854483\n",
      "Iteration 55, loss = 0.26629537\n",
      "Iteration 186, loss = 0.25828806\n",
      "Iteration 56, loss = 0.26640418\n",
      "Iteration 187, loss = 0.25831213\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 57, loss = 0.26582551\n",
      "Iteration 58, loss = 0.26564864\n",
      "Iteration 59, loss = 0.26579287\n",
      "Iteration 1, loss = 0.35257553\n",
      "Iteration 60, loss = 0.26577443\n",
      "Iteration 2, loss = 0.30501564\n",
      "Iteration 61, loss = 0.26600439\n",
      "Iteration 3, loss = 0.28986684\n",
      "Iteration 62, loss = 0.26593024\n",
      "Iteration 63, loss = 0.26528374\n",
      "Iteration 4, loss = 0.28495008\n",
      "Iteration 64, loss = 0.26532073\n",
      "Iteration 5, loss = 0.28277824\n",
      "Iteration 65, loss = 0.26576321\n",
      "Iteration 6, loss = 0.28233612\n",
      "Iteration 66, loss = 0.26519022\n",
      "Iteration 7, loss = 0.28146137\n",
      "Iteration 67, loss = 0.26482950\n",
      "Iteration 68, loss = 0.26547456\n",
      "Iteration 8, loss = 0.28106429\n",
      "Iteration 69, loss = 0.26503701\n",
      "Iteration 70, loss = 0.26472301\n",
      "Iteration 9, loss = 0.28051678\n",
      "Iteration 71, loss = 0.26471964Iteration 10, loss = 0.28022373\n",
      "\n",
      "Iteration 72, loss = 0.26446138\n",
      "Iteration 73, loss = 0.26443216\n",
      "Iteration 11, loss = 0.27963676\n",
      "Iteration 74, loss = 0.26430993\n",
      "Iteration 12, loss = 0.27899880\n",
      "Iteration 75, loss = 0.26426246\n",
      "Iteration 13, loss = 0.27901626\n",
      "Iteration 76, loss = 0.26405467\n",
      "Iteration 14, loss = 0.27803928\n",
      "Iteration 15, loss = 0.27798756\n",
      "Iteration 77, loss = 0.26391093\n",
      "Iteration 16, loss = 0.27737964\n",
      "Iteration 78, loss = 0.26394447\n",
      "Iteration 17, loss = 0.27736187\n",
      "Iteration 79, loss = 0.26385561\n",
      "Iteration 18, loss = 0.27653231\n",
      "Iteration 80, loss = 0.26383273\n",
      "Iteration 19, loss = 0.27619429\n",
      "Iteration 81, loss = 0.26351973\n",
      "Iteration 20, loss = 0.27639965\n",
      "Iteration 82, loss = 0.26353793\n",
      "Iteration 21, loss = 0.27552573\n",
      "Iteration 83, loss = 0.26341082\n",
      "Iteration 22, loss = 0.27504331\n",
      "Iteration 84, loss = 0.26338971\n",
      "Iteration 23, loss = 0.27535736\n",
      "Iteration 85, loss = 0.26325019\n",
      "Iteration 24, loss = 0.27467782\n",
      "Iteration 86, loss = 0.26348120\n",
      "Iteration 25, loss = 0.27442089\n",
      "Iteration 87, loss = 0.26295497\n",
      "Iteration 26, loss = 0.27420954\n",
      "Iteration 88, loss = 0.26325157\n",
      "Iteration 27, loss = 0.27355230\n",
      "Iteration 89, loss = 0.26322842\n",
      "Iteration 28, loss = 0.27343510\n",
      "Iteration 90, loss = 0.26296186\n",
      "Iteration 29, loss = 0.27311417\n",
      "Iteration 91, loss = 0.26285173\n",
      "Iteration 30, loss = 0.27322616\n",
      "Iteration 92, loss = 0.26298216\n",
      "Iteration 31, loss = 0.27234548\n",
      "Iteration 93, loss = 0.26282770\n",
      "Iteration 94, loss = 0.26336043\n",
      "Iteration 32, loss = 0.27319473\n",
      "Iteration 33, loss = 0.27200055\n",
      "Iteration 95, loss = 0.26253298\n",
      "Iteration 34, loss = 0.27210414\n",
      "Iteration 96, loss = 0.26243185\n",
      "Iteration 35, loss = 0.27155233\n",
      "Iteration 97, loss = 0.26248321\n",
      "Iteration 36, loss = 0.27190191\n",
      "Iteration 98, loss = 0.26241092\n",
      "Iteration 37, loss = 0.27140203\n",
      "Iteration 99, loss = 0.26225620\n",
      "Iteration 38, loss = 0.27072046\n",
      "Iteration 100, loss = 0.26186802\n",
      "Iteration 39, loss = 0.27118199\n",
      "Iteration 101, loss = 0.26186951\n",
      "Iteration 40, loss = 0.27000687\n",
      "Iteration 102, loss = 0.26205924\n",
      "Iteration 41, loss = 0.27026352\n",
      "Iteration 103, loss = 0.26213667\n",
      "Iteration 42, loss = 0.27004615\n",
      "Iteration 104, loss = 0.26175641\n",
      "Iteration 43, loss = 0.26951842\n",
      "Iteration 105, loss = 0.26191802\n",
      "Iteration 44, loss = 0.26970236\n",
      "Iteration 106, loss = 0.26206987\n",
      "Iteration 45, loss = 0.26951454\n",
      "Iteration 107, loss = 0.26212631\n",
      "Iteration 46, loss = 0.26895355\n",
      "Iteration 108, loss = 0.26162388\n",
      "Iteration 47, loss = 0.26927544\n",
      "Iteration 109, loss = 0.26147015\n",
      "Iteration 48, loss = 0.26813253\n",
      "Iteration 110, loss = 0.26133831\n",
      "Iteration 49, loss = 0.26837060\n",
      "Iteration 111, loss = 0.26116379\n",
      "Iteration 50, loss = 0.26819550\n",
      "Iteration 112, loss = 0.26137770\n",
      "Iteration 51, loss = 0.26765751\n",
      "Iteration 113, loss = 0.26090785\n",
      "Iteration 52, loss = 0.26757561\n",
      "Iteration 114, loss = 0.26151075\n",
      "Iteration 53, loss = 0.26728733\n",
      "Iteration 115, loss = 0.26111582\n",
      "Iteration 54, loss = 0.26706830\n",
      "Iteration 116, loss = 0.26107689\n",
      "Iteration 55, loss = 0.26690610\n",
      "Iteration 117, loss = 0.26120255\n",
      "Iteration 56, loss = 0.26725043\n",
      "Iteration 118, loss = 0.26121519\n",
      "Iteration 57, loss = 0.26685683\n",
      "Iteration 119, loss = 0.26114315\n",
      "Iteration 58, loss = 0.26622417\n",
      "Iteration 120, loss = 0.26082770\n",
      "Iteration 59, loss = 0.26617755\n",
      "Iteration 121, loss = 0.26113021\n",
      "Iteration 60, loss = 0.26615438\n",
      "Iteration 122, loss = 0.26081408\n",
      "Iteration 61, loss = 0.26626888\n",
      "Iteration 123, loss = 0.26074950\n",
      "Iteration 62, loss = 0.26604892\n",
      "Iteration 124, loss = 0.26111196\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 63, loss = 0.26559836\n",
      "Iteration 64, loss = 0.26579965\n",
      "Iteration 1, loss = 0.35260485\n",
      "Iteration 65, loss = 0.26575169\n",
      "Iteration 2, loss = 0.30391493\n",
      "Iteration 66, loss = 0.26522556\n",
      "Iteration 3, loss = 0.28841074\n",
      "Iteration 67, loss = 0.26498244\n",
      "Iteration 4, loss = 0.28334447\n",
      "Iteration 68, loss = 0.26544605\n",
      "Iteration 5, loss = 0.28105308\n",
      "Iteration 69, loss = 0.26511477\n",
      "Iteration 70, loss = 0.26472978\n",
      "Iteration 6, loss = 0.28053511\n",
      "Iteration 7, loss = 0.28009037\n",
      "Iteration 71, loss = 0.26477386\n",
      "Iteration 8, loss = 0.27961433\n",
      "Iteration 9, loss = 0.27906183\n",
      "Iteration 72, loss = 0.26469874\n",
      "Iteration 73, loss = 0.26465317\n",
      "Iteration 10, loss = 0.27874854\n",
      "Iteration 74, loss = 0.26439136\n",
      "Iteration 11, loss = 0.27837445\n",
      "Iteration 75, loss = 0.26408087\n",
      "Iteration 76, loss = 0.26437379\n",
      "Iteration 12, loss = 0.27757224\n",
      "Iteration 13, loss = 0.27742015\n",
      "Iteration 77, loss = 0.26390250\n",
      "Iteration 14, loss = 0.27648611\n",
      "Iteration 78, loss = 0.26414149\n",
      "Iteration 15, loss = 0.27634081\n",
      "Iteration 79, loss = 0.26417874\n",
      "Iteration 16, loss = 0.27563486\n",
      "Iteration 80, loss = 0.26370131\n",
      "Iteration 17, loss = 0.27551744\n",
      "Iteration 81, loss = 0.26382112\n",
      "Iteration 18, loss = 0.27494703\n",
      "Iteration 82, loss = 0.26375622\n",
      "Iteration 19, loss = 0.27470309\n",
      "Iteration 83, loss = 0.26340170\n",
      "Iteration 20, loss = 0.27481019\n",
      "Iteration 84, loss = 0.26341752\n",
      "Iteration 21, loss = 0.27402593\n",
      "Iteration 85, loss = 0.26364923\n",
      "Iteration 22, loss = 0.27325187\n",
      "Iteration 86, loss = 0.26368654\n",
      "Iteration 23, loss = 0.27340799\n",
      "Iteration 87, loss = 0.26327951\n",
      "Iteration 24, loss = 0.27281618\n",
      "Iteration 88, loss = 0.26348131\n",
      "Iteration 25, loss = 0.27246798\n",
      "Iteration 89, loss = 0.26369792\n",
      "Iteration 26, loss = 0.27184420\n",
      "Iteration 90, loss = 0.26315806\n",
      "Iteration 27, loss = 0.27165644\n",
      "Iteration 91, loss = 0.26310346\n",
      "Iteration 92, loss = 0.26315137\n",
      "Iteration 28, loss = 0.27151455\n",
      "Iteration 29, loss = 0.27104432\n",
      "Iteration 93, loss = 0.26330571\n",
      "Iteration 30, loss = 0.27107089\n",
      "Iteration 94, loss = 0.26303258\n",
      "Iteration 31, loss = 0.27031779\n",
      "Iteration 32, loss = 0.27125536\n",
      "Iteration 95, loss = 0.26284575\n",
      "Iteration 33, loss = 0.27007532\n",
      "Iteration 96, loss = 0.26278447\n",
      "Iteration 34, loss = 0.27021125\n",
      "Iteration 97, loss = 0.26274376\n",
      "Iteration 35, loss = 0.26927334\n",
      "Iteration 98, loss = 0.26260915\n",
      "Iteration 36, loss = 0.26959080\n",
      "Iteration 99, loss = 0.26289453\n",
      "Iteration 37, loss = 0.26907197\n",
      "Iteration 100, loss = 0.26251187\n",
      "Iteration 38, loss = 0.26892668\n",
      "Iteration 101, loss = 0.26238182\n",
      "Iteration 39, loss = 0.26902170\n",
      "Iteration 102, loss = 0.26266975\n",
      "Iteration 40, loss = 0.26819459\n",
      "Iteration 103, loss = 0.26239413\n",
      "Iteration 41, loss = 0.26866151\n",
      "Iteration 104, loss = 0.26219258\n",
      "Iteration 42, loss = 0.26813972\n",
      "Iteration 105, loss = 0.26240057\n",
      "Iteration 106, loss = 0.26231800\n",
      "Iteration 43, loss = 0.26781749\n",
      "Iteration 44, loss = 0.26808291\n",
      "Iteration 107, loss = 0.26254236\n",
      "Iteration 45, loss = 0.26757946\n",
      "Iteration 108, loss = 0.26194306\n",
      "Iteration 46, loss = 0.26744983\n",
      "Iteration 109, loss = 0.26207834\n",
      "Iteration 47, loss = 0.26760129\n",
      "Iteration 110, loss = 0.26228918\n",
      "Iteration 48, loss = 0.26663586\n",
      "Iteration 111, loss = 0.26193270\n",
      "Iteration 49, loss = 0.26691594\n",
      "Iteration 112, loss = 0.26201600\n",
      "Iteration 50, loss = 0.26625231\n",
      "Iteration 113, loss = 0.26149977\n",
      "Iteration 51, loss = 0.26632133\n",
      "Iteration 114, loss = 0.26185456\n",
      "Iteration 52, loss = 0.26647896\n",
      "Iteration 115, loss = 0.26156363\n",
      "Iteration 53, loss = 0.26605123\n",
      "Iteration 116, loss = 0.26182173\n",
      "Iteration 54, loss = 0.26588763\n",
      "Iteration 117, loss = 0.26212575\n",
      "Iteration 55, loss = 0.26550489\n",
      "Iteration 118, loss = 0.26160222\n",
      "Iteration 56, loss = 0.26554340\n",
      "Iteration 119, loss = 0.26204983\n",
      "Iteration 57, loss = 0.26570836\n",
      "Iteration 120, loss = 0.26179355\n",
      "Iteration 58, loss = 0.26518396\n",
      "Iteration 121, loss = 0.26192234\n",
      "Iteration 59, loss = 0.26506841\n",
      "Iteration 122, loss = 0.26160196\n",
      "Iteration 60, loss = 0.26511133\n",
      "Iteration 123, loss = 0.26142763\n",
      "Iteration 61, loss = 0.26490607\n",
      "Iteration 124, loss = 0.26158134\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 62, loss = 0.26507214\n",
      "Iteration 63, loss = 0.26445090\n",
      "Iteration 64, loss = 0.26442008\n",
      "Iteration 1, loss = 0.35192006\n",
      "Iteration 65, loss = 0.26423646\n",
      "Iteration 2, loss = 0.30407695\n",
      "Iteration 3, loss = 0.28917779\n",
      "Iteration 66, loss = 0.26411205\n",
      "Iteration 67, loss = 0.26378179\n",
      "Iteration 4, loss = 0.28444984\n",
      "Iteration 68, loss = 0.26433034\n",
      "Iteration 5, loss = 0.28264424\n",
      "Iteration 6, loss = 0.28155959\n",
      "Iteration 69, loss = 0.26381141\n",
      "Iteration 70, loss = 0.26382721\n",
      "Iteration 7, loss = 0.28142467\n",
      "Iteration 8, loss = 0.28066629\n",
      "Iteration 71, loss = 0.26336156\n",
      "Iteration 72, loss = 0.26368399\n",
      "Iteration 73, loss = 0.26316441\n",
      "Iteration 9, loss = 0.28053090\n",
      "Iteration 74, loss = 0.26327262\n",
      "Iteration 10, loss = 0.27982711\n",
      "Iteration 75, loss = 0.26311419\n",
      "Iteration 11, loss = 0.27984969\n",
      "Iteration 76, loss = 0.26313953\n",
      "Iteration 12, loss = 0.27876980\n",
      "Iteration 77, loss = 0.26290621\n",
      "Iteration 13, loss = 0.27870698\n",
      "Iteration 78, loss = 0.26317800Iteration 14, loss = 0.27785120\n",
      "\n",
      "Iteration 79, loss = 0.26294754\n",
      "Iteration 15, loss = 0.27764511\n",
      "Iteration 80, loss = 0.26254129\n",
      "Iteration 16, loss = 0.27707746\n",
      "Iteration 81, loss = 0.26252089\n",
      "Iteration 17, loss = 0.27673234\n",
      "Iteration 82, loss = 0.26244902\n",
      "Iteration 18, loss = 0.27629906\n",
      "Iteration 83, loss = 0.26226522\n",
      "Iteration 19, loss = 0.27575041\n",
      "Iteration 84, loss = 0.26231681\n",
      "Iteration 20, loss = 0.27558552\n",
      "Iteration 85, loss = 0.26217660\n",
      "Iteration 21, loss = 0.27535709\n",
      "Iteration 86, loss = 0.26230669\n",
      "Iteration 22, loss = 0.27424301\n",
      "Iteration 87, loss = 0.26240703\n",
      "Iteration 23, loss = 0.27406454\n",
      "Iteration 88, loss = 0.26272024\n",
      "Iteration 24, loss = 0.27370735\n",
      "Iteration 89, loss = 0.26221921\n",
      "Iteration 25, loss = 0.27339102\n",
      "Iteration 90, loss = 0.26176100\n",
      "Iteration 26, loss = 0.27261227\n",
      "Iteration 91, loss = 0.26188585\n",
      "Iteration 27, loss = 0.27236720\n",
      "Iteration 92, loss = 0.26201185\n",
      "Iteration 28, loss = 0.27209971\n",
      "Iteration 93, loss = 0.26173036\n",
      "Iteration 29, loss = 0.27192582\n",
      "Iteration 94, loss = 0.26186287\n",
      "Iteration 30, loss = 0.27193274\n",
      "Iteration 95, loss = 0.26151234\n",
      "Iteration 31, loss = 0.27131420\n",
      "Iteration 96, loss = 0.26187063\n",
      "Iteration 32, loss = 0.27148817\n",
      "Iteration 97, loss = 0.26171684\n",
      "Iteration 33, loss = 0.27038299\n",
      "Iteration 98, loss = 0.26151576\n",
      "Iteration 34, loss = 0.27086054\n",
      "Iteration 99, loss = 0.26149848\n",
      "Iteration 35, loss = 0.27001547\n",
      "Iteration 100, loss = 0.26120231\n",
      "Iteration 36, loss = 0.26997815\n",
      "Iteration 101, loss = 0.26124461\n",
      "Iteration 102, loss = 0.26127558\n",
      "Iteration 37, loss = 0.26951136\n",
      "Iteration 103, loss = 0.26123723\n",
      "Iteration 38, loss = 0.26921940\n",
      "Iteration 104, loss = 0.26101122\n",
      "Iteration 39, loss = 0.26954288\n",
      "Iteration 105, loss = 0.26118686\n",
      "Iteration 40, loss = 0.26877954\n",
      "Iteration 106, loss = 0.26115018\n",
      "Iteration 41, loss = 0.26897788\n",
      "Iteration 107, loss = 0.26081666\n",
      "Iteration 42, loss = 0.26854626\n",
      "Iteration 108, loss = 0.26076038\n",
      "Iteration 43, loss = 0.26821637\n",
      "Iteration 109, loss = 0.26089321\n",
      "Iteration 44, loss = 0.26830912\n",
      "Iteration 110, loss = 0.26104967\n",
      "Iteration 45, loss = 0.26786690\n",
      "Iteration 111, loss = 0.26074633\n",
      "Iteration 46, loss = 0.26804682\n",
      "Iteration 112, loss = 0.26058394\n",
      "Iteration 47, loss = 0.26795422\n",
      "Iteration 113, loss = 0.26030797\n",
      "Iteration 48, loss = 0.26753310\n",
      "Iteration 114, loss = 0.26047808\n",
      "Iteration 49, loss = 0.26721671\n",
      "Iteration 115, loss = 0.26021332\n",
      "Iteration 50, loss = 0.26695327\n",
      "Iteration 116, loss = 0.26042602\n",
      "Iteration 51, loss = 0.26677325\n",
      "Iteration 117, loss = 0.26061154\n",
      "Iteration 52, loss = 0.26680580\n",
      "Iteration 118, loss = 0.26023745\n",
      "Iteration 53, loss = 0.26677214\n",
      "Iteration 119, loss = 0.26059989\n",
      "Iteration 54, loss = 0.26653402\n",
      "Iteration 120, loss = 0.26039657\n",
      "Iteration 55, loss = 0.26620044\n",
      "Iteration 121, loss = 0.26034171\n",
      "Iteration 56, loss = 0.26630498\n",
      "Iteration 122, loss = 0.26018096\n",
      "Iteration 57, loss = 0.26620604\n",
      "Iteration 123, loss = 0.26000027\n",
      "Iteration 58, loss = 0.26571713\n",
      "Iteration 124, loss = 0.26017341\n",
      "Iteration 59, loss = 0.26552038\n",
      "Iteration 125, loss = 0.26036098\n",
      "Iteration 60, loss = 0.26563616\n",
      "Iteration 126, loss = 0.25974126\n",
      "Iteration 61, loss = 0.26566931\n",
      "Iteration 127, loss = 0.25974060\n",
      "Iteration 62, loss = 0.26540742\n",
      "Iteration 128, loss = 0.25982769\n",
      "Iteration 63, loss = 0.26506825\n",
      "Iteration 129, loss = 0.25990907\n",
      "Iteration 64, loss = 0.26505816\n",
      "Iteration 130, loss = 0.25985445\n",
      "Iteration 65, loss = 0.26487232\n",
      "Iteration 131, loss = 0.25962700\n",
      "Iteration 66, loss = 0.26484898\n",
      "Iteration 132, loss = 0.25942807\n",
      "Iteration 67, loss = 0.26446559\n",
      "Iteration 133, loss = 0.25984099\n",
      "Iteration 68, loss = 0.26481619\n",
      "Iteration 134, loss = 0.25986690\n",
      "Iteration 69, loss = 0.26446824\n",
      "Iteration 135, loss = 0.25960670\n",
      "Iteration 70, loss = 0.26430511\n",
      "Iteration 136, loss = 0.25967932\n",
      "Iteration 71, loss = 0.26443910\n",
      "Iteration 137, loss = 0.25976730\n",
      "Iteration 72, loss = 0.26431996\n",
      "Iteration 138, loss = 0.25952528\n",
      "Iteration 73, loss = 0.26378576\n",
      "Iteration 139, loss = 0.25932165\n",
      "Iteration 140, loss = 0.25938397\n",
      "Iteration 74, loss = 0.26398078\n",
      "Iteration 141, loss = 0.25922602\n",
      "Iteration 75, loss = 0.26384747\n",
      "Iteration 142, loss = 0.25946449\n",
      "Iteration 76, loss = 0.26395848\n",
      "Iteration 143, loss = 0.25915755\n",
      "Iteration 77, loss = 0.26371750\n",
      "Iteration 144, loss = 0.25913612\n",
      "Iteration 145, loss = 0.25949161\n",
      "Iteration 146, loss = 0.25921542\n",
      "Iteration 78, loss = 0.26378745\n",
      "Iteration 147, loss = 0.25916872\n",
      "Iteration 79, loss = 0.26332824\n",
      "Iteration 80, loss = 0.26322573\n",
      "Iteration 148, loss = 0.25938522\n",
      "Iteration 81, loss = 0.26316578\n",
      "Iteration 149, loss = 0.25961430\n",
      "Iteration 82, loss = 0.26305049\n",
      "Iteration 150, loss = 0.25924655\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 83, loss = 0.26281170\n",
      "Iteration 84, loss = 0.26305637\n",
      "Iteration 1, loss = 0.35237118\n",
      "Iteration 85, loss = 0.26277577\n",
      "Iteration 2, loss = 0.30481855\n",
      "Iteration 86, loss = 0.26287946\n",
      "Iteration 3, loss = 0.29011369\n",
      "Iteration 87, loss = 0.26283789\n",
      "Iteration 4, loss = 0.28517383\n",
      "Iteration 88, loss = 0.26272708\n",
      "Iteration 89, loss = 0.26255931\n",
      "Iteration 5, loss = 0.28298336\n",
      "Iteration 90, loss = 0.26228968\n",
      "Iteration 6, loss = 0.28229295\n",
      "Iteration 7, loss = 0.28187253\n",
      "Iteration 91, loss = 0.26240270\n",
      "Iteration 92, loss = 0.26221474\n",
      "Iteration 8, loss = 0.28122375\n",
      "Iteration 93, loss = 0.26207311\n",
      "Iteration 9, loss = 0.28130083\n",
      "Iteration 94, loss = 0.26211320\n",
      "Iteration 10, loss = 0.28028590\n",
      "Iteration 95, loss = 0.26192370\n",
      "Iteration 11, loss = 0.28041106\n",
      "Iteration 96, loss = 0.26209447\n",
      "Iteration 12, loss = 0.27923507\n",
      "Iteration 97, loss = 0.26215619\n",
      "Iteration 13, loss = 0.27946855\n",
      "Iteration 98, loss = 0.26173531\n",
      "Iteration 14, loss = 0.27859853\n",
      "Iteration 99, loss = 0.26160700\n",
      "Iteration 15, loss = 0.27824267\n",
      "Iteration 100, loss = 0.26153450\n",
      "Iteration 16, loss = 0.27780618\n",
      "Iteration 101, loss = 0.26143399\n",
      "Iteration 17, loss = 0.27727945\n",
      "Iteration 102, loss = 0.26134788\n",
      "Iteration 18, loss = 0.27706139\n",
      "Iteration 103, loss = 0.26124238\n",
      "Iteration 19, loss = 0.27655860\n",
      "Iteration 104, loss = 0.26179033\n",
      "Iteration 20, loss = 0.27622701\n",
      "Iteration 105, loss = 0.26130318\n",
      "Iteration 21, loss = 0.27583797\n",
      "Iteration 106, loss = 0.26112325\n",
      "Iteration 22, loss = 0.27499922\n",
      "Iteration 107, loss = 0.26109593\n",
      "Iteration 23, loss = 0.27455405\n",
      "Iteration 108, loss = 0.26108844\n",
      "Iteration 24, loss = 0.27431629\n",
      "Iteration 109, loss = 0.26124169\n",
      "Iteration 25, loss = 0.27393693\n",
      "Iteration 110, loss = 0.26141442\n",
      "Iteration 26, loss = 0.27310502\n",
      "Iteration 111, loss = 0.26115107\n",
      "Iteration 27, loss = 0.27342294\n",
      "Iteration 112, loss = 0.26097333\n",
      "Iteration 28, loss = 0.27269666\n",
      "Iteration 113, loss = 0.26077563\n",
      "Iteration 29, loss = 0.27259708\n",
      "Iteration 114, loss = 0.26088729\n",
      "Iteration 30, loss = 0.27262619\n",
      "Iteration 115, loss = 0.26054781\n",
      "Iteration 31, loss = 0.27193658\n",
      "Iteration 116, loss = 0.26084983\n",
      "Iteration 32, loss = 0.27196756\n",
      "Iteration 117, loss = 0.26078711\n",
      "Iteration 33, loss = 0.27106062\n",
      "Iteration 118, loss = 0.26073606\n",
      "Iteration 34, loss = 0.27115264\n",
      "Iteration 119, loss = 0.26058361\n",
      "Iteration 35, loss = 0.27026196\n",
      "Iteration 120, loss = 0.26073376\n",
      "Iteration 36, loss = 0.27057323\n",
      "Iteration 121, loss = 0.26048853\n",
      "Iteration 37, loss = 0.26980563\n",
      "Iteration 122, loss = 0.26035859\n",
      "Iteration 38, loss = 0.26968347\n",
      "Iteration 123, loss = 0.26036005\n",
      "Iteration 39, loss = 0.26982086\n",
      "Iteration 124, loss = 0.26046695\n",
      "Iteration 40, loss = 0.26895994\n",
      "Iteration 125, loss = 0.26060925\n",
      "Iteration 41, loss = 0.26937925\n",
      "Iteration 126, loss = 0.26024077\n",
      "Iteration 42, loss = 0.26885232\n",
      "Iteration 127, loss = 0.26013035\n",
      "Iteration 43, loss = 0.26842244\n",
      "Iteration 128, loss = 0.26001944\n",
      "Iteration 44, loss = 0.26851980\n",
      "Iteration 129, loss = 0.26016150\n",
      "Iteration 45, loss = 0.26818191\n",
      "Iteration 130, loss = 0.26031730\n",
      "Iteration 46, loss = 0.26793827\n",
      "Iteration 47, loss = 0.26804051\n",
      "Iteration 131, loss = 0.25999624\n",
      "Iteration 48, loss = 0.26757146\n",
      "Iteration 132, loss = 0.25999589\n",
      "Iteration 49, loss = 0.26715103\n",
      "Iteration 133, loss = 0.25980362\n",
      "Iteration 50, loss = 0.26711602\n",
      "Iteration 134, loss = 0.26012010\n",
      "Iteration 51, loss = 0.26688151\n",
      "Iteration 135, loss = 0.25962470\n",
      "Iteration 52, loss = 0.26674130\n",
      "Iteration 136, loss = 0.26021783\n",
      "Iteration 137, loss = 0.26001564\n",
      "Iteration 53, loss = 0.26668426\n",
      "Iteration 138, loss = 0.25976011\n",
      "Iteration 54, loss = 0.26629003\n",
      "Iteration 139, loss = 0.25972264\n",
      "Iteration 55, loss = 0.26615136\n",
      "Iteration 140, loss = 0.25973907\n",
      "Iteration 56, loss = 0.26615942\n",
      "Iteration 141, loss = 0.25945004\n",
      "Iteration 57, loss = 0.26611562\n",
      "Iteration 142, loss = 0.25959833\n",
      "Iteration 58, loss = 0.26583149\n",
      "Iteration 143, loss = 0.25943105\n",
      "Iteration 59, loss = 0.26568290\n",
      "Iteration 144, loss = 0.25919845\n",
      "Iteration 60, loss = 0.26561462\n",
      "Iteration 61, loss = 0.26572518\n",
      "Iteration 145, loss = 0.26021721\n",
      "Iteration 146, loss = 0.25951635\n",
      "Iteration 62, loss = 0.26513054\n",
      "Iteration 147, loss = 0.25982774\n",
      "Iteration 63, loss = 0.26511838\n",
      "Iteration 148, loss = 0.25937375\n",
      "Iteration 64, loss = 0.26504415\n",
      "Iteration 149, loss = 0.25957245\n",
      "Iteration 65, loss = 0.26480878\n",
      "Iteration 150, loss = 0.25936914\n",
      "Iteration 66, loss = 0.26473947\n",
      "Iteration 151, loss = 0.25964925\n",
      "Iteration 67, loss = 0.26476213\n",
      "Iteration 152, loss = 0.25973968\n",
      "Iteration 68, loss = 0.26469422\n",
      "Iteration 153, loss = 0.25936063\n",
      "Iteration 69, loss = 0.26476520\n",
      "Iteration 154, loss = 0.25925880\n",
      "Iteration 70, loss = 0.26458354\n",
      "Iteration 155, loss = 0.25929441\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 71, loss = 0.26442770\n",
      "Iteration 72, loss = 0.26459907\n",
      "Iteration 73, loss = 0.26415782\n",
      "Iteration 1, loss = 0.35360235\n",
      "Iteration 74, loss = 0.26430175\n",
      "Iteration 2, loss = 0.30532266\n",
      "Iteration 75, loss = 0.26408293\n",
      "Iteration 3, loss = 0.29054201\n",
      "Iteration 76, loss = 0.26450774\n",
      "Iteration 4, loss = 0.28565291\n",
      "Iteration 77, loss = 0.26377378\n",
      "Iteration 5, loss = 0.28352473\n",
      "Iteration 78, loss = 0.26429354\n",
      "Iteration 6, loss = 0.28247987\n",
      "Iteration 79, loss = 0.26368308\n",
      "Iteration 7, loss = 0.28237416\n",
      "Iteration 80, loss = 0.26354108\n",
      "Iteration 8, loss = 0.28142259\n",
      "Iteration 81, loss = 0.26339282\n",
      "Iteration 9, loss = 0.28157175\n",
      "Iteration 82, loss = 0.26343487\n",
      "Iteration 10, loss = 0.28056722\n",
      "Iteration 83, loss = 0.26328671\n",
      "Iteration 11, loss = 0.28029746\n",
      "Iteration 84, loss = 0.26359572\n",
      "Iteration 12, loss = 0.27957691\n",
      "Iteration 85, loss = 0.26304516\n",
      "Iteration 13, loss = 0.27956658\n",
      "Iteration 86, loss = 0.26349848\n",
      "Iteration 14, loss = 0.27878396\n",
      "Iteration 87, loss = 0.26327554\n",
      "Iteration 15, loss = 0.27864750\n",
      "Iteration 88, loss = 0.26324602\n",
      "Iteration 16, loss = 0.27776104\n",
      "Iteration 89, loss = 0.26326634\n",
      "Iteration 17, loss = 0.27740836\n",
      "Iteration 90, loss = 0.26302766\n",
      "Iteration 18, loss = 0.27719653\n",
      "Iteration 91, loss = 0.26316736\n",
      "Iteration 19, loss = 0.27697058\n",
      "Iteration 92, loss = 0.26272753\n",
      "Iteration 20, loss = 0.27623190\n",
      "Iteration 93, loss = 0.26300845\n",
      "Iteration 94, loss = 0.26313371\n",
      "Iteration 21, loss = 0.27633199\n",
      "Iteration 95, loss = 0.26243337\n",
      "Iteration 22, loss = 0.27560585\n",
      "Iteration 96, loss = 0.26293328\n",
      "Iteration 23, loss = 0.27510513\n",
      "Iteration 97, loss = 0.26281436\n",
      "Iteration 24, loss = 0.27519722\n",
      "Iteration 98, loss = 0.26260802\n",
      "Iteration 25, loss = 0.27443917\n",
      "Iteration 99, loss = 0.26255478\n",
      "Iteration 26, loss = 0.27378933\n",
      "Iteration 100, loss = 0.26238751\n",
      "Iteration 27, loss = 0.27402943\n",
      "Iteration 101, loss = 0.26237276\n",
      "Iteration 28, loss = 0.27353282\n",
      "Iteration 102, loss = 0.26236674\n",
      "Iteration 29, loss = 0.27336114\n",
      "Iteration 103, loss = 0.26225950\n",
      "Iteration 30, loss = 0.27333340\n",
      "Iteration 104, loss = 0.26253145\n",
      "Iteration 31, loss = 0.27318506\n",
      "Iteration 105, loss = 0.26220843\n",
      "Iteration 32, loss = 0.27313837\n",
      "Iteration 106, loss = 0.26228753\n",
      "Iteration 33, loss = 0.27222010\n",
      "Iteration 107, loss = 0.26198915\n",
      "Iteration 34, loss = 0.27216297\n",
      "Iteration 108, loss = 0.26230425\n",
      "Iteration 35, loss = 0.27182331\n",
      "Iteration 109, loss = 0.26202235\n",
      "Iteration 36, loss = 0.27211954\n",
      "Iteration 110, loss = 0.26239331\n",
      "Iteration 37, loss = 0.27157865\n",
      "Iteration 111, loss = 0.26199460\n",
      "Iteration 112, loss = 0.26175498\n",
      "Iteration 38, loss = 0.27137314\n",
      "Iteration 39, loss = 0.27152684\n",
      "Iteration 113, loss = 0.26179179\n",
      "Iteration 40, loss = 0.27077940\n",
      "Iteration 114, loss = 0.26173135\n",
      "Iteration 41, loss = 0.27106596\n",
      "Iteration 42, loss = 0.27077152\n",
      "Iteration 115, loss = 0.26179775\n",
      "Iteration 116, loss = 0.26184968\n",
      "Iteration 43, loss = 0.27024355\n",
      "Iteration 117, loss = 0.26155172\n",
      "Iteration 44, loss = 0.27032997Iteration 118, loss = 0.26153876\n",
      "\n",
      "Iteration 119, loss = 0.26164483\n",
      "Iteration 45, loss = 0.26999683\n",
      "Iteration 120, loss = 0.26200333\n",
      "Iteration 46, loss = 0.26994191\n",
      "Iteration 121, loss = 0.26130701\n",
      "Iteration 47, loss = 0.26974299\n",
      "Iteration 122, loss = 0.26134017\n",
      "Iteration 48, loss = 0.26946071\n",
      "Iteration 123, loss = 0.26154174\n",
      "Iteration 49, loss = 0.26893392\n",
      "Iteration 124, loss = 0.26168893\n",
      "Iteration 50, loss = 0.26898373\n",
      "Iteration 51, loss = 0.26855234\n",
      "Iteration 125, loss = 0.26144758\n",
      "Iteration 126, loss = 0.26126016\n",
      "Iteration 52, loss = 0.26861386\n",
      "Iteration 53, loss = 0.26831449\n",
      "Iteration 127, loss = 0.26130428\n",
      "Iteration 128, loss = 0.26119044\n",
      "Iteration 54, loss = 0.26837802\n",
      "Iteration 129, loss = 0.26151631\n",
      "Iteration 55, loss = 0.26798333\n",
      "Iteration 130, loss = 0.26127393\n",
      "Iteration 56, loss = 0.26796605\n",
      "Iteration 131, loss = 0.26121182\n",
      "Iteration 57, loss = 0.26789606\n",
      "Iteration 132, loss = 0.26113226\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 58, loss = 0.26746909\n",
      "Iteration 59, loss = 0.26717303\n",
      "Iteration 60, loss = 0.26740929\n",
      "Iteration 61, loss = 0.26710170\n",
      "Iteration 62, loss = 0.26661541\n",
      "Iteration 63, loss = 0.26675990\n",
      "Iteration 64, loss = 0.26665948\n",
      "Iteration 65, loss = 0.26628714\n",
      "Iteration 66, loss = 0.26633772\n",
      "Iteration 67, loss = 0.26613233\n",
      "Iteration 68, loss = 0.26635051\n",
      "Iteration 69, loss = 0.26611753\n",
      "Iteration 70, loss = 0.26582143\n",
      "Iteration 71, loss = 0.26578841\n",
      "Iteration 72, loss = 0.26602531\n",
      "Iteration 73, loss = 0.26533769\n",
      "Iteration 74, loss = 0.26542345\n",
      "Iteration 75, loss = 0.26537627\n",
      "Iteration 76, loss = 0.26516986\n",
      "Iteration 77, loss = 0.26506383\n",
      "Iteration 78, loss = 0.26536324\n",
      "Iteration 79, loss = 0.26468639\n",
      "Iteration 80, loss = 0.26460503\n",
      "Iteration 81, loss = 0.26445114\n",
      "Iteration 82, loss = 0.26430332\n",
      "Iteration 83, loss = 0.26411153\n",
      "Iteration 84, loss = 0.26451955\n",
      "Iteration 85, loss = 0.26392498\n",
      "Iteration 86, loss = 0.26445233\n",
      "Iteration 87, loss = 0.26394323\n",
      "Iteration 88, loss = 0.26390219\n",
      "Iteration 89, loss = 0.26371300\n",
      "Iteration 90, loss = 0.26346544\n",
      "Iteration 91, loss = 0.26376605\n",
      "Iteration 92, loss = 0.26309049\n",
      "Iteration 93, loss = 0.26341541\n",
      "Iteration 94, loss = 0.26326157\n",
      "Iteration 95, loss = 0.26293571\n",
      "Iteration 96, loss = 0.26369287\n",
      "Iteration 97, loss = 0.26297363\n",
      "Iteration 98, loss = 0.26276030\n",
      "Iteration 99, loss = 0.26270399\n",
      "Iteration 100, loss = 0.26249238\n",
      "Iteration 101, loss = 0.26245370\n",
      "Iteration 102, loss = 0.26225173\n",
      "Iteration 103, loss = 0.26213407\n",
      "Iteration 104, loss = 0.26279773\n",
      "Iteration 105, loss = 0.26189645\n",
      "Iteration 106, loss = 0.26199428\n",
      "Iteration 107, loss = 0.26175987\n",
      "Iteration 108, loss = 0.26161751\n",
      "Iteration 109, loss = 0.26172343\n",
      "Iteration 110, loss = 0.26178034\n",
      "Iteration 111, loss = 0.26144033\n",
      "Iteration 112, loss = 0.26172764\n",
      "Iteration 113, loss = 0.26159214\n",
      "Iteration 114, loss = 0.26124046\n",
      "Iteration 115, loss = 0.26121803\n",
      "Iteration 116, loss = 0.26142482\n",
      "Iteration 117, loss = 0.26096432\n",
      "Iteration 118, loss = 0.26117047\n",
      "Iteration 119, loss = 0.26113657\n",
      "Iteration 120, loss = 0.26140041\n",
      "Iteration 121, loss = 0.26097082\n",
      "Iteration 122, loss = 0.26090741\n",
      "Iteration 123, loss = 0.26080441\n",
      "Iteration 124, loss = 0.26085451\n",
      "Iteration 125, loss = 0.26070519\n",
      "Iteration 126, loss = 0.26055031\n",
      "Iteration 127, loss = 0.26057626\n",
      "Iteration 128, loss = 0.26046832\n",
      "Iteration 129, loss = 0.26052270\n",
      "Iteration 130, loss = 0.26047792\n",
      "Iteration 131, loss = 0.26049842\n",
      "Iteration 132, loss = 0.26012812\n",
      "Iteration 133, loss = 0.26046525\n",
      "Iteration 134, loss = 0.26018900\n",
      "Iteration 135, loss = 0.26019985\n",
      "Iteration 136, loss = 0.26018137\n",
      "Iteration 137, loss = 0.26002356\n",
      "Iteration 138, loss = 0.26014605\n",
      "Iteration 139, loss = 0.25993045\n",
      "Iteration 140, loss = 0.25987414\n",
      "Iteration 141, loss = 0.25979837\n",
      "Iteration 142, loss = 0.26005532\n",
      "Iteration 143, loss = 0.25970461\n",
      "Iteration 144, loss = 0.25976690\n",
      "Iteration 145, loss = 0.26057901\n",
      "Iteration 146, loss = 0.25978631\n",
      "Iteration 147, loss = 0.25997364\n",
      "Iteration 148, loss = 0.25969339\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34701343\n",
      "Iteration 1, loss = 0.34721249\n",
      "Iteration 2, loss = 0.30065772\n",
      "Iteration 2, loss = 0.30068850\n",
      "Iteration 3, loss = 0.28662619\n",
      "Iteration 3, loss = 0.28625053\n",
      "Iteration 4, loss = 0.28203662\n",
      "Iteration 4, loss = 0.28180430\n",
      "Iteration 5, loss = 0.28067386\n",
      "Iteration 5, loss = 0.28056197\n",
      "Iteration 6, loss = 0.27955010\n",
      "Iteration 6, loss = 0.27967324\n",
      "Iteration 7, loss = 0.27876291\n",
      "Iteration 7, loss = 0.27880795\n",
      "Iteration 8, loss = 0.27832084\n",
      "Iteration 8, loss = 0.27816904\n",
      "Iteration 9, loss = 0.27798274\n",
      "Iteration 9, loss = 0.27799602\n",
      "Iteration 10, loss = 0.27741673\n",
      "Iteration 10, loss = 0.27747633\n",
      "Iteration 11, loss = 0.27745722\n",
      "Iteration 11, loss = 0.27769622\n",
      "Iteration 12, loss = 0.27654476\n",
      "Iteration 12, loss = 0.27641501\n",
      "Iteration 13, loss = 0.27624111\n",
      "Iteration 13, loss = 0.27635925\n",
      "Iteration 14, loss = 0.27604635\n",
      "Iteration 14, loss = 0.27592274\n",
      "Iteration 15, loss = 0.27589755\n",
      "Iteration 15, loss = 0.27558553\n",
      "Iteration 16, loss = 0.27518397\n",
      "Iteration 16, loss = 0.27504771\n",
      "Iteration 17, loss = 0.27515628\n",
      "Iteration 17, loss = 0.27498311\n",
      "Iteration 18, loss = 0.27443318\n",
      "Iteration 18, loss = 0.27443124\n",
      "Iteration 19, loss = 0.27372346\n",
      "Iteration 19, loss = 0.27346131\n",
      "Iteration 20, loss = 0.27387209\n",
      "Iteration 20, loss = 0.27354283\n",
      "Iteration 21, loss = 0.27338577\n",
      "Iteration 21, loss = 0.27286628\n",
      "Iteration 22, loss = 0.27283426\n",
      "Iteration 22, loss = 0.27236603\n",
      "Iteration 23, loss = 0.27266187\n",
      "Iteration 23, loss = 0.27223920\n",
      "Iteration 24, loss = 0.27254006\n",
      "Iteration 24, loss = 0.27197816\n",
      "Iteration 25, loss = 0.27228060\n",
      "Iteration 25, loss = 0.27169344\n",
      "Iteration 26, loss = 0.27197789\n",
      "Iteration 26, loss = 0.27120471\n",
      "Iteration 27, loss = 0.27160278\n",
      "Iteration 27, loss = 0.27101038\n",
      "Iteration 28, loss = 0.27109280\n",
      "Iteration 28, loss = 0.27062348\n",
      "Iteration 29, loss = 0.27116518\n",
      "Iteration 29, loss = 0.27062149\n",
      "Iteration 30, loss = 0.27090424\n",
      "Iteration 30, loss = 0.27016542\n",
      "Iteration 31, loss = 0.27039279\n",
      "Iteration 31, loss = 0.26973423\n",
      "Iteration 32, loss = 0.27007028\n",
      "Iteration 32, loss = 0.26938218\n",
      "Iteration 33, loss = 0.27007141\n",
      "Iteration 33, loss = 0.26935655\n",
      "Iteration 34, loss = 0.26977638\n",
      "Iteration 34, loss = 0.26887977\n",
      "Iteration 35, loss = 0.26945869\n",
      "Iteration 35, loss = 0.26855125\n",
      "Iteration 36, loss = 0.26929816\n",
      "Iteration 36, loss = 0.26832466\n",
      "Iteration 37, loss = 0.26895924\n",
      "Iteration 37, loss = 0.26784718\n",
      "Iteration 38, loss = 0.26897683\n",
      "Iteration 38, loss = 0.26815416\n",
      "Iteration 39, loss = 0.26864689\n",
      "Iteration 39, loss = 0.26742206\n",
      "Iteration 40, loss = 0.26730261\n",
      "Iteration 40, loss = 0.26835372\n",
      "Iteration 41, loss = 0.26721168\n",
      "Iteration 41, loss = 0.26846744\n",
      "Iteration 42, loss = 0.26688930\n",
      "Iteration 42, loss = 0.26823973\n",
      "Iteration 43, loss = 0.26636788\n",
      "Iteration 43, loss = 0.26787571\n",
      "Iteration 44, loss = 0.26632433\n",
      "Iteration 44, loss = 0.26769666\n",
      "Iteration 45, loss = 0.26620784\n",
      "Iteration 45, loss = 0.26785654\n",
      "Iteration 46, loss = 0.26602769\n",
      "Iteration 46, loss = 0.26766535\n",
      "Iteration 47, loss = 0.26575575\n",
      "Iteration 47, loss = 0.26744941\n",
      "Iteration 48, loss = 0.26534919\n",
      "Iteration 49, loss = 0.26540956\n",
      "Iteration 48, loss = 0.26721100\n",
      "Iteration 49, loss = 0.26741001\n",
      "Iteration 50, loss = 0.26503157\n",
      "Iteration 50, loss = 0.26743318\n",
      "Iteration 51, loss = 0.26491398\n",
      "Iteration 51, loss = 0.26693108\n",
      "Iteration 52, loss = 0.26656652\n",
      "Iteration 52, loss = 0.26461947\n",
      "Iteration 53, loss = 0.26499824\n",
      "Iteration 53, loss = 0.26705238\n",
      "Iteration 54, loss = 0.26493310\n",
      "Iteration 54, loss = 0.26698731\n",
      "Iteration 55, loss = 0.26426997\n",
      "Iteration 55, loss = 0.26645153\n",
      "Iteration 56, loss = 0.26437384\n",
      "Iteration 56, loss = 0.26666320\n",
      "Iteration 57, loss = 0.26394408\n",
      "Iteration 57, loss = 0.26595944\n",
      "Iteration 58, loss = 0.26358257\n",
      "Iteration 58, loss = 0.26584592\n",
      "Iteration 59, loss = 0.26373583\n",
      "Iteration 59, loss = 0.26609768\n",
      "Iteration 60, loss = 0.26428835\n",
      "Iteration 60, loss = 0.26651392\n",
      "Iteration 61, loss = 0.26343243\n",
      "Iteration 61, loss = 0.26560605\n",
      "Iteration 62, loss = 0.26347320\n",
      "Iteration 62, loss = 0.26593995\n",
      "Iteration 63, loss = 0.26310879\n",
      "Iteration 63, loss = 0.26564362\n",
      "Iteration 64, loss = 0.26296304\n",
      "Iteration 64, loss = 0.26523447\n",
      "Iteration 65, loss = 0.26288305\n",
      "Iteration 65, loss = 0.26536667\n",
      "Iteration 66, loss = 0.26297749\n",
      "Iteration 66, loss = 0.26535094\n",
      "Iteration 67, loss = 0.26265631\n",
      "Iteration 67, loss = 0.26489140\n",
      "Iteration 68, loss = 0.26280071\n",
      "Iteration 68, loss = 0.26511636\n",
      "Iteration 69, loss = 0.26265025\n",
      "Iteration 69, loss = 0.26508938\n",
      "Iteration 70, loss = 0.26283251\n",
      "Iteration 71, loss = 0.26243849\n",
      "Iteration 70, loss = 0.26492421\n",
      "Iteration 72, loss = 0.26226124\n",
      "Iteration 71, loss = 0.26478297\n",
      "Iteration 73, loss = 0.26223194\n",
      "Iteration 72, loss = 0.26447829\n",
      "Iteration 74, loss = 0.26232689\n",
      "Iteration 73, loss = 0.26474695\n",
      "Iteration 74, loss = 0.26461170\n",
      "Iteration 75, loss = 0.26216520\n",
      "Iteration 75, loss = 0.26451763\n",
      "Iteration 76, loss = 0.26237410\n",
      "Iteration 76, loss = 0.26445051\n",
      "Iteration 77, loss = 0.26227761\n",
      "Iteration 78, loss = 0.26248085\n",
      "Iteration 77, loss = 0.26422134\n",
      "Iteration 79, loss = 0.26209499\n",
      "Iteration 78, loss = 0.26439831\n",
      "Iteration 80, loss = 0.26188833\n",
      "Iteration 79, loss = 0.26476875\n",
      "Iteration 81, loss = 0.26187375\n",
      "Iteration 80, loss = 0.26420475\n",
      "Iteration 82, loss = 0.26161016\n",
      "Iteration 81, loss = 0.26395594\n",
      "Iteration 83, loss = 0.26209364\n",
      "Iteration 82, loss = 0.26382101\n",
      "Iteration 84, loss = 0.26141736\n",
      "Iteration 83, loss = 0.26445154\n",
      "Iteration 85, loss = 0.26136211\n",
      "Iteration 84, loss = 0.26362052\n",
      "Iteration 86, loss = 0.26144072\n",
      "Iteration 85, loss = 0.26380784\n",
      "Iteration 87, loss = 0.26112957\n",
      "Iteration 86, loss = 0.26369358\n",
      "Iteration 88, loss = 0.26112218\n",
      "Iteration 87, loss = 0.26370736\n",
      "Iteration 89, loss = 0.26145922\n",
      "Iteration 88, loss = 0.26356165\n",
      "Iteration 90, loss = 0.26105034\n",
      "Iteration 89, loss = 0.26401703\n",
      "Iteration 91, loss = 0.26100257\n",
      "Iteration 90, loss = 0.26361340Iteration 92, loss = 0.26100275\n",
      "\n",
      "Iteration 93, loss = 0.26098280\n",
      "Iteration 91, loss = 0.26352057\n",
      "Iteration 92, loss = 0.26334603\n",
      "Iteration 94, loss = 0.26104168\n",
      "Iteration 93, loss = 0.26331939\n",
      "Iteration 95, loss = 0.26090766\n",
      "Iteration 94, loss = 0.26362394\n",
      "Iteration 96, loss = 0.26087659\n",
      "Iteration 97, loss = 0.26084422\n",
      "Iteration 95, loss = 0.26334087\n",
      "Iteration 98, loss = 0.26082045\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 96, loss = 0.26305991\n",
      "Iteration 97, loss = 0.26316428\n",
      "Iteration 1, loss = 0.34587661\n",
      "Iteration 98, loss = 0.26348417\n",
      "Iteration 2, loss = 0.29983644\n",
      "Iteration 99, loss = 0.26313470\n",
      "Iteration 3, loss = 0.28684040\n",
      "Iteration 100, loss = 0.26300788\n",
      "Iteration 101, loss = 0.26281655\n",
      "Iteration 4, loss = 0.28248787\n",
      "Iteration 5, loss = 0.28057680\n",
      "Iteration 102, loss = 0.26320064\n",
      "Iteration 6, loss = 0.27958551\n",
      "Iteration 103, loss = 0.26280149\n",
      "Iteration 7, loss = 0.27919151\n",
      "Iteration 104, loss = 0.26291272\n",
      "Iteration 8, loss = 0.27870595\n",
      "Iteration 9, loss = 0.27823317\n",
      "Iteration 105, loss = 0.26269959\n",
      "Iteration 10, loss = 0.27783161\n",
      "Iteration 106, loss = 0.26273619\n",
      "Iteration 11, loss = 0.27794309\n",
      "Iteration 107, loss = 0.26250479\n",
      "Iteration 12, loss = 0.27719660\n",
      "Iteration 108, loss = 0.26262695\n",
      "Iteration 13, loss = 0.27655047\n",
      "Iteration 109, loss = 0.26226673\n",
      "Iteration 14, loss = 0.27630168\n",
      "Iteration 110, loss = 0.26288144\n",
      "Iteration 15, loss = 0.27570908\n",
      "Iteration 111, loss = 0.26252061\n",
      "Iteration 16, loss = 0.27572930\n",
      "Iteration 112, loss = 0.26289736\n",
      "Iteration 17, loss = 0.27539189\n",
      "Iteration 113, loss = 0.26239586\n",
      "Iteration 18, loss = 0.27499591\n",
      "Iteration 114, loss = 0.26235635\n",
      "Iteration 19, loss = 0.27415999\n",
      "Iteration 115, loss = 0.26201924\n",
      "Iteration 20, loss = 0.27413763\n",
      "Iteration 116, loss = 0.26218523\n",
      "Iteration 21, loss = 0.27359001\n",
      "Iteration 117, loss = 0.26227131\n",
      "Iteration 22, loss = 0.27356199\n",
      "Iteration 118, loss = 0.26240655\n",
      "Iteration 23, loss = 0.27315275\n",
      "Iteration 24, loss = 0.27303164\n",
      "Iteration 119, loss = 0.26208681\n",
      "Iteration 25, loss = 0.27265535\n",
      "Iteration 120, loss = 0.26216680\n",
      "Iteration 26, loss = 0.27202844\n",
      "Iteration 121, loss = 0.26195751\n",
      "Iteration 27, loss = 0.27171013\n",
      "Iteration 122, loss = 0.26202833\n",
      "Iteration 28, loss = 0.27123256\n",
      "Iteration 123, loss = 0.26200955\n",
      "Iteration 29, loss = 0.27112242\n",
      "Iteration 124, loss = 0.26188468\n",
      "Iteration 30, loss = 0.27100734\n",
      "Iteration 125, loss = 0.26221683\n",
      "Iteration 31, loss = 0.27049802\n",
      "Iteration 126, loss = 0.26188109\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 32, loss = 0.27055042\n",
      "Iteration 33, loss = 0.26988319\n",
      "Iteration 1, loss = 0.34628326\n",
      "Iteration 34, loss = 0.26970647\n",
      "Iteration 2, loss = 0.29971909\n",
      "Iteration 35, loss = 0.26949770\n",
      "Iteration 3, loss = 0.28649597\n",
      "Iteration 36, loss = 0.26927907\n",
      "Iteration 4, loss = 0.28200425\n",
      "Iteration 37, loss = 0.26872455\n",
      "Iteration 5, loss = 0.28023875\n",
      "Iteration 38, loss = 0.26898894\n",
      "Iteration 6, loss = 0.27901272\n",
      "Iteration 39, loss = 0.26873381\n",
      "Iteration 7, loss = 0.27883876\n",
      "Iteration 40, loss = 0.26843913\n",
      "Iteration 41, loss = 0.26817256\n",
      "Iteration 8, loss = 0.27831683\n",
      "Iteration 42, loss = 0.26804037\n",
      "Iteration 9, loss = 0.27801625\n",
      "Iteration 43, loss = 0.26805340\n",
      "Iteration 44, loss = 0.26775265\n",
      "Iteration 10, loss = 0.27745979\n",
      "Iteration 45, loss = 0.26742467\n",
      "Iteration 11, loss = 0.27752209\n",
      "Iteration 46, loss = 0.26717768\n",
      "Iteration 12, loss = 0.27694215\n",
      "Iteration 47, loss = 0.26706222\n",
      "Iteration 13, loss = 0.27642899\n",
      "Iteration 48, loss = 0.26710624\n",
      "Iteration 14, loss = 0.27602208\n",
      "Iteration 49, loss = 0.26704097\n",
      "Iteration 15, loss = 0.27551526\n",
      "Iteration 50, loss = 0.26661289\n",
      "Iteration 16, loss = 0.27526310\n",
      "Iteration 51, loss = 0.26661112\n",
      "Iteration 17, loss = 0.27520012\n",
      "Iteration 18, loss = 0.27490010\n",
      "Iteration 52, loss = 0.26658797\n",
      "Iteration 53, loss = 0.26630575\n",
      "Iteration 19, loss = 0.27411296\n",
      "Iteration 54, loss = 0.26615692\n",
      "Iteration 20, loss = 0.27437828\n",
      "Iteration 55, loss = 0.26615099\n",
      "Iteration 21, loss = 0.27353341\n",
      "Iteration 56, loss = 0.26586597\n",
      "Iteration 22, loss = 0.27382652\n",
      "Iteration 23, loss = 0.27322347\n",
      "Iteration 57, loss = 0.26573593\n",
      "Iteration 24, loss = 0.27282560\n",
      "Iteration 58, loss = 0.26568180\n",
      "Iteration 25, loss = 0.27264533\n",
      "Iteration 59, loss = 0.26556916\n",
      "Iteration 26, loss = 0.27200895\n",
      "Iteration 60, loss = 0.26565020\n",
      "Iteration 27, loss = 0.27185748\n",
      "Iteration 61, loss = 0.26564573\n",
      "Iteration 28, loss = 0.27138990\n",
      "Iteration 62, loss = 0.26526027\n",
      "Iteration 29, loss = 0.27136847\n",
      "Iteration 30, loss = 0.27131568\n",
      "Iteration 63, loss = 0.26466919\n",
      "Iteration 31, loss = 0.27114752\n",
      "Iteration 64, loss = 0.26481399\n",
      "Iteration 32, loss = 0.27079942\n",
      "Iteration 65, loss = 0.26465544Iteration 33, loss = 0.27027170\n",
      "\n",
      "Iteration 34, loss = 0.27015755\n",
      "Iteration 66, loss = 0.26518942\n",
      "Iteration 67, loss = 0.26471338\n",
      "Iteration 35, loss = 0.27000234\n",
      "Iteration 68, loss = 0.26466147\n",
      "Iteration 36, loss = 0.26955612\n",
      "Iteration 69, loss = 0.26421733\n",
      "Iteration 37, loss = 0.26926881\n",
      "Iteration 70, loss = 0.26392747\n",
      "Iteration 38, loss = 0.26936737\n",
      "Iteration 71, loss = 0.26387045\n",
      "Iteration 39, loss = 0.26903345\n",
      "Iteration 72, loss = 0.26360991\n",
      "Iteration 40, loss = 0.26890886\n",
      "Iteration 73, loss = 0.26382171\n",
      "Iteration 41, loss = 0.26852812\n",
      "Iteration 74, loss = 0.26367779\n",
      "Iteration 42, loss = 0.26867901\n",
      "Iteration 75, loss = 0.26322194\n",
      "Iteration 43, loss = 0.26837199\n",
      "Iteration 76, loss = 0.26353028\n",
      "Iteration 44, loss = 0.26838373\n",
      "Iteration 77, loss = 0.26300167\n",
      "Iteration 78, loss = 0.26309010\n",
      "Iteration 45, loss = 0.26789872\n",
      "Iteration 79, loss = 0.26301809\n",
      "Iteration 46, loss = 0.26750978\n",
      "Iteration 80, loss = 0.26285728\n",
      "Iteration 47, loss = 0.26753797\n",
      "Iteration 81, loss = 0.26274988\n",
      "Iteration 48, loss = 0.26757055\n",
      "Iteration 49, loss = 0.26736456\n",
      "Iteration 82, loss = 0.26280225\n",
      "Iteration 83, loss = 0.26289499\n",
      "Iteration 50, loss = 0.26700401\n",
      "Iteration 84, loss = 0.26275494\n",
      "Iteration 51, loss = 0.26686601\n",
      "Iteration 85, loss = 0.26261246Iteration 52, loss = 0.26676650\n",
      "\n",
      "Iteration 86, loss = 0.26271949Iteration 53, loss = 0.26632220\n",
      "\n",
      "Iteration 54, loss = 0.26612933\n",
      "Iteration 87, loss = 0.26232756\n",
      "Iteration 55, loss = 0.26618167\n",
      "Iteration 88, loss = 0.26221542\n",
      "Iteration 89, loss = 0.26213287Iteration 56, loss = 0.26597401\n",
      "\n",
      "Iteration 57, loss = 0.26575174\n",
      "Iteration 90, loss = 0.26217623\n",
      "Iteration 91, loss = 0.26212412\n",
      "Iteration 58, loss = 0.26557508\n",
      "Iteration 59, loss = 0.26538927\n",
      "Iteration 92, loss = 0.26191995\n",
      "Iteration 60, loss = 0.26531832\n",
      "Iteration 93, loss = 0.26182059\n",
      "Iteration 61, loss = 0.26580074\n",
      "Iteration 94, loss = 0.26176876\n",
      "Iteration 62, loss = 0.26510971\n",
      "Iteration 95, loss = 0.26185612\n",
      "Iteration 96, loss = 0.26175248\n",
      "Iteration 63, loss = 0.26455681\n",
      "Iteration 97, loss = 0.26149878\n",
      "Iteration 64, loss = 0.26490212\n",
      "Iteration 98, loss = 0.26179520\n",
      "Iteration 65, loss = 0.26455050\n",
      "Iteration 99, loss = 0.26126546\n",
      "Iteration 66, loss = 0.26520419\n",
      "Iteration 100, loss = 0.26171260\n",
      "Iteration 67, loss = 0.26462275\n",
      "Iteration 101, loss = 0.26140204\n",
      "Iteration 68, loss = 0.26451986\n",
      "Iteration 102, loss = 0.26145778\n",
      "Iteration 69, loss = 0.26433587\n",
      "Iteration 103, loss = 0.26119844\n",
      "Iteration 70, loss = 0.26411201\n",
      "Iteration 104, loss = 0.26095372\n",
      "Iteration 71, loss = 0.26415242\n",
      "Iteration 72, loss = 0.26380809\n",
      "Iteration 105, loss = 0.26129938\n",
      "Iteration 73, loss = 0.26446236\n",
      "Iteration 106, loss = 0.26122780\n",
      "Iteration 74, loss = 0.26382041\n",
      "Iteration 107, loss = 0.26114167\n",
      "Iteration 75, loss = 0.26366116\n",
      "Iteration 108, loss = 0.26076867\n",
      "Iteration 76, loss = 0.26380184\n",
      "Iteration 109, loss = 0.26107417\n",
      "Iteration 77, loss = 0.26324863\n",
      "Iteration 110, loss = 0.26086410\n",
      "Iteration 78, loss = 0.26334946\n",
      "Iteration 111, loss = 0.26106016\n",
      "Iteration 79, loss = 0.26321587\n",
      "Iteration 80, loss = 0.26323282\n",
      "Iteration 112, loss = 0.26129277\n",
      "Iteration 81, loss = 0.26287811\n",
      "Iteration 113, loss = 0.26075941\n",
      "Iteration 82, loss = 0.26312709\n",
      "Iteration 114, loss = 0.26070178\n",
      "Iteration 83, loss = 0.26392345\n",
      "Iteration 115, loss = 0.26090910\n",
      "Iteration 84, loss = 0.26298919\n",
      "Iteration 116, loss = 0.26048612\n",
      "Iteration 85, loss = 0.26280544\n",
      "Iteration 117, loss = 0.26057398\n",
      "Iteration 86, loss = 0.26321835\n",
      "Iteration 118, loss = 0.26069471\n",
      "Iteration 87, loss = 0.26255904\n",
      "Iteration 119, loss = 0.26060544\n",
      "Iteration 88, loss = 0.26275932\n",
      "Iteration 120, loss = 0.26069228\n",
      "Iteration 89, loss = 0.26270568\n",
      "Iteration 121, loss = 0.26043019\n",
      "Iteration 90, loss = 0.26273783\n",
      "Iteration 122, loss = 0.26022957\n",
      "Iteration 91, loss = 0.26228873\n",
      "Iteration 123, loss = 0.26022721\n",
      "Iteration 124, loss = 0.26025255\n",
      "Iteration 92, loss = 0.26227795\n",
      "Iteration 93, loss = 0.26208740\n",
      "Iteration 125, loss = 0.26024946\n",
      "Iteration 94, loss = 0.26242847\n",
      "Iteration 126, loss = 0.26034866\n",
      "Iteration 95, loss = 0.26211500\n",
      "Iteration 127, loss = 0.26049787\n",
      "Iteration 96, loss = 0.26203210\n",
      "Iteration 128, loss = 0.26024396\n",
      "Iteration 97, loss = 0.26241639\n",
      "Iteration 129, loss = 0.26016347\n",
      "Iteration 130, loss = 0.26017659\n",
      "Iteration 98, loss = 0.26220001\n",
      "Iteration 131, loss = 0.26024803\n",
      "Iteration 99, loss = 0.26183874\n",
      "Iteration 132, loss = 0.26027636\n",
      "Iteration 100, loss = 0.26219927\n",
      "Iteration 133, loss = 0.26000544\n",
      "Iteration 101, loss = 0.26209463\n",
      "Iteration 102, loss = 0.26169653\n",
      "Iteration 134, loss = 0.26001050\n",
      "Iteration 103, loss = 0.26195777\n",
      "Iteration 135, loss = 0.25982793\n",
      "Iteration 104, loss = 0.26161906\n",
      "Iteration 136, loss = 0.25995961\n",
      "Iteration 105, loss = 0.26184983\n",
      "Iteration 137, loss = 0.25979680\n",
      "Iteration 106, loss = 0.26169694\n",
      "Iteration 138, loss = 0.25968708\n",
      "Iteration 107, loss = 0.26171677\n",
      "Iteration 139, loss = 0.25989532\n",
      "Iteration 108, loss = 0.26153324\n",
      "Iteration 140, loss = 0.25966527\n",
      "Iteration 109, loss = 0.26153209\n",
      "Iteration 141, loss = 0.25966356\n",
      "Iteration 110, loss = 0.26190264\n",
      "Iteration 142, loss = 0.25971815\n",
      "Iteration 111, loss = 0.26165269\n",
      "Iteration 143, loss = 0.25974499\n",
      "Iteration 112, loss = 0.26142940\n",
      "Iteration 144, loss = 0.25968789\n",
      "Iteration 113, loss = 0.26148521\n",
      "Iteration 145, loss = 0.25983755\n",
      "Iteration 114, loss = 0.26106707\n",
      "Iteration 146, loss = 0.25968752\n",
      "Iteration 115, loss = 0.26128462\n",
      "Iteration 147, loss = 0.25947740\n",
      "Iteration 116, loss = 0.26107684\n",
      "Iteration 148, loss = 0.25924205\n",
      "Iteration 117, loss = 0.26137066\n",
      "Iteration 149, loss = 0.25951661\n",
      "Iteration 150, loss = 0.25937406\n",
      "Iteration 118, loss = 0.26114456\n",
      "Iteration 151, loss = 0.25944237\n",
      "Iteration 119, loss = 0.26125727\n",
      "Iteration 120, loss = 0.26105496\n",
      "Iteration 152, loss = 0.25921891\n",
      "Iteration 153, loss = 0.25926679\n",
      "Iteration 121, loss = 0.26094499\n",
      "Iteration 122, loss = 0.26084897\n",
      "Iteration 154, loss = 0.25955088Iteration 123, loss = 0.26114860\n",
      "\n",
      "Iteration 124, loss = 0.26083618\n",
      "Iteration 155, loss = 0.25928647\n",
      "Iteration 156, loss = 0.25914739\n",
      "Iteration 125, loss = 0.26096021\n",
      "Iteration 126, loss = 0.26092583\n",
      "Iteration 157, loss = 0.25915869\n",
      "Iteration 127, loss = 0.26056746\n",
      "Iteration 158, loss = 0.25923728\n",
      "Iteration 159, loss = 0.25922508\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 128, loss = 0.26077814\n",
      "Iteration 129, loss = 0.26079869\n",
      "Iteration 130, loss = 0.26073913\n",
      "Iteration 1, loss = 0.34633673\n",
      "Iteration 2, loss = 0.29967580\n",
      "Iteration 131, loss = 0.26072677\n",
      "Iteration 3, loss = 0.28652081\n",
      "Iteration 132, loss = 0.26094570\n",
      "Iteration 4, loss = 0.28197900\n",
      "Iteration 133, loss = 0.26059684\n",
      "Iteration 5, loss = 0.27997624\n",
      "Iteration 134, loss = 0.26076685\n",
      "Iteration 6, loss = 0.27912361\n",
      "Iteration 135, loss = 0.26038624\n",
      "Iteration 7, loss = 0.27866023\n",
      "Iteration 136, loss = 0.26057195\n",
      "Iteration 8, loss = 0.27819820\n",
      "Iteration 137, loss = 0.26048196\n",
      "Iteration 9, loss = 0.27778610\n",
      "Iteration 138, loss = 0.26040206\n",
      "Iteration 10, loss = 0.27743804\n",
      "Iteration 139, loss = 0.26013365\n",
      "Iteration 11, loss = 0.27710447\n",
      "Iteration 140, loss = 0.26032908\n",
      "Iteration 12, loss = 0.27651341\n",
      "Iteration 141, loss = 0.26037861\n",
      "Iteration 13, loss = 0.27608222\n",
      "Iteration 142, loss = 0.26007840\n",
      "Iteration 14, loss = 0.27559181\n",
      "Iteration 143, loss = 0.26027436\n",
      "Iteration 15, loss = 0.27522593\n",
      "Iteration 144, loss = 0.26026539\n",
      "Iteration 16, loss = 0.27462698\n",
      "Iteration 145, loss = 0.26070840\n",
      "Iteration 17, loss = 0.27455658\n",
      "Iteration 146, loss = 0.26008414\n",
      "Iteration 18, loss = 0.27417561\n",
      "Iteration 147, loss = 0.26011283\n",
      "Iteration 19, loss = 0.27336508\n",
      "Iteration 148, loss = 0.25978044\n",
      "Iteration 20, loss = 0.27374759\n",
      "Iteration 149, loss = 0.26035403\n",
      "Iteration 21, loss = 0.27247988\n",
      "Iteration 150, loss = 0.26006949\n",
      "Iteration 22, loss = 0.27277843\n",
      "Iteration 151, loss = 0.26000043\n",
      "Iteration 23, loss = 0.27207435\n",
      "Iteration 152, loss = 0.25989714\n",
      "Iteration 24, loss = 0.27184419\n",
      "Iteration 153, loss = 0.26014009\n",
      "Iteration 25, loss = 0.27180054\n",
      "Iteration 154, loss = 0.26020369\n",
      "Iteration 26, loss = 0.27116805\n",
      "Iteration 155, loss = 0.25985246\n",
      "Iteration 27, loss = 0.27056187\n",
      "Iteration 156, loss = 0.25968969\n",
      "Iteration 28, loss = 0.27040610\n",
      "Iteration 157, loss = 0.25971124\n",
      "Iteration 29, loss = 0.27004775\n",
      "Iteration 158, loss = 0.25983929\n",
      "Iteration 30, loss = 0.27006331\n",
      "Iteration 159, loss = 0.25990345\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 31, loss = 0.26982217\n",
      "Iteration 32, loss = 0.26947878\n",
      "Iteration 1, loss = 0.34682492\n",
      "Iteration 33, loss = 0.26906703\n",
      "Iteration 2, loss = 0.30041115\n",
      "Iteration 34, loss = 0.26880437\n",
      "Iteration 3, loss = 0.28726943\n",
      "Iteration 35, loss = 0.26825905\n",
      "Iteration 4, loss = 0.28250533\n",
      "Iteration 36, loss = 0.26819879\n",
      "Iteration 5, loss = 0.28077169\n",
      "Iteration 37, loss = 0.26784324\n",
      "Iteration 6, loss = 0.27987094\n",
      "Iteration 38, loss = 0.26806527\n",
      "Iteration 7, loss = 0.27941968\n",
      "Iteration 39, loss = 0.26730307\n",
      "Iteration 8, loss = 0.27896684\n",
      "Iteration 40, loss = 0.26783817\n",
      "Iteration 9, loss = 0.27887795\n",
      "Iteration 41, loss = 0.26661763\n",
      "Iteration 10, loss = 0.27797996\n",
      "Iteration 42, loss = 0.26718814\n",
      "Iteration 11, loss = 0.27762909\n",
      "Iteration 43, loss = 0.26693526\n",
      "Iteration 12, loss = 0.27734532\n",
      "Iteration 44, loss = 0.26669178\n",
      "Iteration 13, loss = 0.27709987\n",
      "Iteration 45, loss = 0.26595703\n",
      "Iteration 14, loss = 0.27655974\n",
      "Iteration 46, loss = 0.26582822\n",
      "Iteration 47, loss = 0.26593057\n",
      "Iteration 15, loss = 0.27639865\n",
      "Iteration 16, loss = 0.27594380\n",
      "Iteration 48, loss = 0.26599901\n",
      "Iteration 17, loss = 0.27568574\n",
      "Iteration 49, loss = 0.26557197\n",
      "Iteration 18, loss = 0.27563859\n",
      "Iteration 50, loss = 0.26540260\n",
      "Iteration 19, loss = 0.27461080\n",
      "Iteration 51, loss = 0.26557644\n",
      "Iteration 20, loss = 0.27527329\n",
      "Iteration 52, loss = 0.26531222\n",
      "Iteration 21, loss = 0.27393349\n",
      "Iteration 53, loss = 0.26496871\n",
      "Iteration 22, loss = 0.27401673\n",
      "Iteration 54, loss = 0.26472114\n",
      "Iteration 23, loss = 0.27336919\n",
      "Iteration 55, loss = 0.26481282\n",
      "Iteration 56, loss = 0.26492461\n",
      "Iteration 24, loss = 0.27275341\n",
      "Iteration 57, loss = 0.26439552\n",
      "Iteration 25, loss = 0.27269878\n",
      "Iteration 58, loss = 0.26451981\n",
      "Iteration 26, loss = 0.27242947\n",
      "Iteration 59, loss = 0.26411158\n",
      "Iteration 27, loss = 0.27188127\n",
      "Iteration 60, loss = 0.26425616\n",
      "Iteration 28, loss = 0.27166928\n",
      "Iteration 61, loss = 0.26464126\n",
      "Iteration 29, loss = 0.27152744\n",
      "Iteration 62, loss = 0.26387526\n",
      "Iteration 30, loss = 0.27130932\n",
      "Iteration 63, loss = 0.26356796\n",
      "Iteration 31, loss = 0.27131570\n",
      "Iteration 64, loss = 0.26380007\n",
      "Iteration 32, loss = 0.27110553\n",
      "Iteration 65, loss = 0.26354555\n",
      "Iteration 33, loss = 0.27071641\n",
      "Iteration 66, loss = 0.26455539\n",
      "Iteration 34, loss = 0.27052038\n",
      "Iteration 67, loss = 0.26335608\n",
      "Iteration 35, loss = 0.26984417\n",
      "Iteration 68, loss = 0.26361355\n",
      "Iteration 36, loss = 0.26976370\n",
      "Iteration 69, loss = 0.26313219\n",
      "Iteration 37, loss = 0.26978492\n",
      "Iteration 70, loss = 0.26307573\n",
      "Iteration 38, loss = 0.26989533\n",
      "Iteration 71, loss = 0.26319117\n",
      "Iteration 39, loss = 0.26909733\n",
      "Iteration 72, loss = 0.26296614\n",
      "Iteration 40, loss = 0.26938576\n",
      "Iteration 73, loss = 0.26328590\n",
      "Iteration 41, loss = 0.26877235\n",
      "Iteration 74, loss = 0.26293744\n",
      "Iteration 42, loss = 0.26905516\n",
      "Iteration 43, loss = 0.26865746\n",
      "Iteration 75, loss = 0.26271339\n",
      "Iteration 44, loss = 0.26852922\n",
      "Iteration 76, loss = 0.26319968\n",
      "Iteration 45, loss = 0.26824506\n",
      "Iteration 77, loss = 0.26233434\n",
      "Iteration 46, loss = 0.26821060\n",
      "Iteration 78, loss = 0.26245794\n",
      "Iteration 47, loss = 0.26852454\n",
      "Iteration 79, loss = 0.26255774\n",
      "Iteration 48, loss = 0.26794917\n",
      "Iteration 80, loss = 0.26269222\n",
      "Iteration 49, loss = 0.26770520\n",
      "Iteration 81, loss = 0.26216075\n",
      "Iteration 50, loss = 0.26758399\n",
      "Iteration 82, loss = 0.26225246\n",
      "Iteration 51, loss = 0.26778734\n",
      "Iteration 83, loss = 0.26264465\n",
      "Iteration 52, loss = 0.26757120\n",
      "Iteration 84, loss = 0.26200690\n",
      "Iteration 53, loss = 0.26727214\n",
      "Iteration 85, loss = 0.26198606\n",
      "Iteration 54, loss = 0.26723959\n",
      "Iteration 86, loss = 0.26256280\n",
      "Iteration 55, loss = 0.26710072\n",
      "Iteration 87, loss = 0.26189385\n",
      "Iteration 56, loss = 0.26748440\n",
      "Iteration 88, loss = 0.26196250\n",
      "Iteration 57, loss = 0.26672326\n",
      "Iteration 89, loss = 0.26197883\n",
      "Iteration 58, loss = 0.26693923\n",
      "Iteration 59, loss = 0.26648708\n",
      "Iteration 90, loss = 0.26172937\n",
      "Iteration 60, loss = 0.26642642\n",
      "Iteration 91, loss = 0.26148230\n",
      "Iteration 61, loss = 0.26655032\n",
      "Iteration 92, loss = 0.26140055\n",
      "Iteration 62, loss = 0.26650943\n",
      "Iteration 93, loss = 0.26133314\n",
      "Iteration 63, loss = 0.26620111\n",
      "Iteration 94, loss = 0.26136503\n",
      "Iteration 64, loss = 0.26606228\n",
      "Iteration 95, loss = 0.26118988\n",
      "Iteration 65, loss = 0.26602921\n",
      "Iteration 96, loss = 0.26137567\n",
      "Iteration 66, loss = 0.26651011\n",
      "Iteration 97, loss = 0.26118935\n",
      "Iteration 67, loss = 0.26574484\n",
      "Iteration 98, loss = 0.26149346\n",
      "Iteration 68, loss = 0.26578173\n",
      "Iteration 99, loss = 0.26102443\n",
      "Iteration 69, loss = 0.26551216\n",
      "Iteration 100, loss = 0.26117487\n",
      "Iteration 70, loss = 0.26566761\n",
      "Iteration 101, loss = 0.26133640\n",
      "Iteration 71, loss = 0.26561875\n",
      "Iteration 102, loss = 0.26084460\n",
      "Iteration 72, loss = 0.26561553\n",
      "Iteration 103, loss = 0.26106360\n",
      "Iteration 73, loss = 0.26586981\n",
      "Iteration 104, loss = 0.26127915\n",
      "Iteration 74, loss = 0.26536144\n",
      "Iteration 75, loss = 0.26521451\n",
      "Iteration 105, loss = 0.26112939\n",
      "Iteration 106, loss = 0.26082165\n",
      "Iteration 76, loss = 0.26547595\n",
      "Iteration 77, loss = 0.26494659\n",
      "Iteration 107, loss = 0.26079684\n",
      "Iteration 78, loss = 0.26507663\n",
      "Iteration 108, loss = 0.26075297\n",
      "Iteration 79, loss = 0.26479909\n",
      "Iteration 109, loss = 0.26073459\n",
      "Iteration 110, loss = 0.26069553\n",
      "Iteration 111, loss = 0.26054495\n",
      "Iteration 80, loss = 0.26495364\n",
      "Iteration 81, loss = 0.26480998\n",
      "Iteration 112, loss = 0.26083518\n",
      "Iteration 82, loss = 0.26474449\n",
      "Iteration 113, loss = 0.26099142\n",
      "Iteration 83, loss = 0.26524079\n",
      "Iteration 114, loss = 0.26026665\n",
      "Iteration 84, loss = 0.26462038\n",
      "Iteration 115, loss = 0.26043974\n",
      "Iteration 85, loss = 0.26453630\n",
      "Iteration 116, loss = 0.26026530\n",
      "Iteration 86, loss = 0.26455994\n",
      "Iteration 117, loss = 0.26075949\n",
      "Iteration 87, loss = 0.26434858\n",
      "Iteration 118, loss = 0.26011547\n",
      "Iteration 88, loss = 0.26448484\n",
      "Iteration 119, loss = 0.26056719\n",
      "Iteration 89, loss = 0.26425578\n",
      "Iteration 120, loss = 0.26068078\n",
      "Iteration 90, loss = 0.26413302\n",
      "Iteration 121, loss = 0.26017339\n",
      "Iteration 91, loss = 0.26406619\n",
      "Iteration 122, loss = 0.25971041\n",
      "Iteration 92, loss = 0.26385737\n",
      "Iteration 123, loss = 0.26001903\n",
      "Iteration 93, loss = 0.26371050\n",
      "Iteration 124, loss = 0.26003134\n",
      "Iteration 94, loss = 0.26384719\n",
      "Iteration 125, loss = 0.26028830\n",
      "Iteration 95, loss = 0.26367744\n",
      "Iteration 126, loss = 0.26013814\n",
      "Iteration 96, loss = 0.26377094\n",
      "Iteration 127, loss = 0.25985500\n",
      "Iteration 97, loss = 0.26372407\n",
      "Iteration 128, loss = 0.25966436\n",
      "Iteration 98, loss = 0.26371094\n",
      "Iteration 99, loss = 0.26359838\n",
      "Iteration 100, loss = 0.26356586Iteration 129, loss = 0.25992631\n",
      "\n",
      "Iteration 101, loss = 0.26384613\n",
      "Iteration 130, loss = 0.25969082\n",
      "Iteration 131, loss = 0.26022253\n",
      "Iteration 102, loss = 0.26348155\n",
      "Iteration 132, loss = 0.26007740\n",
      "Iteration 103, loss = 0.26340272\n",
      "Iteration 133, loss = 0.25978453\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 104, loss = 0.26349089\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34611796\n",
      "Iteration 1, loss = 0.34637413\n",
      "Iteration 2, loss = 0.29958619\n",
      "Iteration 2, loss = 0.30031107\n",
      "Iteration 3, loss = 0.28666953\n",
      "Iteration 3, loss = 0.28662089\n",
      "Iteration 4, loss = 0.28197344\n",
      "Iteration 4, loss = 0.28196822\n",
      "Iteration 5, loss = 0.28024443\n",
      "Iteration 5, loss = 0.28028352\n",
      "Iteration 6, loss = 0.27956623\n",
      "Iteration 6, loss = 0.27977982\n",
      "Iteration 7, loss = 0.27890921\n",
      "Iteration 7, loss = 0.27873763\n",
      "Iteration 8, loss = 0.27844905\n",
      "Iteration 8, loss = 0.27822767\n",
      "Iteration 9, loss = 0.27841765\n",
      "Iteration 9, loss = 0.27868323\n",
      "Iteration 10, loss = 0.27750567\n",
      "Iteration 10, loss = 0.27718031\n",
      "Iteration 11, loss = 0.27691861\n",
      "Iteration 11, loss = 0.27639586\n",
      "Iteration 12, loss = 0.27710493\n",
      "Iteration 12, loss = 0.27660178\n",
      "Iteration 13, loss = 0.27668498\n",
      "Iteration 13, loss = 0.27639682\n",
      "Iteration 14, loss = 0.27570877\n",
      "Iteration 14, loss = 0.27527103\n",
      "Iteration 15, loss = 0.27556019\n",
      "Iteration 15, loss = 0.27520194\n",
      "Iteration 16, loss = 0.27502308\n",
      "Iteration 16, loss = 0.27478202\n",
      "Iteration 17, loss = 0.27480166\n",
      "Iteration 17, loss = 0.27402358\n",
      "Iteration 18, loss = 0.27497667\n",
      "Iteration 18, loss = 0.27422376\n",
      "Iteration 19, loss = 0.27359698\n",
      "Iteration 19, loss = 0.27323810\n",
      "Iteration 20, loss = 0.27403101\n",
      "Iteration 20, loss = 0.27311333\n",
      "Iteration 21, loss = 0.27272044\n",
      "Iteration 21, loss = 0.27240148\n",
      "Iteration 22, loss = 0.27307778\n",
      "Iteration 22, loss = 0.27267472\n",
      "Iteration 23, loss = 0.27224746\n",
      "Iteration 24, loss = 0.27172693\n",
      "Iteration 23, loss = 0.27199003\n",
      "Iteration 24, loss = 0.27160119\n",
      "Iteration 25, loss = 0.27166921\n",
      "Iteration 25, loss = 0.27103329\n",
      "Iteration 26, loss = 0.27148066\n",
      "Iteration 27, loss = 0.27062085\n",
      "Iteration 26, loss = 0.27109626\n",
      "Iteration 28, loss = 0.27085927\n",
      "Iteration 27, loss = 0.27034570\n",
      "Iteration 29, loss = 0.27057948\n",
      "Iteration 28, loss = 0.27040015\n",
      "Iteration 30, loss = 0.27031840\n",
      "Iteration 29, loss = 0.26987208\n",
      "Iteration 31, loss = 0.27039232\n",
      "Iteration 30, loss = 0.26948851\n",
      "Iteration 32, loss = 0.26997172\n",
      "Iteration 31, loss = 0.26972066\n",
      "Iteration 33, loss = 0.26958714\n",
      "Iteration 34, loss = 0.26937147\n",
      "Iteration 32, loss = 0.26930453\n",
      "Iteration 35, loss = 0.26878490\n",
      "Iteration 33, loss = 0.26924633\n",
      "Iteration 36, loss = 0.26852821\n",
      "Iteration 34, loss = 0.26872709\n",
      "Iteration 37, loss = 0.26826899\n",
      "Iteration 35, loss = 0.26816284\n",
      "Iteration 38, loss = 0.26864189\n",
      "Iteration 36, loss = 0.26804336\n",
      "Iteration 39, loss = 0.26781062\n",
      "Iteration 37, loss = 0.26791088\n",
      "Iteration 40, loss = 0.26792961\n",
      "Iteration 38, loss = 0.26781018\n",
      "Iteration 41, loss = 0.26738843\n",
      "Iteration 39, loss = 0.26736453\n",
      "Iteration 42, loss = 0.26772840\n",
      "Iteration 40, loss = 0.26730942\n",
      "Iteration 43, loss = 0.26722001\n",
      "Iteration 41, loss = 0.26701392\n",
      "Iteration 44, loss = 0.26696372\n",
      "Iteration 45, loss = 0.26655072\n",
      "Iteration 42, loss = 0.26699173\n",
      "Iteration 46, loss = 0.26650911\n",
      "Iteration 43, loss = 0.26719363\n",
      "Iteration 47, loss = 0.26655005\n",
      "Iteration 44, loss = 0.26666731\n",
      "Iteration 48, loss = 0.26613235\n",
      "Iteration 45, loss = 0.26644344\n",
      "Iteration 49, loss = 0.26594473\n",
      "Iteration 46, loss = 0.26626432\n",
      "Iteration 50, loss = 0.26589838\n",
      "Iteration 47, loss = 0.26656865\n",
      "Iteration 51, loss = 0.26603210\n",
      "Iteration 48, loss = 0.26590935\n",
      "Iteration 52, loss = 0.26599974\n",
      "Iteration 49, loss = 0.26607406\n",
      "Iteration 53, loss = 0.26534051\n",
      "Iteration 54, loss = 0.26511901\n",
      "Iteration 50, loss = 0.26594531\n",
      "Iteration 51, loss = 0.26627469\n",
      "Iteration 55, loss = 0.26559844\n",
      "Iteration 52, loss = 0.26616210\n",
      "Iteration 56, loss = 0.26519767\n",
      "Iteration 53, loss = 0.26534985\n",
      "Iteration 57, loss = 0.26491510\n",
      "Iteration 54, loss = 0.26526449\n",
      "Iteration 58, loss = 0.26514115\n",
      "Iteration 55, loss = 0.26569597\n",
      "Iteration 59, loss = 0.26454578\n",
      "Iteration 56, loss = 0.26570324\n",
      "Iteration 60, loss = 0.26469559\n",
      "Iteration 57, loss = 0.26524460\n",
      "Iteration 61, loss = 0.26464072\n",
      "Iteration 58, loss = 0.26547078\n",
      "Iteration 62, loss = 0.26425065\n",
      "Iteration 59, loss = 0.26478929\n",
      "Iteration 63, loss = 0.26390859\n",
      "Iteration 60, loss = 0.26496835\n",
      "Iteration 61, loss = 0.26506066Iteration 64, loss = 0.26412407\n",
      "\n",
      "Iteration 62, loss = 0.26490240\n",
      "Iteration 65, loss = 0.26386334\n",
      "Iteration 63, loss = 0.26402337\n",
      "Iteration 66, loss = 0.26437042\n",
      "Iteration 64, loss = 0.26476813\n",
      "Iteration 67, loss = 0.26368484\n",
      "Iteration 65, loss = 0.26430109\n",
      "Iteration 68, loss = 0.26350618\n",
      "Iteration 66, loss = 0.26464579\n",
      "Iteration 69, loss = 0.26354662\n",
      "Iteration 67, loss = 0.26433104\n",
      "Iteration 70, loss = 0.26325621\n",
      "Iteration 68, loss = 0.26413187\n",
      "Iteration 71, loss = 0.26372356\n",
      "Iteration 69, loss = 0.26439055\n",
      "Iteration 72, loss = 0.26335070\n",
      "Iteration 70, loss = 0.26389553\n",
      "Iteration 73, loss = 0.26320968\n",
      "Iteration 71, loss = 0.26406084\n",
      "Iteration 74, loss = 0.26341026\n",
      "Iteration 75, loss = 0.26318096\n",
      "Iteration 72, loss = 0.26403411\n",
      "Iteration 76, loss = 0.26323669\n",
      "Iteration 73, loss = 0.26392507\n",
      "Iteration 77, loss = 0.26300888\n",
      "Iteration 74, loss = 0.26376974\n",
      "Iteration 78, loss = 0.26297270\n",
      "Iteration 79, loss = 0.26275395\n",
      "Iteration 75, loss = 0.26365570\n",
      "Iteration 80, loss = 0.26265908\n",
      "Iteration 76, loss = 0.26411390\n",
      "Iteration 81, loss = 0.26276117\n",
      "Iteration 77, loss = 0.26365157\n",
      "Iteration 82, loss = 0.26252686\n",
      "Iteration 78, loss = 0.26366619\n",
      "Iteration 83, loss = 0.26294728\n",
      "Iteration 79, loss = 0.26338793\n",
      "Iteration 84, loss = 0.26251364\n",
      "Iteration 80, loss = 0.26321980\n",
      "Iteration 85, loss = 0.26227532\n",
      "Iteration 81, loss = 0.26343483\n",
      "Iteration 86, loss = 0.26254423\n",
      "Iteration 82, loss = 0.26325332\n",
      "Iteration 87, loss = 0.26226150\n",
      "Iteration 83, loss = 0.26367379\n",
      "Iteration 88, loss = 0.26238140\n",
      "Iteration 84, loss = 0.26310454\n",
      "Iteration 89, loss = 0.26205495\n",
      "Iteration 85, loss = 0.26280337\n",
      "Iteration 90, loss = 0.26194108\n",
      "Iteration 86, loss = 0.26311687\n",
      "Iteration 91, loss = 0.26199201\n",
      "Iteration 87, loss = 0.26304959\n",
      "Iteration 88, loss = 0.26295945\n",
      "Iteration 92, loss = 0.26167334\n",
      "Iteration 89, loss = 0.26271688\n",
      "Iteration 93, loss = 0.26182503\n",
      "Iteration 94, loss = 0.26172288\n",
      "Iteration 90, loss = 0.26268436\n",
      "Iteration 91, loss = 0.26293886\n",
      "Iteration 95, loss = 0.26148105\n",
      "Iteration 92, loss = 0.26246415\n",
      "Iteration 96, loss = 0.26174292\n",
      "Iteration 93, loss = 0.26230252\n",
      "Iteration 97, loss = 0.26159744\n",
      "Iteration 94, loss = 0.26233543\n",
      "Iteration 98, loss = 0.26150714\n",
      "Iteration 95, loss = 0.26227681\n",
      "Iteration 99, loss = 0.26142215\n",
      "Iteration 96, loss = 0.26258633\n",
      "Iteration 100, loss = 0.26156893\n",
      "Iteration 97, loss = 0.26245708\n",
      "Iteration 101, loss = 0.26150703\n",
      "Iteration 98, loss = 0.26233970\n",
      "Iteration 102, loss = 0.26127894\n",
      "Iteration 99, loss = 0.26217560\n",
      "Iteration 103, loss = 0.26116650\n",
      "Iteration 100, loss = 0.26235095\n",
      "Iteration 104, loss = 0.26123277\n",
      "Iteration 101, loss = 0.26226114\n",
      "Iteration 105, loss = 0.26152324\n",
      "Iteration 102, loss = 0.26199675\n",
      "Iteration 103, loss = 0.26205831\n",
      "Iteration 106, loss = 0.26129214\n",
      "Iteration 104, loss = 0.26194412\n",
      "Iteration 107, loss = 0.26112219\n",
      "Iteration 105, loss = 0.26245347\n",
      "Iteration 108, loss = 0.26120817\n",
      "Iteration 106, loss = 0.26192052\n",
      "Iteration 109, loss = 0.26118287\n",
      "Iteration 107, loss = 0.26174668\n",
      "Iteration 108, loss = 0.26199343\n",
      "Iteration 110, loss = 0.26096863\n",
      "Iteration 109, loss = 0.26206325\n",
      "Iteration 111, loss = 0.26087796\n",
      "Iteration 110, loss = 0.26166611\n",
      "Iteration 112, loss = 0.26109142\n",
      "Iteration 111, loss = 0.26149319\n",
      "Iteration 113, loss = 0.26120322\n",
      "Iteration 112, loss = 0.26181556\n",
      "Iteration 114, loss = 0.26060436\n",
      "Iteration 113, loss = 0.26190634\n",
      "Iteration 115, loss = 0.26057176\n",
      "Iteration 114, loss = 0.26145351\n",
      "Iteration 116, loss = 0.26070308\n",
      "Iteration 115, loss = 0.26119847Iteration 117, loss = 0.26088542\n",
      "\n",
      "Iteration 118, loss = 0.26083282\n",
      "Iteration 116, loss = 0.26162587\n",
      "Iteration 119, loss = 0.26079039\n",
      "Iteration 117, loss = 0.26158312\n",
      "Iteration 120, loss = 0.26121365\n",
      "Iteration 118, loss = 0.26182176\n",
      "Iteration 119, loss = 0.26156240\n",
      "Iteration 121, loss = 0.26016350\n",
      "Iteration 120, loss = 0.26191567\n",
      "Iteration 122, loss = 0.26017723\n",
      "Iteration 121, loss = 0.26101345\n",
      "Iteration 123, loss = 0.26030510\n",
      "Iteration 122, loss = 0.26087290\n",
      "Iteration 124, loss = 0.26033751\n",
      "Iteration 123, loss = 0.26096269\n",
      "Iteration 125, loss = 0.26022216\n",
      "Iteration 124, loss = 0.26084403\n",
      "Iteration 126, loss = 0.26020277\n",
      "Iteration 125, loss = 0.26087474\n",
      "Iteration 127, loss = 0.26004897\n",
      "Iteration 126, loss = 0.26127995\n",
      "Iteration 128, loss = 0.26017661\n",
      "Iteration 127, loss = 0.26101075\n",
      "Iteration 129, loss = 0.25983786\n",
      "Iteration 128, loss = 0.26126779\n",
      "Iteration 130, loss = 0.25969480\n",
      "Iteration 129, loss = 0.26096571\n",
      "Iteration 131, loss = 0.25996924\n",
      "Iteration 130, loss = 0.26064782\n",
      "Iteration 132, loss = 0.25996147\n",
      "Iteration 131, loss = 0.26079071\n",
      "Iteration 133, loss = 0.26035690\n",
      "Iteration 132, loss = 0.26080371\n",
      "Iteration 134, loss = 0.26002745\n",
      "Iteration 133, loss = 0.26105676\n",
      "Iteration 135, loss = 0.25983241\n",
      "Iteration 134, loss = 0.26096671\n",
      "Iteration 136, loss = 0.25959059\n",
      "Iteration 135, loss = 0.26122112\n",
      "Iteration 137, loss = 0.25958145\n",
      "Iteration 136, loss = 0.26044530\n",
      "Iteration 138, loss = 0.25938389\n",
      "Iteration 137, loss = 0.26056002\n",
      "Iteration 139, loss = 0.25933590\n",
      "Iteration 138, loss = 0.26051687\n",
      "Iteration 140, loss = 0.25941381\n",
      "Iteration 139, loss = 0.26043334\n",
      "Iteration 141, loss = 0.25921457\n",
      "Iteration 140, loss = 0.26044377\n",
      "Iteration 142, loss = 0.25930183\n",
      "Iteration 141, loss = 0.26010053\n",
      "Iteration 143, loss = 0.25930147\n",
      "Iteration 142, loss = 0.26022506\n",
      "Iteration 144, loss = 0.25918497\n",
      "Iteration 143, loss = 0.26031413\n",
      "Iteration 145, loss = 0.25927568\n",
      "Iteration 144, loss = 0.26023944\n",
      "Iteration 146, loss = 0.25889518\n",
      "Iteration 145, loss = 0.26034872\n",
      "Iteration 147, loss = 0.25920415\n",
      "Iteration 146, loss = 0.26028941\n",
      "Iteration 148, loss = 0.25900252\n",
      "Iteration 147, loss = 0.26001552\n",
      "Iteration 149, loss = 0.25919388\n",
      "Iteration 148, loss = 0.26010900\n",
      "Iteration 150, loss = 0.25883472\n",
      "Iteration 149, loss = 0.26021061\n",
      "Iteration 151, loss = 0.25897856\n",
      "Iteration 150, loss = 0.25993506\n",
      "Iteration 152, loss = 0.25855847\n",
      "Iteration 151, loss = 0.26020822\n",
      "Iteration 153, loss = 0.25885223\n",
      "Iteration 152, loss = 0.25967878\n",
      "Iteration 154, loss = 0.25883447\n",
      "Iteration 153, loss = 0.25972551\n",
      "Iteration 155, loss = 0.25862810\n",
      "Iteration 154, loss = 0.26008283\n",
      "Iteration 156, loss = 0.25854899\n",
      "Iteration 155, loss = 0.25963911\n",
      "Iteration 157, loss = 0.25868484\n",
      "Iteration 156, loss = 0.25998922\n",
      "Iteration 158, loss = 0.25857509\n",
      "Iteration 157, loss = 0.25987297\n",
      "Iteration 159, loss = 0.25853311\n",
      "Iteration 158, loss = 0.25959525\n",
      "Iteration 160, loss = 0.25857056\n",
      "Iteration 159, loss = 0.26013864\n",
      "Iteration 161, loss = 0.25834398\n",
      "Iteration 160, loss = 0.25973844\n",
      "Iteration 161, loss = 0.25974297\n",
      "Iteration 162, loss = 0.25855075\n",
      "Iteration 162, loss = 0.25965796\n",
      "Iteration 163, loss = 0.25878785\n",
      "Iteration 163, loss = 0.26002149\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 164, loss = 0.25847276\n",
      "Iteration 165, loss = 0.25873822\n",
      "Iteration 1, loss = 0.34685833\n",
      "Iteration 166, loss = 0.25891823\n",
      "Iteration 2, loss = 0.30049986\n",
      "Iteration 167, loss = 0.25842716\n",
      "Iteration 3, loss = 0.28681019\n",
      "Iteration 4, loss = 0.28238982\n",
      "Iteration 168, loss = 0.25841966\n",
      "Iteration 5, loss = 0.28065420\n",
      "Iteration 169, loss = 0.25816359\n",
      "Iteration 6, loss = 0.27989463\n",
      "Iteration 170, loss = 0.25785159\n",
      "Iteration 7, loss = 0.27923464\n",
      "Iteration 171, loss = 0.25824805\n",
      "Iteration 8, loss = 0.27873616\n",
      "Iteration 172, loss = 0.25825033\n",
      "Iteration 9, loss = 0.27902922\n",
      "Iteration 173, loss = 0.25798221\n",
      "Iteration 10, loss = 0.27795009\n",
      "Iteration 174, loss = 0.25824550\n",
      "Iteration 11, loss = 0.27726581\n",
      "Iteration 175, loss = 0.25795104\n",
      "Iteration 12, loss = 0.27735513\n",
      "Iteration 176, loss = 0.25789497\n",
      "Iteration 13, loss = 0.27714040\n",
      "Iteration 177, loss = 0.25800887\n",
      "Iteration 14, loss = 0.27640154\n",
      "Iteration 178, loss = 0.25789509\n",
      "Iteration 15, loss = 0.27617978\n",
      "Iteration 16, loss = 0.27602779\n",
      "Iteration 179, loss = 0.25802828\n",
      "Iteration 17, loss = 0.27495340\n",
      "Iteration 180, loss = 0.25800873\n",
      "Iteration 18, loss = 0.27468355\n",
      "Iteration 181, loss = 0.25774182\n",
      "Iteration 19, loss = 0.27444719\n",
      "Iteration 20, loss = 0.27379411\n",
      "Iteration 182, loss = 0.25794582\n",
      "Iteration 183, loss = 0.25797814\n",
      "Iteration 21, loss = 0.27346610\n",
      "Iteration 184, loss = 0.25773644\n",
      "Iteration 22, loss = 0.27354841\n",
      "Iteration 185, loss = 0.25776259\n",
      "Iteration 23, loss = 0.27274651\n",
      "Iteration 186, loss = 0.25781690\n",
      "Iteration 24, loss = 0.27238315\n",
      "Iteration 187, loss = 0.25773566\n",
      "Iteration 25, loss = 0.27192010\n",
      "Iteration 188, loss = 0.25769389\n",
      "Iteration 26, loss = 0.27194458\n",
      "Iteration 189, loss = 0.25766140\n",
      "Iteration 27, loss = 0.27144688\n",
      "Iteration 190, loss = 0.25762302\n",
      "Iteration 28, loss = 0.27124963\n",
      "Iteration 191, loss = 0.25789795\n",
      "Iteration 29, loss = 0.27064394\n",
      "Iteration 192, loss = 0.25758324\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 30, loss = 0.27038914\n",
      "Iteration 31, loss = 0.27060330\n",
      "Iteration 32, loss = 0.26998111\n",
      "Iteration 1, loss = 0.34556951\n",
      "Iteration 33, loss = 0.26998306\n",
      "Iteration 2, loss = 0.29965443\n",
      "Iteration 34, loss = 0.26942994\n",
      "Iteration 3, loss = 0.28695147\n",
      "Iteration 35, loss = 0.26902146\n",
      "Iteration 4, loss = 0.28249983\n",
      "Iteration 36, loss = 0.26892286\n",
      "Iteration 5, loss = 0.28057103\n",
      "Iteration 37, loss = 0.26856608\n",
      "Iteration 6, loss = 0.28037122\n",
      "Iteration 38, loss = 0.26868529\n",
      "Iteration 7, loss = 0.27938823\n",
      "Iteration 39, loss = 0.26820773\n",
      "Iteration 8, loss = 0.27928835\n",
      "Iteration 40, loss = 0.26807691\n",
      "Iteration 9, loss = 0.27884185\n",
      "Iteration 41, loss = 0.26794819\n",
      "Iteration 10, loss = 0.27806276\n",
      "Iteration 42, loss = 0.26784724\n",
      "Iteration 11, loss = 0.27763962\n",
      "Iteration 43, loss = 0.26760449\n",
      "Iteration 12, loss = 0.27737758\n",
      "Iteration 44, loss = 0.26763961\n",
      "Iteration 13, loss = 0.27702854\n",
      "Iteration 45, loss = 0.26736325\n",
      "Iteration 14, loss = 0.27679887\n",
      "Iteration 46, loss = 0.26714323\n",
      "Iteration 15, loss = 0.27680858\n",
      "Iteration 47, loss = 0.26724915\n",
      "Iteration 16, loss = 0.27635294\n",
      "Iteration 48, loss = 0.26672041\n",
      "Iteration 17, loss = 0.27539721\n",
      "Iteration 49, loss = 0.26676711\n",
      "Iteration 18, loss = 0.27533938\n",
      "Iteration 50, loss = 0.26690284\n",
      "Iteration 19, loss = 0.27489601\n",
      "Iteration 51, loss = 0.26685029\n",
      "Iteration 20, loss = 0.27439703\n",
      "Iteration 52, loss = 0.26711431\n",
      "Iteration 21, loss = 0.27421140\n",
      "Iteration 53, loss = 0.26625061\n",
      "Iteration 22, loss = 0.27410664\n",
      "Iteration 54, loss = 0.26612679\n",
      "Iteration 23, loss = 0.27362687\n",
      "Iteration 55, loss = 0.26647657\n",
      "Iteration 24, loss = 0.27343304\n",
      "Iteration 56, loss = 0.26662355\n",
      "Iteration 25, loss = 0.27279105\n",
      "Iteration 57, loss = 0.26595345\n",
      "Iteration 26, loss = 0.27264790\n",
      "Iteration 27, loss = 0.27214713\n",
      "Iteration 58, loss = 0.26597941\n",
      "Iteration 59, loss = 0.26567107\n",
      "Iteration 28, loss = 0.27249976\n",
      "Iteration 60, loss = 0.26591638\n",
      "Iteration 29, loss = 0.27163769\n",
      "Iteration 61, loss = 0.26588015\n",
      "Iteration 30, loss = 0.27141808\n",
      "Iteration 31, loss = 0.27160073\n",
      "Iteration 62, loss = 0.26558553\n",
      "Iteration 32, loss = 0.27120038\n",
      "Iteration 63, loss = 0.26506967\n",
      "Iteration 33, loss = 0.27129727\n",
      "Iteration 64, loss = 0.26575359\n",
      "Iteration 34, loss = 0.27055810\n",
      "Iteration 65, loss = 0.26513539\n",
      "Iteration 35, loss = 0.26998496\n",
      "Iteration 66, loss = 0.26508792\n",
      "Iteration 36, loss = 0.26993183\n",
      "Iteration 67, loss = 0.26531111\n",
      "Iteration 37, loss = 0.26967491\n",
      "Iteration 68, loss = 0.26486092\n",
      "Iteration 38, loss = 0.26963477\n",
      "Iteration 69, loss = 0.26483805\n",
      "Iteration 39, loss = 0.26928531\n",
      "Iteration 70, loss = 0.26474982\n",
      "Iteration 40, loss = 0.26919533\n",
      "Iteration 71, loss = 0.26458373\n",
      "Iteration 72, loss = 0.26462464\n",
      "Iteration 41, loss = 0.26903209\n",
      "Iteration 42, loss = 0.26894994\n",
      "Iteration 73, loss = 0.26454474\n",
      "Iteration 43, loss = 0.26852071\n",
      "Iteration 44, loss = 0.26849772\n",
      "Iteration 74, loss = 0.26448647Iteration 45, loss = 0.26815778\n",
      "\n",
      "Iteration 46, loss = 0.26799166\n",
      "Iteration 75, loss = 0.26444535\n",
      "Iteration 47, loss = 0.26779557\n",
      "Iteration 76, loss = 0.26466161\n",
      "Iteration 48, loss = 0.26732911\n",
      "Iteration 77, loss = 0.26427772\n",
      "Iteration 49, loss = 0.26747412\n",
      "Iteration 50, loss = 0.26746821\n",
      "Iteration 78, loss = 0.26444007\n",
      "Iteration 51, loss = 0.26733605\n",
      "Iteration 79, loss = 0.26400960\n",
      "Iteration 52, loss = 0.26739454\n",
      "Iteration 53, loss = 0.26681061\n",
      "Iteration 80, loss = 0.26391105\n",
      "Iteration 54, loss = 0.26661176\n",
      "Iteration 81, loss = 0.26405660\n",
      "Iteration 55, loss = 0.26683014\n",
      "Iteration 82, loss = 0.26397995\n",
      "Iteration 56, loss = 0.26670375\n",
      "Iteration 83, loss = 0.26437247\n",
      "Iteration 57, loss = 0.26642885\n",
      "Iteration 84, loss = 0.26377388\n",
      "Iteration 58, loss = 0.26619769\n",
      "Iteration 85, loss = 0.26367834\n",
      "Iteration 59, loss = 0.26575155\n",
      "Iteration 86, loss = 0.26418148\n",
      "Iteration 60, loss = 0.26585239\n",
      "Iteration 87, loss = 0.26380254\n",
      "Iteration 61, loss = 0.26617084\n",
      "Iteration 88, loss = 0.26368695\n",
      "Iteration 62, loss = 0.26583438\n",
      "Iteration 89, loss = 0.26326205\n",
      "Iteration 63, loss = 0.26515408Iteration 90, loss = 0.26347106\n",
      "\n",
      "Iteration 91, loss = 0.26356130\n",
      "Iteration 64, loss = 0.26584733\n",
      "Iteration 92, loss = 0.26330486\n",
      "Iteration 65, loss = 0.26536469\n",
      "Iteration 93, loss = 0.26298925\n",
      "Iteration 66, loss = 0.26518611\n",
      "Iteration 94, loss = 0.26312238\n",
      "Iteration 67, loss = 0.26541248\n",
      "Iteration 95, loss = 0.26301083\n",
      "Iteration 68, loss = 0.26499237\n",
      "Iteration 96, loss = 0.26330749\n",
      "Iteration 69, loss = 0.26514969\n",
      "Iteration 97, loss = 0.26324518\n",
      "Iteration 70, loss = 0.26477950\n",
      "Iteration 98, loss = 0.26313578\n",
      "Iteration 71, loss = 0.26456651\n",
      "Iteration 99, loss = 0.26310366\n",
      "Iteration 72, loss = 0.26480915\n",
      "Iteration 100, loss = 0.26286211\n",
      "Iteration 73, loss = 0.26465931\n",
      "Iteration 101, loss = 0.26307864\n",
      "Iteration 74, loss = 0.26463459\n",
      "Iteration 102, loss = 0.26269397\n",
      "Iteration 75, loss = 0.26451574\n",
      "Iteration 103, loss = 0.26264085\n",
      "Iteration 76, loss = 0.26479277\n",
      "Iteration 104, loss = 0.26259756\n",
      "Iteration 77, loss = 0.26438279\n",
      "Iteration 105, loss = 0.26319933\n",
      "Iteration 78, loss = 0.26452252\n",
      "Iteration 106, loss = 0.26286511\n",
      "Iteration 79, loss = 0.26405675\n",
      "Iteration 107, loss = 0.26278784\n",
      "Iteration 80, loss = 0.26410284\n",
      "Iteration 108, loss = 0.26279762\n",
      "Iteration 81, loss = 0.26403506\n",
      "Iteration 109, loss = 0.26292037\n",
      "Iteration 110, loss = 0.26266512\n",
      "Iteration 82, loss = 0.26431013\n",
      "Iteration 111, loss = 0.26232627\n",
      "Iteration 83, loss = 0.26438048\n",
      "Iteration 112, loss = 0.26257765\n",
      "Iteration 84, loss = 0.26391821\n",
      "Iteration 113, loss = 0.26281537\n",
      "Iteration 114, loss = 0.26238373\n",
      "Iteration 85, loss = 0.26400921\n",
      "Iteration 115, loss = 0.26218722\n",
      "Iteration 86, loss = 0.26425068\n",
      "Iteration 87, loss = 0.26385152\n",
      "Iteration 116, loss = 0.26236289\n",
      "Iteration 88, loss = 0.26404940\n",
      "Iteration 117, loss = 0.26272714\n",
      "Iteration 89, loss = 0.26384373\n",
      "Iteration 118, loss = 0.26265436\n",
      "Iteration 90, loss = 0.26372601\n",
      "Iteration 119, loss = 0.26237154\n",
      "Iteration 91, loss = 0.26392033\n",
      "Iteration 120, loss = 0.26330759\n",
      "Iteration 121, loss = 0.26203719\n",
      "Iteration 92, loss = 0.26335774\n",
      "Iteration 93, loss = 0.26325366\n",
      "Iteration 122, loss = 0.26206961\n",
      "Iteration 123, loss = 0.26219208\n",
      "Iteration 94, loss = 0.26329973\n",
      "Iteration 95, loss = 0.26346831\n",
      "Iteration 124, loss = 0.26209895\n",
      "Iteration 96, loss = 0.26356137\n",
      "Iteration 125, loss = 0.26197154\n",
      "Iteration 97, loss = 0.26333910\n",
      "Iteration 126, loss = 0.26220897\n",
      "Iteration 98, loss = 0.26314212\n",
      "Iteration 127, loss = 0.26227336\n",
      "Iteration 99, loss = 0.26329645\n",
      "Iteration 128, loss = 0.26230935\n",
      "Iteration 129, loss = 0.26204736\n",
      "Iteration 100, loss = 0.26316387\n",
      "Iteration 130, loss = 0.26223479\n",
      "Iteration 101, loss = 0.26324696\n",
      "Iteration 131, loss = 0.26191210\n",
      "Iteration 102, loss = 0.26290167\n",
      "Iteration 132, loss = 0.26185924\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 103, loss = 0.26280617\n",
      "Iteration 104, loss = 0.26297351\n",
      "Iteration 105, loss = 0.26291949\n",
      "Iteration 106, loss = 0.26305314\n",
      "Iteration 107, loss = 0.26275287\n",
      "Iteration 108, loss = 0.26283370\n",
      "Iteration 109, loss = 0.26292942\n",
      "Iteration 110, loss = 0.26278350\n",
      "Iteration 111, loss = 0.26247770\n",
      "Iteration 112, loss = 0.26253968\n",
      "Iteration 113, loss = 0.26269721\n",
      "Iteration 114, loss = 0.26251613\n",
      "Iteration 115, loss = 0.26257548\n",
      "Iteration 116, loss = 0.26242033\n",
      "Iteration 117, loss = 0.26255227\n",
      "Iteration 118, loss = 0.26285856\n",
      "Iteration 119, loss = 0.26239009\n",
      "Iteration 120, loss = 0.26317834\n",
      "Iteration 121, loss = 0.26210790\n",
      "Iteration 122, loss = 0.26213152\n",
      "Iteration 123, loss = 0.26223460\n",
      "Iteration 124, loss = 0.26235136\n",
      "Iteration 125, loss = 0.26219282\n",
      "Iteration 126, loss = 0.26215522\n",
      "Iteration 127, loss = 0.26222600\n",
      "Iteration 128, loss = 0.26253960\n",
      "Iteration 129, loss = 0.26211988\n",
      "Iteration 130, loss = 0.26205852\n",
      "Iteration 131, loss = 0.26221331\n",
      "Iteration 132, loss = 0.26181192\n",
      "Iteration 133, loss = 0.26229910\n",
      "Iteration 134, loss = 0.26237591\n",
      "Iteration 135, loss = 0.26212379\n",
      "Iteration 136, loss = 0.26189450\n",
      "Iteration 137, loss = 0.26186591\n",
      "Iteration 138, loss = 0.26200022\n",
      "Iteration 139, loss = 0.26197685\n",
      "Iteration 140, loss = 0.26205404\n",
      "Iteration 141, loss = 0.26196794\n",
      "Iteration 142, loss = 0.26175825\n",
      "Iteration 143, loss = 0.26191761\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33514630\n",
      "Iteration 1, loss = 0.33564692\n",
      "Iteration 2, loss = 0.28978036\n",
      "Iteration 2, loss = 0.29076472\n",
      "Iteration 3, loss = 0.27848175\n",
      "Iteration 3, loss = 0.27947617\n",
      "Iteration 4, loss = 0.27573418Iteration 4, loss = 0.27476849\n",
      "\n",
      "Iteration 5, loss = 0.27433709\n",
      "Iteration 5, loss = 0.27334826\n",
      "Iteration 6, loss = 0.27319112\n",
      "Iteration 6, loss = 0.27226024\n",
      "Iteration 7, loss = 0.27266018\n",
      "Iteration 7, loss = 0.27178860\n",
      "Iteration 8, loss = 0.27254642\n",
      "Iteration 8, loss = 0.27206120\n",
      "Iteration 9, loss = 0.27144841\n",
      "Iteration 9, loss = 0.27075505\n",
      "Iteration 10, loss = 0.27115274\n",
      "Iteration 10, loss = 0.27074208\n",
      "Iteration 11, loss = 0.27018376\n",
      "Iteration 11, loss = 0.27077221\n",
      "Iteration 12, loss = 0.27037502\n",
      "Iteration 12, loss = 0.27004425\n",
      "Iteration 13, loss = 0.26981133\n",
      "Iteration 13, loss = 0.26929351\n",
      "Iteration 14, loss = 0.26952180\n",
      "Iteration 14, loss = 0.26922206\n",
      "Iteration 15, loss = 0.26892938\n",
      "Iteration 15, loss = 0.26847274\n",
      "Iteration 16, loss = 0.26927160\n",
      "Iteration 17, loss = 0.26837684\n",
      "Iteration 16, loss = 0.26874967\n",
      "Iteration 18, loss = 0.26795777\n",
      "Iteration 17, loss = 0.26808524\n",
      "Iteration 19, loss = 0.26752311\n",
      "Iteration 18, loss = 0.26751187\n",
      "Iteration 20, loss = 0.26714161\n",
      "Iteration 21, loss = 0.26686966\n",
      "Iteration 19, loss = 0.26707251\n",
      "Iteration 22, loss = 0.26703562\n",
      "Iteration 20, loss = 0.26679278\n",
      "Iteration 21, loss = 0.26672137\n",
      "Iteration 23, loss = 0.26596602\n",
      "Iteration 24, loss = 0.26595643\n",
      "Iteration 22, loss = 0.26653531\n",
      "Iteration 25, loss = 0.26520923\n",
      "Iteration 23, loss = 0.26584089\n",
      "Iteration 26, loss = 0.26520125\n",
      "Iteration 24, loss = 0.26602909\n",
      "Iteration 27, loss = 0.26462319\n",
      "Iteration 25, loss = 0.26494088\n",
      "Iteration 28, loss = 0.26443086\n",
      "Iteration 26, loss = 0.26475111\n",
      "Iteration 29, loss = 0.26396597\n",
      "Iteration 27, loss = 0.26451589\n",
      "Iteration 30, loss = 0.26379128\n",
      "Iteration 28, loss = 0.26428920\n",
      "Iteration 31, loss = 0.26338240\n",
      "Iteration 29, loss = 0.26357098\n",
      "Iteration 32, loss = 0.26291855\n",
      "Iteration 30, loss = 0.26343998\n",
      "Iteration 33, loss = 0.26257616\n",
      "Iteration 31, loss = 0.26312138\n",
      "Iteration 34, loss = 0.26254269\n",
      "Iteration 32, loss = 0.26256860\n",
      "Iteration 35, loss = 0.26244480\n",
      "Iteration 33, loss = 0.26244033\n",
      "Iteration 36, loss = 0.26160384\n",
      "Iteration 34, loss = 0.26206482\n",
      "Iteration 37, loss = 0.26159074\n",
      "Iteration 35, loss = 0.26199376\n",
      "Iteration 38, loss = 0.26157205\n",
      "Iteration 36, loss = 0.26114878\n",
      "Iteration 39, loss = 0.26083497\n",
      "Iteration 37, loss = 0.26112654\n",
      "Iteration 40, loss = 0.26085802\n",
      "Iteration 38, loss = 0.26101977\n",
      "Iteration 41, loss = 0.26022422\n",
      "Iteration 39, loss = 0.26040036\n",
      "Iteration 42, loss = 0.26055895\n",
      "Iteration 43, loss = 0.26024903\n",
      "Iteration 40, loss = 0.26023661\n",
      "Iteration 41, loss = 0.25960867\n",
      "Iteration 44, loss = 0.25952936\n",
      "Iteration 42, loss = 0.25954434\n",
      "Iteration 43, loss = 0.25960206\n",
      "Iteration 45, loss = 0.25953650\n",
      "Iteration 44, loss = 0.25890592\n",
      "Iteration 45, loss = 0.25896481\n",
      "Iteration 46, loss = 0.25943871\n",
      "Iteration 46, loss = 0.25847195\n",
      "Iteration 47, loss = 0.25902474\n",
      "Iteration 47, loss = 0.25813424\n",
      "Iteration 48, loss = 0.25917751\n",
      "Iteration 48, loss = 0.25818917\n",
      "Iteration 49, loss = 0.25900684\n",
      "Iteration 49, loss = 0.25818188\n",
      "Iteration 50, loss = 0.25873633\n",
      "Iteration 50, loss = 0.25755403\n",
      "Iteration 51, loss = 0.25882238\n",
      "Iteration 51, loss = 0.25760196\n",
      "Iteration 52, loss = 0.25825907\n",
      "Iteration 52, loss = 0.25707244\n",
      "Iteration 53, loss = 0.25826295\n",
      "Iteration 53, loss = 0.25702908\n",
      "Iteration 54, loss = 0.25857321\n",
      "Iteration 55, loss = 0.25783434\n",
      "Iteration 54, loss = 0.25741535\n",
      "Iteration 56, loss = 0.25815189\n",
      "Iteration 55, loss = 0.25686145\n",
      "Iteration 57, loss = 0.25820384\n",
      "Iteration 56, loss = 0.25671816\n",
      "Iteration 58, loss = 0.25767741\n",
      "Iteration 57, loss = 0.25665472\n",
      "Iteration 59, loss = 0.25777979\n",
      "Iteration 58, loss = 0.25636389\n",
      "Iteration 60, loss = 0.25811052\n",
      "Iteration 59, loss = 0.25649079\n",
      "Iteration 61, loss = 0.25762123\n",
      "Iteration 60, loss = 0.25678332\n",
      "Iteration 62, loss = 0.25750778\n",
      "Iteration 61, loss = 0.25634094\n",
      "Iteration 63, loss = 0.25752980\n",
      "Iteration 64, loss = 0.25714821\n",
      "Iteration 62, loss = 0.25598195\n",
      "Iteration 65, loss = 0.25731757\n",
      "Iteration 63, loss = 0.25610716\n",
      "Iteration 66, loss = 0.25720353\n",
      "Iteration 64, loss = 0.25577642\n",
      "Iteration 65, loss = 0.25561022\n",
      "Iteration 67, loss = 0.25714053\n",
      "Iteration 66, loss = 0.25557902\n",
      "Iteration 68, loss = 0.25708382\n",
      "Iteration 67, loss = 0.25571381\n",
      "Iteration 69, loss = 0.25667673\n",
      "Iteration 68, loss = 0.25571795\n",
      "Iteration 70, loss = 0.25648692\n",
      "Iteration 71, loss = 0.25664052\n",
      "Iteration 69, loss = 0.25523149\n",
      "Iteration 72, loss = 0.25658904\n",
      "Iteration 70, loss = 0.25514341\n",
      "Iteration 73, loss = 0.25643196\n",
      "Iteration 71, loss = 0.25524570\n",
      "Iteration 74, loss = 0.25652945\n",
      "Iteration 72, loss = 0.25525286\n",
      "Iteration 75, loss = 0.25598438\n",
      "Iteration 73, loss = 0.25509950\n",
      "Iteration 74, loss = 0.25500586\n",
      "Iteration 76, loss = 0.25635286\n",
      "Iteration 77, loss = 0.25597253\n",
      "Iteration 75, loss = 0.25476793\n",
      "Iteration 78, loss = 0.25616934\n",
      "Iteration 76, loss = 0.25470953\n",
      "Iteration 79, loss = 0.25596044\n",
      "Iteration 77, loss = 0.25473962\n",
      "Iteration 80, loss = 0.25581413\n",
      "Iteration 78, loss = 0.25458869\n",
      "Iteration 81, loss = 0.25597986\n",
      "Iteration 79, loss = 0.25457823\n",
      "Iteration 82, loss = 0.25567745\n",
      "Iteration 80, loss = 0.25457103\n",
      "Iteration 83, loss = 0.25572641\n",
      "Iteration 81, loss = 0.25461412\n",
      "Iteration 84, loss = 0.25555068\n",
      "Iteration 82, loss = 0.25440531\n",
      "Iteration 83, loss = 0.25438665\n",
      "Iteration 85, loss = 0.25558291\n",
      "Iteration 84, loss = 0.25410095\n",
      "Iteration 86, loss = 0.25556611\n",
      "Iteration 85, loss = 0.25430590\n",
      "Iteration 87, loss = 0.25531821\n",
      "Iteration 88, loss = 0.25540806\n",
      "Iteration 86, loss = 0.25426652\n",
      "Iteration 89, loss = 0.25527946\n",
      "Iteration 87, loss = 0.25402165\n",
      "Iteration 90, loss = 0.25511873\n",
      "Iteration 88, loss = 0.25443136\n",
      "Iteration 91, loss = 0.25505157\n",
      "Iteration 89, loss = 0.25415478\n",
      "Iteration 92, loss = 0.25499016\n",
      "Iteration 90, loss = 0.25385962\n",
      "Iteration 93, loss = 0.25493745\n",
      "Iteration 91, loss = 0.25413306\n",
      "Iteration 94, loss = 0.25477710\n",
      "Iteration 92, loss = 0.25375788\n",
      "Iteration 95, loss = 0.25476052\n",
      "Iteration 93, loss = 0.25391450\n",
      "Iteration 96, loss = 0.25483257\n",
      "Iteration 94, loss = 0.25368711\n",
      "Iteration 97, loss = 0.25479712\n",
      "Iteration 95, loss = 0.25358538\n",
      "Iteration 98, loss = 0.25461267\n",
      "Iteration 96, loss = 0.25367032\n",
      "Iteration 99, loss = 0.25450503\n",
      "Iteration 97, loss = 0.25365457\n",
      "Iteration 100, loss = 0.25451258\n",
      "Iteration 98, loss = 0.25370911\n",
      "Iteration 101, loss = 0.25452044\n",
      "Iteration 99, loss = 0.25358583\n",
      "Iteration 102, loss = 0.25477342\n",
      "Iteration 100, loss = 0.25335101\n",
      "Iteration 103, loss = 0.25436105\n",
      "Iteration 101, loss = 0.25355393\n",
      "Iteration 104, loss = 0.25449938\n",
      "Iteration 102, loss = 0.25349281\n",
      "Iteration 105, loss = 0.25471590\n",
      "Iteration 103, loss = 0.25319557\n",
      "Iteration 106, loss = 0.25443687\n",
      "Iteration 104, loss = 0.25342777\n",
      "Iteration 107, loss = 0.25413363\n",
      "Iteration 105, loss = 0.25356181\n",
      "Iteration 108, loss = 0.25422321\n",
      "Iteration 109, loss = 0.25426645\n",
      "Iteration 106, loss = 0.25314349\n",
      "Iteration 110, loss = 0.25431721\n",
      "Iteration 107, loss = 0.25293800\n",
      "Iteration 111, loss = 0.25413641\n",
      "Iteration 108, loss = 0.25311546\n",
      "Iteration 112, loss = 0.25404315\n",
      "Iteration 109, loss = 0.25322238\n",
      "Iteration 113, loss = 0.25396203\n",
      "Iteration 114, loss = 0.25408826\n",
      "Iteration 110, loss = 0.25296974\n",
      "Iteration 115, loss = 0.25421189\n",
      "Iteration 111, loss = 0.25288830\n",
      "Iteration 116, loss = 0.25399254\n",
      "Iteration 112, loss = 0.25282475\n",
      "Iteration 117, loss = 0.25367147\n",
      "Iteration 113, loss = 0.25290173\n",
      "Iteration 118, loss = 0.25371538\n",
      "Iteration 114, loss = 0.25297038\n",
      "Iteration 119, loss = 0.25389326\n",
      "Iteration 115, loss = 0.25307291\n",
      "Iteration 120, loss = 0.25389097\n",
      "Iteration 116, loss = 0.25286680\n",
      "Iteration 121, loss = 0.25396219\n",
      "Iteration 117, loss = 0.25276784\n",
      "Iteration 122, loss = 0.25368031\n",
      "Iteration 118, loss = 0.25272013\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 123, loss = 0.25363242\n",
      "Iteration 124, loss = 0.25387005\n",
      "Iteration 1, loss = 0.33477906\n",
      "Iteration 125, loss = 0.25398613\n",
      "Iteration 2, loss = 0.28759581\n",
      "Iteration 126, loss = 0.25367961\n",
      "Iteration 3, loss = 0.27616131\n",
      "Iteration 127, loss = 0.25385506\n",
      "Iteration 4, loss = 0.27289412\n",
      "Iteration 128, loss = 0.25370595\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 5, loss = 0.27096692\n",
      "Iteration 6, loss = 0.26995223\n",
      "Iteration 1, loss = 0.33598648\n",
      "Iteration 7, loss = 0.26921283\n",
      "Iteration 2, loss = 0.29059595\n",
      "Iteration 8, loss = 0.26931965\n",
      "Iteration 3, loss = 0.27984156\n",
      "Iteration 9, loss = 0.26830315\n",
      "Iteration 4, loss = 0.27620551\n",
      "Iteration 10, loss = 0.26793627\n",
      "Iteration 5, loss = 0.27450272\n",
      "Iteration 11, loss = 0.26751624\n",
      "Iteration 6, loss = 0.27334518\n",
      "Iteration 12, loss = 0.26730961\n",
      "Iteration 7, loss = 0.27288919\n",
      "Iteration 13, loss = 0.26664309\n",
      "Iteration 8, loss = 0.27262938\n",
      "Iteration 14, loss = 0.26670855\n",
      "Iteration 9, loss = 0.27175954\n",
      "Iteration 15, loss = 0.26592370\n",
      "Iteration 10, loss = 0.27151727\n",
      "Iteration 16, loss = 0.26626006\n",
      "Iteration 11, loss = 0.27092036\n",
      "Iteration 17, loss = 0.26533485\n",
      "Iteration 12, loss = 0.27083695\n",
      "Iteration 18, loss = 0.26510797\n",
      "Iteration 13, loss = 0.26998338\n",
      "Iteration 19, loss = 0.26453347\n",
      "Iteration 14, loss = 0.26983201\n",
      "Iteration 20, loss = 0.26420699\n",
      "Iteration 15, loss = 0.26942329\n",
      "Iteration 21, loss = 0.26436833\n",
      "Iteration 16, loss = 0.26981191\n",
      "Iteration 22, loss = 0.26390468\n",
      "Iteration 17, loss = 0.26853333\n",
      "Iteration 23, loss = 0.26311451\n",
      "Iteration 18, loss = 0.26862854\n",
      "Iteration 24, loss = 0.26334424\n",
      "Iteration 19, loss = 0.26787975\n",
      "Iteration 25, loss = 0.26254130\n",
      "Iteration 20, loss = 0.26800360\n",
      "Iteration 26, loss = 0.26243458\n",
      "Iteration 21, loss = 0.26760744Iteration 27, loss = 0.26200584\n",
      "\n",
      "Iteration 28, loss = 0.26218445\n",
      "Iteration 22, loss = 0.26731302\n",
      "Iteration 29, loss = 0.26139446\n",
      "Iteration 23, loss = 0.26664140\n",
      "Iteration 30, loss = 0.26114010\n",
      "Iteration 24, loss = 0.26677538\n",
      "Iteration 31, loss = 0.26073613\n",
      "Iteration 25, loss = 0.26598840\n",
      "Iteration 32, loss = 0.26045075\n",
      "Iteration 26, loss = 0.26589012\n",
      "Iteration 33, loss = 0.26017423\n",
      "Iteration 27, loss = 0.26583367\n",
      "Iteration 34, loss = 0.25999310\n",
      "Iteration 28, loss = 0.26567754\n",
      "Iteration 35, loss = 0.25963641\n",
      "Iteration 36, loss = 0.25916419\n",
      "Iteration 29, loss = 0.26499950\n",
      "Iteration 37, loss = 0.25885899\n",
      "Iteration 30, loss = 0.26465819\n",
      "Iteration 38, loss = 0.25896541\n",
      "Iteration 31, loss = 0.26440008\n",
      "Iteration 39, loss = 0.25829403\n",
      "Iteration 32, loss = 0.26415595\n",
      "Iteration 40, loss = 0.25820444\n",
      "Iteration 33, loss = 0.26391324\n",
      "Iteration 41, loss = 0.25761841\n",
      "Iteration 34, loss = 0.26384335\n",
      "Iteration 42, loss = 0.25734474\n",
      "Iteration 35, loss = 0.26328365\n",
      "Iteration 43, loss = 0.25764237\n",
      "Iteration 36, loss = 0.26290325\n",
      "Iteration 44, loss = 0.25722873\n",
      "Iteration 37, loss = 0.26270074\n",
      "Iteration 45, loss = 0.25712941\n",
      "Iteration 38, loss = 0.26275938\n",
      "Iteration 46, loss = 0.25680010\n",
      "Iteration 39, loss = 0.26228363\n",
      "Iteration 47, loss = 0.25638529\n",
      "Iteration 40, loss = 0.26249886\n",
      "Iteration 48, loss = 0.25630204\n",
      "Iteration 41, loss = 0.26215273\n",
      "Iteration 42, loss = 0.26161880\n",
      "Iteration 49, loss = 0.25629077\n",
      "Iteration 43, loss = 0.26172965\n",
      "Iteration 50, loss = 0.25585718\n",
      "Iteration 44, loss = 0.26169244\n",
      "Iteration 51, loss = 0.25608417\n",
      "Iteration 45, loss = 0.26135550\n",
      "Iteration 52, loss = 0.25557590\n",
      "Iteration 46, loss = 0.26134484\n",
      "Iteration 53, loss = 0.25555172\n",
      "Iteration 47, loss = 0.26094868\n",
      "Iteration 54, loss = 0.25599228\n",
      "Iteration 48, loss = 0.26070781\n",
      "Iteration 55, loss = 0.25534823\n",
      "Iteration 49, loss = 0.26082529\n",
      "Iteration 56, loss = 0.25544065\n",
      "Iteration 50, loss = 0.26061229\n",
      "Iteration 57, loss = 0.25516581\n",
      "Iteration 51, loss = 0.26064557\n",
      "Iteration 58, loss = 0.25502717\n",
      "Iteration 52, loss = 0.26025683\n",
      "Iteration 59, loss = 0.25493885\n",
      "Iteration 53, loss = 0.26029625\n",
      "Iteration 60, loss = 0.25543130\n",
      "Iteration 54, loss = 0.26047879\n",
      "Iteration 61, loss = 0.25486190\n",
      "Iteration 55, loss = 0.26012730\n",
      "Iteration 62, loss = 0.25435508\n",
      "Iteration 56, loss = 0.25998275\n",
      "Iteration 63, loss = 0.25504075\n",
      "Iteration 57, loss = 0.26017088\n",
      "Iteration 64, loss = 0.25450988\n",
      "Iteration 58, loss = 0.25985828\n",
      "Iteration 65, loss = 0.25430958\n",
      "Iteration 59, loss = 0.25979911\n",
      "Iteration 66, loss = 0.25411077\n",
      "Iteration 60, loss = 0.25987557\n",
      "Iteration 67, loss = 0.25410265\n",
      "Iteration 61, loss = 0.25952414\n",
      "Iteration 68, loss = 0.25431187\n",
      "Iteration 62, loss = 0.25907199\n",
      "Iteration 69, loss = 0.25379024\n",
      "Iteration 63, loss = 0.25972188\n",
      "Iteration 70, loss = 0.25377697\n",
      "Iteration 64, loss = 0.25933623\n",
      "Iteration 71, loss = 0.25378333\n",
      "Iteration 65, loss = 0.25909966\n",
      "Iteration 72, loss = 0.25372201\n",
      "Iteration 66, loss = 0.25892722\n",
      "Iteration 73, loss = 0.25337463\n",
      "Iteration 67, loss = 0.25880012\n",
      "Iteration 74, loss = 0.25356128\n",
      "Iteration 68, loss = 0.25853342\n",
      "Iteration 75, loss = 0.25335313\n",
      "Iteration 69, loss = 0.25862100\n",
      "Iteration 76, loss = 0.25337579\n",
      "Iteration 70, loss = 0.25869284\n",
      "Iteration 77, loss = 0.25325754\n",
      "Iteration 71, loss = 0.25830677\n",
      "Iteration 78, loss = 0.25322164\n",
      "Iteration 72, loss = 0.25835190\n",
      "Iteration 79, loss = 0.25312200\n",
      "Iteration 73, loss = 0.25793100\n",
      "Iteration 80, loss = 0.25310919\n",
      "Iteration 74, loss = 0.25820370\n",
      "Iteration 81, loss = 0.25299332\n",
      "Iteration 75, loss = 0.25812229\n",
      "Iteration 82, loss = 0.25282665\n",
      "Iteration 76, loss = 0.25830558\n",
      "Iteration 83, loss = 0.25285719\n",
      "Iteration 77, loss = 0.25787632\n",
      "Iteration 84, loss = 0.25257639\n",
      "Iteration 78, loss = 0.25825524\n",
      "Iteration 85, loss = 0.25262604\n",
      "Iteration 79, loss = 0.25783195\n",
      "Iteration 86, loss = 0.25254560\n",
      "Iteration 80, loss = 0.25807763\n",
      "Iteration 87, loss = 0.25269441\n",
      "Iteration 81, loss = 0.25758962\n",
      "Iteration 88, loss = 0.25272010\n",
      "Iteration 82, loss = 0.25771748\n",
      "Iteration 89, loss = 0.25226160\n",
      "Iteration 83, loss = 0.25758679\n",
      "Iteration 90, loss = 0.25232470\n",
      "Iteration 84, loss = 0.25762357\n",
      "Iteration 91, loss = 0.25221938\n",
      "Iteration 85, loss = 0.25737989Iteration 92, loss = 0.25182040\n",
      "\n",
      "Iteration 93, loss = 0.25218310\n",
      "Iteration 86, loss = 0.25744009\n",
      "Iteration 94, loss = 0.25178881\n",
      "Iteration 95, loss = 0.25207808\n",
      "Iteration 87, loss = 0.25742431\n",
      "Iteration 96, loss = 0.25185043\n",
      "Iteration 88, loss = 0.25760027\n",
      "Iteration 97, loss = 0.25169325\n",
      "Iteration 89, loss = 0.25719150\n",
      "Iteration 98, loss = 0.25172551\n",
      "Iteration 90, loss = 0.25710426\n",
      "Iteration 91, loss = 0.25715173\n",
      "Iteration 99, loss = 0.25166227\n",
      "Iteration 92, loss = 0.25652523\n",
      "Iteration 100, loss = 0.25148056\n",
      "Iteration 93, loss = 0.25735804\n",
      "Iteration 101, loss = 0.25143201\n",
      "Iteration 94, loss = 0.25694852\n",
      "Iteration 102, loss = 0.25148909\n",
      "Iteration 95, loss = 0.25700782\n",
      "Iteration 103, loss = 0.25117594\n",
      "Iteration 96, loss = 0.25681937\n",
      "Iteration 104, loss = 0.25171244\n",
      "Iteration 97, loss = 0.25660053\n",
      "Iteration 105, loss = 0.25146177\n",
      "Iteration 98, loss = 0.25700018\n",
      "Iteration 106, loss = 0.25118314\n",
      "Iteration 99, loss = 0.25664849\n",
      "Iteration 107, loss = 0.25102224\n",
      "Iteration 100, loss = 0.25687816\n",
      "Iteration 108, loss = 0.25114029\n",
      "Iteration 101, loss = 0.25659581\n",
      "Iteration 109, loss = 0.25104207\n",
      "Iteration 102, loss = 0.25662140\n",
      "Iteration 110, loss = 0.25089179\n",
      "Iteration 103, loss = 0.25638342\n",
      "Iteration 111, loss = 0.25086714\n",
      "Iteration 104, loss = 0.25667739\n",
      "Iteration 112, loss = 0.25078195\n",
      "Iteration 105, loss = 0.25659275\n",
      "Iteration 113, loss = 0.25097488\n",
      "Iteration 106, loss = 0.25629391\n",
      "Iteration 114, loss = 0.25097605\n",
      "Iteration 107, loss = 0.25621685\n",
      "Iteration 115, loss = 0.25107381\n",
      "Iteration 108, loss = 0.25625815\n",
      "Iteration 116, loss = 0.25074900\n",
      "Iteration 109, loss = 0.25618915\n",
      "Iteration 117, loss = 0.25084501\n",
      "Iteration 110, loss = 0.25625246\n",
      "Iteration 118, loss = 0.25069573\n",
      "Iteration 111, loss = 0.25605808\n",
      "Iteration 119, loss = 0.25099291\n",
      "Iteration 112, loss = 0.25606511\n",
      "Iteration 120, loss = 0.25069686\n",
      "Iteration 113, loss = 0.25637075\n",
      "Iteration 114, loss = 0.25619448\n",
      "Iteration 121, loss = 0.25070674\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 115, loss = 0.25632497\n",
      "Iteration 116, loss = 0.25594167\n",
      "Iteration 1, loss = 0.33466365\n",
      "Iteration 117, loss = 0.25571296\n",
      "Iteration 2, loss = 0.28866399\n",
      "Iteration 118, loss = 0.25572335\n",
      "Iteration 3, loss = 0.27694320\n",
      "Iteration 119, loss = 0.25600071\n",
      "Iteration 4, loss = 0.27337334\n",
      "Iteration 120, loss = 0.25592589\n",
      "Iteration 5, loss = 0.27168841\n",
      "Iteration 121, loss = 0.25602001\n",
      "Iteration 6, loss = 0.27092362\n",
      "Iteration 122, loss = 0.25582276\n",
      "Iteration 7, loss = 0.27075653\n",
      "Iteration 123, loss = 0.25565082\n",
      "Iteration 8, loss = 0.26918321\n",
      "Iteration 124, loss = 0.25569944\n",
      "Iteration 9, loss = 0.26930721\n",
      "Iteration 125, loss = 0.25618289\n",
      "Iteration 10, loss = 0.26863634\n",
      "Iteration 126, loss = 0.25553101\n",
      "Iteration 11, loss = 0.26815802\n",
      "Iteration 127, loss = 0.25554464\n",
      "Iteration 12, loss = 0.26779191\n",
      "Iteration 128, loss = 0.25553861\n",
      "Iteration 13, loss = 0.26723311\n",
      "Iteration 129, loss = 0.25549086\n",
      "Iteration 14, loss = 0.26714197\n",
      "Iteration 130, loss = 0.25538751\n",
      "Iteration 15, loss = 0.26626108\n",
      "Iteration 131, loss = 0.25539712\n",
      "Iteration 132, loss = 0.25540040\n",
      "Iteration 16, loss = 0.26590939\n",
      "Iteration 17, loss = 0.26571849\n",
      "Iteration 133, loss = 0.25519427\n",
      "Iteration 18, loss = 0.26540617\n",
      "Iteration 134, loss = 0.25537560\n",
      "Iteration 135, loss = 0.25546306\n",
      "Iteration 19, loss = 0.26535356\n",
      "Iteration 20, loss = 0.26410109\n",
      "Iteration 136, loss = 0.25561749\n",
      "Iteration 137, loss = 0.25530549\n",
      "Iteration 21, loss = 0.26417521\n",
      "Iteration 138, loss = 0.25524561\n",
      "Iteration 22, loss = 0.26374516\n",
      "Iteration 139, loss = 0.25505650\n",
      "Iteration 23, loss = 0.26355927\n",
      "Iteration 24, loss = 0.26269629\n",
      "Iteration 140, loss = 0.25519410\n",
      "Iteration 25, loss = 0.26252857\n",
      "Iteration 141, loss = 0.25501231\n",
      "Iteration 142, loss = 0.25521742\n",
      "Iteration 26, loss = 0.26209529\n",
      "Iteration 143, loss = 0.25512410\n",
      "Iteration 27, loss = 0.26205391\n",
      "Iteration 144, loss = 0.25477956\n",
      "Iteration 28, loss = 0.26157096\n",
      "Iteration 145, loss = 0.25500369\n",
      "Iteration 29, loss = 0.26115652\n",
      "Iteration 146, loss = 0.25524907\n",
      "Iteration 30, loss = 0.26052285\n",
      "Iteration 147, loss = 0.25489085\n",
      "Iteration 31, loss = 0.26028664\n",
      "Iteration 148, loss = 0.25501697\n",
      "Iteration 32, loss = 0.25993242\n",
      "Iteration 149, loss = 0.25480964\n",
      "Iteration 33, loss = 0.26012161\n",
      "Iteration 150, loss = 0.25520553\n",
      "Iteration 34, loss = 0.25943231\n",
      "Iteration 151, loss = 0.25496233\n",
      "Iteration 35, loss = 0.25933856\n",
      "Iteration 152, loss = 0.25501997\n",
      "Iteration 36, loss = 0.25923579\n",
      "Iteration 153, loss = 0.25524306\n",
      "Iteration 37, loss = 0.25891405\n",
      "Iteration 154, loss = 0.25493610\n",
      "Iteration 38, loss = 0.25844296\n",
      "Iteration 155, loss = 0.25500155\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 39, loss = 0.25840714\n",
      "Iteration 40, loss = 0.25827218\n",
      "Iteration 1, loss = 0.33482665\n",
      "Iteration 41, loss = 0.25794063\n",
      "Iteration 42, loss = 0.25793654\n",
      "Iteration 2, loss = 0.28988375\n",
      "Iteration 3, loss = 0.27875696\n",
      "Iteration 43, loss = 0.25753432\n",
      "Iteration 4, loss = 0.27563980\n",
      "Iteration 44, loss = 0.25726643\n",
      "Iteration 5, loss = 0.27402735\n",
      "Iteration 45, loss = 0.25722600\n",
      "Iteration 6, loss = 0.27314283\n",
      "Iteration 46, loss = 0.25712743\n",
      "Iteration 7, loss = 0.27319637\n",
      "Iteration 47, loss = 0.25661755\n",
      "Iteration 8, loss = 0.27167139\n",
      "Iteration 48, loss = 0.25681096\n",
      "Iteration 9, loss = 0.27162101\n",
      "Iteration 49, loss = 0.25629881\n",
      "Iteration 10, loss = 0.27078402\n",
      "Iteration 50, loss = 0.25644137\n",
      "Iteration 11, loss = 0.27055480\n",
      "Iteration 51, loss = 0.25614092\n",
      "Iteration 12, loss = 0.27004256\n",
      "Iteration 52, loss = 0.25587987\n",
      "Iteration 13, loss = 0.26975202\n",
      "Iteration 53, loss = 0.25610158\n",
      "Iteration 14, loss = 0.26979842\n",
      "Iteration 54, loss = 0.25604230\n",
      "Iteration 55, loss = 0.25569466\n",
      "Iteration 15, loss = 0.26904817\n",
      "Iteration 16, loss = 0.26862533\n",
      "Iteration 56, loss = 0.25545044\n",
      "Iteration 17, loss = 0.26853412\n",
      "Iteration 57, loss = 0.25546466\n",
      "Iteration 18, loss = 0.26821300\n",
      "Iteration 58, loss = 0.25571928\n",
      "Iteration 19, loss = 0.26798416\n",
      "Iteration 59, loss = 0.25534864\n",
      "Iteration 20, loss = 0.26743201\n",
      "Iteration 60, loss = 0.25500531\n",
      "Iteration 21, loss = 0.26723679\n",
      "Iteration 61, loss = 0.25525517\n",
      "Iteration 22, loss = 0.26709171\n",
      "Iteration 62, loss = 0.25483914\n",
      "Iteration 23, loss = 0.26706065\n",
      "Iteration 63, loss = 0.25485208\n",
      "Iteration 24, loss = 0.26611647\n",
      "Iteration 25, loss = 0.26621107\n",
      "Iteration 64, loss = 0.25467443\n",
      "Iteration 65, loss = 0.25461937\n",
      "Iteration 26, loss = 0.26588915\n",
      "Iteration 66, loss = 0.25507437\n",
      "Iteration 27, loss = 0.26567511\n",
      "Iteration 67, loss = 0.25442320\n",
      "Iteration 28, loss = 0.26535784\n",
      "Iteration 68, loss = 0.25440094\n",
      "Iteration 29, loss = 0.26508331\n",
      "Iteration 69, loss = 0.25430558\n",
      "Iteration 30, loss = 0.26437966\n",
      "Iteration 70, loss = 0.25397967\n",
      "Iteration 31, loss = 0.26416240\n",
      "Iteration 71, loss = 0.25414066\n",
      "Iteration 72, loss = 0.25397155\n",
      "Iteration 32, loss = 0.26376530\n",
      "Iteration 33, loss = 0.26367895\n",
      "Iteration 73, loss = 0.25406461\n",
      "Iteration 34, loss = 0.26328479\n",
      "Iteration 74, loss = 0.25411597\n",
      "Iteration 35, loss = 0.26295748\n",
      "Iteration 36, loss = 0.26270375\n",
      "Iteration 75, loss = 0.25395712\n",
      "Iteration 76, loss = 0.25379960\n",
      "Iteration 37, loss = 0.26252308\n",
      "Iteration 77, loss = 0.25364789\n",
      "Iteration 38, loss = 0.26220916\n",
      "Iteration 78, loss = 0.25356013\n",
      "Iteration 39, loss = 0.26199108\n",
      "Iteration 79, loss = 0.25363989\n",
      "Iteration 40, loss = 0.26157025\n",
      "Iteration 80, loss = 0.25394515\n",
      "Iteration 41, loss = 0.26131199\n",
      "Iteration 81, loss = 0.25351579\n",
      "Iteration 42, loss = 0.26120994\n",
      "Iteration 82, loss = 0.25385278\n",
      "Iteration 83, loss = 0.25340080\n",
      "Iteration 43, loss = 0.26123972\n",
      "Iteration 84, loss = 0.25345965\n",
      "Iteration 44, loss = 0.26060244\n",
      "Iteration 85, loss = 0.25323881\n",
      "Iteration 45, loss = 0.26082581\n",
      "Iteration 86, loss = 0.25362012\n",
      "Iteration 46, loss = 0.26045967\n",
      "Iteration 47, loss = 0.25999971\n",
      "Iteration 87, loss = 0.25342461\n",
      "Iteration 48, loss = 0.25999808\n",
      "Iteration 88, loss = 0.25306829\n",
      "Iteration 49, loss = 0.25948927\n",
      "Iteration 89, loss = 0.25324210\n",
      "Iteration 90, loss = 0.25314945\n",
      "Iteration 50, loss = 0.25947817\n",
      "Iteration 91, loss = 0.25277349\n",
      "Iteration 51, loss = 0.25930343\n",
      "Iteration 92, loss = 0.25299744\n",
      "Iteration 52, loss = 0.25909661\n",
      "Iteration 53, loss = 0.25937957\n",
      "Iteration 93, loss = 0.25289901\n",
      "Iteration 94, loss = 0.25283591\n",
      "Iteration 54, loss = 0.25934129\n",
      "Iteration 95, loss = 0.25278175\n",
      "Iteration 55, loss = 0.25887253\n",
      "Iteration 96, loss = 0.25266226\n",
      "Iteration 56, loss = 0.25899192\n",
      "Iteration 97, loss = 0.25265742\n",
      "Iteration 57, loss = 0.25863952\n",
      "Iteration 98, loss = 0.25253067\n",
      "Iteration 58, loss = 0.25879226\n",
      "Iteration 99, loss = 0.25239745\n",
      "Iteration 59, loss = 0.25856713\n",
      "Iteration 100, loss = 0.25250255\n",
      "Iteration 60, loss = 0.25806336\n",
      "Iteration 101, loss = 0.25315191\n",
      "Iteration 61, loss = 0.25856484\n",
      "Iteration 102, loss = 0.25230608\n",
      "Iteration 62, loss = 0.25787403\n",
      "Iteration 103, loss = 0.25257263\n",
      "Iteration 63, loss = 0.25775414\n",
      "Iteration 104, loss = 0.25237982\n",
      "Iteration 64, loss = 0.25792596\n",
      "Iteration 105, loss = 0.25257394\n",
      "Iteration 65, loss = 0.25768254\n",
      "Iteration 106, loss = 0.25240681\n",
      "Iteration 66, loss = 0.25780216\n",
      "Iteration 107, loss = 0.25230815\n",
      "Iteration 67, loss = 0.25763291\n",
      "Iteration 108, loss = 0.25209837\n",
      "Iteration 68, loss = 0.25750921\n",
      "Iteration 109, loss = 0.25230123\n",
      "Iteration 69, loss = 0.25726817\n",
      "Iteration 110, loss = 0.25229922\n",
      "Iteration 70, loss = 0.25704231\n",
      "Iteration 111, loss = 0.25223080\n",
      "Iteration 71, loss = 0.25710412\n",
      "Iteration 112, loss = 0.25250432\n",
      "Iteration 72, loss = 0.25714560\n",
      "Iteration 113, loss = 0.25208923\n",
      "Iteration 73, loss = 0.25687964\n",
      "Iteration 114, loss = 0.25215354\n",
      "Iteration 74, loss = 0.25725038\n",
      "Iteration 115, loss = 0.25184325\n",
      "Iteration 116, loss = 0.25184628\n",
      "Iteration 75, loss = 0.25714301\n",
      "Iteration 76, loss = 0.25679611\n",
      "Iteration 117, loss = 0.25181487\n",
      "Iteration 118, loss = 0.25172776\n",
      "Iteration 77, loss = 0.25676845\n",
      "Iteration 119, loss = 0.25175920\n",
      "Iteration 78, loss = 0.25666376\n",
      "Iteration 120, loss = 0.25203484\n",
      "Iteration 79, loss = 0.25666013\n",
      "Iteration 121, loss = 0.25185590\n",
      "Iteration 80, loss = 0.25678205\n",
      "Iteration 122, loss = 0.25181564\n",
      "Iteration 81, loss = 0.25646929\n",
      "Iteration 123, loss = 0.25197206\n",
      "Iteration 82, loss = 0.25712950\n",
      "Iteration 124, loss = 0.25182942\n",
      "Iteration 83, loss = 0.25652512\n",
      "Iteration 125, loss = 0.25179317\n",
      "Iteration 84, loss = 0.25618025\n",
      "Iteration 126, loss = 0.25149063\n",
      "Iteration 85, loss = 0.25627282\n",
      "Iteration 127, loss = 0.25181252\n",
      "Iteration 128, loss = 0.25170552\n",
      "Iteration 86, loss = 0.25638258\n",
      "Iteration 129, loss = 0.25162432\n",
      "Iteration 87, loss = 0.25663584\n",
      "Iteration 130, loss = 0.25164434\n",
      "Iteration 131, loss = 0.25173733\n",
      "Iteration 88, loss = 0.25612911\n",
      "Iteration 89, loss = 0.25637923\n",
      "Iteration 132, loss = 0.25138393\n",
      "Iteration 90, loss = 0.25602309\n",
      "Iteration 133, loss = 0.25184443\n",
      "Iteration 91, loss = 0.25599811\n",
      "Iteration 134, loss = 0.25139819\n",
      "Iteration 135, loss = 0.25134733\n",
      "Iteration 92, loss = 0.25600298\n",
      "Iteration 136, loss = 0.25155155\n",
      "Iteration 93, loss = 0.25587922\n",
      "Iteration 137, loss = 0.25160007\n",
      "Iteration 138, loss = 0.25141684\n",
      "Iteration 94, loss = 0.25593070\n",
      "Iteration 139, loss = 0.25138136\n",
      "Iteration 95, loss = 0.25595508\n",
      "Iteration 140, loss = 0.25147395\n",
      "Iteration 141, loss = 0.25122698\n",
      "Iteration 96, loss = 0.25611630\n",
      "Iteration 142, loss = 0.25134406\n",
      "Iteration 97, loss = 0.25590369\n",
      "Iteration 143, loss = 0.25172727\n",
      "Iteration 98, loss = 0.25562008\n",
      "Iteration 144, loss = 0.25136148\n",
      "Iteration 99, loss = 0.25584337\n",
      "Iteration 145, loss = 0.25125733\n",
      "Iteration 100, loss = 0.25586838\n",
      "Iteration 146, loss = 0.25117201\n",
      "Iteration 101, loss = 0.25608792\n",
      "Iteration 147, loss = 0.25117428\n",
      "Iteration 102, loss = 0.25558915\n",
      "Iteration 148, loss = 0.25134762\n",
      "Iteration 103, loss = 0.25593709\n",
      "Iteration 149, loss = 0.25134470\n",
      "Iteration 104, loss = 0.25551197\n",
      "Iteration 150, loss = 0.25119944\n",
      "Iteration 105, loss = 0.25582343\n",
      "Iteration 151, loss = 0.25130274\n",
      "Iteration 106, loss = 0.25561047\n",
      "Iteration 152, loss = 0.25092004\n",
      "Iteration 107, loss = 0.25566164\n",
      "Iteration 153, loss = 0.25096552\n",
      "Iteration 108, loss = 0.25543260\n",
      "Iteration 154, loss = 0.25115547\n",
      "Iteration 109, loss = 0.25555917\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 155, loss = 0.25119945\n",
      "Iteration 156, loss = 0.25112628\n",
      "Iteration 1, loss = 0.33499359\n",
      "Iteration 157, loss = 0.25110760\n",
      "Iteration 2, loss = 0.28994890\n",
      "Iteration 158, loss = 0.25088305\n",
      "Iteration 3, loss = 0.27864341\n",
      "Iteration 159, loss = 0.25107021\n",
      "Iteration 4, loss = 0.27504974\n",
      "Iteration 160, loss = 0.25124713\n",
      "Iteration 5, loss = 0.27333025\n",
      "Iteration 161, loss = 0.25074005\n",
      "Iteration 6, loss = 0.27263960\n",
      "Iteration 162, loss = 0.25085359\n",
      "Iteration 163, loss = 0.25115595\n",
      "Iteration 7, loss = 0.27230798\n",
      "Iteration 8, loss = 0.27127954\n",
      "Iteration 164, loss = 0.25103997\n",
      "Iteration 9, loss = 0.27112655\n",
      "Iteration 165, loss = 0.25094114\n",
      "Iteration 166, loss = 0.25079283\n",
      "Iteration 10, loss = 0.27015549\n",
      "Iteration 167, loss = 0.25076602\n",
      "Iteration 11, loss = 0.26992778\n",
      "Iteration 168, loss = 0.25085845\n",
      "Iteration 12, loss = 0.26931645\n",
      "Iteration 169, loss = 0.25085353\n",
      "Iteration 13, loss = 0.26887029\n",
      "Iteration 170, loss = 0.25094129\n",
      "Iteration 14, loss = 0.26873247\n",
      "Iteration 171, loss = 0.25066644\n",
      "Iteration 15, loss = 0.26803953\n",
      "Iteration 172, loss = 0.25071490\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 16, loss = 0.26717014\n",
      "Iteration 17, loss = 0.26718611\n",
      "Iteration 1, loss = 0.33463204\n",
      "Iteration 18, loss = 0.26672624\n",
      "Iteration 2, loss = 0.28900102\n",
      "Iteration 19, loss = 0.26656553\n",
      "Iteration 3, loss = 0.27737265\n",
      "Iteration 20, loss = 0.26591217\n",
      "Iteration 4, loss = 0.27382238\n",
      "Iteration 5, loss = 0.27232307\n",
      "Iteration 21, loss = 0.26554205\n",
      "Iteration 6, loss = 0.27140883\n",
      "Iteration 22, loss = 0.26537768\n",
      "Iteration 7, loss = 0.27135288\n",
      "Iteration 23, loss = 0.26533443\n",
      "Iteration 24, loss = 0.26465421\n",
      "Iteration 8, loss = 0.27017354\n",
      "Iteration 25, loss = 0.26463238\n",
      "Iteration 9, loss = 0.27011827\n",
      "Iteration 10, loss = 0.26929434\n",
      "Iteration 26, loss = 0.26408564\n",
      "Iteration 27, loss = 0.26405355\n",
      "Iteration 11, loss = 0.26922387\n",
      "Iteration 28, loss = 0.26371158\n",
      "Iteration 12, loss = 0.26867454\n",
      "Iteration 13, loss = 0.26840674\n",
      "Iteration 29, loss = 0.26373636\n",
      "Iteration 14, loss = 0.26799548\n",
      "Iteration 30, loss = 0.26280331\n",
      "Iteration 15, loss = 0.26759938\n",
      "Iteration 31, loss = 0.26257635\n",
      "Iteration 32, loss = 0.26215813\n",
      "Iteration 16, loss = 0.26671254\n",
      "Iteration 17, loss = 0.26696925\n",
      "Iteration 33, loss = 0.26241896\n",
      "Iteration 18, loss = 0.26652110\n",
      "Iteration 34, loss = 0.26186422\n",
      "Iteration 35, loss = 0.26157946\n",
      "Iteration 19, loss = 0.26618502\n",
      "Iteration 36, loss = 0.26133091\n",
      "Iteration 20, loss = 0.26578125\n",
      "Iteration 37, loss = 0.26153365\n",
      "Iteration 21, loss = 0.26542586\n",
      "Iteration 38, loss = 0.26111871\n",
      "Iteration 22, loss = 0.26536583\n",
      "Iteration 39, loss = 0.26069514\n",
      "Iteration 23, loss = 0.26518912\n",
      "Iteration 40, loss = 0.26031818\n",
      "Iteration 24, loss = 0.26442349\n",
      "Iteration 41, loss = 0.26023823\n",
      "Iteration 25, loss = 0.26444905\n",
      "Iteration 42, loss = 0.26034431\n",
      "Iteration 26, loss = 0.26385524\n",
      "Iteration 43, loss = 0.26005837\n",
      "Iteration 27, loss = 0.26376041\n",
      "Iteration 44, loss = 0.25948315\n",
      "Iteration 28, loss = 0.26335846\n",
      "Iteration 29, loss = 0.26329812\n",
      "Iteration 45, loss = 0.25970846\n",
      "Iteration 30, loss = 0.26269281\n",
      "Iteration 46, loss = 0.25951153\n",
      "Iteration 31, loss = 0.26211931\n",
      "Iteration 47, loss = 0.25897980\n",
      "Iteration 32, loss = 0.26204956\n",
      "Iteration 48, loss = 0.25911300\n",
      "Iteration 33, loss = 0.26198573\n",
      "Iteration 49, loss = 0.25854907\n",
      "Iteration 34, loss = 0.26176085\n",
      "Iteration 50, loss = 0.25873304\n",
      "Iteration 35, loss = 0.26127536\n",
      "Iteration 51, loss = 0.25851257\n",
      "Iteration 36, loss = 0.26141156\n",
      "Iteration 52, loss = 0.25859202\n",
      "Iteration 37, loss = 0.26104711\n",
      "Iteration 53, loss = 0.25854939\n",
      "Iteration 38, loss = 0.26059834\n",
      "Iteration 54, loss = 0.25865854\n",
      "Iteration 55, loss = 0.25810053\n",
      "Iteration 39, loss = 0.26024246\n",
      "Iteration 40, loss = 0.26006392\n",
      "Iteration 56, loss = 0.25828509\n",
      "Iteration 41, loss = 0.25971398\n",
      "Iteration 57, loss = 0.25775793\n",
      "Iteration 42, loss = 0.25972723\n",
      "Iteration 58, loss = 0.25783059\n",
      "Iteration 43, loss = 0.25970456\n",
      "Iteration 59, loss = 0.25753016\n",
      "Iteration 44, loss = 0.25906639\n",
      "Iteration 60, loss = 0.25736301\n",
      "Iteration 45, loss = 0.25907613\n",
      "Iteration 61, loss = 0.25752899\n",
      "Iteration 46, loss = 0.25885943\n",
      "Iteration 62, loss = 0.25700923\n",
      "Iteration 47, loss = 0.25839356\n",
      "Iteration 63, loss = 0.25699747\n",
      "Iteration 48, loss = 0.25837898\n",
      "Iteration 64, loss = 0.25722818\n",
      "Iteration 49, loss = 0.25781539\n",
      "Iteration 65, loss = 0.25692309\n",
      "Iteration 50, loss = 0.25763075\n",
      "Iteration 66, loss = 0.25713607\n",
      "Iteration 51, loss = 0.25753944\n",
      "Iteration 67, loss = 0.25697280\n",
      "Iteration 52, loss = 0.25736223\n",
      "Iteration 68, loss = 0.25648642\n",
      "Iteration 53, loss = 0.25739568\n",
      "Iteration 69, loss = 0.25628207\n",
      "Iteration 54, loss = 0.25745349\n",
      "Iteration 70, loss = 0.25625030\n",
      "Iteration 55, loss = 0.25703041\n",
      "Iteration 71, loss = 0.25626924\n",
      "Iteration 56, loss = 0.25707223\n",
      "Iteration 72, loss = 0.25643370\n",
      "Iteration 73, loss = 0.25585281\n",
      "Iteration 57, loss = 0.25627281\n",
      "Iteration 74, loss = 0.25651889\n",
      "Iteration 58, loss = 0.25619503\n",
      "Iteration 75, loss = 0.25626473\n",
      "Iteration 59, loss = 0.25614653\n",
      "Iteration 76, loss = 0.25605321\n",
      "Iteration 60, loss = 0.25600246\n",
      "Iteration 77, loss = 0.25588365\n",
      "Iteration 61, loss = 0.25616128\n",
      "Iteration 78, loss = 0.25578104\n",
      "Iteration 62, loss = 0.25546831\n",
      "Iteration 79, loss = 0.25584407\n",
      "Iteration 63, loss = 0.25541342\n",
      "Iteration 80, loss = 0.25579034\n",
      "Iteration 64, loss = 0.25545126\n",
      "Iteration 81, loss = 0.25585154\n",
      "Iteration 65, loss = 0.25518315\n",
      "Iteration 82, loss = 0.25599255\n",
      "Iteration 66, loss = 0.25528167\n",
      "Iteration 83, loss = 0.25559940\n",
      "Iteration 67, loss = 0.25525228\n",
      "Iteration 84, loss = 0.25535932\n",
      "Iteration 68, loss = 0.25485317\n",
      "Iteration 85, loss = 0.25554620\n",
      "Iteration 69, loss = 0.25457672\n",
      "Iteration 86, loss = 0.25532354\n",
      "Iteration 70, loss = 0.25483388\n",
      "Iteration 87, loss = 0.25543981\n",
      "Iteration 71, loss = 0.25478747\n",
      "Iteration 88, loss = 0.25541354\n",
      "Iteration 72, loss = 0.25481868\n",
      "Iteration 89, loss = 0.25541963\n",
      "Iteration 73, loss = 0.25431553\n",
      "Iteration 90, loss = 0.25516245\n",
      "Iteration 74, loss = 0.25493645\n",
      "Iteration 91, loss = 0.25492517\n",
      "Iteration 75, loss = 0.25488070\n",
      "Iteration 92, loss = 0.25493745\n",
      "Iteration 76, loss = 0.25433614\n",
      "Iteration 93, loss = 0.25496691\n",
      "Iteration 77, loss = 0.25422512\n",
      "Iteration 78, loss = 0.25409777\n",
      "Iteration 94, loss = 0.25491725\n",
      "Iteration 79, loss = 0.25424083\n",
      "Iteration 95, loss = 0.25509997\n",
      "Iteration 80, loss = 0.25420047\n",
      "Iteration 96, loss = 0.25489005\n",
      "Iteration 81, loss = 0.25388598\n",
      "Iteration 97, loss = 0.25510925\n",
      "Iteration 82, loss = 0.25419288\n",
      "Iteration 98, loss = 0.25456384\n",
      "Iteration 83, loss = 0.25375858\n",
      "Iteration 99, loss = 0.25458931\n",
      "Iteration 84, loss = 0.25359683\n",
      "Iteration 100, loss = 0.25472777\n",
      "Iteration 85, loss = 0.25361503\n",
      "Iteration 101, loss = 0.25501279\n",
      "Iteration 86, loss = 0.25350348\n",
      "Iteration 102, loss = 0.25483784\n",
      "Iteration 87, loss = 0.25360950\n",
      "Iteration 103, loss = 0.25487254\n",
      "Iteration 88, loss = 0.25371831\n",
      "Iteration 104, loss = 0.25449777\n",
      "Iteration 105, loss = 0.25455602\n",
      "Iteration 89, loss = 0.25389506\n",
      "Iteration 90, loss = 0.25330026\n",
      "Iteration 106, loss = 0.25463603\n",
      "Iteration 91, loss = 0.25317904\n",
      "Iteration 107, loss = 0.25446175\n",
      "Iteration 92, loss = 0.25298553\n",
      "Iteration 108, loss = 0.25420213\n",
      "Iteration 109, loss = 0.25463697\n",
      "Iteration 93, loss = 0.25321171\n",
      "Iteration 110, loss = 0.25429309\n",
      "Iteration 94, loss = 0.25317514\n",
      "Iteration 111, loss = 0.25412979\n",
      "Iteration 95, loss = 0.25338640\n",
      "Iteration 112, loss = 0.25445932\n",
      "Iteration 96, loss = 0.25317204\n",
      "Iteration 113, loss = 0.25417297\n",
      "Iteration 97, loss = 0.25311293\n",
      "Iteration 114, loss = 0.25414975\n",
      "Iteration 98, loss = 0.25297859\n",
      "Iteration 115, loss = 0.25395486\n",
      "Iteration 99, loss = 0.25281118\n",
      "Iteration 116, loss = 0.25380093\n",
      "Iteration 100, loss = 0.25297848\n",
      "Iteration 117, loss = 0.25408498\n",
      "Iteration 101, loss = 0.25311246\n",
      "Iteration 118, loss = 0.25356563\n",
      "Iteration 102, loss = 0.25271032\n",
      "Iteration 119, loss = 0.25338826\n",
      "Iteration 103, loss = 0.25277251\n",
      "Iteration 120, loss = 0.25364058\n",
      "Iteration 104, loss = 0.25244494\n",
      "Iteration 105, loss = 0.25250920\n",
      "Iteration 121, loss = 0.25355189\n",
      "Iteration 106, loss = 0.25271301\n",
      "Iteration 122, loss = 0.25367883\n",
      "Iteration 107, loss = 0.25263720\n",
      "Iteration 123, loss = 0.25357768Iteration 108, loss = 0.25226951\n",
      "\n",
      "Iteration 109, loss = 0.25270125\n",
      "Iteration 124, loss = 0.25331196\n",
      "Iteration 125, loss = 0.25338939\n",
      "Iteration 110, loss = 0.25226150\n",
      "Iteration 126, loss = 0.25331535\n",
      "Iteration 111, loss = 0.25230468\n",
      "Iteration 127, loss = 0.25304369\n",
      "Iteration 112, loss = 0.25219333\n",
      "Iteration 128, loss = 0.25320662\n",
      "Iteration 113, loss = 0.25228186\n",
      "Iteration 129, loss = 0.25273228\n",
      "Iteration 114, loss = 0.25253968\n",
      "Iteration 130, loss = 0.25317875\n",
      "Iteration 115, loss = 0.25199674\n",
      "Iteration 116, loss = 0.25201596\n",
      "Iteration 131, loss = 0.25302977\n",
      "Iteration 132, loss = 0.25284096\n",
      "Iteration 117, loss = 0.25213184\n",
      "Iteration 133, loss = 0.25310932\n",
      "Iteration 118, loss = 0.25186628\n",
      "Iteration 134, loss = 0.25253298\n",
      "Iteration 119, loss = 0.25172190\n",
      "Iteration 135, loss = 0.25262036\n",
      "Iteration 120, loss = 0.25198023\n",
      "Iteration 136, loss = 0.25267709\n",
      "Iteration 121, loss = 0.25176156\n",
      "Iteration 137, loss = 0.25276730\n",
      "Iteration 138, loss = 0.25278130\n",
      "Iteration 122, loss = 0.25179108\n",
      "Iteration 139, loss = 0.25248264\n",
      "Iteration 123, loss = 0.25205533\n",
      "Iteration 140, loss = 0.25269272\n",
      "Iteration 124, loss = 0.25164066\n",
      "Iteration 141, loss = 0.25266614\n",
      "Iteration 125, loss = 0.25203792\n",
      "Iteration 142, loss = 0.25241557\n",
      "Iteration 126, loss = 0.25167610\n",
      "Iteration 143, loss = 0.25316656\n",
      "Iteration 127, loss = 0.25155633\n",
      "Iteration 144, loss = 0.25244033\n",
      "Iteration 128, loss = 0.25172372\n",
      "Iteration 145, loss = 0.25242423\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 129, loss = 0.25150766\n",
      "Iteration 130, loss = 0.25164623\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33471351\n",
      "Iteration 2, loss = 0.28852742\n",
      "Iteration 3, loss = 0.27630630\n",
      "Iteration 1, loss = 0.33399018\n",
      "Iteration 4, loss = 0.27238858\n",
      "Iteration 5, loss = 0.27110475\n",
      "Iteration 2, loss = 0.28990392\n",
      "Iteration 3, loss = 0.27879315\n",
      "Iteration 6, loss = 0.27002623\n",
      "Iteration 4, loss = 0.27511421\n",
      "Iteration 7, loss = 0.27010553\n",
      "Iteration 5, loss = 0.27393040\n",
      "Iteration 8, loss = 0.26890293\n",
      "Iteration 6, loss = 0.27297191\n",
      "Iteration 9, loss = 0.26882692\n",
      "Iteration 7, loss = 0.27336268\n",
      "Iteration 10, loss = 0.26824767\n",
      "Iteration 11, loss = 0.26811371\n",
      "Iteration 8, loss = 0.27192281\n",
      "Iteration 12, loss = 0.26738594\n",
      "Iteration 9, loss = 0.27145048\n",
      "Iteration 13, loss = 0.26745493\n",
      "Iteration 10, loss = 0.27089847\n",
      "Iteration 14, loss = 0.26680286\n",
      "Iteration 11, loss = 0.27063065\n",
      "Iteration 15, loss = 0.26640583\n",
      "Iteration 12, loss = 0.26985027\n",
      "Iteration 16, loss = 0.26574117\n",
      "Iteration 13, loss = 0.26977631\n",
      "Iteration 17, loss = 0.26602978\n",
      "Iteration 14, loss = 0.26889876\n",
      "Iteration 18, loss = 0.26508643\n",
      "Iteration 15, loss = 0.26874034\n",
      "Iteration 19, loss = 0.26502055\n",
      "Iteration 16, loss = 0.26825056\n",
      "Iteration 20, loss = 0.26482612\n",
      "Iteration 17, loss = 0.26799098\n",
      "Iteration 21, loss = 0.26434924\n",
      "Iteration 18, loss = 0.26715318\n",
      "Iteration 22, loss = 0.26448375\n",
      "Iteration 19, loss = 0.26697704\n",
      "Iteration 23, loss = 0.26401339\n",
      "Iteration 20, loss = 0.26636124\n",
      "Iteration 24, loss = 0.26343021\n",
      "Iteration 21, loss = 0.26592252\n",
      "Iteration 25, loss = 0.26343150\n",
      "Iteration 22, loss = 0.26583728\n",
      "Iteration 26, loss = 0.26288786\n",
      "Iteration 23, loss = 0.26525103\n",
      "Iteration 27, loss = 0.26298927\n",
      "Iteration 24, loss = 0.26460181\n",
      "Iteration 28, loss = 0.26266980\n",
      "Iteration 29, loss = 0.26264464\n",
      "Iteration 25, loss = 0.26442382\n",
      "Iteration 26, loss = 0.26388313\n",
      "Iteration 30, loss = 0.26204615\n",
      "Iteration 27, loss = 0.26364585\n",
      "Iteration 31, loss = 0.26156244\n",
      "Iteration 28, loss = 0.26329424\n",
      "Iteration 32, loss = 0.26143178\n",
      "Iteration 29, loss = 0.26325679\n",
      "Iteration 33, loss = 0.26163796\n",
      "Iteration 30, loss = 0.26276968\n",
      "Iteration 34, loss = 0.26127644\n",
      "Iteration 31, loss = 0.26228022\n",
      "Iteration 35, loss = 0.26085672\n",
      "Iteration 32, loss = 0.26227798\n",
      "Iteration 36, loss = 0.26104203\n",
      "Iteration 33, loss = 0.26199940\n",
      "Iteration 37, loss = 0.26097500\n",
      "Iteration 34, loss = 0.26175911\n",
      "Iteration 38, loss = 0.26022804\n",
      "Iteration 35, loss = 0.26141396\n",
      "Iteration 39, loss = 0.26006021\n",
      "Iteration 36, loss = 0.26169851\n",
      "Iteration 40, loss = 0.25983868\n",
      "Iteration 37, loss = 0.26150751\n",
      "Iteration 41, loss = 0.25954464\n",
      "Iteration 38, loss = 0.26079550\n",
      "Iteration 42, loss = 0.25943624\n",
      "Iteration 39, loss = 0.26083681Iteration 43, loss = 0.25927116\n",
      "\n",
      "Iteration 44, loss = 0.25893048\n",
      "Iteration 40, loss = 0.26072359\n",
      "Iteration 45, loss = 0.25908484\n",
      "Iteration 41, loss = 0.26027930\n",
      "Iteration 46, loss = 0.25839391\n",
      "Iteration 42, loss = 0.26027599\n",
      "Iteration 47, loss = 0.25822228\n",
      "Iteration 43, loss = 0.26008545\n",
      "Iteration 48, loss = 0.25839205\n",
      "Iteration 44, loss = 0.26017179\n",
      "Iteration 49, loss = 0.25772812\n",
      "Iteration 45, loss = 0.26000712\n",
      "Iteration 50, loss = 0.25758311\n",
      "Iteration 46, loss = 0.25975853\n",
      "Iteration 51, loss = 0.25764117\n",
      "Iteration 47, loss = 0.25933431\n",
      "Iteration 52, loss = 0.25750526\n",
      "Iteration 48, loss = 0.25982090\n",
      "Iteration 53, loss = 0.25741416\n",
      "Iteration 54, loss = 0.25726960\n",
      "Iteration 49, loss = 0.25905781\n",
      "Iteration 50, loss = 0.25889933Iteration 55, loss = 0.25696208\n",
      "\n",
      "Iteration 51, loss = 0.25868722\n",
      "Iteration 56, loss = 0.25742369\n",
      "Iteration 52, loss = 0.25888726\n",
      "Iteration 57, loss = 0.25660933\n",
      "Iteration 58, loss = 0.25665139\n",
      "Iteration 53, loss = 0.25856375\n",
      "Iteration 59, loss = 0.25676648\n",
      "Iteration 54, loss = 0.25849184\n",
      "Iteration 60, loss = 0.25649442\n",
      "Iteration 61, loss = 0.25644979\n",
      "Iteration 55, loss = 0.25829188\n",
      "Iteration 62, loss = 0.25602590\n",
      "Iteration 56, loss = 0.25869282\n",
      "Iteration 63, loss = 0.25586637\n",
      "Iteration 57, loss = 0.25791633\n",
      "Iteration 64, loss = 0.25617313\n",
      "Iteration 58, loss = 0.25815421\n",
      "Iteration 65, loss = 0.25575674\n",
      "Iteration 59, loss = 0.25809381\n",
      "Iteration 66, loss = 0.25582619\n",
      "Iteration 60, loss = 0.25776544\n",
      "Iteration 67, loss = 0.25585511\n",
      "Iteration 61, loss = 0.25759396\n",
      "Iteration 68, loss = 0.25550938\n",
      "Iteration 62, loss = 0.25748562\n",
      "Iteration 69, loss = 0.25522970\n",
      "Iteration 63, loss = 0.25715594\n",
      "Iteration 64, loss = 0.25761102\n",
      "Iteration 70, loss = 0.25547442\n",
      "Iteration 65, loss = 0.25748797\n",
      "Iteration 71, loss = 0.25549875\n",
      "Iteration 66, loss = 0.25742298\n",
      "Iteration 72, loss = 0.25566811\n",
      "Iteration 67, loss = 0.25719046\n",
      "Iteration 73, loss = 0.25512619\n",
      "Iteration 68, loss = 0.25679560\n",
      "Iteration 74, loss = 0.25533039\n",
      "Iteration 75, loss = 0.25538357\n",
      "Iteration 69, loss = 0.25654938\n",
      "Iteration 76, loss = 0.25490901\n",
      "Iteration 70, loss = 0.25702442\n",
      "Iteration 77, loss = 0.25491582\n",
      "Iteration 71, loss = 0.25702090\n",
      "Iteration 72, loss = 0.25766119\n",
      "Iteration 78, loss = 0.25473532\n",
      "Iteration 73, loss = 0.25670045\n",
      "Iteration 79, loss = 0.25470112\n",
      "Iteration 80, loss = 0.25467330\n",
      "Iteration 74, loss = 0.25667948\n",
      "Iteration 81, loss = 0.25452055\n",
      "Iteration 75, loss = 0.25677595\n",
      "Iteration 82, loss = 0.25451735\n",
      "Iteration 76, loss = 0.25672055\n",
      "Iteration 83, loss = 0.25448444\n",
      "Iteration 77, loss = 0.25636856\n",
      "Iteration 84, loss = 0.25437992\n",
      "Iteration 78, loss = 0.25634312\n",
      "Iteration 85, loss = 0.25413316\n",
      "Iteration 79, loss = 0.25616795\n",
      "Iteration 86, loss = 0.25420487\n",
      "Iteration 80, loss = 0.25597193\n",
      "Iteration 87, loss = 0.25432842\n",
      "Iteration 81, loss = 0.25607595\n",
      "Iteration 88, loss = 0.25428241\n",
      "Iteration 82, loss = 0.25589792\n",
      "Iteration 89, loss = 0.25451519\n",
      "Iteration 83, loss = 0.25582173\n",
      "Iteration 90, loss = 0.25385686\n",
      "Iteration 84, loss = 0.25596828\n",
      "Iteration 85, loss = 0.25578272\n",
      "Iteration 91, loss = 0.25390752\n",
      "Iteration 92, loss = 0.25378249\n",
      "Iteration 86, loss = 0.25566293\n",
      "Iteration 93, loss = 0.25394048\n",
      "Iteration 87, loss = 0.25584297\n",
      "Iteration 88, loss = 0.25561646\n",
      "Iteration 94, loss = 0.25370504\n",
      "Iteration 89, loss = 0.25567401\n",
      "Iteration 95, loss = 0.25373613\n",
      "Iteration 90, loss = 0.25529894\n",
      "Iteration 96, loss = 0.25375860\n",
      "Iteration 97, loss = 0.25398622\n",
      "Iteration 91, loss = 0.25543952\n",
      "Iteration 98, loss = 0.25344087\n",
      "Iteration 92, loss = 0.25522753\n",
      "Iteration 99, loss = 0.25373185\n",
      "Iteration 93, loss = 0.25532057\n",
      "Iteration 100, loss = 0.25378106\n",
      "Iteration 94, loss = 0.25514419\n",
      "Iteration 101, loss = 0.25387327\n",
      "Iteration 95, loss = 0.25516934\n",
      "Iteration 102, loss = 0.25342071\n",
      "Iteration 96, loss = 0.25518580\n",
      "Iteration 103, loss = 0.25349211\n",
      "Iteration 104, loss = 0.25335303\n",
      "Iteration 97, loss = 0.25532267\n",
      "Iteration 105, loss = 0.25346460\n",
      "Iteration 98, loss = 0.25507765\n",
      "Iteration 106, loss = 0.25353558\n",
      "Iteration 99, loss = 0.25482332\n",
      "Iteration 107, loss = 0.25347751\n",
      "Iteration 100, loss = 0.25486992\n",
      "Iteration 108, loss = 0.25305658\n",
      "Iteration 101, loss = 0.25509175\n",
      "Iteration 109, loss = 0.25338876\n",
      "Iteration 102, loss = 0.25488960\n",
      "Iteration 110, loss = 0.25306762\n",
      "Iteration 103, loss = 0.25494488\n",
      "Iteration 111, loss = 0.25316362\n",
      "Iteration 104, loss = 0.25462746\n",
      "Iteration 112, loss = 0.25296150\n",
      "Iteration 105, loss = 0.25461251\n",
      "Iteration 113, loss = 0.25308722\n",
      "Iteration 106, loss = 0.25459168\n",
      "Iteration 114, loss = 0.25317304\n",
      "Iteration 107, loss = 0.25476550\n",
      "Iteration 108, loss = 0.25441611\n",
      "Iteration 115, loss = 0.25294262\n",
      "Iteration 116, loss = 0.25318932Iteration 109, loss = 0.25474917\n",
      "\n",
      "Iteration 117, loss = 0.25277822\n",
      "Iteration 110, loss = 0.25433579\n",
      "Iteration 118, loss = 0.25270630\n",
      "Iteration 111, loss = 0.25418245\n",
      "Iteration 119, loss = 0.25296770\n",
      "Iteration 120, loss = 0.25281290\n",
      "Iteration 112, loss = 0.25439764\n",
      "Iteration 121, loss = 0.25271895\n",
      "Iteration 113, loss = 0.25427778\n",
      "Iteration 114, loss = 0.25437423\n",
      "Iteration 122, loss = 0.25276970\n",
      "Iteration 115, loss = 0.25415584\n",
      "Iteration 123, loss = 0.25268044\n",
      "Iteration 124, loss = 0.25277238\n",
      "Iteration 116, loss = 0.25407392\n",
      "Iteration 125, loss = 0.25268717\n",
      "Iteration 117, loss = 0.25413018\n",
      "Iteration 126, loss = 0.25266790\n",
      "Iteration 118, loss = 0.25400212\n",
      "Iteration 127, loss = 0.25260042\n",
      "Iteration 119, loss = 0.25385467\n",
      "Iteration 128, loss = 0.25253569\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 120, loss = 0.25396155\n",
      "Iteration 121, loss = 0.25399514\n",
      "Iteration 122, loss = 0.25411333\n",
      "Iteration 123, loss = 0.25389362\n",
      "Iteration 124, loss = 0.25383547\n",
      "Iteration 125, loss = 0.25393244\n",
      "Iteration 126, loss = 0.25365860\n",
      "Iteration 127, loss = 0.25374181\n",
      "Iteration 128, loss = 0.25357940\n",
      "Iteration 129, loss = 0.25352233\n",
      "Iteration 130, loss = 0.25396904\n",
      "Iteration 131, loss = 0.25365368\n",
      "Iteration 132, loss = 0.25362460\n",
      "Iteration 133, loss = 0.25351816\n",
      "Iteration 134, loss = 0.25330948\n",
      "Iteration 135, loss = 0.25365201\n",
      "Iteration 136, loss = 0.25391638\n",
      "Iteration 137, loss = 0.25335092\n",
      "Iteration 138, loss = 0.25356315\n",
      "Iteration 139, loss = 0.25362165\n",
      "Iteration 140, loss = 0.25321208\n",
      "Iteration 141, loss = 0.25326825\n",
      "Iteration 142, loss = 0.25327000\n",
      "Iteration 143, loss = 0.25373749\n",
      "Iteration 144, loss = 0.25332653\n",
      "Iteration 145, loss = 0.25318228\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34153053\n",
      "Iteration 1, loss = 0.34141961\n",
      "Iteration 2, loss = 0.29675106\n",
      "Iteration 2, loss = 0.29718776\n",
      "Iteration 3, loss = 0.28544182\n",
      "Iteration 3, loss = 0.28624478\n",
      "Iteration 4, loss = 0.28276629\n",
      "Iteration 4, loss = 0.28313205\n",
      "Iteration 5, loss = 0.28155624\n",
      "Iteration 5, loss = 0.28144830\n",
      "Iteration 6, loss = 0.28006069\n",
      "Iteration 6, loss = 0.28079251\n",
      "Iteration 7, loss = 0.27927827\n",
      "Iteration 7, loss = 0.27983545\n",
      "Iteration 8, loss = 0.27898784\n",
      "Iteration 8, loss = 0.27982338\n",
      "Iteration 9, loss = 0.27859776\n",
      "Iteration 9, loss = 0.27927441\n",
      "Iteration 10, loss = 0.27842632\n",
      "Iteration 10, loss = 0.27888935\n",
      "Iteration 11, loss = 0.27784522\n",
      "Iteration 11, loss = 0.27856224\n",
      "Iteration 12, loss = 0.27763428\n",
      "Iteration 12, loss = 0.27867154\n",
      "Iteration 13, loss = 0.27673294\n",
      "Iteration 13, loss = 0.27765586\n",
      "Iteration 14, loss = 0.27709767\n",
      "Iteration 14, loss = 0.27783349\n",
      "Iteration 15, loss = 0.27607407\n",
      "Iteration 15, loss = 0.27699794\n",
      "Iteration 16, loss = 0.27659161\n",
      "Iteration 16, loss = 0.27731110\n",
      "Iteration 17, loss = 0.27600476\n",
      "Iteration 17, loss = 0.27665755\n",
      "Iteration 18, loss = 0.27615080\n",
      "Iteration 19, loss = 0.27511897\n",
      "Iteration 18, loss = 0.27714665\n",
      "Iteration 20, loss = 0.27499546\n",
      "Iteration 19, loss = 0.27590312\n",
      "Iteration 21, loss = 0.27463968\n",
      "Iteration 20, loss = 0.27565756\n",
      "Iteration 22, loss = 0.27436305\n",
      "Iteration 21, loss = 0.27547933\n",
      "Iteration 23, loss = 0.27452006\n",
      "Iteration 22, loss = 0.27518098\n",
      "Iteration 24, loss = 0.27430264\n",
      "Iteration 23, loss = 0.27528944\n",
      "Iteration 24, loss = 0.27497072\n",
      "Iteration 25, loss = 0.27354578\n",
      "Iteration 25, loss = 0.27410594\n",
      "Iteration 26, loss = 0.27337730\n",
      "Iteration 26, loss = 0.27425380\n",
      "Iteration 27, loss = 0.27311826\n",
      "Iteration 27, loss = 0.27397829\n",
      "Iteration 28, loss = 0.27258290\n",
      "Iteration 28, loss = 0.27355685\n",
      "Iteration 29, loss = 0.27345023\n",
      "Iteration 29, loss = 0.27286324\n",
      "Iteration 30, loss = 0.27237804\n",
      "Iteration 30, loss = 0.27305673\n",
      "Iteration 31, loss = 0.27190038\n",
      "Iteration 31, loss = 0.27277126\n",
      "Iteration 32, loss = 0.27249911\n",
      "Iteration 32, loss = 0.27301715\n",
      "Iteration 33, loss = 0.27172791\n",
      "Iteration 33, loss = 0.27233639\n",
      "Iteration 34, loss = 0.27137596\n",
      "Iteration 34, loss = 0.27215101\n",
      "Iteration 35, loss = 0.27101833\n",
      "Iteration 35, loss = 0.27184726\n",
      "Iteration 36, loss = 0.27096105\n",
      "Iteration 36, loss = 0.27154903\n",
      "Iteration 37, loss = 0.27055375\n",
      "Iteration 37, loss = 0.27142500\n",
      "Iteration 38, loss = 0.27064013\n",
      "Iteration 38, loss = 0.27104163\n",
      "Iteration 39, loss = 0.27042256\n",
      "Iteration 39, loss = 0.27123971\n",
      "Iteration 40, loss = 0.27029150\n",
      "Iteration 40, loss = 0.27107695\n",
      "Iteration 41, loss = 0.26993829\n",
      "Iteration 41, loss = 0.27042153\n",
      "Iteration 42, loss = 0.26985246\n",
      "Iteration 42, loss = 0.26996904\n",
      "Iteration 43, loss = 0.26960389\n",
      "Iteration 43, loss = 0.26985977\n",
      "Iteration 44, loss = 0.26990737\n",
      "Iteration 44, loss = 0.26931907\n",
      "Iteration 45, loss = 0.26935464\n",
      "Iteration 45, loss = 0.26897259\n",
      "Iteration 46, loss = 0.26901033\n",
      "Iteration 46, loss = 0.26860279\n",
      "Iteration 47, loss = 0.26911857\n",
      "Iteration 47, loss = 0.26893225\n",
      "Iteration 48, loss = 0.26890467\n",
      "Iteration 48, loss = 0.26901307\n",
      "Iteration 49, loss = 0.26855544\n",
      "Iteration 50, loss = 0.26813856\n",
      "Iteration 49, loss = 0.26832844\n",
      "Iteration 51, loss = 0.26832585\n",
      "Iteration 50, loss = 0.26820576\n",
      "Iteration 52, loss = 0.26781717\n",
      "Iteration 51, loss = 0.26830263\n",
      "Iteration 53, loss = 0.26728895\n",
      "Iteration 52, loss = 0.26791653\n",
      "Iteration 54, loss = 0.26767416\n",
      "Iteration 53, loss = 0.26763197\n",
      "Iteration 55, loss = 0.26756170\n",
      "Iteration 54, loss = 0.26809156\n",
      "Iteration 56, loss = 0.26721200\n",
      "Iteration 55, loss = 0.26797572\n",
      "Iteration 57, loss = 0.26688103\n",
      "Iteration 56, loss = 0.26735190\n",
      "Iteration 58, loss = 0.26697734\n",
      "Iteration 57, loss = 0.26730691\n",
      "Iteration 59, loss = 0.26665255\n",
      "Iteration 58, loss = 0.26722216\n",
      "Iteration 60, loss = 0.26688477\n",
      "Iteration 59, loss = 0.26712879\n",
      "Iteration 61, loss = 0.26656411\n",
      "Iteration 60, loss = 0.26727166\n",
      "Iteration 62, loss = 0.26633685\n",
      "Iteration 61, loss = 0.26696018\n",
      "Iteration 63, loss = 0.26615896\n",
      "Iteration 62, loss = 0.26686876\n",
      "Iteration 64, loss = 0.26632056\n",
      "Iteration 63, loss = 0.26658171\n",
      "Iteration 65, loss = 0.26588999\n",
      "Iteration 64, loss = 0.26656510\n",
      "Iteration 66, loss = 0.26593034\n",
      "Iteration 65, loss = 0.26648464\n",
      "Iteration 67, loss = 0.26580621\n",
      "Iteration 66, loss = 0.26637551\n",
      "Iteration 68, loss = 0.26571778\n",
      "Iteration 67, loss = 0.26632683\n",
      "Iteration 69, loss = 0.26536674\n",
      "Iteration 68, loss = 0.26623836\n",
      "Iteration 70, loss = 0.26583525\n",
      "Iteration 71, loss = 0.26554502\n",
      "Iteration 69, loss = 0.26599465\n",
      "Iteration 72, loss = 0.26555653\n",
      "Iteration 70, loss = 0.26613129\n",
      "Iteration 73, loss = 0.26536690\n",
      "Iteration 71, loss = 0.26576034\n",
      "Iteration 74, loss = 0.26522765\n",
      "Iteration 72, loss = 0.26554735\n",
      "Iteration 75, loss = 0.26494940\n",
      "Iteration 73, loss = 0.26544431\n",
      "Iteration 76, loss = 0.26564783\n",
      "Iteration 74, loss = 0.26542950\n",
      "Iteration 77, loss = 0.26506414\n",
      "Iteration 75, loss = 0.26541272\n",
      "Iteration 78, loss = 0.26507038\n",
      "Iteration 76, loss = 0.26575716\n",
      "Iteration 79, loss = 0.26519531\n",
      "Iteration 80, loss = 0.26488600\n",
      "Iteration 77, loss = 0.26508097\n",
      "Iteration 81, loss = 0.26478450\n",
      "Iteration 78, loss = 0.26544984\n",
      "Iteration 82, loss = 0.26504302\n",
      "Iteration 79, loss = 0.26498624\n",
      "Iteration 83, loss = 0.26485675\n",
      "Iteration 80, loss = 0.26494193\n",
      "Iteration 84, loss = 0.26465062\n",
      "Iteration 85, loss = 0.26462075\n",
      "Iteration 81, loss = 0.26519723\n",
      "Iteration 86, loss = 0.26452999\n",
      "Iteration 82, loss = 0.26544883\n",
      "Iteration 87, loss = 0.26422762\n",
      "Iteration 83, loss = 0.26490794\n",
      "Iteration 84, loss = 0.26499586\n",
      "Iteration 88, loss = 0.26418318\n",
      "Iteration 85, loss = 0.26466830\n",
      "Iteration 89, loss = 0.26449718\n",
      "Iteration 86, loss = 0.26470741\n",
      "Iteration 90, loss = 0.26470933\n",
      "Iteration 87, loss = 0.26444046\n",
      "Iteration 91, loss = 0.26447681\n",
      "Iteration 88, loss = 0.26436266\n",
      "Iteration 92, loss = 0.26417555\n",
      "Iteration 89, loss = 0.26451360\n",
      "Iteration 93, loss = 0.26417752\n",
      "Iteration 90, loss = 0.26496507\n",
      "Iteration 94, loss = 0.26402191\n",
      "Iteration 95, loss = 0.26411763\n",
      "Iteration 91, loss = 0.26437908\n",
      "Iteration 92, loss = 0.26426726\n",
      "Iteration 96, loss = 0.26425874\n",
      "Iteration 93, loss = 0.26432608\n",
      "Iteration 94, loss = 0.26408422\n",
      "Iteration 97, loss = 0.26367930\n",
      "Iteration 95, loss = 0.26412275\n",
      "Iteration 98, loss = 0.26394892\n",
      "Iteration 96, loss = 0.26395257\n",
      "Iteration 99, loss = 0.26362234\n",
      "Iteration 97, loss = 0.26377070\n",
      "Iteration 100, loss = 0.26384099\n",
      "Iteration 98, loss = 0.26392616\n",
      "Iteration 101, loss = 0.26411775\n",
      "Iteration 99, loss = 0.26385679\n",
      "Iteration 102, loss = 0.26401471\n",
      "Iteration 100, loss = 0.26398336\n",
      "Iteration 103, loss = 0.26354775\n",
      "Iteration 101, loss = 0.26406227\n",
      "Iteration 104, loss = 0.26363191\n",
      "Iteration 102, loss = 0.26394121\n",
      "Iteration 105, loss = 0.26354516\n",
      "Iteration 103, loss = 0.26368522\n",
      "Iteration 106, loss = 0.26345508\n",
      "Iteration 104, loss = 0.26372373\n",
      "Iteration 107, loss = 0.26350359\n",
      "Iteration 105, loss = 0.26361247\n",
      "Iteration 108, loss = 0.26331247\n",
      "Iteration 106, loss = 0.26359207\n",
      "Iteration 109, loss = 0.26356334\n",
      "Iteration 107, loss = 0.26365592\n",
      "Iteration 110, loss = 0.26344250\n",
      "Iteration 108, loss = 0.26360528\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 111, loss = 0.26329131\n",
      "Iteration 112, loss = 0.26317211\n",
      "Iteration 113, loss = 0.26311156\n",
      "Iteration 1, loss = 0.34110313\n",
      "Iteration 114, loss = 0.26328886\n",
      "Iteration 2, loss = 0.29764658\n",
      "Iteration 115, loss = 0.26311886\n",
      "Iteration 3, loss = 0.28747810\n",
      "Iteration 116, loss = 0.26304516\n",
      "Iteration 4, loss = 0.28419911\n",
      "Iteration 117, loss = 0.26288415\n",
      "Iteration 5, loss = 0.28285832\n",
      "Iteration 6, loss = 0.28219432\n",
      "Iteration 118, loss = 0.26335638\n",
      "Iteration 7, loss = 0.28130556\n",
      "Iteration 119, loss = 0.26341856\n",
      "Iteration 8, loss = 0.28108602\n",
      "Iteration 120, loss = 0.26308376\n",
      "Iteration 9, loss = 0.28083444\n",
      "Iteration 121, loss = 0.26295809\n",
      "Iteration 122, loss = 0.26286020\n",
      "Iteration 10, loss = 0.28029574\n",
      "Iteration 123, loss = 0.26293362\n",
      "Iteration 11, loss = 0.27955587\n",
      "Iteration 124, loss = 0.26308591\n",
      "Iteration 12, loss = 0.27954510\n",
      "Iteration 125, loss = 0.26309303\n",
      "Iteration 13, loss = 0.27886473Iteration 126, loss = 0.26265070\n",
      "\n",
      "Iteration 14, loss = 0.27873835\n",
      "Iteration 127, loss = 0.26245684\n",
      "Iteration 128, loss = 0.26274006\n",
      "Iteration 15, loss = 0.27829009\n",
      "Iteration 129, loss = 0.26324004\n",
      "Iteration 16, loss = 0.27854262\n",
      "Iteration 130, loss = 0.26261420\n",
      "Iteration 131, loss = 0.26254822\n",
      "Iteration 17, loss = 0.27772569\n",
      "Iteration 132, loss = 0.26237638\n",
      "Iteration 18, loss = 0.27813548\n",
      "Iteration 133, loss = 0.26270143\n",
      "Iteration 19, loss = 0.27688410\n",
      "Iteration 134, loss = 0.26247334\n",
      "Iteration 20, loss = 0.27681044\n",
      "Iteration 135, loss = 0.26247596\n",
      "Iteration 21, loss = 0.27656265\n",
      "Iteration 136, loss = 0.26257226\n",
      "Iteration 22, loss = 0.27626570\n",
      "Iteration 137, loss = 0.26250706\n",
      "Iteration 23, loss = 0.27643550\n",
      "Iteration 138, loss = 0.26220305\n",
      "Iteration 24, loss = 0.27622199\n",
      "Iteration 139, loss = 0.26251011\n",
      "Iteration 25, loss = 0.27540834\n",
      "Iteration 26, loss = 0.27558397\n",
      "Iteration 140, loss = 0.26256134\n",
      "Iteration 27, loss = 0.27538233\n",
      "Iteration 141, loss = 0.26228347\n",
      "Iteration 142, loss = 0.26235938\n",
      "Iteration 28, loss = 0.27485731\n",
      "Iteration 143, loss = 0.26267220\n",
      "Iteration 29, loss = 0.27498329\n",
      "Iteration 144, loss = 0.26257845\n",
      "Iteration 30, loss = 0.27462375\n",
      "Iteration 145, loss = 0.26279978\n",
      "Iteration 31, loss = 0.27411088\n",
      "Iteration 146, loss = 0.26234358\n",
      "Iteration 32, loss = 0.27396454\n",
      "Iteration 147, loss = 0.26210975\n",
      "Iteration 33, loss = 0.27364882\n",
      "Iteration 148, loss = 0.26251607\n",
      "Iteration 34, loss = 0.27358958\n",
      "Iteration 149, loss = 0.26213200\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 35, loss = 0.27321729\n",
      "Iteration 36, loss = 0.27304610\n",
      "Iteration 1, loss = 0.34164414\n",
      "Iteration 37, loss = 0.27313132\n",
      "Iteration 2, loss = 0.29714029\n",
      "Iteration 38, loss = 0.27269202\n",
      "Iteration 39, loss = 0.27274485\n",
      "Iteration 3, loss = 0.28611947\n",
      "Iteration 40, loss = 0.27253434\n",
      "Iteration 4, loss = 0.28271475\n",
      "Iteration 41, loss = 0.27218030\n",
      "Iteration 5, loss = 0.28114347\n",
      "Iteration 6, loss = 0.28025410\n",
      "Iteration 42, loss = 0.27189086\n",
      "Iteration 7, loss = 0.27949615\n",
      "Iteration 43, loss = 0.27185199\n",
      "Iteration 8, loss = 0.27925185\n",
      "Iteration 44, loss = 0.27181789\n",
      "Iteration 9, loss = 0.27896415\n",
      "Iteration 45, loss = 0.27119375\n",
      "Iteration 10, loss = 0.27844284\n",
      "Iteration 46, loss = 0.27098656\n",
      "Iteration 11, loss = 0.27764211\n",
      "Iteration 47, loss = 0.27168108\n",
      "Iteration 12, loss = 0.27767076\n",
      "Iteration 48, loss = 0.27128129\n",
      "Iteration 13, loss = 0.27719113\n",
      "Iteration 49, loss = 0.27088430\n",
      "Iteration 14, loss = 0.27707249\n",
      "Iteration 50, loss = 0.27044307\n",
      "Iteration 15, loss = 0.27651972\n",
      "Iteration 51, loss = 0.27054395\n",
      "Iteration 16, loss = 0.27648552\n",
      "Iteration 52, loss = 0.27026240\n",
      "Iteration 17, loss = 0.27595099\n",
      "Iteration 53, loss = 0.26982573\n",
      "Iteration 18, loss = 0.27606181\n",
      "Iteration 19, loss = 0.27526754\n",
      "Iteration 54, loss = 0.26980734\n",
      "Iteration 20, loss = 0.27494650\n",
      "Iteration 55, loss = 0.26985663\n",
      "Iteration 21, loss = 0.27489397\n",
      "Iteration 56, loss = 0.26957670\n",
      "Iteration 57, loss = 0.26941853\n",
      "Iteration 22, loss = 0.27443748\n",
      "Iteration 23, loss = 0.27442128\n",
      "Iteration 58, loss = 0.26954465\n",
      "Iteration 59, loss = 0.26913359\n",
      "Iteration 24, loss = 0.27407725\n",
      "Iteration 60, loss = 0.26954138\n",
      "Iteration 25, loss = 0.27344739\n",
      "Iteration 61, loss = 0.26938720\n",
      "Iteration 26, loss = 0.27383742\n",
      "Iteration 62, loss = 0.26863230\n",
      "Iteration 27, loss = 0.27368278\n",
      "Iteration 63, loss = 0.26860961\n",
      "Iteration 28, loss = 0.27296534\n",
      "Iteration 64, loss = 0.26852169\n",
      "Iteration 29, loss = 0.27284670\n",
      "Iteration 30, loss = 0.27259213\n",
      "Iteration 65, loss = 0.26815560\n",
      "Iteration 31, loss = 0.27198712\n",
      "Iteration 66, loss = 0.26842158\n",
      "Iteration 32, loss = 0.27187393\n",
      "Iteration 67, loss = 0.26839769\n",
      "Iteration 33, loss = 0.27207050\n",
      "Iteration 68, loss = 0.26837452\n",
      "Iteration 69, loss = 0.26771440\n",
      "Iteration 34, loss = 0.27178430\n",
      "Iteration 35, loss = 0.27120985\n",
      "Iteration 70, loss = 0.26805414\n",
      "Iteration 36, loss = 0.27103011\n",
      "Iteration 37, loss = 0.27097300\n",
      "Iteration 71, loss = 0.26772104\n",
      "Iteration 38, loss = 0.27056587\n",
      "Iteration 72, loss = 0.26771426\n",
      "Iteration 39, loss = 0.27111663\n",
      "Iteration 73, loss = 0.26789736\n",
      "Iteration 40, loss = 0.27069660\n",
      "Iteration 74, loss = 0.26776926\n",
      "Iteration 41, loss = 0.27016893\n",
      "Iteration 75, loss = 0.26722146\n",
      "Iteration 42, loss = 0.26998039\n",
      "Iteration 76, loss = 0.26788965\n",
      "Iteration 43, loss = 0.26969269\n",
      "Iteration 77, loss = 0.26733664\n",
      "Iteration 44, loss = 0.26990508\n",
      "Iteration 78, loss = 0.26742197\n",
      "Iteration 45, loss = 0.26927801\n",
      "Iteration 46, loss = 0.26923162\n",
      "Iteration 79, loss = 0.26734680\n",
      "Iteration 47, loss = 0.26953579\n",
      "Iteration 48, loss = 0.26914729\n",
      "Iteration 49, loss = 0.26881436\n",
      "Iteration 80, loss = 0.26740062\n",
      "Iteration 50, loss = 0.26869181\n",
      "Iteration 81, loss = 0.26736468\n",
      "Iteration 51, loss = 0.26864212\n",
      "Iteration 82, loss = 0.26709985\n",
      "Iteration 52, loss = 0.26855306\n",
      "Iteration 83, loss = 0.26754124\n",
      "Iteration 53, loss = 0.26804261\n",
      "Iteration 84, loss = 0.26722728\n",
      "Iteration 54, loss = 0.26797763\n",
      "Iteration 85, loss = 0.26704403\n",
      "Iteration 55, loss = 0.26802561\n",
      "Iteration 86, loss = 0.26712292\n",
      "Iteration 56, loss = 0.26784607\n",
      "Iteration 87, loss = 0.26655444\n",
      "Iteration 57, loss = 0.26788908\n",
      "Iteration 88, loss = 0.26638069\n",
      "Iteration 58, loss = 0.26790950\n",
      "Iteration 59, loss = 0.26751570\n",
      "Iteration 89, loss = 0.26668067\n",
      "Iteration 60, loss = 0.26786744\n",
      "Iteration 90, loss = 0.26705966\n",
      "Iteration 61, loss = 0.26763999\n",
      "Iteration 91, loss = 0.26689309\n",
      "Iteration 62, loss = 0.26716663\n",
      "Iteration 92, loss = 0.26637005\n",
      "Iteration 63, loss = 0.26695128\n",
      "Iteration 93, loss = 0.26661648\n",
      "Iteration 64, loss = 0.26698101\n",
      "Iteration 94, loss = 0.26650203\n",
      "Iteration 65, loss = 0.26706474\n",
      "Iteration 95, loss = 0.26615775\n",
      "Iteration 66, loss = 0.26706737\n",
      "Iteration 96, loss = 0.26639475\n",
      "Iteration 67, loss = 0.26671312\n",
      "Iteration 97, loss = 0.26620173\n",
      "Iteration 68, loss = 0.26685025\n",
      "Iteration 98, loss = 0.26614440\n",
      "Iteration 69, loss = 0.26633324\n",
      "Iteration 70, loss = 0.26675398\n",
      "Iteration 99, loss = 0.26587176\n",
      "Iteration 100, loss = 0.26619056\n",
      "Iteration 71, loss = 0.26638496\n",
      "Iteration 101, loss = 0.26650551\n",
      "Iteration 72, loss = 0.26655541\n",
      "Iteration 102, loss = 0.26591924\n",
      "Iteration 103, loss = 0.26557073\n",
      "Iteration 73, loss = 0.26667395\n",
      "Iteration 74, loss = 0.26671747\n",
      "Iteration 104, loss = 0.26575685\n",
      "Iteration 75, loss = 0.26631065\n",
      "Iteration 105, loss = 0.26581705\n",
      "Iteration 76, loss = 0.26639313\n",
      "Iteration 106, loss = 0.26573324\n",
      "Iteration 77, loss = 0.26639675\n",
      "Iteration 107, loss = 0.26577010\n",
      "Iteration 78, loss = 0.26615037\n",
      "Iteration 108, loss = 0.26573142\n",
      "Iteration 79, loss = 0.26599015\n",
      "Iteration 109, loss = 0.26592645\n",
      "Iteration 110, loss = 0.26549221Iteration 80, loss = 0.26621140\n",
      "\n",
      "Iteration 111, loss = 0.26568141\n",
      "Iteration 81, loss = 0.26621692\n",
      "Iteration 112, loss = 0.26563382\n",
      "Iteration 82, loss = 0.26597264\n",
      "Iteration 113, loss = 0.26517830\n",
      "Iteration 114, loss = 0.26533042\n",
      "Iteration 83, loss = 0.26625135\n",
      "Iteration 115, loss = 0.26531975\n",
      "Iteration 84, loss = 0.26609887\n",
      "Iteration 116, loss = 0.26521445\n",
      "Iteration 85, loss = 0.26591199\n",
      "Iteration 117, loss = 0.26486241\n",
      "Iteration 86, loss = 0.26566325\n",
      "Iteration 87, loss = 0.26546694\n",
      "Iteration 118, loss = 0.26549168\n",
      "Iteration 88, loss = 0.26536772\n",
      "Iteration 119, loss = 0.26547004\n",
      "Iteration 120, loss = 0.26511915\n",
      "Iteration 89, loss = 0.26567230\n",
      "Iteration 121, loss = 0.26526856\n",
      "Iteration 90, loss = 0.26572074\n",
      "Iteration 122, loss = 0.26497525\n",
      "Iteration 91, loss = 0.26576707\n",
      "Iteration 123, loss = 0.26521125\n",
      "Iteration 92, loss = 0.26518460\n",
      "Iteration 93, loss = 0.26547249\n",
      "Iteration 124, loss = 0.26500343\n",
      "Iteration 94, loss = 0.26564146\n",
      "Iteration 125, loss = 0.26510866\n",
      "Iteration 95, loss = 0.26522061\n",
      "Iteration 126, loss = 0.26485623\n",
      "Iteration 96, loss = 0.26510652\n",
      "Iteration 127, loss = 0.26469497\n",
      "Iteration 128, loss = 0.26492873\n",
      "Iteration 97, loss = 0.26496187\n",
      "Iteration 129, loss = 0.26513898\n",
      "Iteration 98, loss = 0.26485684\n",
      "Iteration 130, loss = 0.26459651\n",
      "Iteration 99, loss = 0.26479376\n",
      "Iteration 131, loss = 0.26455604\n",
      "Iteration 100, loss = 0.26508896\n",
      "Iteration 132, loss = 0.26460731\n",
      "Iteration 101, loss = 0.26508017\n",
      "Iteration 133, loss = 0.26451546\n",
      "Iteration 102, loss = 0.26483758\n",
      "Iteration 134, loss = 0.26449151\n",
      "Iteration 103, loss = 0.26459238\n",
      "Iteration 135, loss = 0.26458184\n",
      "Iteration 104, loss = 0.26474939\n",
      "Iteration 136, loss = 0.26452556\n",
      "Iteration 105, loss = 0.26485879\n",
      "Iteration 137, loss = 0.26473136\n",
      "Iteration 106, loss = 0.26467059\n",
      "Iteration 138, loss = 0.26445403\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 107, loss = 0.26451032\n",
      "Iteration 108, loss = 0.26462171\n",
      "Iteration 1, loss = 0.34168760\n",
      "Iteration 109, loss = 0.26484670\n",
      "Iteration 2, loss = 0.29762079\n",
      "Iteration 110, loss = 0.26458965\n",
      "Iteration 3, loss = 0.28675447\n",
      "Iteration 111, loss = 0.26445117\n",
      "Iteration 4, loss = 0.28362492\n",
      "Iteration 112, loss = 0.26433999\n",
      "Iteration 5, loss = 0.28246052\n",
      "Iteration 113, loss = 0.26438249\n",
      "Iteration 6, loss = 0.28139802\n",
      "Iteration 114, loss = 0.26441934\n",
      "Iteration 7, loss = 0.28064659\n",
      "Iteration 115, loss = 0.26428185\n",
      "Iteration 8, loss = 0.28016151\n",
      "Iteration 116, loss = 0.26415411\n",
      "Iteration 9, loss = 0.27979449\n",
      "Iteration 117, loss = 0.26398894\n",
      "Iteration 10, loss = 0.27934776Iteration 118, loss = 0.26401570\n",
      "\n",
      "Iteration 11, loss = 0.27846685\n",
      "Iteration 119, loss = 0.26413340\n",
      "Iteration 12, loss = 0.27836902\n",
      "Iteration 120, loss = 0.26388451\n",
      "Iteration 13, loss = 0.27785261\n",
      "Iteration 121, loss = 0.26390909\n",
      "Iteration 122, loss = 0.26414285\n",
      "Iteration 14, loss = 0.27762490\n",
      "Iteration 15, loss = 0.27720623\n",
      "Iteration 123, loss = 0.26395252\n",
      "Iteration 16, loss = 0.27696004\n",
      "Iteration 124, loss = 0.26411317\n",
      "Iteration 17, loss = 0.27659193\n",
      "Iteration 125, loss = 0.26411812\n",
      "Iteration 18, loss = 0.27631896\n",
      "Iteration 126, loss = 0.26374014\n",
      "Iteration 19, loss = 0.27578428\n",
      "Iteration 127, loss = 0.26347871\n",
      "Iteration 20, loss = 0.27531266\n",
      "Iteration 128, loss = 0.26374177\n",
      "Iteration 129, loss = 0.26390304\n",
      "Iteration 21, loss = 0.27520095\n",
      "Iteration 22, loss = 0.27476375\n",
      "Iteration 130, loss = 0.26377695\n",
      "Iteration 23, loss = 0.27476088\n",
      "Iteration 131, loss = 0.26379559\n",
      "Iteration 24, loss = 0.27430016\n",
      "Iteration 132, loss = 0.26371189\n",
      "Iteration 25, loss = 0.27356041\n",
      "Iteration 133, loss = 0.26354599\n",
      "Iteration 26, loss = 0.27406443\n",
      "Iteration 134, loss = 0.26362530\n",
      "Iteration 27, loss = 0.27404018\n",
      "Iteration 135, loss = 0.26366177\n",
      "Iteration 28, loss = 0.27330109\n",
      "Iteration 136, loss = 0.26348617\n",
      "Iteration 29, loss = 0.27315708\n",
      "Iteration 137, loss = 0.26373152\n",
      "Iteration 30, loss = 0.27274920\n",
      "Iteration 138, loss = 0.26342398\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 31, loss = 0.27208481\n",
      "Iteration 32, loss = 0.27211705\n",
      "Iteration 1, loss = 0.34139193\n",
      "Iteration 33, loss = 0.27198612\n",
      "Iteration 2, loss = 0.29770263\n",
      "Iteration 3, loss = 0.28691486\n",
      "Iteration 34, loss = 0.27203142\n",
      "Iteration 4, loss = 0.28393570\n",
      "Iteration 35, loss = 0.27116695\n",
      "Iteration 5, loss = 0.28274683\n",
      "Iteration 36, loss = 0.27113536\n",
      "Iteration 6, loss = 0.28162090\n",
      "Iteration 37, loss = 0.27119757\n",
      "Iteration 7, loss = 0.28112998\n",
      "Iteration 38, loss = 0.27034714\n",
      "Iteration 8, loss = 0.28057110\n",
      "Iteration 39, loss = 0.27102431\n",
      "Iteration 9, loss = 0.28018825\n",
      "Iteration 40, loss = 0.27026165\n",
      "Iteration 10, loss = 0.27992862\n",
      "Iteration 41, loss = 0.27001447\n",
      "Iteration 11, loss = 0.27918999\n",
      "Iteration 42, loss = 0.26981718\n",
      "Iteration 12, loss = 0.27888390\n",
      "Iteration 43, loss = 0.26972004\n",
      "Iteration 13, loss = 0.27876260\n",
      "Iteration 44, loss = 0.26993845\n",
      "Iteration 14, loss = 0.27839484\n",
      "Iteration 45, loss = 0.26900508\n",
      "Iteration 15, loss = 0.27784875\n",
      "Iteration 46, loss = 0.26893376\n",
      "Iteration 16, loss = 0.27784972\n",
      "Iteration 47, loss = 0.26886851\n",
      "Iteration 17, loss = 0.27770632\n",
      "Iteration 48, loss = 0.26909215\n",
      "Iteration 18, loss = 0.27716612\n",
      "Iteration 19, loss = 0.27704056\n",
      "Iteration 49, loss = 0.26845155\n",
      "Iteration 20, loss = 0.27642139\n",
      "Iteration 50, loss = 0.26830981\n",
      "Iteration 21, loss = 0.27637719\n",
      "Iteration 51, loss = 0.26806598\n",
      "Iteration 22, loss = 0.27600099\n",
      "Iteration 52, loss = 0.26795552\n",
      "Iteration 23, loss = 0.27569297\n",
      "Iteration 24, loss = 0.27542435\n",
      "Iteration 53, loss = 0.26785524\n",
      "Iteration 25, loss = 0.27495516\n",
      "Iteration 54, loss = 0.26755499\n",
      "Iteration 26, loss = 0.27538031\n",
      "Iteration 55, loss = 0.26745030\n",
      "Iteration 27, loss = 0.27483405\n",
      "Iteration 56, loss = 0.26752754\n",
      "Iteration 28, loss = 0.27487505\n",
      "Iteration 57, loss = 0.26731810\n",
      "Iteration 58, loss = 0.26705224\n",
      "Iteration 29, loss = 0.27444215\n",
      "Iteration 59, loss = 0.26689287\n",
      "Iteration 60, loss = 0.26751445\n",
      "Iteration 30, loss = 0.27421618\n",
      "Iteration 61, loss = 0.26737176\n",
      "Iteration 31, loss = 0.27352065\n",
      "Iteration 62, loss = 0.26644373\n",
      "Iteration 32, loss = 0.27364555\n",
      "Iteration 63, loss = 0.26620545\n",
      "Iteration 33, loss = 0.27350558\n",
      "Iteration 64, loss = 0.26631986\n",
      "Iteration 34, loss = 0.27335057\n",
      "Iteration 65, loss = 0.26627171\n",
      "Iteration 35, loss = 0.27275383\n",
      "Iteration 66, loss = 0.26609250\n",
      "Iteration 36, loss = 0.27279818\n",
      "Iteration 67, loss = 0.26597903\n",
      "Iteration 37, loss = 0.27240478\n",
      "Iteration 68, loss = 0.26633612\n",
      "Iteration 38, loss = 0.27170772\n",
      "Iteration 69, loss = 0.26561882\n",
      "Iteration 39, loss = 0.27274697\n",
      "Iteration 70, loss = 0.26566252\n",
      "Iteration 40, loss = 0.27201909\n",
      "Iteration 71, loss = 0.26563542\n",
      "Iteration 41, loss = 0.27153174\n",
      "Iteration 42, loss = 0.27165835\n",
      "Iteration 72, loss = 0.26563364\n",
      "Iteration 43, loss = 0.27115788\n",
      "Iteration 73, loss = 0.26578861\n",
      "Iteration 44, loss = 0.27103823\n",
      "Iteration 74, loss = 0.26557504\n",
      "Iteration 45, loss = 0.27029107\n",
      "Iteration 75, loss = 0.26514500\n",
      "Iteration 76, loss = 0.26516205\n",
      "Iteration 46, loss = 0.27043825\n",
      "Iteration 77, loss = 0.26529765\n",
      "Iteration 47, loss = 0.26984019\n",
      "Iteration 78, loss = 0.26496171\n",
      "Iteration 79, loss = 0.26516426\n",
      "Iteration 48, loss = 0.27019445\n",
      "Iteration 80, loss = 0.26496033\n",
      "Iteration 49, loss = 0.26975661\n",
      "Iteration 81, loss = 0.26484017\n",
      "Iteration 50, loss = 0.26948840\n",
      "Iteration 82, loss = 0.26485026\n",
      "Iteration 51, loss = 0.26950851\n",
      "Iteration 52, loss = 0.26907691\n",
      "Iteration 83, loss = 0.26492073\n",
      "Iteration 53, loss = 0.26876550\n",
      "Iteration 84, loss = 0.26484844\n",
      "Iteration 54, loss = 0.26868794\n",
      "Iteration 85, loss = 0.26450646\n",
      "Iteration 55, loss = 0.26888026\n",
      "Iteration 56, loss = 0.26883844\n",
      "Iteration 86, loss = 0.26479269\n",
      "Iteration 57, loss = 0.26857248\n",
      "Iteration 87, loss = 0.26447235\n",
      "Iteration 58, loss = 0.26820701\n",
      "Iteration 88, loss = 0.26426461\n",
      "Iteration 59, loss = 0.26785308\n",
      "Iteration 89, loss = 0.26454355\n",
      "Iteration 90, loss = 0.26434785\n",
      "Iteration 60, loss = 0.26828734\n",
      "Iteration 91, loss = 0.26435354\n",
      "Iteration 61, loss = 0.26836121\n",
      "Iteration 92, loss = 0.26405982\n",
      "Iteration 62, loss = 0.26773819\n",
      "Iteration 93, loss = 0.26400397\n",
      "Iteration 63, loss = 0.26751623\n",
      "Iteration 94, loss = 0.26422315\n",
      "Iteration 64, loss = 0.26754486\n",
      "Iteration 95, loss = 0.26374805\n",
      "Iteration 65, loss = 0.26726469\n",
      "Iteration 96, loss = 0.26413446\n",
      "Iteration 66, loss = 0.26724421\n",
      "Iteration 97, loss = 0.26364541\n",
      "Iteration 67, loss = 0.26726731\n",
      "Iteration 98, loss = 0.26376728\n",
      "Iteration 68, loss = 0.26744040\n",
      "Iteration 99, loss = 0.26345697\n",
      "Iteration 69, loss = 0.26694756\n",
      "Iteration 100, loss = 0.26381060\n",
      "Iteration 70, loss = 0.26709206\n",
      "Iteration 101, loss = 0.26346995\n",
      "Iteration 102, loss = 0.26350007\n",
      "Iteration 71, loss = 0.26692453\n",
      "Iteration 103, loss = 0.26332069\n",
      "Iteration 72, loss = 0.26684176\n",
      "Iteration 104, loss = 0.26332241\n",
      "Iteration 73, loss = 0.26748484\n",
      "Iteration 105, loss = 0.26344243\n",
      "Iteration 74, loss = 0.26690041\n",
      "Iteration 106, loss = 0.26334062\n",
      "Iteration 75, loss = 0.26627304\n",
      "Iteration 107, loss = 0.26322065\n",
      "Iteration 76, loss = 0.26645920\n",
      "Iteration 108, loss = 0.26321447\n",
      "Iteration 77, loss = 0.26639272\n",
      "Iteration 109, loss = 0.26309565\n",
      "Iteration 78, loss = 0.26621699\n",
      "Iteration 110, loss = 0.26301634\n",
      "Iteration 79, loss = 0.26626868\n",
      "Iteration 111, loss = 0.26291874\n",
      "Iteration 80, loss = 0.26648725\n",
      "Iteration 112, loss = 0.26292510\n",
      "Iteration 81, loss = 0.26602854\n",
      "Iteration 113, loss = 0.26311420\n",
      "Iteration 82, loss = 0.26594101\n",
      "Iteration 114, loss = 0.26287730\n",
      "Iteration 83, loss = 0.26595866\n",
      "Iteration 115, loss = 0.26304700\n",
      "Iteration 84, loss = 0.26580448\n",
      "Iteration 116, loss = 0.26280733\n",
      "Iteration 85, loss = 0.26573037\n",
      "Iteration 117, loss = 0.26251853\n",
      "Iteration 86, loss = 0.26558830\n",
      "Iteration 87, loss = 0.26584070\n",
      "Iteration 118, loss = 0.26280935\n",
      "Iteration 88, loss = 0.26549526\n",
      "Iteration 119, loss = 0.26287829\n",
      "Iteration 89, loss = 0.26556561\n",
      "Iteration 120, loss = 0.26265225\n",
      "Iteration 90, loss = 0.26519524\n",
      "Iteration 121, loss = 0.26264039\n",
      "Iteration 91, loss = 0.26574664\n",
      "Iteration 122, loss = 0.26278987\n",
      "Iteration 92, loss = 0.26526890\n",
      "Iteration 123, loss = 0.26244762\n",
      "Iteration 93, loss = 0.26527332\n",
      "Iteration 124, loss = 0.26255714\n",
      "Iteration 94, loss = 0.26582091\n",
      "Iteration 125, loss = 0.26275312\n",
      "Iteration 95, loss = 0.26509878\n",
      "Iteration 126, loss = 0.26252900\n",
      "Iteration 96, loss = 0.26486184\n",
      "Iteration 127, loss = 0.26254453\n",
      "Iteration 97, loss = 0.26500471\n",
      "Iteration 128, loss = 0.26229775\n",
      "Iteration 98, loss = 0.26489688\n",
      "Iteration 129, loss = 0.26244102\n",
      "Iteration 99, loss = 0.26462035\n",
      "Iteration 130, loss = 0.26233915\n",
      "Iteration 100, loss = 0.26494080\n",
      "Iteration 131, loss = 0.26246642\n",
      "Iteration 101, loss = 0.26477965\n",
      "Iteration 132, loss = 0.26239446\n",
      "Iteration 102, loss = 0.26458284\n",
      "Iteration 133, loss = 0.26232865\n",
      "Iteration 103, loss = 0.26445376\n",
      "Iteration 134, loss = 0.26221342\n",
      "Iteration 104, loss = 0.26474264\n",
      "Iteration 105, loss = 0.26467829\n",
      "Iteration 135, loss = 0.26230845\n",
      "Iteration 106, loss = 0.26453310\n",
      "Iteration 136, loss = 0.26234285\n",
      "Iteration 107, loss = 0.26480192\n",
      "Iteration 137, loss = 0.26227554\n",
      "Iteration 108, loss = 0.26456669\n",
      "Iteration 138, loss = 0.26221762\n",
      "Iteration 109, loss = 0.26456059\n",
      "Iteration 139, loss = 0.26215682\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 110, loss = 0.26412805\n",
      "Iteration 111, loss = 0.26437999\n",
      "Iteration 1, loss = 0.34201335\n",
      "Iteration 112, loss = 0.26415651\n",
      "Iteration 2, loss = 0.29803835\n",
      "Iteration 113, loss = 0.26422951\n",
      "Iteration 3, loss = 0.28704948\n",
      "Iteration 114, loss = 0.26439037\n",
      "Iteration 4, loss = 0.28400953\n",
      "Iteration 115, loss = 0.26429361\n",
      "Iteration 5, loss = 0.28251772\n",
      "Iteration 6, loss = 0.28152917Iteration 116, loss = 0.26421012\n",
      "\n",
      "Iteration 7, loss = 0.28093000Iteration 117, loss = 0.26400253\n",
      "\n",
      "Iteration 8, loss = 0.28053700\n",
      "Iteration 9, loss = 0.28022674\n",
      "Iteration 118, loss = 0.26456704\n",
      "Iteration 119, loss = 0.26406852\n",
      "Iteration 10, loss = 0.27957187\n",
      "Iteration 120, loss = 0.26390087\n",
      "Iteration 11, loss = 0.27911024\n",
      "Iteration 121, loss = 0.26393627\n",
      "Iteration 12, loss = 0.27872911\n",
      "Iteration 122, loss = 0.26412896\n",
      "Iteration 13, loss = 0.27879318\n",
      "Iteration 123, loss = 0.26389697\n",
      "Iteration 14, loss = 0.27797298\n",
      "Iteration 124, loss = 0.26439420\n",
      "Iteration 15, loss = 0.27762201\n",
      "Iteration 125, loss = 0.26421501\n",
      "Iteration 16, loss = 0.27750533\n",
      "Iteration 126, loss = 0.26394496\n",
      "Iteration 17, loss = 0.27754846\n",
      "Iteration 127, loss = 0.26396909\n",
      "Iteration 18, loss = 0.27702973\n",
      "Iteration 128, loss = 0.26393845\n",
      "Iteration 19, loss = 0.27677809\n",
      "Iteration 129, loss = 0.26373718\n",
      "Iteration 20, loss = 0.27664030\n",
      "Iteration 21, loss = 0.27653356\n",
      "Iteration 130, loss = 0.26381034\n",
      "Iteration 131, loss = 0.26392189\n",
      "Iteration 22, loss = 0.27611581\n",
      "Iteration 132, loss = 0.26443296\n",
      "Iteration 23, loss = 0.27568005\n",
      "Iteration 133, loss = 0.26348694\n",
      "Iteration 24, loss = 0.27545506\n",
      "Iteration 134, loss = 0.26370401Iteration 25, loss = 0.27495109\n",
      "\n",
      "Iteration 26, loss = 0.27531662\n",
      "Iteration 135, loss = 0.26380024\n",
      "Iteration 27, loss = 0.27471289\n",
      "Iteration 136, loss = 0.26373149\n",
      "Iteration 28, loss = 0.27468004\n",
      "Iteration 137, loss = 0.26358653\n",
      "Iteration 29, loss = 0.27422551\n",
      "Iteration 138, loss = 0.26378014\n",
      "Iteration 30, loss = 0.27397925\n",
      "Iteration 139, loss = 0.26372918\n",
      "Iteration 31, loss = 0.27334677\n",
      "Iteration 140, loss = 0.26359702\n",
      "Iteration 32, loss = 0.27321945\n",
      "Iteration 141, loss = 0.26341581\n",
      "Iteration 33, loss = 0.27305445\n",
      "Iteration 142, loss = 0.26385975\n",
      "Iteration 34, loss = 0.27292573\n",
      "Iteration 143, loss = 0.26362792\n",
      "Iteration 35, loss = 0.27244100\n",
      "Iteration 144, loss = 0.26350749\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 36, loss = 0.27229926\n",
      "Iteration 1, loss = 0.34129519\n",
      "Iteration 37, loss = 0.27195585\n",
      "Iteration 2, loss = 0.29838587\n",
      "Iteration 38, loss = 0.27124930\n",
      "Iteration 3, loss = 0.28833872\n",
      "Iteration 4, loss = 0.28535753\n",
      "Iteration 39, loss = 0.27166157\n",
      "Iteration 5, loss = 0.28367326\n",
      "Iteration 40, loss = 0.27116012\n",
      "Iteration 6, loss = 0.28274895\n",
      "Iteration 41, loss = 0.27091104\n",
      "Iteration 7, loss = 0.28236657\n",
      "Iteration 8, loss = 0.28219832\n",
      "Iteration 42, loss = 0.27076366\n",
      "Iteration 9, loss = 0.28131502\n",
      "Iteration 43, loss = 0.27089457\n",
      "Iteration 10, loss = 0.28086975\n",
      "Iteration 44, loss = 0.27044529\n",
      "Iteration 11, loss = 0.28097924\n",
      "Iteration 45, loss = 0.26998672\n",
      "Iteration 12, loss = 0.28021267\n",
      "Iteration 46, loss = 0.26988849\n",
      "Iteration 13, loss = 0.27980062\n",
      "Iteration 47, loss = 0.26959963\n",
      "Iteration 14, loss = 0.27926978\n",
      "Iteration 48, loss = 0.26957424\n",
      "Iteration 15, loss = 0.27915150\n",
      "Iteration 16, loss = 0.27886597\n",
      "Iteration 49, loss = 0.26943298\n",
      "Iteration 17, loss = 0.27827892\n",
      "Iteration 50, loss = 0.26936096\n",
      "Iteration 51, loss = 0.26941543\n",
      "Iteration 18, loss = 0.27813463\n",
      "Iteration 19, loss = 0.27785602\n",
      "Iteration 52, loss = 0.26896127\n",
      "Iteration 20, loss = 0.27764113\n",
      "Iteration 53, loss = 0.26896154\n",
      "Iteration 21, loss = 0.27759593\n",
      "Iteration 54, loss = 0.26864235\n",
      "Iteration 22, loss = 0.27711785\n",
      "Iteration 55, loss = 0.26934554\n",
      "Iteration 23, loss = 0.27687803\n",
      "Iteration 56, loss = 0.26897653\n",
      "Iteration 24, loss = 0.27692992\n",
      "Iteration 57, loss = 0.26851957\n",
      "Iteration 25, loss = 0.27669987\n",
      "Iteration 58, loss = 0.26828114\n",
      "Iteration 26, loss = 0.27641554\n",
      "Iteration 59, loss = 0.26819329\n",
      "Iteration 27, loss = 0.27608416\n",
      "Iteration 60, loss = 0.26844765\n",
      "Iteration 28, loss = 0.27595995\n",
      "Iteration 29, loss = 0.27583722\n",
      "Iteration 61, loss = 0.26845548\n",
      "Iteration 30, loss = 0.27550179\n",
      "Iteration 62, loss = 0.26800251\n",
      "Iteration 31, loss = 0.27550654\n",
      "Iteration 63, loss = 0.26789601\n",
      "Iteration 32, loss = 0.27523775\n",
      "Iteration 64, loss = 0.26800188\n",
      "Iteration 33, loss = 0.27510188\n",
      "Iteration 65, loss = 0.26765658\n",
      "Iteration 34, loss = 0.27528164\n",
      "Iteration 66, loss = 0.26750718\n",
      "Iteration 35, loss = 0.27459800\n",
      "Iteration 67, loss = 0.26749161\n",
      "Iteration 36, loss = 0.27415974\n",
      "Iteration 68, loss = 0.26742491\n",
      "Iteration 37, loss = 0.27424276\n",
      "Iteration 69, loss = 0.26724032\n",
      "Iteration 38, loss = 0.27428360\n",
      "Iteration 70, loss = 0.26731400\n",
      "Iteration 39, loss = 0.27417939\n",
      "Iteration 71, loss = 0.26720714\n",
      "Iteration 40, loss = 0.27351474\n",
      "Iteration 72, loss = 0.26690343\n",
      "Iteration 41, loss = 0.27328688\n",
      "Iteration 73, loss = 0.26788106\n",
      "Iteration 42, loss = 0.27295047\n",
      "Iteration 74, loss = 0.26702155\n",
      "Iteration 43, loss = 0.27264915\n",
      "Iteration 75, loss = 0.26646912\n",
      "Iteration 44, loss = 0.27292444\n",
      "Iteration 76, loss = 0.26642823\n",
      "Iteration 45, loss = 0.27250435\n",
      "Iteration 77, loss = 0.26642563\n",
      "Iteration 46, loss = 0.27272087\n",
      "Iteration 78, loss = 0.26633532\n",
      "Iteration 47, loss = 0.27213451\n",
      "Iteration 79, loss = 0.26632414\n",
      "Iteration 48, loss = 0.27211571\n",
      "Iteration 49, loss = 0.27179809\n",
      "Iteration 80, loss = 0.26608761\n",
      "Iteration 50, loss = 0.27158175\n",
      "Iteration 81, loss = 0.26595031\n",
      "Iteration 51, loss = 0.27121027\n",
      "Iteration 82, loss = 0.26598166\n",
      "Iteration 52, loss = 0.27117239\n",
      "Iteration 83, loss = 0.26599159\n",
      "Iteration 53, loss = 0.27103095\n",
      "Iteration 84, loss = 0.26578329\n",
      "Iteration 54, loss = 0.27097811\n",
      "Iteration 85, loss = 0.26560282\n",
      "Iteration 55, loss = 0.27062768\n",
      "Iteration 86, loss = 0.26551175\n",
      "Iteration 56, loss = 0.27021666\n",
      "Iteration 87, loss = 0.26561211\n",
      "Iteration 57, loss = 0.27023503\n",
      "Iteration 88, loss = 0.26563212\n",
      "Iteration 58, loss = 0.27043214\n",
      "Iteration 89, loss = 0.26581015\n",
      "Iteration 59, loss = 0.26987315\n",
      "Iteration 90, loss = 0.26533378\n",
      "Iteration 60, loss = 0.27007209\n",
      "Iteration 61, loss = 0.26958636\n",
      "Iteration 62, loss = 0.26956356\n",
      "Iteration 91, loss = 0.26542343\n",
      "Iteration 63, loss = 0.26941182\n",
      "Iteration 92, loss = 0.26489089\n",
      "Iteration 64, loss = 0.26954693\n",
      "Iteration 93, loss = 0.26492142\n",
      "Iteration 65, loss = 0.26915230\n",
      "Iteration 94, loss = 0.26564706\n",
      "Iteration 66, loss = 0.26880538\n",
      "Iteration 67, loss = 0.26894266\n",
      "Iteration 95, loss = 0.26483479\n",
      "Iteration 68, loss = 0.26850430\n",
      "Iteration 96, loss = 0.26466980\n",
      "Iteration 69, loss = 0.26875415\n",
      "Iteration 97, loss = 0.26463334\n",
      "Iteration 70, loss = 0.26840949\n",
      "Iteration 98, loss = 0.26476969\n",
      "Iteration 99, loss = 0.26417345\n",
      "Iteration 71, loss = 0.26842132\n",
      "Iteration 100, loss = 0.26461435\n",
      "Iteration 72, loss = 0.26825653\n",
      "Iteration 73, loss = 0.26928766\n",
      "Iteration 101, loss = 0.26397944\n",
      "Iteration 74, loss = 0.26788351\n",
      "Iteration 102, loss = 0.26414475\n",
      "Iteration 75, loss = 0.26867430\n",
      "Iteration 103, loss = 0.26384811\n",
      "Iteration 76, loss = 0.26775397\n",
      "Iteration 104, loss = 0.26397783\n",
      "Iteration 77, loss = 0.26777411\n",
      "Iteration 105, loss = 0.26363486\n",
      "Iteration 78, loss = 0.26763745\n",
      "Iteration 106, loss = 0.26367101\n",
      "Iteration 79, loss = 0.26795344\n",
      "Iteration 107, loss = 0.26354851\n",
      "Iteration 80, loss = 0.26773321\n",
      "Iteration 108, loss = 0.26331264\n",
      "Iteration 81, loss = 0.26782915\n",
      "Iteration 109, loss = 0.26367784\n",
      "Iteration 82, loss = 0.26753459\n",
      "Iteration 83, loss = 0.26778858\n",
      "Iteration 110, loss = 0.26318002\n",
      "Iteration 84, loss = 0.26743196\n",
      "Iteration 111, loss = 0.26330157\n",
      "Iteration 85, loss = 0.26747161\n",
      "Iteration 112, loss = 0.26301544\n",
      "Iteration 86, loss = 0.26730129\n",
      "Iteration 113, loss = 0.26298305\n",
      "Iteration 87, loss = 0.26710142\n",
      "Iteration 114, loss = 0.26291113\n",
      "Iteration 88, loss = 0.26701616\n",
      "Iteration 115, loss = 0.26302506\n",
      "Iteration 89, loss = 0.26708740\n",
      "Iteration 116, loss = 0.26293796\n",
      "Iteration 90, loss = 0.26716650\n",
      "Iteration 117, loss = 0.26285593\n",
      "Iteration 91, loss = 0.26711435\n",
      "Iteration 118, loss = 0.26339381\n",
      "Iteration 92, loss = 0.26679447\n",
      "Iteration 93, loss = 0.26657701\n",
      "Iteration 119, loss = 0.26294982\n",
      "Iteration 94, loss = 0.26690672\n",
      "Iteration 120, loss = 0.26261962\n",
      "Iteration 95, loss = 0.26687877\n",
      "Iteration 121, loss = 0.26268674\n",
      "Iteration 96, loss = 0.26681009\n",
      "Iteration 122, loss = 0.26275551\n",
      "Iteration 97, loss = 0.26690702\n",
      "Iteration 123, loss = 0.26249308\n",
      "Iteration 98, loss = 0.26653458\n",
      "Iteration 124, loss = 0.26272778\n",
      "Iteration 99, loss = 0.26672711\n",
      "Iteration 125, loss = 0.26273196\n",
      "Iteration 100, loss = 0.26646420\n",
      "Iteration 126, loss = 0.26241268\n",
      "Iteration 101, loss = 0.26644368\n",
      "Iteration 127, loss = 0.26274959\n",
      "Iteration 102, loss = 0.26667678\n",
      "Iteration 128, loss = 0.26236285\n",
      "Iteration 103, loss = 0.26682449\n",
      "Iteration 129, loss = 0.26251354\n",
      "Iteration 104, loss = 0.26664641\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 130, loss = 0.26258864\n",
      "Iteration 1, loss = 0.34106386\n",
      "Iteration 131, loss = 0.26253834\n",
      "Iteration 2, loss = 0.29693645\n",
      "Iteration 132, loss = 0.26288719\n",
      "Iteration 3, loss = 0.28622441\n",
      "Iteration 133, loss = 0.26205408\n",
      "Iteration 4, loss = 0.28318992\n",
      "Iteration 5, loss = 0.28161240\n",
      "Iteration 134, loss = 0.26240983\n",
      "Iteration 6, loss = 0.28048177\n",
      "Iteration 7, loss = 0.28007980\n",
      "Iteration 135, loss = 0.26211695\n",
      "Iteration 8, loss = 0.28002354\n",
      "Iteration 9, loss = 0.27940669\n",
      "Iteration 136, loss = 0.26252474\n",
      "Iteration 137, loss = 0.26205384\n",
      "Iteration 138, loss = 0.26230943\n",
      "Iteration 10, loss = 0.27897879\n",
      "Iteration 139, loss = 0.26214983\n",
      "Iteration 11, loss = 0.27918424\n",
      "Iteration 12, loss = 0.27839607\n",
      "Iteration 140, loss = 0.26279928\n",
      "Iteration 13, loss = 0.27794658\n",
      "Iteration 141, loss = 0.26190126\n",
      "Iteration 142, loss = 0.26216273\n",
      "Iteration 14, loss = 0.27761591\n",
      "Iteration 143, loss = 0.26216421\n",
      "Iteration 15, loss = 0.27756253\n",
      "Iteration 144, loss = 0.26197854\n",
      "Iteration 16, loss = 0.27722081\n",
      "Iteration 145, loss = 0.26201502\n",
      "Iteration 17, loss = 0.27691565\n",
      "Iteration 146, loss = 0.26200235\n",
      "Iteration 18, loss = 0.27659679\n",
      "Iteration 147, loss = 0.26208567\n",
      "Iteration 19, loss = 0.27666012\n",
      "Iteration 148, loss = 0.26205290\n",
      "Iteration 20, loss = 0.27600490\n",
      "Iteration 149, loss = 0.26206135\n",
      "Iteration 21, loss = 0.27598449\n",
      "Iteration 150, loss = 0.26217464\n",
      "Iteration 22, loss = 0.27571052\n",
      "Iteration 151, loss = 0.26182291\n",
      "Iteration 23, loss = 0.27525076\n",
      "Iteration 152, loss = 0.26194786\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 24, loss = 0.27552739\n",
      "Iteration 25, loss = 0.27511417\n",
      "Iteration 1, loss = 0.34202191\n",
      "Iteration 26, loss = 0.27481323\n",
      "Iteration 2, loss = 0.29698228\n",
      "Iteration 3, loss = 0.28584898\n",
      "Iteration 27, loss = 0.27453035\n",
      "Iteration 4, loss = 0.28248745\n",
      "Iteration 28, loss = 0.27450010\n",
      "Iteration 5, loss = 0.28079760\n",
      "Iteration 29, loss = 0.27418225\n",
      "Iteration 6, loss = 0.27985271\n",
      "Iteration 30, loss = 0.27379855\n",
      "Iteration 7, loss = 0.27932631\n",
      "Iteration 31, loss = 0.27393177\n",
      "Iteration 8, loss = 0.27945503\n",
      "Iteration 32, loss = 0.27336578\n",
      "Iteration 9, loss = 0.27850914\n",
      "Iteration 33, loss = 0.27337850\n",
      "Iteration 10, loss = 0.27809136\n",
      "Iteration 34, loss = 0.27345048\n",
      "Iteration 11, loss = 0.27820763\n",
      "Iteration 35, loss = 0.27291240\n",
      "Iteration 12, loss = 0.27736605\n",
      "Iteration 36, loss = 0.27243562\n",
      "Iteration 13, loss = 0.27679351\n",
      "Iteration 37, loss = 0.27278171\n",
      "Iteration 14, loss = 0.27689272\n",
      "Iteration 38, loss = 0.27263876\n",
      "Iteration 15, loss = 0.27635020\n",
      "Iteration 39, loss = 0.27267048\n",
      "Iteration 16, loss = 0.27626701\n",
      "Iteration 40, loss = 0.27205192\n",
      "Iteration 17, loss = 0.27586348\n",
      "Iteration 41, loss = 0.27195361\n",
      "Iteration 18, loss = 0.27580188\n",
      "Iteration 42, loss = 0.27166490\n",
      "Iteration 19, loss = 0.27560931\n",
      "Iteration 20, loss = 0.27501048\n",
      "Iteration 43, loss = 0.27117230\n",
      "Iteration 21, loss = 0.27490698\n",
      "Iteration 44, loss = 0.27134355\n",
      "Iteration 22, loss = 0.27475159\n",
      "Iteration 45, loss = 0.27086388\n",
      "Iteration 23, loss = 0.27434117\n",
      "Iteration 46, loss = 0.27119963\n",
      "Iteration 24, loss = 0.27445785\n",
      "Iteration 47, loss = 0.27110045\n",
      "Iteration 25, loss = 0.27443020\n",
      "Iteration 48, loss = 0.27084232\n",
      "Iteration 26, loss = 0.27393989\n",
      "Iteration 49, loss = 0.27046487\n",
      "Iteration 27, loss = 0.27384740\n",
      "Iteration 28, loss = 0.27363767\n",
      "Iteration 50, loss = 0.27041351\n",
      "Iteration 29, loss = 0.27345001\n",
      "Iteration 51, loss = 0.27009370\n",
      "Iteration 30, loss = 0.27322943\n",
      "Iteration 52, loss = 0.26998727\n",
      "Iteration 31, loss = 0.27307417\n",
      "Iteration 53, loss = 0.26999895\n",
      "Iteration 54, loss = 0.27023428\n",
      "Iteration 32, loss = 0.27292215\n",
      "Iteration 55, loss = 0.26964298\n",
      "Iteration 33, loss = 0.27270310\n",
      "Iteration 56, loss = 0.26926041\n",
      "Iteration 34, loss = 0.27275151\n",
      "Iteration 57, loss = 0.26944267\n",
      "Iteration 35, loss = 0.27237921\n",
      "Iteration 58, loss = 0.26974118\n",
      "Iteration 36, loss = 0.27184422\n",
      "Iteration 37, loss = 0.27203221\n",
      "Iteration 59, loss = 0.26913872\n",
      "Iteration 38, loss = 0.27188990\n",
      "Iteration 60, loss = 0.26899114\n",
      "Iteration 39, loss = 0.27188988\n",
      "Iteration 61, loss = 0.26911655\n",
      "Iteration 62, loss = 0.26918761\n",
      "Iteration 40, loss = 0.27135593\n",
      "Iteration 41, loss = 0.27103572\n",
      "Iteration 63, loss = 0.26864864\n",
      "Iteration 42, loss = 0.27058355\n",
      "Iteration 64, loss = 0.26872075\n",
      "Iteration 43, loss = 0.27019568\n",
      "Iteration 65, loss = 0.26860892\n",
      "Iteration 44, loss = 0.27050889\n",
      "Iteration 66, loss = 0.26812603\n",
      "Iteration 45, loss = 0.27006067\n",
      "Iteration 67, loss = 0.26846316\n",
      "Iteration 46, loss = 0.27008685\n",
      "Iteration 68, loss = 0.26807882\n",
      "Iteration 47, loss = 0.26958047\n",
      "Iteration 69, loss = 0.26815009\n",
      "Iteration 48, loss = 0.26962302\n",
      "Iteration 70, loss = 0.26794504\n",
      "Iteration 49, loss = 0.26903686\n",
      "Iteration 71, loss = 0.26817075\n",
      "Iteration 50, loss = 0.26898642\n",
      "Iteration 72, loss = 0.26784796\n",
      "Iteration 51, loss = 0.26872882\n",
      "Iteration 73, loss = 0.26815093\n",
      "Iteration 52, loss = 0.26859987\n",
      "Iteration 74, loss = 0.26752550\n",
      "Iteration 53, loss = 0.26834309\n",
      "Iteration 75, loss = 0.26790397\n",
      "Iteration 54, loss = 0.26851896\n",
      "Iteration 76, loss = 0.26715593\n",
      "Iteration 55, loss = 0.26785209\n",
      "Iteration 77, loss = 0.26710538\n",
      "Iteration 56, loss = 0.26756210\n",
      "Iteration 78, loss = 0.26728745\n",
      "Iteration 57, loss = 0.26752464\n",
      "Iteration 79, loss = 0.26774735\n",
      "Iteration 58, loss = 0.26786465\n",
      "Iteration 80, loss = 0.26708010\n",
      "Iteration 59, loss = 0.26723538\n",
      "Iteration 60, loss = 0.26666126\n",
      "Iteration 81, loss = 0.26722454\n",
      "Iteration 61, loss = 0.26742963\n",
      "Iteration 82, loss = 0.26700012\n",
      "Iteration 62, loss = 0.26697862\n",
      "Iteration 83, loss = 0.26744388\n",
      "Iteration 63, loss = 0.26645244\n",
      "Iteration 64, loss = 0.26649831\n",
      "Iteration 84, loss = 0.26678246\n",
      "Iteration 85, loss = 0.26729963\n",
      "Iteration 65, loss = 0.26622406\n",
      "Iteration 66, loss = 0.26616005\n",
      "Iteration 86, loss = 0.26672617\n",
      "Iteration 67, loss = 0.26598685\n",
      "Iteration 87, loss = 0.26671938\n",
      "Iteration 68, loss = 0.26573235\n",
      "Iteration 88, loss = 0.26655980\n",
      "Iteration 89, loss = 0.26637076\n",
      "Iteration 69, loss = 0.26579994\n",
      "Iteration 70, loss = 0.26558196\n",
      "Iteration 90, loss = 0.26648639\n",
      "Iteration 71, loss = 0.26552323\n",
      "Iteration 91, loss = 0.26638955\n",
      "Iteration 72, loss = 0.26553587\n",
      "Iteration 92, loss = 0.26644683\n",
      "Iteration 73, loss = 0.26532229\n",
      "Iteration 93, loss = 0.26611178\n",
      "Iteration 74, loss = 0.26524324\n",
      "Iteration 94, loss = 0.26635586\n",
      "Iteration 75, loss = 0.26525570\n",
      "Iteration 95, loss = 0.26609327\n",
      "Iteration 76, loss = 0.26467677\n",
      "Iteration 96, loss = 0.26583535\n",
      "Iteration 77, loss = 0.26474619\n",
      "Iteration 97, loss = 0.26616579\n",
      "Iteration 78, loss = 0.26466288\n",
      "Iteration 98, loss = 0.26583303\n",
      "Iteration 79, loss = 0.26527485\n",
      "Iteration 99, loss = 0.26604389\n",
      "Iteration 80, loss = 0.26457975\n",
      "Iteration 100, loss = 0.26597635\n",
      "Iteration 81, loss = 0.26506227\n",
      "Iteration 101, loss = 0.26581499\n",
      "Iteration 82, loss = 0.26449801\n",
      "Iteration 83, loss = 0.26454607\n",
      "Iteration 102, loss = 0.26592915\n",
      "Iteration 84, loss = 0.26423581\n",
      "Iteration 103, loss = 0.26618376\n",
      "Iteration 85, loss = 0.26480131\n",
      "Iteration 104, loss = 0.26581834\n",
      "Iteration 86, loss = 0.26433054\n",
      "Iteration 105, loss = 0.26542344\n",
      "Iteration 87, loss = 0.26406107\n",
      "Iteration 106, loss = 0.26556914\n",
      "Iteration 88, loss = 0.26418330\n",
      "Iteration 89, loss = 0.26435967\n",
      "Iteration 107, loss = 0.26574301\n",
      "Iteration 90, loss = 0.26405300\n",
      "Iteration 108, loss = 0.26538512\n",
      "Iteration 91, loss = 0.26406159\n",
      "Iteration 109, loss = 0.26544281\n",
      "Iteration 92, loss = 0.26399895\n",
      "Iteration 110, loss = 0.26555145\n",
      "Iteration 93, loss = 0.26383727\n",
      "Iteration 111, loss = 0.26555428\n",
      "Iteration 94, loss = 0.26459621\n",
      "Iteration 112, loss = 0.26526794\n",
      "Iteration 95, loss = 0.26372946\n",
      "Iteration 113, loss = 0.26550107\n",
      "Iteration 96, loss = 0.26384422\n",
      "Iteration 97, loss = 0.26387030\n",
      "Iteration 98, loss = 0.26342421\n",
      "Iteration 114, loss = 0.26522730\n",
      "Iteration 99, loss = 0.26361387\n",
      "Iteration 115, loss = 0.26539687\n",
      "Iteration 100, loss = 0.26352656\n",
      "Iteration 116, loss = 0.26536323\n",
      "Iteration 101, loss = 0.26339897\n",
      "Iteration 117, loss = 0.26517552\n",
      "Iteration 102, loss = 0.26343717\n",
      "Iteration 118, loss = 0.26519542\n",
      "Iteration 103, loss = 0.26377674\n",
      "Iteration 119, loss = 0.26510978\n",
      "Iteration 104, loss = 0.26347878\n",
      "Iteration 120, loss = 0.26497474\n",
      "Iteration 105, loss = 0.26311864\n",
      "Iteration 121, loss = 0.26504663\n",
      "Iteration 106, loss = 0.26305682\n",
      "Iteration 122, loss = 0.26511875\n",
      "Iteration 107, loss = 0.26369909\n",
      "Iteration 123, loss = 0.26508920\n",
      "Iteration 108, loss = 0.26312766\n",
      "Iteration 124, loss = 0.26466556\n",
      "Iteration 109, loss = 0.26311502\n",
      "Iteration 125, loss = 0.26479146\n",
      "Iteration 110, loss = 0.26288275\n",
      "Iteration 126, loss = 0.26469677\n",
      "Iteration 111, loss = 0.26313122\n",
      "Iteration 127, loss = 0.26466882\n",
      "Iteration 112, loss = 0.26282069\n",
      "Iteration 128, loss = 0.26487748\n",
      "Iteration 113, loss = 0.26332757\n",
      "Iteration 129, loss = 0.26477731\n",
      "Iteration 114, loss = 0.26278739\n",
      "Iteration 130, loss = 0.26475814\n",
      "Iteration 115, loss = 0.26293921\n",
      "Iteration 116, loss = 0.26309060\n",
      "Iteration 131, loss = 0.26490268\n",
      "Iteration 117, loss = 0.26298703\n",
      "Iteration 132, loss = 0.26488668\n",
      "Iteration 133, loss = 0.26452351\n",
      "Iteration 118, loss = 0.26279935\n",
      "Iteration 119, loss = 0.26277801\n",
      "Iteration 134, loss = 0.26449095\n",
      "Iteration 120, loss = 0.26257379\n",
      "Iteration 135, loss = 0.26467207\n",
      "Iteration 121, loss = 0.26252472\n",
      "Iteration 136, loss = 0.26408801\n",
      "Iteration 122, loss = 0.26245005\n",
      "Iteration 137, loss = 0.26417117\n",
      "Iteration 123, loss = 0.26266906\n",
      "Iteration 138, loss = 0.26423060\n",
      "Iteration 124, loss = 0.26237608\n",
      "Iteration 139, loss = 0.26427912\n",
      "Iteration 125, loss = 0.26254335\n",
      "Iteration 140, loss = 0.26399983\n",
      "Iteration 126, loss = 0.26247373\n",
      "Iteration 141, loss = 0.26412866\n",
      "Iteration 127, loss = 0.26229546\n",
      "Iteration 142, loss = 0.26408538\n",
      "Iteration 128, loss = 0.26244938\n",
      "Iteration 129, loss = 0.26280480\n",
      "Iteration 143, loss = 0.26406655\n",
      "Iteration 130, loss = 0.26243666\n",
      "Iteration 131, loss = 0.26302171\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 144, loss = 0.26400061\n",
      "Iteration 145, loss = 0.26437161\n",
      "Iteration 146, loss = 0.26413686\n",
      "Iteration 147, loss = 0.26402559\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34301579\n",
      "Iteration 1, loss = 0.34145940\n",
      "Iteration 2, loss = 0.29651823\n",
      "Iteration 2, loss = 0.29375952\n",
      "Iteration 3, loss = 0.28688553\n",
      "Iteration 3, loss = 0.28391188\n",
      "Iteration 4, loss = 0.28426715\n",
      "Iteration 4, loss = 0.28110657\n",
      "Iteration 5, loss = 0.28432868\n",
      "Iteration 5, loss = 0.28094356\n",
      "Iteration 6, loss = 0.28264813\n",
      "Iteration 6, loss = 0.27953307\n",
      "Iteration 7, loss = 0.28211688\n",
      "Iteration 7, loss = 0.27876322\n",
      "Iteration 8, loss = 0.28212462\n",
      "Iteration 8, loss = 0.27859749\n",
      "Iteration 9, loss = 0.28188509\n",
      "Iteration 9, loss = 0.27867284\n",
      "Iteration 10, loss = 0.28100758\n",
      "Iteration 10, loss = 0.27774087\n",
      "Iteration 11, loss = 0.28072550\n",
      "Iteration 11, loss = 0.27883164\n",
      "Iteration 12, loss = 0.28033269\n",
      "Iteration 12, loss = 0.27734281\n",
      "Iteration 13, loss = 0.28011791\n",
      "Iteration 13, loss = 0.27713891\n",
      "Iteration 14, loss = 0.27660179\n",
      "Iteration 14, loss = 0.27984917\n",
      "Iteration 15, loss = 0.27960048\n",
      "Iteration 15, loss = 0.27616134\n",
      "Iteration 16, loss = 0.27950205\n",
      "Iteration 16, loss = 0.27602720\n",
      "Iteration 17, loss = 0.27945706\n",
      "Iteration 17, loss = 0.27603963\n",
      "Iteration 18, loss = 0.27883870\n",
      "Iteration 18, loss = 0.27550950\n",
      "Iteration 19, loss = 0.27842957\n",
      "Iteration 19, loss = 0.27533858\n",
      "Iteration 20, loss = 0.27852051\n",
      "Iteration 20, loss = 0.27520065\n",
      "Iteration 21, loss = 0.27779371\n",
      "Iteration 21, loss = 0.27462302\n",
      "Iteration 22, loss = 0.27756819\n",
      "Iteration 22, loss = 0.27454789\n",
      "Iteration 23, loss = 0.27735238\n",
      "Iteration 23, loss = 0.27407902\n",
      "Iteration 24, loss = 0.27973804\n",
      "Iteration 24, loss = 0.27628634\n",
      "Iteration 25, loss = 0.27700611\n",
      "Iteration 25, loss = 0.27379013\n",
      "Iteration 26, loss = 0.27699770\n",
      "Iteration 26, loss = 0.27374908\n",
      "Iteration 27, loss = 0.27669126\n",
      "Iteration 28, loss = 0.27631959\n",
      "Iteration 27, loss = 0.27333168\n",
      "Iteration 29, loss = 0.27620923\n",
      "Iteration 28, loss = 0.27300240\n",
      "Iteration 30, loss = 0.27589760\n",
      "Iteration 29, loss = 0.27272003\n",
      "Iteration 31, loss = 0.27758097\n",
      "Iteration 30, loss = 0.27242683\n",
      "Iteration 32, loss = 0.27573247\n",
      "Iteration 31, loss = 0.27232331\n",
      "Iteration 33, loss = 0.27561878\n",
      "Iteration 32, loss = 0.27212661\n",
      "Iteration 34, loss = 0.27499828\n",
      "Iteration 33, loss = 0.27212594\n",
      "Iteration 35, loss = 0.27496873\n",
      "Iteration 34, loss = 0.27137654\n",
      "Iteration 36, loss = 0.27514906\n",
      "Iteration 37, loss = 0.27485247\n",
      "Iteration 35, loss = 0.27130938\n",
      "Iteration 38, loss = 0.27467355\n",
      "Iteration 36, loss = 0.27137655\n",
      "Iteration 39, loss = 0.27464365\n",
      "Iteration 37, loss = 0.27074613\n",
      "Iteration 40, loss = 0.27477053\n",
      "Iteration 38, loss = 0.27040379\n",
      "Iteration 39, loss = 0.27028938\n",
      "Iteration 41, loss = 0.27491733\n",
      "Iteration 40, loss = 0.26996636\n",
      "Iteration 42, loss = 0.27393465\n",
      "Iteration 41, loss = 0.27028048\n",
      "Iteration 43, loss = 0.27411325\n",
      "Iteration 42, loss = 0.26940130\n",
      "Iteration 44, loss = 0.27389864\n",
      "Iteration 43, loss = 0.26925618\n",
      "Iteration 45, loss = 0.27463142\n",
      "Iteration 44, loss = 0.26909570\n",
      "Iteration 46, loss = 0.27422585\n",
      "Iteration 45, loss = 0.27000049\n",
      "Iteration 47, loss = 0.27392214\n",
      "Iteration 46, loss = 0.26883365\n",
      "Iteration 48, loss = 0.27346278\n",
      "Iteration 47, loss = 0.26845983\n",
      "Iteration 49, loss = 0.27330214\n",
      "Iteration 48, loss = 0.26820887\n",
      "Iteration 50, loss = 0.27386697\n",
      "Iteration 49, loss = 0.26796605\n",
      "Iteration 51, loss = 0.27313153\n",
      "Iteration 50, loss = 0.26842647\n",
      "Iteration 51, loss = 0.26762157\n",
      "Iteration 52, loss = 0.27304084\n",
      "Iteration 53, loss = 0.27377044\n",
      "Iteration 52, loss = 0.26730199\n",
      "Iteration 54, loss = 0.27321785\n",
      "Iteration 53, loss = 0.27075844\n",
      "Iteration 54, loss = 0.26689112\n",
      "Iteration 55, loss = 0.27271328\n",
      "Iteration 55, loss = 0.26669896\n",
      "Iteration 56, loss = 0.27224486\n",
      "Iteration 56, loss = 0.26617290\n",
      "Iteration 57, loss = 0.27255724\n",
      "Iteration 57, loss = 0.26665862\n",
      "Iteration 58, loss = 0.27252682\n",
      "Iteration 58, loss = 0.26625031\n",
      "Iteration 59, loss = 0.27186446\n",
      "Iteration 59, loss = 0.26581508\n",
      "Iteration 60, loss = 0.27250365\n",
      "Iteration 60, loss = 0.26588492\n",
      "Iteration 61, loss = 0.27189910\n",
      "Iteration 61, loss = 0.26549666\n",
      "Iteration 62, loss = 0.26552681\n",
      "Iteration 62, loss = 0.27166773\n",
      "Iteration 63, loss = 0.26752890\n",
      "Iteration 63, loss = 0.27217165\n",
      "Iteration 64, loss = 0.26547742\n",
      "Iteration 64, loss = 0.27179771\n",
      "Iteration 65, loss = 0.26506669\n",
      "Iteration 65, loss = 0.27143823\n",
      "Iteration 66, loss = 0.26488753\n",
      "Iteration 66, loss = 0.27121733\n",
      "Iteration 67, loss = 0.26514668\n",
      "Iteration 67, loss = 0.27140890\n",
      "Iteration 68, loss = 0.26556431\n",
      "Iteration 68, loss = 0.27161450\n",
      "Iteration 69, loss = 0.26583828\n",
      "Iteration 69, loss = 0.27180878\n",
      "Iteration 70, loss = 0.26457996\n",
      "Iteration 71, loss = 0.26460100\n",
      "Iteration 70, loss = 0.27087809\n",
      "Iteration 72, loss = 0.26452661\n",
      "Iteration 73, loss = 0.26463200\n",
      "Iteration 71, loss = 0.27071464\n",
      "Iteration 74, loss = 0.26452763\n",
      "Iteration 75, loss = 0.26396650\n",
      "Iteration 72, loss = 0.27058116\n",
      "Iteration 76, loss = 0.26403180\n",
      "Iteration 77, loss = 0.26506178\n",
      "Iteration 73, loss = 0.27050669\n",
      "Iteration 78, loss = 0.26470672\n",
      "Iteration 74, loss = 0.27019188\n",
      "Iteration 79, loss = 0.26432849\n",
      "Iteration 75, loss = 0.26996331Iteration 80, loss = 0.26396561\n",
      "\n",
      "Iteration 81, loss = 0.26373925\n",
      "Iteration 82, loss = 0.26361017\n",
      "Iteration 76, loss = 0.27010532\n",
      "Iteration 83, loss = 0.26339689\n",
      "Iteration 77, loss = 0.27198313\n",
      "Iteration 84, loss = 0.26476775\n",
      "Iteration 78, loss = 0.27013880\n",
      "Iteration 85, loss = 0.26353101\n",
      "Iteration 79, loss = 0.27018327\n",
      "Iteration 86, loss = 0.26342663\n",
      "Iteration 80, loss = 0.26972541\n",
      "Iteration 87, loss = 0.26405161\n",
      "Iteration 81, loss = 0.26970536\n",
      "Iteration 88, loss = 0.26403247\n",
      "Iteration 82, loss = 0.26957172\n",
      "Iteration 89, loss = 0.26349344\n",
      "Iteration 83, loss = 0.26937971\n",
      "Iteration 90, loss = 0.26354032\n",
      "Iteration 91, loss = 0.26282660\n",
      "Iteration 84, loss = 0.27047293\n",
      "Iteration 92, loss = 0.26297298\n",
      "Iteration 85, loss = 0.26936733\n",
      "Iteration 86, loss = 0.26921379\n",
      "Iteration 93, loss = 0.26337551\n",
      "Iteration 94, loss = 0.26287237\n",
      "Iteration 87, loss = 0.26965206\n",
      "Iteration 88, loss = 0.26942498\n",
      "Iteration 95, loss = 0.26275769\n",
      "Iteration 89, loss = 0.26937329\n",
      "Iteration 96, loss = 0.26298942\n",
      "Iteration 90, loss = 0.26918352\n",
      "Iteration 97, loss = 0.26274234\n",
      "Iteration 91, loss = 0.26884260\n",
      "Iteration 98, loss = 0.26286403\n",
      "Iteration 92, loss = 0.26878767\n",
      "Iteration 99, loss = 0.26278765\n",
      "Iteration 93, loss = 0.26903807\n",
      "Iteration 100, loss = 0.26283055\n",
      "Iteration 94, loss = 0.26880379\n",
      "Iteration 101, loss = 0.26376569\n",
      "Iteration 95, loss = 0.26860263\n",
      "Iteration 102, loss = 0.26357142\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 96, loss = 0.26899248\n",
      "Iteration 97, loss = 0.26844533\n",
      "Iteration 98, loss = 0.26859929\n",
      "Iteration 99, loss = 0.26846591\n",
      "Iteration 1, loss = 0.34245136\n",
      "Iteration 2, loss = 0.29587545\n",
      "Iteration 100, loss = 0.26858119\n",
      "Iteration 3, loss = 0.28612741\n",
      "Iteration 101, loss = 0.26990618\n",
      "Iteration 4, loss = 0.28351287\n",
      "Iteration 102, loss = 0.26944742\n",
      "Iteration 103, loss = 0.26838040\n",
      "Iteration 5, loss = 0.28370930\n",
      "Iteration 104, loss = 0.26827404\n",
      "Iteration 6, loss = 0.28215469\n",
      "Iteration 105, loss = 0.26825712\n",
      "Iteration 7, loss = 0.28140553\n",
      "Iteration 106, loss = 0.26829461\n",
      "Iteration 8, loss = 0.28122866\n",
      "Iteration 107, loss = 0.26825602\n",
      "Iteration 9, loss = 0.28101918\n",
      "Iteration 10, loss = 0.28029355\n",
      "Iteration 108, loss = 0.26816973\n",
      "Iteration 11, loss = 0.28132685\n",
      "Iteration 12, loss = 0.27996002\n",
      "Iteration 109, loss = 0.26812537\n",
      "Iteration 110, loss = 0.26822984\n",
      "Iteration 13, loss = 0.27958831\n",
      "Iteration 111, loss = 0.26798966\n",
      "Iteration 14, loss = 0.27888085\n",
      "Iteration 112, loss = 0.26782554\n",
      "Iteration 15, loss = 0.27871815\n",
      "Iteration 113, loss = 0.26784747\n",
      "Iteration 16, loss = 0.27858554\n",
      "Iteration 114, loss = 0.26768343\n",
      "Iteration 17, loss = 0.27858866\n",
      "Iteration 115, loss = 0.26813202\n",
      "Iteration 18, loss = 0.27801678\n",
      "Iteration 116, loss = 0.26784190\n",
      "Iteration 19, loss = 0.27782449\n",
      "Iteration 117, loss = 0.26763690\n",
      "Iteration 20, loss = 0.27766523\n",
      "Iteration 118, loss = 0.26781995\n",
      "Iteration 21, loss = 0.27700360\n",
      "Iteration 22, loss = 0.27689411\n",
      "Iteration 119, loss = 0.26758119\n",
      "Iteration 23, loss = 0.27644325\n",
      "Iteration 120, loss = 0.26732476\n",
      "Iteration 24, loss = 0.27844845\n",
      "Iteration 121, loss = 0.26805837\n",
      "Iteration 122, loss = 0.26776416\n",
      "Iteration 25, loss = 0.27629923\n",
      "Iteration 123, loss = 0.26754139\n",
      "Iteration 26, loss = 0.27623292\n",
      "Iteration 124, loss = 0.26735721\n",
      "Iteration 125, loss = 0.26758066\n",
      "Iteration 27, loss = 0.27548776\n",
      "Iteration 126, loss = 0.26710529\n",
      "Iteration 28, loss = 0.27540292\n",
      "Iteration 127, loss = 0.26742036\n",
      "Iteration 29, loss = 0.27528507\n",
      "Iteration 128, loss = 0.26703092\n",
      "Iteration 30, loss = 0.27487293\n",
      "Iteration 129, loss = 0.26812134\n",
      "Iteration 31, loss = 0.27458283\n",
      "Iteration 130, loss = 0.26714631\n",
      "Iteration 32, loss = 0.27463210\n",
      "Iteration 131, loss = 0.26790769\n",
      "Iteration 33, loss = 0.27441215\n",
      "Iteration 132, loss = 0.26742297\n",
      "Iteration 34, loss = 0.27391535\n",
      "Iteration 133, loss = 0.26701905\n",
      "Iteration 35, loss = 0.27405600\n",
      "Iteration 134, loss = 0.26701256\n",
      "Iteration 36, loss = 0.27392234\n",
      "Iteration 135, loss = 0.26695662\n",
      "Iteration 136, loss = 0.26698705\n",
      "Iteration 37, loss = 0.27346538\n",
      "Iteration 137, loss = 0.26737011\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 38, loss = 0.27282164\n",
      "Iteration 1, loss = 0.34226255\n",
      "Iteration 39, loss = 0.27282010\n",
      "Iteration 40, loss = 0.27261099\n",
      "Iteration 2, loss = 0.29611494\n",
      "Iteration 41, loss = 0.27251183\n",
      "Iteration 3, loss = 0.28703874\n",
      "Iteration 42, loss = 0.27208323\n",
      "Iteration 4, loss = 0.28428904\n",
      "Iteration 43, loss = 0.27194400\n",
      "Iteration 5, loss = 0.28449440\n",
      "Iteration 44, loss = 0.27216798\n",
      "Iteration 6, loss = 0.28271198\n",
      "Iteration 45, loss = 0.27224591\n",
      "Iteration 7, loss = 0.28221367\n",
      "Iteration 46, loss = 0.27140777\n",
      "Iteration 8, loss = 0.28205791\n",
      "Iteration 9, loss = 0.28197095\n",
      "Iteration 47, loss = 0.27129184\n",
      "Iteration 10, loss = 0.28135987\n",
      "Iteration 48, loss = 0.27099672\n",
      "Iteration 11, loss = 0.28229104\n",
      "Iteration 49, loss = 0.27085333\n",
      "Iteration 12, loss = 0.28119927\n",
      "Iteration 50, loss = 0.27131548\n",
      "Iteration 13, loss = 0.28066944\n",
      "Iteration 51, loss = 0.27053133\n",
      "Iteration 14, loss = 0.28006797\n",
      "Iteration 52, loss = 0.27037035\n",
      "Iteration 15, loss = 0.27997010\n",
      "Iteration 53, loss = 0.27445951\n",
      "Iteration 16, loss = 0.27965168\n",
      "Iteration 54, loss = 0.27021283\n",
      "Iteration 17, loss = 0.27982259\n",
      "Iteration 55, loss = 0.27000876\n",
      "Iteration 18, loss = 0.27902462\n",
      "Iteration 56, loss = 0.26970850\n",
      "Iteration 19, loss = 0.27892498\n",
      "Iteration 57, loss = 0.27029416\n",
      "Iteration 20, loss = 0.27849436\n",
      "Iteration 58, loss = 0.26999275\n",
      "Iteration 21, loss = 0.27789362\n",
      "Iteration 59, loss = 0.26970620\n",
      "Iteration 60, loss = 0.26981161\n",
      "Iteration 61, loss = 0.26961089\n",
      "Iteration 22, loss = 0.27776798\n",
      "Iteration 23, loss = 0.27713242\n",
      "Iteration 62, loss = 0.26928722\n",
      "Iteration 24, loss = 0.27912318\n",
      "Iteration 63, loss = 0.27047924\n",
      "Iteration 25, loss = 0.27667319\n",
      "Iteration 64, loss = 0.27004829\n",
      "Iteration 26, loss = 0.27636646\n",
      "Iteration 65, loss = 0.26914367\n",
      "Iteration 27, loss = 0.27587977\n",
      "Iteration 66, loss = 0.26886015\n",
      "Iteration 28, loss = 0.27547357\n",
      "Iteration 67, loss = 0.26922818\n",
      "Iteration 29, loss = 0.27506415\n",
      "Iteration 68, loss = 0.26989844\n",
      "Iteration 30, loss = 0.27453216\n",
      "Iteration 69, loss = 0.26999142\n",
      "Iteration 31, loss = 0.27426433\n",
      "Iteration 70, loss = 0.26875866\n",
      "Iteration 32, loss = 0.27396869\n",
      "Iteration 71, loss = 0.26898170\n",
      "Iteration 72, loss = 0.26927149\n",
      "Iteration 33, loss = 0.27377687\n",
      "Iteration 73, loss = 0.26851062\n",
      "Iteration 34, loss = 0.27304441\n",
      "Iteration 74, loss = 0.26845905\n",
      "Iteration 35, loss = 0.27280512\n",
      "Iteration 75, loss = 0.26828011\n",
      "Iteration 36, loss = 0.27292181\n",
      "Iteration 76, loss = 0.26827515\n",
      "Iteration 37, loss = 0.27224004\n",
      "Iteration 77, loss = 0.26894996\n",
      "Iteration 38, loss = 0.27171632\n",
      "Iteration 78, loss = 0.26946695\n",
      "Iteration 39, loss = 0.27140813\n",
      "Iteration 79, loss = 0.26829378\n",
      "Iteration 40, loss = 0.27096611\n",
      "Iteration 80, loss = 0.26812446\n",
      "Iteration 41, loss = 0.27076188\n",
      "Iteration 81, loss = 0.26790656\n",
      "Iteration 42, loss = 0.27042894\n",
      "Iteration 43, loss = 0.27012534\n",
      "Iteration 82, loss = 0.26789434\n",
      "Iteration 44, loss = 0.27019608\n",
      "Iteration 83, loss = 0.26779557\n",
      "Iteration 45, loss = 0.27060922\n",
      "Iteration 84, loss = 0.26823553\n",
      "Iteration 46, loss = 0.26952931\n",
      "Iteration 85, loss = 0.26774175\n",
      "Iteration 47, loss = 0.26934232\n",
      "Iteration 86, loss = 0.26800614\n",
      "Iteration 48, loss = 0.26954147\n",
      "Iteration 87, loss = 0.26805817\n",
      "Iteration 49, loss = 0.27009631\n",
      "Iteration 88, loss = 0.26768275\n",
      "Iteration 89, loss = 0.26795171\n",
      "Iteration 50, loss = 0.26996068\n",
      "Iteration 51, loss = 0.26883689\n",
      "Iteration 90, loss = 0.26803251\n",
      "Iteration 91, loss = 0.26714627\n",
      "Iteration 52, loss = 0.26850511\n",
      "Iteration 92, loss = 0.26731776\n",
      "Iteration 53, loss = 0.27211212\n",
      "Iteration 93, loss = 0.26738856\n",
      "Iteration 54, loss = 0.26831882\n",
      "Iteration 94, loss = 0.26725161\n",
      "Iteration 55, loss = 0.26785938\n",
      "Iteration 95, loss = 0.26716091\n",
      "Iteration 56, loss = 0.26766401\n",
      "Iteration 96, loss = 0.26735329\n",
      "Iteration 57, loss = 0.26805770\n",
      "Iteration 97, loss = 0.26701306\n",
      "Iteration 58, loss = 0.26774656\n",
      "Iteration 98, loss = 0.26737225\n",
      "Iteration 59, loss = 0.26729509\n",
      "Iteration 60, loss = 0.26764925\n",
      "Iteration 99, loss = 0.26763247\n",
      "Iteration 100, loss = 0.26718459\n",
      "Iteration 61, loss = 0.26735355\n",
      "Iteration 62, loss = 0.26705272\n",
      "Iteration 101, loss = 0.26729981\n",
      "Iteration 63, loss = 0.26887034\n",
      "Iteration 102, loss = 0.26763952\n",
      "Iteration 64, loss = 0.26700232\n",
      "Iteration 103, loss = 0.26673863\n",
      "Iteration 65, loss = 0.26747540\n",
      "Iteration 104, loss = 0.26671499\n",
      "Iteration 66, loss = 0.26679501\n",
      "Iteration 105, loss = 0.26668855\n",
      "Iteration 67, loss = 0.26700017\n",
      "Iteration 106, loss = 0.26693106\n",
      "Iteration 68, loss = 0.26775791\n",
      "Iteration 107, loss = 0.26758042\n",
      "Iteration 69, loss = 0.26759056\n",
      "Iteration 108, loss = 0.26695273\n",
      "Iteration 70, loss = 0.26619020\n",
      "Iteration 109, loss = 0.26686364\n",
      "Iteration 71, loss = 0.26654838\n",
      "Iteration 110, loss = 0.26669942\n",
      "Iteration 72, loss = 0.26713810\n",
      "Iteration 111, loss = 0.26658768\n",
      "Iteration 73, loss = 0.26613634\n",
      "Iteration 112, loss = 0.26640424\n",
      "Iteration 74, loss = 0.26597706\n",
      "Iteration 113, loss = 0.26639020\n",
      "Iteration 75, loss = 0.26596546\n",
      "Iteration 114, loss = 0.26625803\n",
      "Iteration 76, loss = 0.26580392\n",
      "Iteration 115, loss = 0.26630067\n",
      "Iteration 116, loss = 0.26624853\n",
      "Iteration 77, loss = 0.26664184\n",
      "Iteration 117, loss = 0.26617112\n",
      "Iteration 78, loss = 0.26683081\n",
      "Iteration 118, loss = 0.26635909\n",
      "Iteration 79, loss = 0.26572204\n",
      "Iteration 119, loss = 0.26596844\n",
      "Iteration 80, loss = 0.26559409\n",
      "Iteration 120, loss = 0.26605179\n",
      "Iteration 81, loss = 0.26549751\n",
      "Iteration 121, loss = 0.26622004\n",
      "Iteration 82, loss = 0.26547860\n",
      "Iteration 83, loss = 0.26551084\n",
      "Iteration 122, loss = 0.26628568\n",
      "Iteration 84, loss = 0.26603813\n",
      "Iteration 123, loss = 0.26610951\n",
      "Iteration 85, loss = 0.26556590\n",
      "Iteration 124, loss = 0.26597384\n",
      "Iteration 86, loss = 0.26555948\n",
      "Iteration 125, loss = 0.26606061\n",
      "Iteration 87, loss = 0.26542737\n",
      "Iteration 126, loss = 0.26591161\n",
      "Iteration 88, loss = 0.26563989\n",
      "Iteration 127, loss = 0.26593686\n",
      "Iteration 89, loss = 0.26582544\n",
      "Iteration 128, loss = 0.26582887\n",
      "Iteration 90, loss = 0.26561594\n",
      "Iteration 129, loss = 0.26653874\n",
      "Iteration 91, loss = 0.26464759\n",
      "Iteration 130, loss = 0.26566529\n",
      "Iteration 92, loss = 0.26478190\n",
      "Iteration 131, loss = 0.26626392\n",
      "Iteration 93, loss = 0.26527291\n",
      "Iteration 132, loss = 0.26612190\n",
      "Iteration 94, loss = 0.26472352\n",
      "Iteration 133, loss = 0.26555201\n",
      "Iteration 95, loss = 0.26474165\n",
      "Iteration 96, loss = 0.26496430\n",
      "Iteration 134, loss = 0.26565573\n",
      "Iteration 135, loss = 0.26525054\n",
      "Iteration 97, loss = 0.26464437\n",
      "Iteration 136, loss = 0.26554897\n",
      "Iteration 98, loss = 0.26484411\n",
      "Iteration 137, loss = 0.26555691\n",
      "Iteration 99, loss = 0.26500864\n",
      "Iteration 138, loss = 0.26557511\n",
      "Iteration 100, loss = 0.26456620\n",
      "Iteration 139, loss = 0.26518078\n",
      "Iteration 101, loss = 0.26467342\n",
      "Iteration 140, loss = 0.26618624\n",
      "Iteration 102, loss = 0.26584267\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 141, loss = 0.26558683\n",
      "Iteration 142, loss = 0.26570722\n",
      "Iteration 143, loss = 0.26509168\n",
      "Iteration 1, loss = 0.34218212\n",
      "Iteration 144, loss = 0.26504723\n",
      "Iteration 2, loss = 0.29416905\n",
      "Iteration 145, loss = 0.26498029\n",
      "Iteration 3, loss = 0.28415730\n",
      "Iteration 146, loss = 0.26518257\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 4, loss = 0.28108472\n",
      "Iteration 5, loss = 0.28072876\n",
      "Iteration 6, loss = 0.27928531\n",
      "Iteration 1, loss = 0.34165012\n",
      "Iteration 7, loss = 0.27877665\n",
      "Iteration 8, loss = 0.27840068\n",
      "Iteration 2, loss = 0.29390865\n",
      "Iteration 9, loss = 0.27849036\n",
      "Iteration 3, loss = 0.28392641\n",
      "Iteration 10, loss = 0.27808013\n",
      "Iteration 4, loss = 0.28142970\n",
      "Iteration 11, loss = 0.27882440\n",
      "Iteration 5, loss = 0.28111070\n",
      "Iteration 12, loss = 0.27740329\n",
      "Iteration 6, loss = 0.27962047\n",
      "Iteration 13, loss = 0.27688541\n",
      "Iteration 7, loss = 0.27929234\n",
      "Iteration 14, loss = 0.27639677\n",
      "Iteration 8, loss = 0.27886034\n",
      "Iteration 15, loss = 0.27622345\n",
      "Iteration 9, loss = 0.27893170\n",
      "Iteration 16, loss = 0.27589022\n",
      "Iteration 10, loss = 0.27824500\n",
      "Iteration 17, loss = 0.27585577\n",
      "Iteration 18, loss = 0.27544843\n",
      "Iteration 11, loss = 0.27939419\n",
      "Iteration 19, loss = 0.27520929\n",
      "Iteration 12, loss = 0.27780652\n",
      "Iteration 20, loss = 0.27493183\n",
      "Iteration 13, loss = 0.27707664\n",
      "Iteration 21, loss = 0.27441676\n",
      "Iteration 14, loss = 0.27677939\n",
      "Iteration 22, loss = 0.27400078\n",
      "Iteration 15, loss = 0.27666302\n",
      "Iteration 23, loss = 0.27363681\n",
      "Iteration 24, loss = 0.27539664\n",
      "Iteration 16, loss = 0.27631679\n",
      "Iteration 17, loss = 0.27658545\n",
      "Iteration 18, loss = 0.27552735\n",
      "Iteration 25, loss = 0.27331302\n",
      "Iteration 19, loss = 0.27557116\n",
      "Iteration 26, loss = 0.27316694\n",
      "Iteration 20, loss = 0.27559365\n",
      "Iteration 21, loss = 0.27500092\n",
      "Iteration 27, loss = 0.27269776\n",
      "Iteration 22, loss = 0.27463185\n",
      "Iteration 28, loss = 0.27225725\n",
      "Iteration 29, loss = 0.27220109\n",
      "Iteration 23, loss = 0.27420140\n",
      "Iteration 30, loss = 0.27154598\n",
      "Iteration 24, loss = 0.27547812\n",
      "Iteration 31, loss = 0.27144340\n",
      "Iteration 25, loss = 0.27381523\n",
      "Iteration 32, loss = 0.27113467\n",
      "Iteration 26, loss = 0.27391092\n",
      "Iteration 27, loss = 0.27345194\n",
      "Iteration 33, loss = 0.27086897\n",
      "Iteration 28, loss = 0.27309978\n",
      "Iteration 34, loss = 0.27060575\n",
      "Iteration 29, loss = 0.27389643\n",
      "Iteration 35, loss = 0.27024594\n",
      "Iteration 30, loss = 0.27260045\n",
      "Iteration 36, loss = 0.27030625\n",
      "Iteration 31, loss = 0.27238262\n",
      "Iteration 37, loss = 0.26998826\n",
      "Iteration 38, loss = 0.26933304\n",
      "Iteration 32, loss = 0.27226542\n",
      "Iteration 33, loss = 0.27194682\n",
      "Iteration 39, loss = 0.26899781\n",
      "Iteration 34, loss = 0.27194719\n",
      "Iteration 40, loss = 0.26904299\n",
      "Iteration 35, loss = 0.27146553\n",
      "Iteration 41, loss = 0.26877861\n",
      "Iteration 42, loss = 0.26854502\n",
      "Iteration 36, loss = 0.27295655\n",
      "Iteration 37, loss = 0.27133664\n",
      "Iteration 43, loss = 0.26843175\n",
      "Iteration 44, loss = 0.26830606\n",
      "Iteration 38, loss = 0.27075866\n",
      "Iteration 45, loss = 0.26866587\n",
      "Iteration 39, loss = 0.27076659\n",
      "Iteration 46, loss = 0.26788176\n",
      "Iteration 40, loss = 0.27177363\n",
      "Iteration 41, loss = 0.27037717\n",
      "Iteration 47, loss = 0.26773230\n",
      "Iteration 48, loss = 0.26771512\n",
      "Iteration 42, loss = 0.27034213\n",
      "Iteration 43, loss = 0.27014921\n",
      "Iteration 49, loss = 0.26878869\n",
      "Iteration 44, loss = 0.27010871\n",
      "Iteration 50, loss = 0.26838455\n",
      "Iteration 45, loss = 0.26993757\n",
      "Iteration 51, loss = 0.26732778\n",
      "Iteration 46, loss = 0.26984573\n",
      "Iteration 52, loss = 0.26713587\n",
      "Iteration 47, loss = 0.26970054\n",
      "Iteration 53, loss = 0.27030814\n",
      "Iteration 48, loss = 0.26978988\n",
      "Iteration 54, loss = 0.26701959\n",
      "Iteration 49, loss = 0.27000737\n",
      "Iteration 55, loss = 0.26636729\n",
      "Iteration 50, loss = 0.27049484\n",
      "Iteration 56, loss = 0.26623835\n",
      "Iteration 51, loss = 0.26929521\n",
      "Iteration 57, loss = 0.26665600\n",
      "Iteration 52, loss = 0.26915604\n",
      "Iteration 58, loss = 0.26630968\n",
      "Iteration 53, loss = 0.27157183\n",
      "Iteration 59, loss = 0.26609290\n",
      "Iteration 54, loss = 0.26908450\n",
      "Iteration 60, loss = 0.26607489\n",
      "Iteration 55, loss = 0.26862792\n",
      "Iteration 61, loss = 0.26626560\n",
      "Iteration 56, loss = 0.26856582\n",
      "Iteration 57, loss = 0.26881471\n",
      "Iteration 58, loss = 0.26826378\n",
      "Iteration 62, loss = 0.26570285\n",
      "Iteration 59, loss = 0.26841886\n",
      "Iteration 63, loss = 0.26731481\n",
      "Iteration 60, loss = 0.26831809\n",
      "Iteration 61, loss = 0.26825457\n",
      "Iteration 64, loss = 0.26610516\n",
      "Iteration 62, loss = 0.26797326\n",
      "Iteration 65, loss = 0.26573986\n",
      "Iteration 66, loss = 0.26544959\n",
      "Iteration 63, loss = 0.26906614\n",
      "Iteration 67, loss = 0.26558439\n",
      "Iteration 64, loss = 0.26836428\n",
      "Iteration 68, loss = 0.26638780\n",
      "Iteration 65, loss = 0.26773886\n",
      "Iteration 69, loss = 0.26627839\n",
      "Iteration 66, loss = 0.26821047\n",
      "Iteration 67, loss = 0.26786619\n",
      "Iteration 70, loss = 0.26500553\n",
      "Iteration 68, loss = 0.26859410\n",
      "Iteration 71, loss = 0.26554727\n",
      "Iteration 69, loss = 0.26834563\n",
      "Iteration 72, loss = 0.26599705\n",
      "Iteration 70, loss = 0.26724709\n",
      "Iteration 73, loss = 0.26490803\n",
      "Iteration 74, loss = 0.26486262\n",
      "Iteration 71, loss = 0.26765393\n",
      "Iteration 75, loss = 0.26495349\n",
      "Iteration 72, loss = 0.26798597\n",
      "Iteration 76, loss = 0.26453756\n",
      "Iteration 73, loss = 0.26702878\n",
      "Iteration 77, loss = 0.26638395\n",
      "Iteration 74, loss = 0.26724689\n",
      "Iteration 78, loss = 0.26571364\n",
      "Iteration 75, loss = 0.26701147\n",
      "Iteration 79, loss = 0.26449751\n",
      "Iteration 76, loss = 0.26683358\n",
      "Iteration 80, loss = 0.26453840\n",
      "Iteration 77, loss = 0.26792239\n",
      "Iteration 81, loss = 0.26415736\n",
      "Iteration 78, loss = 0.26756381\n",
      "Iteration 82, loss = 0.26409470\n",
      "Iteration 79, loss = 0.26700463\n",
      "Iteration 83, loss = 0.26433860\n",
      "Iteration 80, loss = 0.26675910\n",
      "Iteration 81, loss = 0.26657847\n",
      "Iteration 84, loss = 0.26458040\n",
      "Iteration 82, loss = 0.26644129\n",
      "Iteration 85, loss = 0.26413796\n",
      "Iteration 83, loss = 0.26670692\n",
      "Iteration 86, loss = 0.26437939\n",
      "Iteration 84, loss = 0.26708123\n",
      "Iteration 87, loss = 0.26433480\n",
      "Iteration 85, loss = 0.26644166\n",
      "Iteration 88, loss = 0.26413596\n",
      "Iteration 86, loss = 0.26673101\n",
      "Iteration 89, loss = 0.26384589\n",
      "Iteration 87, loss = 0.26653091\n",
      "Iteration 90, loss = 0.26382441\n",
      "Iteration 88, loss = 0.26650549\n",
      "Iteration 91, loss = 0.26337551\n",
      "Iteration 89, loss = 0.26650515\n",
      "Iteration 92, loss = 0.26352647\n",
      "Iteration 90, loss = 0.26615468\n",
      "Iteration 93, loss = 0.26365173\n",
      "Iteration 91, loss = 0.26584265\n",
      "Iteration 94, loss = 0.26360795\n",
      "Iteration 92, loss = 0.26594394\n",
      "Iteration 95, loss = 0.26317628\n",
      "Iteration 93, loss = 0.26603858\n",
      "Iteration 96, loss = 0.26347192\n",
      "Iteration 94, loss = 0.26578557\n",
      "Iteration 97, loss = 0.26297112\n",
      "Iteration 95, loss = 0.26571900\n",
      "Iteration 98, loss = 0.26310960\n",
      "Iteration 96, loss = 0.26600691\n",
      "Iteration 99, loss = 0.26340729\n",
      "Iteration 97, loss = 0.26554458\n",
      "Iteration 100, loss = 0.26327183\n",
      "Iteration 98, loss = 0.26584397\n",
      "Iteration 101, loss = 0.26368157\n",
      "Iteration 99, loss = 0.26574717\n",
      "Iteration 102, loss = 0.26550600\n",
      "Iteration 100, loss = 0.26612024\n",
      "Iteration 103, loss = 0.26273954\n",
      "Iteration 101, loss = 0.26581067\n",
      "Iteration 102, loss = 0.26755637\n",
      "Iteration 104, loss = 0.26240969\n",
      "Iteration 105, loss = 0.26227572\n",
      "Iteration 103, loss = 0.26536191\n",
      "Iteration 106, loss = 0.26245657\n",
      "Iteration 104, loss = 0.26514241\n",
      "Iteration 107, loss = 0.26260934\n",
      "Iteration 105, loss = 0.26514497\n",
      "Iteration 108, loss = 0.26223133\n",
      "Iteration 106, loss = 0.26547999\n",
      "Iteration 109, loss = 0.26240925\n",
      "Iteration 107, loss = 0.26541436\n",
      "Iteration 110, loss = 0.26217355\n",
      "Iteration 108, loss = 0.26518674\n",
      "Iteration 111, loss = 0.26202343\n",
      "Iteration 109, loss = 0.26532128\n",
      "Iteration 112, loss = 0.26187799\n",
      "Iteration 110, loss = 0.26505308\n",
      "Iteration 113, loss = 0.26205057\n",
      "Iteration 111, loss = 0.26492374\n",
      "Iteration 114, loss = 0.26200408\n",
      "Iteration 112, loss = 0.26505962\n",
      "Iteration 115, loss = 0.26212953\n",
      "Iteration 113, loss = 0.26490800\n",
      "Iteration 116, loss = 0.26182882\n",
      "Iteration 114, loss = 0.26478948\n",
      "Iteration 117, loss = 0.26182583\n",
      "Iteration 115, loss = 0.26490033\n",
      "Iteration 118, loss = 0.26185777\n",
      "Iteration 119, loss = 0.26153244\n",
      "Iteration 116, loss = 0.26472940\n",
      "Iteration 120, loss = 0.26155073\n",
      "Iteration 117, loss = 0.26468584\n",
      "Iteration 121, loss = 0.26238415\n",
      "Iteration 118, loss = 0.26457921\n",
      "Iteration 122, loss = 0.26169738\n",
      "Iteration 119, loss = 0.26457215\n",
      "Iteration 123, loss = 0.26161466\n",
      "Iteration 120, loss = 0.26489939\n",
      "Iteration 124, loss = 0.26176588\n",
      "Iteration 121, loss = 0.26485862\n",
      "Iteration 125, loss = 0.26159661\n",
      "Iteration 122, loss = 0.26474119\n",
      "Iteration 126, loss = 0.26126954\n",
      "Iteration 123, loss = 0.26468830\n",
      "Iteration 127, loss = 0.26148857Iteration 124, loss = 0.26453871\n",
      "\n",
      "Iteration 128, loss = 0.26160988\n",
      "Iteration 125, loss = 0.26446339\n",
      "Iteration 126, loss = 0.26420229\n",
      "Iteration 129, loss = 0.26247573\n",
      "Iteration 130, loss = 0.26171438\n",
      "Iteration 127, loss = 0.26432406\n",
      "Iteration 131, loss = 0.26238686\n",
      "Iteration 128, loss = 0.26443563\n",
      "Iteration 132, loss = 0.26114160\n",
      "Iteration 129, loss = 0.26539659\n",
      "Iteration 133, loss = 0.26106575\n",
      "Iteration 130, loss = 0.26450250\n",
      "Iteration 131, loss = 0.26430995\n",
      "Iteration 134, loss = 0.26108214\n",
      "Iteration 132, loss = 0.26419128\n",
      "Iteration 135, loss = 0.26096853\n",
      "Iteration 133, loss = 0.26403678\n",
      "Iteration 136, loss = 0.26133222\n",
      "Iteration 134, loss = 0.26413556\n",
      "Iteration 137, loss = 0.26103197\n",
      "Iteration 138, loss = 0.26095622\n",
      "Iteration 135, loss = 0.26410616\n",
      "Iteration 139, loss = 0.26093793\n",
      "Iteration 136, loss = 0.26442507\n",
      "Iteration 137, loss = 0.26394226\n",
      "Iteration 140, loss = 0.26137162\n",
      "Iteration 138, loss = 0.26406938\n",
      "Iteration 141, loss = 0.26108806\n",
      "Iteration 139, loss = 0.26407502\n",
      "Iteration 142, loss = 0.26189354\n",
      "Iteration 143, loss = 0.26083948\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 140, loss = 0.26442693\n",
      "Iteration 141, loss = 0.26402304\n",
      "Iteration 1, loss = 0.34175172\n",
      "Iteration 142, loss = 0.26474196Iteration 2, loss = 0.29336547\n",
      "\n",
      "Iteration 3, loss = 0.28338415\n",
      "Iteration 143, loss = 0.26374660\n",
      "Iteration 4, loss = 0.28076351\n",
      "Iteration 5, loss = 0.27970917\n",
      "Iteration 144, loss = 0.26398299\n",
      "Iteration 145, loss = 0.26376251\n",
      "Iteration 6, loss = 0.27946294\n",
      "Iteration 146, loss = 0.26372340\n",
      "Iteration 7, loss = 0.27852413\n",
      "Iteration 8, loss = 0.27799231\n",
      "Iteration 147, loss = 0.26426305\n",
      "Iteration 9, loss = 0.27798132\n",
      "Iteration 148, loss = 0.26387437\n",
      "Iteration 10, loss = 0.27755027\n",
      "Iteration 11, loss = 0.27846748\n",
      "Iteration 149, loss = 0.26357920\n",
      "Iteration 12, loss = 0.27680459\n",
      "Iteration 150, loss = 0.26372546\n",
      "Iteration 151, loss = 0.26361170\n",
      "Iteration 13, loss = 0.27624419\n",
      "Iteration 152, loss = 0.26351118\n",
      "Iteration 14, loss = 0.27602573\n",
      "Iteration 153, loss = 0.26414331\n",
      "Iteration 15, loss = 0.27598557\n",
      "Iteration 154, loss = 0.26345172\n",
      "Iteration 16, loss = 0.27548824\n",
      "Iteration 155, loss = 0.26377655\n",
      "Iteration 17, loss = 0.27541975\n",
      "Iteration 156, loss = 0.26321291\n",
      "Iteration 18, loss = 0.27470992\n",
      "Iteration 157, loss = 0.26352773\n",
      "Iteration 19, loss = 0.27471695\n",
      "Iteration 158, loss = 0.26349451\n",
      "Iteration 20, loss = 0.27447700\n",
      "Iteration 159, loss = 0.26380928\n",
      "Iteration 21, loss = 0.27396966\n",
      "Iteration 160, loss = 0.26338292\n",
      "Iteration 22, loss = 0.27372903\n",
      "Iteration 23, loss = 0.27327851\n",
      "Iteration 161, loss = 0.26345366\n",
      "Iteration 24, loss = 0.27498304\n",
      "Iteration 162, loss = 0.26340980\n",
      "Iteration 25, loss = 0.27287402\n",
      "Iteration 163, loss = 0.26384024\n",
      "Iteration 26, loss = 0.27292118\n",
      "Iteration 164, loss = 0.26329422\n",
      "Iteration 27, loss = 0.27213555\n",
      "Iteration 165, loss = 0.26325676\n",
      "Iteration 28, loss = 0.27200822\n",
      "Iteration 166, loss = 0.26338847\n",
      "Iteration 29, loss = 0.27273672\n",
      "Iteration 167, loss = 0.26343336\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 30, loss = 0.27117410\n",
      "Iteration 31, loss = 0.27102052\n",
      "Iteration 1, loss = 0.34262065\n",
      "Iteration 32, loss = 0.27063307\n",
      "Iteration 2, loss = 0.29510472\n",
      "Iteration 33, loss = 0.27052187\n",
      "Iteration 3, loss = 0.28540793\n",
      "Iteration 34, loss = 0.27044614\n",
      "Iteration 4, loss = 0.28258614\n",
      "Iteration 35, loss = 0.26993529\n",
      "Iteration 5, loss = 0.28179930\n",
      "Iteration 36, loss = 0.27118331\n",
      "Iteration 6, loss = 0.28161165\n",
      "Iteration 37, loss = 0.27033559\n",
      "Iteration 7, loss = 0.28079487\n",
      "Iteration 38, loss = 0.26921719\n",
      "Iteration 8, loss = 0.28047452\n",
      "Iteration 39, loss = 0.26912383\n",
      "Iteration 9, loss = 0.28028469\n",
      "Iteration 40, loss = 0.27036076\n",
      "Iteration 10, loss = 0.28010206\n",
      "Iteration 41, loss = 0.26878798\n",
      "Iteration 11, loss = 0.28094356\n",
      "Iteration 42, loss = 0.26842696\n",
      "Iteration 12, loss = 0.27915097\n",
      "Iteration 43, loss = 0.26893225\n",
      "Iteration 13, loss = 0.27858749\n",
      "Iteration 44, loss = 0.26802839\n",
      "Iteration 14, loss = 0.27825800\n",
      "Iteration 45, loss = 0.26816566\n",
      "Iteration 15, loss = 0.27838932\n",
      "Iteration 46, loss = 0.26761948\n",
      "Iteration 16, loss = 0.27784988\n",
      "Iteration 47, loss = 0.26730172\n",
      "Iteration 48, loss = 0.26733626\n",
      "Iteration 17, loss = 0.27772706\n",
      "Iteration 18, loss = 0.27728191\n",
      "Iteration 49, loss = 0.26840130\n",
      "Iteration 19, loss = 0.27714849\n",
      "Iteration 50, loss = 0.26749757\n",
      "Iteration 20, loss = 0.27714068\n",
      "Iteration 51, loss = 0.26682897\n",
      "Iteration 21, loss = 0.27658326\n",
      "Iteration 52, loss = 0.26650715\n",
      "Iteration 22, loss = 0.27609360\n",
      "Iteration 53, loss = 0.26998230\n",
      "Iteration 23, loss = 0.27575667\n",
      "Iteration 54, loss = 0.26618010\n",
      "Iteration 24, loss = 0.27745232\n",
      "Iteration 55, loss = 0.26582642\n",
      "Iteration 56, loss = 0.26566955\n",
      "Iteration 25, loss = 0.27524815\n",
      "Iteration 26, loss = 0.27539714\n",
      "Iteration 57, loss = 0.26600547\n",
      "Iteration 27, loss = 0.27457552\n",
      "Iteration 58, loss = 0.26541894\n",
      "Iteration 28, loss = 0.27423116\n",
      "Iteration 59, loss = 0.26537352\n",
      "Iteration 29, loss = 0.27463634\n",
      "Iteration 60, loss = 0.26501164\n",
      "Iteration 30, loss = 0.27339887\n",
      "Iteration 61, loss = 0.26531387\n",
      "Iteration 31, loss = 0.27342628\n",
      "Iteration 62, loss = 0.26493572\n",
      "Iteration 32, loss = 0.27282422\n",
      "Iteration 63, loss = 0.26629708\n",
      "Iteration 33, loss = 0.27258822\n",
      "Iteration 64, loss = 0.26538613\n",
      "Iteration 34, loss = 0.27242613\n",
      "Iteration 65, loss = 0.26481302\n",
      "Iteration 35, loss = 0.27217964\n",
      "Iteration 66, loss = 0.26504320\n",
      "Iteration 36, loss = 0.27470204\n",
      "Iteration 67, loss = 0.26488006\n",
      "Iteration 37, loss = 0.27269923\n",
      "Iteration 68, loss = 0.26556905\n",
      "Iteration 69, loss = 0.26621429\n",
      "Iteration 38, loss = 0.27148191\n",
      "Iteration 70, loss = 0.26432213\n",
      "Iteration 39, loss = 0.27135417\n",
      "Iteration 40, loss = 0.27234736\n",
      "Iteration 71, loss = 0.26450069\n",
      "Iteration 41, loss = 0.27077648\n",
      "Iteration 72, loss = 0.26473705\n",
      "Iteration 42, loss = 0.27089911\n",
      "Iteration 73, loss = 0.26401724\n",
      "Iteration 43, loss = 0.27168481\n",
      "Iteration 74, loss = 0.26419565\n",
      "Iteration 44, loss = 0.27058119\n",
      "Iteration 45, loss = 0.27070648\n",
      "Iteration 75, loss = 0.26418394\n",
      "Iteration 46, loss = 0.27014014\n",
      "Iteration 76, loss = 0.26398119\n",
      "Iteration 47, loss = 0.26994605Iteration 77, loss = 0.26511989\n",
      "\n",
      "Iteration 48, loss = 0.27005178Iteration 78, loss = 0.26539249\n",
      "\n",
      "Iteration 49, loss = 0.27032338\n",
      "Iteration 79, loss = 0.26399111\n",
      "Iteration 80, loss = 0.26385076\n",
      "Iteration 50, loss = 0.27005079\n",
      "Iteration 81, loss = 0.26361587\n",
      "Iteration 51, loss = 0.26980698\n",
      "Iteration 82, loss = 0.26339856\n",
      "Iteration 52, loss = 0.26921506\n",
      "Iteration 83, loss = 0.26345568\n",
      "Iteration 53, loss = 0.27238554\n",
      "Iteration 84, loss = 0.26427303\n",
      "Iteration 54, loss = 0.26892693\n",
      "Iteration 85, loss = 0.26321521\n",
      "Iteration 55, loss = 0.26868231\n",
      "Iteration 86, loss = 0.26425517\n",
      "Iteration 56, loss = 0.26841327\n",
      "Iteration 87, loss = 0.26337210\n",
      "Iteration 57, loss = 0.26878288\n",
      "Iteration 88, loss = 0.26306721\n",
      "Iteration 58, loss = 0.26823340\n",
      "Iteration 89, loss = 0.26338481\n",
      "Iteration 59, loss = 0.26830381\n",
      "Iteration 90, loss = 0.26337649\n",
      "Iteration 60, loss = 0.26778172\n",
      "Iteration 91, loss = 0.26310247\n",
      "Iteration 61, loss = 0.26792455\n",
      "Iteration 92, loss = 0.26286084\n",
      "Iteration 93, loss = 0.26319824\n",
      "Iteration 62, loss = 0.26769990\n",
      "Iteration 94, loss = 0.26302002\n",
      "Iteration 95, loss = 0.26281171\n",
      "Iteration 63, loss = 0.26825056\n",
      "Iteration 96, loss = 0.26275261\n",
      "Iteration 64, loss = 0.26817261\n",
      "Iteration 97, loss = 0.26273226\n",
      "Iteration 65, loss = 0.26757694\n",
      "Iteration 98, loss = 0.26268404\n",
      "Iteration 66, loss = 0.26763082\n",
      "Iteration 99, loss = 0.26292187\n",
      "Iteration 67, loss = 0.26769907\n",
      "Iteration 100, loss = 0.26262669\n",
      "Iteration 68, loss = 0.26841613\n",
      "Iteration 101, loss = 0.26316208\n",
      "Iteration 69, loss = 0.26719739\n",
      "Iteration 102, loss = 0.26499354\n",
      "Iteration 70, loss = 0.26725238\n",
      "Iteration 103, loss = 0.26256138\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 71, loss = 0.26697613\n",
      "Iteration 72, loss = 0.26708631\n",
      "Iteration 1, loss = 0.34246031\n",
      "Iteration 73, loss = 0.26674275\n",
      "Iteration 2, loss = 0.29528088\n",
      "Iteration 74, loss = 0.26666066\n",
      "Iteration 3, loss = 0.28566147\n",
      "Iteration 75, loss = 0.26679609\n",
      "Iteration 4, loss = 0.28295944\n",
      "Iteration 76, loss = 0.26662299\n",
      "Iteration 5, loss = 0.28191069\n",
      "Iteration 77, loss = 0.26711102\n",
      "Iteration 6, loss = 0.28182063\n",
      "Iteration 7, loss = 0.28104741\n",
      "Iteration 78, loss = 0.26820949\n",
      "Iteration 8, loss = 0.28106226\n",
      "Iteration 79, loss = 0.26617797\n",
      "Iteration 9, loss = 0.28030668\n",
      "Iteration 80, loss = 0.26630695\n",
      "Iteration 10, loss = 0.27992129\n",
      "Iteration 81, loss = 0.26624955\n",
      "Iteration 11, loss = 0.28048540\n",
      "Iteration 82, loss = 0.26593091\n",
      "Iteration 12, loss = 0.27892885\n",
      "Iteration 13, loss = 0.27840797\n",
      "Iteration 83, loss = 0.26609337\n",
      "Iteration 14, loss = 0.27797443\n",
      "Iteration 84, loss = 0.26707162\n",
      "Iteration 15, loss = 0.27812326\n",
      "Iteration 16, loss = 0.27750597\n",
      "Iteration 85, loss = 0.26592938\n",
      "Iteration 17, loss = 0.27706835\n",
      "Iteration 86, loss = 0.26660332\n",
      "Iteration 18, loss = 0.27683622\n",
      "Iteration 87, loss = 0.26600308\n",
      "Iteration 19, loss = 0.27650196\n",
      "Iteration 20, loss = 0.27639921\n",
      "Iteration 88, loss = 0.26560103\n",
      "Iteration 21, loss = 0.27611135\n",
      "Iteration 22, loss = 0.27523011\n",
      "Iteration 89, loss = 0.26618836\n",
      "Iteration 23, loss = 0.27480799\n",
      "Iteration 90, loss = 0.26583201\n",
      "Iteration 91, loss = 0.26572787\n",
      "Iteration 92, loss = 0.26544095\n",
      "Iteration 24, loss = 0.27411709\n",
      "Iteration 93, loss = 0.26522180\n",
      "Iteration 25, loss = 0.27405621Iteration 94, loss = 0.26529005\n",
      "\n",
      "Iteration 95, loss = 0.26526297\n",
      "Iteration 26, loss = 0.27392363\n",
      "Iteration 96, loss = 0.26603694\n",
      "Iteration 27, loss = 0.27328374\n",
      "Iteration 97, loss = 0.26527431\n",
      "Iteration 28, loss = 0.27303708\n",
      "Iteration 98, loss = 0.26534435\n",
      "Iteration 29, loss = 0.27362945\n",
      "Iteration 99, loss = 0.26538428\n",
      "Iteration 30, loss = 0.27209202\n",
      "Iteration 31, loss = 0.27211506\n",
      "Iteration 100, loss = 0.26515320\n",
      "Iteration 32, loss = 0.27146031\n",
      "Iteration 101, loss = 0.26569723\n",
      "Iteration 33, loss = 0.27135813\n",
      "Iteration 102, loss = 0.26754113\n",
      "Iteration 34, loss = 0.27124830\n",
      "Iteration 103, loss = 0.26492933\n",
      "Iteration 35, loss = 0.27081339\n",
      "Iteration 104, loss = 0.26486672\n",
      "Iteration 36, loss = 0.27366059\n",
      "Iteration 105, loss = 0.26491152\n",
      "Iteration 37, loss = 0.27097851\n",
      "Iteration 106, loss = 0.26509612\n",
      "Iteration 38, loss = 0.26997541\n",
      "Iteration 107, loss = 0.26485175\n",
      "Iteration 39, loss = 0.26968112\n",
      "Iteration 108, loss = 0.26485920\n",
      "Iteration 40, loss = 0.27068293\n",
      "Iteration 109, loss = 0.26475920\n",
      "Iteration 41, loss = 0.26891448\n",
      "Iteration 110, loss = 0.26444412\n",
      "Iteration 42, loss = 0.26875252\n",
      "Iteration 111, loss = 0.26437791\n",
      "Iteration 43, loss = 0.26966507\n",
      "Iteration 112, loss = 0.26448054\n",
      "Iteration 44, loss = 0.26857108\n",
      "Iteration 113, loss = 0.26429278\n",
      "Iteration 45, loss = 0.26817370\n",
      "Iteration 114, loss = 0.26447819\n",
      "Iteration 46, loss = 0.26792564\n",
      "Iteration 115, loss = 0.26462122\n",
      "Iteration 47, loss = 0.26776516\n",
      "Iteration 116, loss = 0.26421068\n",
      "Iteration 117, loss = 0.26407652\n",
      "Iteration 48, loss = 0.26772035\n",
      "Iteration 118, loss = 0.26410997\n",
      "Iteration 49, loss = 0.26870635\n",
      "Iteration 119, loss = 0.26412699\n",
      "Iteration 120, loss = 0.26414346\n",
      "Iteration 121, loss = 0.26457622\n",
      "Iteration 50, loss = 0.26815438\n",
      "Iteration 122, loss = 0.26421394\n",
      "Iteration 123, loss = 0.26399690\n",
      "Iteration 51, loss = 0.26750694\n",
      "Iteration 52, loss = 0.26696215\n",
      "Iteration 124, loss = 0.26398900\n",
      "Iteration 53, loss = 0.26774900\n",
      "Iteration 125, loss = 0.26386475\n",
      "Iteration 54, loss = 0.26654143\n",
      "Iteration 126, loss = 0.26356270\n",
      "Iteration 55, loss = 0.26652941\n",
      "Iteration 127, loss = 0.26388073\n",
      "Iteration 128, loss = 0.26411723\n",
      "Iteration 56, loss = 0.26611843\n",
      "Iteration 57, loss = 0.26628235\n",
      "Iteration 129, loss = 0.26517364\n",
      "Iteration 58, loss = 0.26594263\n",
      "Iteration 130, loss = 0.26398142\n",
      "Iteration 59, loss = 0.26588835\n",
      "Iteration 131, loss = 0.26405848\n",
      "Iteration 132, loss = 0.26325507\n",
      "Iteration 60, loss = 0.26541297\n",
      "Iteration 133, loss = 0.26345635\n",
      "Iteration 61, loss = 0.26563126\n",
      "Iteration 134, loss = 0.26385749\n",
      "Iteration 135, loss = 0.26383031\n",
      "Iteration 62, loss = 0.26536280\n",
      "Iteration 136, loss = 0.26345808\n",
      "Iteration 63, loss = 0.26514102\n",
      "Iteration 137, loss = 0.26329740\n",
      "Iteration 64, loss = 0.26555646\n",
      "Iteration 138, loss = 0.26317534\n",
      "Iteration 65, loss = 0.26510469\n",
      "Iteration 139, loss = 0.26329623\n",
      "Iteration 66, loss = 0.26545053\n",
      "Iteration 140, loss = 0.26326870\n",
      "Iteration 67, loss = 0.26512196\n",
      "Iteration 141, loss = 0.26291558\n",
      "Iteration 68, loss = 0.26470406\n",
      "Iteration 142, loss = 0.26310278\n",
      "Iteration 69, loss = 0.26480043\n",
      "Iteration 143, loss = 0.26286707\n",
      "Iteration 144, loss = 0.26311227\n",
      "Iteration 70, loss = 0.26490578\n",
      "Iteration 145, loss = 0.26314404\n",
      "Iteration 71, loss = 0.26448970\n",
      "Iteration 72, loss = 0.26496567\n",
      "Iteration 146, loss = 0.26301945\n",
      "Iteration 147, loss = 0.26361177\n",
      "Iteration 73, loss = 0.26433438\n",
      "Iteration 148, loss = 0.26341004\n",
      "Iteration 74, loss = 0.26414505\n",
      "Iteration 149, loss = 0.26291294\n",
      "Iteration 75, loss = 0.26427880\n",
      "Iteration 150, loss = 0.26311984\n",
      "Iteration 76, loss = 0.26418544\n",
      "Iteration 151, loss = 0.26282274\n",
      "Iteration 152, loss = 0.26295093\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 77, loss = 0.26517776\n",
      "Iteration 78, loss = 0.26618425\n",
      "Iteration 79, loss = 0.26370369\n",
      "Iteration 1, loss = 0.34244651\n",
      "Iteration 80, loss = 0.26416230\n",
      "Iteration 81, loss = 0.26355862\n",
      "Iteration 2, loss = 0.29505896\n",
      "Iteration 82, loss = 0.26346406\n",
      "Iteration 3, loss = 0.28497603\n",
      "Iteration 83, loss = 0.26333013\n",
      "Iteration 4, loss = 0.28238573\n",
      "Iteration 84, loss = 0.26410920\n",
      "Iteration 5, loss = 0.28219499\n",
      "Iteration 85, loss = 0.26329058\n",
      "Iteration 6, loss = 0.28153092\n",
      "Iteration 86, loss = 0.26386238\n",
      "Iteration 7, loss = 0.28064641\n",
      "Iteration 87, loss = 0.26334134\n",
      "Iteration 8, loss = 0.28001329\n",
      "Iteration 88, loss = 0.26304414\n",
      "Iteration 9, loss = 0.28024223\n",
      "Iteration 89, loss = 0.26346321\n",
      "Iteration 10, loss = 0.27983315\n",
      "Iteration 90, loss = 0.26318310\n",
      "Iteration 11, loss = 0.27937483\n",
      "Iteration 91, loss = 0.26326036\n",
      "Iteration 12, loss = 0.27904116\n",
      "Iteration 92, loss = 0.26301616\n",
      "Iteration 13, loss = 0.27858098\n",
      "Iteration 93, loss = 0.26268755\n",
      "Iteration 14, loss = 0.27841709\n",
      "Iteration 94, loss = 0.26300160\n",
      "Iteration 15, loss = 0.27812446\n",
      "Iteration 95, loss = 0.26271126\n",
      "Iteration 16, loss = 0.27794299\n",
      "Iteration 96, loss = 0.26411760\n",
      "Iteration 17, loss = 0.27805274\n",
      "Iteration 97, loss = 0.26281113\n",
      "Iteration 98, loss = 0.26275177\n",
      "Iteration 18, loss = 0.27708994\n",
      "Iteration 99, loss = 0.26301315\n",
      "Iteration 19, loss = 0.27649661\n",
      "Iteration 100, loss = 0.26258184\n",
      "Iteration 20, loss = 0.27638969\n",
      "Iteration 101, loss = 0.26303342\n",
      "Iteration 21, loss = 0.27602316\n",
      "Iteration 22, loss = 0.27568534Iteration 102, loss = 0.26441509\n",
      "\n",
      "Iteration 103, loss = 0.26240100Iteration 23, loss = 0.27607684\n",
      "\n",
      "Iteration 104, loss = 0.26243918\n",
      "Iteration 24, loss = 0.27644688\n",
      "Iteration 105, loss = 0.26237575\n",
      "Iteration 25, loss = 0.27504098\n",
      "Iteration 106, loss = 0.26260617\n",
      "Iteration 107, loss = 0.26264978\n",
      "Iteration 26, loss = 0.27551198\n",
      "Iteration 108, loss = 0.26242504\n",
      "Iteration 27, loss = 0.27486381\n",
      "Iteration 109, loss = 0.26248554\n",
      "Iteration 110, loss = 0.26209750\n",
      "Iteration 28, loss = 0.27466293\n",
      "Iteration 111, loss = 0.26204870\n",
      "Iteration 29, loss = 0.27439301\n",
      "Iteration 112, loss = 0.26220314\n",
      "Iteration 30, loss = 0.27430484\n",
      "Iteration 113, loss = 0.26202298\n",
      "Iteration 114, loss = 0.26198831\n",
      "Iteration 31, loss = 0.27361308\n",
      "Iteration 115, loss = 0.26239746\n",
      "Iteration 32, loss = 0.27380227\n",
      "Iteration 116, loss = 0.26197319\n",
      "Iteration 117, loss = 0.26200160\n",
      "Iteration 33, loss = 0.27356265\n",
      "Iteration 118, loss = 0.26197221\n",
      "Iteration 119, loss = 0.26199163\n",
      "Iteration 34, loss = 0.27363207\n",
      "Iteration 120, loss = 0.26234643\n",
      "Iteration 121, loss = 0.26206873\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 35, loss = 0.27288001\n",
      "Iteration 36, loss = 0.27277704\n",
      "Iteration 37, loss = 0.27234156\n",
      "Iteration 38, loss = 0.27225144\n",
      "Iteration 39, loss = 0.27184626\n",
      "Iteration 40, loss = 0.27309725\n",
      "Iteration 41, loss = 0.27253104\n",
      "Iteration 42, loss = 0.27147478\n",
      "Iteration 43, loss = 0.27127821\n",
      "Iteration 44, loss = 0.27081092\n",
      "Iteration 45, loss = 0.27094607\n",
      "Iteration 46, loss = 0.27180028\n",
      "Iteration 47, loss = 0.27052296\n",
      "Iteration 48, loss = 0.27051904\n",
      "Iteration 49, loss = 0.27030686\n",
      "Iteration 50, loss = 0.26988162\n",
      "Iteration 51, loss = 0.27003815\n",
      "Iteration 52, loss = 0.27021608\n",
      "Iteration 53, loss = 0.26962164\n",
      "Iteration 54, loss = 0.26938577\n",
      "Iteration 55, loss = 0.26920794\n",
      "Iteration 56, loss = 0.26898246\n",
      "Iteration 57, loss = 0.26866163\n",
      "Iteration 58, loss = 0.26858535\n",
      "Iteration 59, loss = 0.26828515\n",
      "Iteration 60, loss = 0.26811410\n",
      "Iteration 61, loss = 0.26871419\n",
      "Iteration 62, loss = 0.26801328\n",
      "Iteration 63, loss = 0.26787883\n",
      "Iteration 64, loss = 0.26785283\n",
      "Iteration 65, loss = 0.26778603\n",
      "Iteration 66, loss = 0.26759613\n",
      "Iteration 67, loss = 0.26735744\n",
      "Iteration 68, loss = 0.26705598\n",
      "Iteration 69, loss = 0.26695441\n",
      "Iteration 70, loss = 0.26670043\n",
      "Iteration 71, loss = 0.26655301\n",
      "Iteration 72, loss = 0.26668060\n",
      "Iteration 73, loss = 0.26638689\n",
      "Iteration 74, loss = 0.26625206\n",
      "Iteration 75, loss = 0.26672253\n",
      "Iteration 76, loss = 0.26729632\n",
      "Iteration 77, loss = 0.26611857\n",
      "Iteration 78, loss = 0.26576528\n",
      "Iteration 79, loss = 0.26577503\n",
      "Iteration 80, loss = 0.26581733\n",
      "Iteration 81, loss = 0.26576429\n",
      "Iteration 82, loss = 0.26551970\n",
      "Iteration 83, loss = 0.26542861\n",
      "Iteration 84, loss = 0.26566863\n",
      "Iteration 85, loss = 0.26521500\n",
      "Iteration 86, loss = 0.26482984\n",
      "Iteration 87, loss = 0.26525480\n",
      "Iteration 88, loss = 0.26534202\n",
      "Iteration 89, loss = 0.26507721\n",
      "Iteration 90, loss = 0.26481440\n",
      "Iteration 91, loss = 0.26445407\n",
      "Iteration 92, loss = 0.26459685\n",
      "Iteration 93, loss = 0.26438652\n",
      "Iteration 94, loss = 0.26422277\n",
      "Iteration 95, loss = 0.26418809\n",
      "Iteration 96, loss = 0.26427012\n",
      "Iteration 97, loss = 0.26415884\n",
      "Iteration 98, loss = 0.26461478\n",
      "Iteration 99, loss = 0.26403988\n",
      "Iteration 100, loss = 0.26405323\n",
      "Iteration 101, loss = 0.26392349\n",
      "Iteration 102, loss = 0.26386537\n",
      "Iteration 103, loss = 0.26357913\n",
      "Iteration 104, loss = 0.26373021\n",
      "Iteration 105, loss = 0.26448703\n",
      "Iteration 106, loss = 0.26372631\n",
      "Iteration 107, loss = 0.26418580\n",
      "Iteration 108, loss = 0.26384152\n",
      "Iteration 109, loss = 0.26339245\n",
      "Iteration 110, loss = 0.26408194\n",
      "Iteration 111, loss = 0.26361281\n",
      "Iteration 112, loss = 0.26356444\n",
      "Iteration 113, loss = 0.26325812\n",
      "Iteration 114, loss = 0.26320757\n",
      "Iteration 115, loss = 0.26346248\n",
      "Iteration 116, loss = 0.26346348\n",
      "Iteration 117, loss = 0.26357434\n",
      "Iteration 118, loss = 0.26328870\n",
      "Iteration 119, loss = 0.26358212\n",
      "Iteration 120, loss = 0.26368554\n",
      "Iteration 121, loss = 0.26322368\n",
      "Iteration 122, loss = 0.26353496\n",
      "Iteration 123, loss = 0.26320054\n",
      "Iteration 124, loss = 0.26367998\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33421447\n",
      "Iteration 1, loss = 0.33640688\n",
      "Iteration 2, loss = 0.28866583\n",
      "Iteration 3, loss = 0.28012941\n",
      "Iteration 2, loss = 0.28963217\n",
      "Iteration 4, loss = 0.27776437\n",
      "Iteration 5, loss = 0.27667235\n",
      "Iteration 3, loss = 0.28070303\n",
      "Iteration 6, loss = 0.27611092\n",
      "Iteration 4, loss = 0.27844741\n",
      "Iteration 7, loss = 0.27560204\n",
      "Iteration 8, loss = 0.27548008\n",
      "Iteration 5, loss = 0.27708827\n",
      "Iteration 6, loss = 0.27655978\n",
      "Iteration 9, loss = 0.27455179\n",
      "Iteration 7, loss = 0.27605283\n",
      "Iteration 10, loss = 0.27410142\n",
      "Iteration 8, loss = 0.27559779\n",
      "Iteration 11, loss = 0.27370959\n",
      "Iteration 9, loss = 0.27526708\n",
      "Iteration 12, loss = 0.27342833\n",
      "Iteration 13, loss = 0.27273476\n",
      "Iteration 10, loss = 0.27479460\n",
      "Iteration 14, loss = 0.27235706\n",
      "Iteration 11, loss = 0.27434034\n",
      "Iteration 15, loss = 0.27216729\n",
      "Iteration 16, loss = 0.27154669\n",
      "Iteration 12, loss = 0.27381431\n",
      "Iteration 13, loss = 0.27339855\n",
      "Iteration 17, loss = 0.27122982\n",
      "Iteration 14, loss = 0.27300189\n",
      "Iteration 18, loss = 0.27081060\n",
      "Iteration 15, loss = 0.27254171\n",
      "Iteration 19, loss = 0.26995067\n",
      "Iteration 16, loss = 0.27248538\n",
      "Iteration 20, loss = 0.26963077\n",
      "Iteration 17, loss = 0.27152501\n",
      "Iteration 21, loss = 0.26936553\n",
      "Iteration 22, loss = 0.26908184\n",
      "Iteration 18, loss = 0.27078627\n",
      "Iteration 19, loss = 0.27015792\n",
      "Iteration 23, loss = 0.26876426\n",
      "Iteration 24, loss = 0.26789912\n",
      "Iteration 25, loss = 0.26748625\n",
      "Iteration 20, loss = 0.26965031\n",
      "Iteration 26, loss = 0.26718362\n",
      "Iteration 21, loss = 0.26910512\n",
      "Iteration 27, loss = 0.26711746\n",
      "Iteration 22, loss = 0.26865102\n",
      "Iteration 28, loss = 0.26663914\n",
      "Iteration 23, loss = 0.26823575\n",
      "Iteration 29, loss = 0.26666720\n",
      "Iteration 24, loss = 0.26831151\n",
      "Iteration 30, loss = 0.26608366\n",
      "Iteration 25, loss = 0.26820482\n",
      "Iteration 31, loss = 0.26532513\n",
      "Iteration 32, loss = 0.26509994\n",
      "Iteration 26, loss = 0.26756503\n",
      "Iteration 33, loss = 0.26482612\n",
      "Iteration 34, loss = 0.26458611\n",
      "Iteration 27, loss = 0.26739784\n",
      "Iteration 35, loss = 0.26430861\n",
      "Iteration 28, loss = 0.26706087\n",
      "Iteration 36, loss = 0.26401654\n",
      "Iteration 29, loss = 0.26666482\n",
      "Iteration 37, loss = 0.26401995\n",
      "Iteration 30, loss = 0.26681658\n",
      "Iteration 38, loss = 0.26375467\n",
      "Iteration 31, loss = 0.26617110\n",
      "Iteration 39, loss = 0.26375175\n",
      "Iteration 32, loss = 0.26612695\n",
      "Iteration 40, loss = 0.26384358\n",
      "Iteration 41, loss = 0.26368226\n",
      "Iteration 33, loss = 0.26571213\n",
      "Iteration 34, loss = 0.26583591\n",
      "Iteration 42, loss = 0.26316246\n",
      "Iteration 35, loss = 0.26558724\n",
      "Iteration 36, loss = 0.26521453\n",
      "Iteration 43, loss = 0.26326668\n",
      "Iteration 37, loss = 0.26550024\n",
      "Iteration 44, loss = 0.26317064\n",
      "Iteration 45, loss = 0.26319507\n",
      "Iteration 38, loss = 0.26508748\n",
      "Iteration 46, loss = 0.26277259\n",
      "Iteration 39, loss = 0.26516697\n",
      "Iteration 47, loss = 0.26358747\n",
      "Iteration 40, loss = 0.26509321\n",
      "Iteration 48, loss = 0.26320623\n",
      "Iteration 41, loss = 0.26497672\n",
      "Iteration 49, loss = 0.26259364\n",
      "Iteration 42, loss = 0.26490121\n",
      "Iteration 50, loss = 0.26286809\n",
      "Iteration 43, loss = 0.26483034\n",
      "Iteration 44, loss = 0.26473424\n",
      "Iteration 51, loss = 0.26284029\n",
      "Iteration 45, loss = 0.26437326\n",
      "Iteration 52, loss = 0.26228034\n",
      "Iteration 46, loss = 0.26427943\n",
      "Iteration 53, loss = 0.26232455\n",
      "Iteration 47, loss = 0.26413633\n",
      "Iteration 48, loss = 0.26409471\n",
      "Iteration 54, loss = 0.26217055\n",
      "Iteration 49, loss = 0.26418062\n",
      "Iteration 55, loss = 0.26246777\n",
      "Iteration 50, loss = 0.26401638\n",
      "Iteration 51, loss = 0.26390789\n",
      "Iteration 56, loss = 0.26210160\n",
      "Iteration 52, loss = 0.26371692\n",
      "Iteration 57, loss = 0.26209362\n",
      "Iteration 53, loss = 0.26353739\n",
      "Iteration 58, loss = 0.26213928\n",
      "Iteration 54, loss = 0.26365810\n",
      "Iteration 59, loss = 0.26226474\n",
      "Iteration 60, loss = 0.26171765\n",
      "Iteration 55, loss = 0.26373360\n",
      "Iteration 61, loss = 0.26180346\n",
      "Iteration 56, loss = 0.26323872\n",
      "Iteration 62, loss = 0.26173720\n",
      "Iteration 63, loss = 0.26150306\n",
      "Iteration 57, loss = 0.26349280\n",
      "Iteration 58, loss = 0.26315430\n",
      "Iteration 64, loss = 0.26164808\n",
      "Iteration 59, loss = 0.26332086\n",
      "Iteration 60, loss = 0.26296771\n",
      "Iteration 65, loss = 0.26148014\n",
      "Iteration 61, loss = 0.26284963\n",
      "Iteration 66, loss = 0.26145834\n",
      "Iteration 67, loss = 0.26147997\n",
      "Iteration 62, loss = 0.26305133Iteration 68, loss = 0.26140103\n",
      "\n",
      "Iteration 69, loss = 0.26113015\n",
      "Iteration 70, loss = 0.26173024\n",
      "Iteration 63, loss = 0.26272151\n",
      "Iteration 71, loss = 0.26124661\n",
      "Iteration 64, loss = 0.26248818\n",
      "Iteration 72, loss = 0.26152626\n",
      "Iteration 73, loss = 0.26130262\n",
      "Iteration 65, loss = 0.26249221\n",
      "Iteration 66, loss = 0.26267668\n",
      "Iteration 74, loss = 0.26119978\n",
      "Iteration 75, loss = 0.26109127\n",
      "Iteration 67, loss = 0.26248884\n",
      "Iteration 68, loss = 0.26288347\n",
      "Iteration 76, loss = 0.26094726\n",
      "Iteration 69, loss = 0.26257192\n",
      "Iteration 70, loss = 0.26252165\n",
      "Iteration 77, loss = 0.26099453\n",
      "Iteration 71, loss = 0.26233819\n",
      "Iteration 78, loss = 0.26071150\n",
      "Iteration 72, loss = 0.26186184\n",
      "Iteration 79, loss = 0.26084017\n",
      "Iteration 73, loss = 0.26189226Iteration 80, loss = 0.26070352\n",
      "\n",
      "Iteration 81, loss = 0.26079900\n",
      "Iteration 82, loss = 0.26072668\n",
      "Iteration 74, loss = 0.26214619\n",
      "Iteration 83, loss = 0.26095825\n",
      "Iteration 75, loss = 0.26197122\n",
      "Iteration 84, loss = 0.26105172\n",
      "Iteration 85, loss = 0.26055237\n",
      "Iteration 76, loss = 0.26186556\n",
      "Iteration 86, loss = 0.26069806\n",
      "Iteration 87, loss = 0.26052088\n",
      "Iteration 77, loss = 0.26186757\n",
      "Iteration 88, loss = 0.26030272\n",
      "Iteration 78, loss = 0.26165910\n",
      "Iteration 89, loss = 0.26042064\n",
      "Iteration 90, loss = 0.26026498\n",
      "Iteration 79, loss = 0.26198050\n",
      "Iteration 91, loss = 0.26016835\n",
      "Iteration 92, loss = 0.26025527\n",
      "Iteration 80, loss = 0.26243079\n",
      "Iteration 93, loss = 0.26018516\n",
      "Iteration 81, loss = 0.26174417\n",
      "Iteration 94, loss = 0.26026167\n",
      "Iteration 82, loss = 0.26160271\n",
      "Iteration 95, loss = 0.26018003\n",
      "Iteration 83, loss = 0.26137896\n",
      "Iteration 96, loss = 0.26025229\n",
      "Iteration 84, loss = 0.26149926\n",
      "Iteration 97, loss = 0.26004500\n",
      "Iteration 85, loss = 0.26182316\n",
      "Iteration 98, loss = 0.26018693\n",
      "Iteration 86, loss = 0.26170057\n",
      "Iteration 99, loss = 0.26001389\n",
      "Iteration 87, loss = 0.26142610\n",
      "Iteration 100, loss = 0.25965185\n",
      "Iteration 88, loss = 0.26140484\n",
      "Iteration 101, loss = 0.25985350\n",
      "Iteration 89, loss = 0.26165481\n",
      "Iteration 102, loss = 0.26004132\n",
      "Iteration 90, loss = 0.26125897\n",
      "Iteration 103, loss = 0.25995492\n",
      "Iteration 91, loss = 0.26140907\n",
      "Iteration 104, loss = 0.25988192\n",
      "Iteration 92, loss = 0.26118824\n",
      "Iteration 105, loss = 0.25978294\n",
      "Iteration 93, loss = 0.26136274\n",
      "Iteration 106, loss = 0.26008974\n",
      "Iteration 94, loss = 0.26119106\n",
      "Iteration 107, loss = 0.25969699\n",
      "Iteration 95, loss = 0.26128335\n",
      "Iteration 108, loss = 0.25961305\n",
      "Iteration 96, loss = 0.26101723\n",
      "Iteration 109, loss = 0.25980413\n",
      "Iteration 97, loss = 0.26129913\n",
      "Iteration 110, loss = 0.25963056\n",
      "Iteration 98, loss = 0.26133953\n",
      "Iteration 99, loss = 0.26101694\n",
      "Iteration 111, loss = 0.25952940\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 100, loss = 0.26142256\n",
      "Iteration 101, loss = 0.26109746\n",
      "Iteration 1, loss = 0.33511115\n",
      "Iteration 102, loss = 0.26126871\n",
      "Iteration 2, loss = 0.28897219\n",
      "Iteration 103, loss = 0.26103879\n",
      "Iteration 3, loss = 0.28009022\n",
      "Iteration 104, loss = 0.26101066\n",
      "Iteration 4, loss = 0.27791435\n",
      "Iteration 105, loss = 0.26105381\n",
      "Iteration 106, loss = 0.26091713\n",
      "Iteration 5, loss = 0.27655694\n",
      "Iteration 107, loss = 0.26116835\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 6, loss = 0.27619379\n",
      "Iteration 7, loss = 0.27542210\n",
      "Iteration 8, loss = 0.27537208\n",
      "Iteration 1, loss = 0.33630668\n",
      "Iteration 9, loss = 0.27414921\n",
      "Iteration 2, loss = 0.28991128\n",
      "Iteration 10, loss = 0.27405626\n",
      "Iteration 3, loss = 0.28104097\n",
      "Iteration 11, loss = 0.27358388\n",
      "Iteration 12, loss = 0.27317983\n",
      "Iteration 4, loss = 0.27861107\n",
      "Iteration 5, loss = 0.27717012\n",
      "Iteration 13, loss = 0.27277106\n",
      "Iteration 6, loss = 0.27681116\n",
      "Iteration 14, loss = 0.27251563\n",
      "Iteration 7, loss = 0.27600155\n",
      "Iteration 15, loss = 0.27215176\n",
      "Iteration 16, loss = 0.27145652\n",
      "Iteration 8, loss = 0.27594664\n",
      "Iteration 17, loss = 0.27112399\n",
      "Iteration 9, loss = 0.27460532\n",
      "Iteration 18, loss = 0.27112587\n",
      "Iteration 10, loss = 0.27456717\n",
      "Iteration 19, loss = 0.27023479\n",
      "Iteration 11, loss = 0.27421031\n",
      "Iteration 20, loss = 0.26982906\n",
      "Iteration 12, loss = 0.27369669\n",
      "Iteration 21, loss = 0.26944930\n",
      "Iteration 13, loss = 0.27310458\n",
      "Iteration 22, loss = 0.26957809\n",
      "Iteration 14, loss = 0.27268101\n",
      "Iteration 15, loss = 0.27249882\n",
      "Iteration 23, loss = 0.26891447\n",
      "Iteration 16, loss = 0.27201933\n",
      "Iteration 24, loss = 0.26854851\n",
      "Iteration 17, loss = 0.27149224\n",
      "Iteration 25, loss = 0.26798937\n",
      "Iteration 18, loss = 0.27132438\n",
      "Iteration 26, loss = 0.26764420\n",
      "Iteration 19, loss = 0.27035916\n",
      "Iteration 27, loss = 0.26767385\n",
      "Iteration 20, loss = 0.27009269\n",
      "Iteration 28, loss = 0.26725210\n",
      "Iteration 21, loss = 0.26963171\n",
      "Iteration 29, loss = 0.26678388\n",
      "Iteration 22, loss = 0.26963835\n",
      "Iteration 30, loss = 0.26663055\n",
      "Iteration 23, loss = 0.26888659\n",
      "Iteration 31, loss = 0.26608830\n",
      "Iteration 24, loss = 0.26847245\n",
      "Iteration 32, loss = 0.26577502\n",
      "Iteration 33, loss = 0.26565445\n",
      "Iteration 25, loss = 0.26825904\n",
      "Iteration 26, loss = 0.26800855\n",
      "Iteration 34, loss = 0.26532916\n",
      "Iteration 27, loss = 0.26794668\n",
      "Iteration 35, loss = 0.26478358\n",
      "Iteration 28, loss = 0.26754871\n",
      "Iteration 36, loss = 0.26443069\n",
      "Iteration 29, loss = 0.26705119\n",
      "Iteration 37, loss = 0.26455141\n",
      "Iteration 38, loss = 0.26405946\n",
      "Iteration 30, loss = 0.26702078\n",
      "Iteration 39, loss = 0.26392179\n",
      "Iteration 31, loss = 0.26670582\n",
      "Iteration 40, loss = 0.26398113\n",
      "Iteration 32, loss = 0.26605686\n",
      "Iteration 41, loss = 0.26391276\n",
      "Iteration 33, loss = 0.26604996\n",
      "Iteration 42, loss = 0.26326029\n",
      "Iteration 34, loss = 0.26607376\n",
      "Iteration 43, loss = 0.26313294\n",
      "Iteration 35, loss = 0.26558853\n",
      "Iteration 44, loss = 0.26305958\n",
      "Iteration 36, loss = 0.26545883\n",
      "Iteration 45, loss = 0.26287101\n",
      "Iteration 37, loss = 0.26545273\n",
      "Iteration 38, loss = 0.26496340\n",
      "Iteration 46, loss = 0.26279047\n",
      "Iteration 39, loss = 0.26496194\n",
      "Iteration 47, loss = 0.26362962\n",
      "Iteration 40, loss = 0.26500299\n",
      "Iteration 48, loss = 0.26281927\n",
      "Iteration 49, loss = 0.26257580\n",
      "Iteration 41, loss = 0.26467169\n",
      "Iteration 50, loss = 0.26256937\n",
      "Iteration 42, loss = 0.26421071\n",
      "Iteration 51, loss = 0.26297979\n",
      "Iteration 43, loss = 0.26416274\n",
      "Iteration 52, loss = 0.26203897\n",
      "Iteration 44, loss = 0.26374668\n",
      "Iteration 53, loss = 0.26190850\n",
      "Iteration 45, loss = 0.26392429\n",
      "Iteration 54, loss = 0.26196869\n",
      "Iteration 46, loss = 0.26352570\n",
      "Iteration 55, loss = 0.26214287\n",
      "Iteration 47, loss = 0.26404692\n",
      "Iteration 56, loss = 0.26156993\n",
      "Iteration 48, loss = 0.26381396\n",
      "Iteration 57, loss = 0.26184470\n",
      "Iteration 49, loss = 0.26333148\n",
      "Iteration 58, loss = 0.26184905\n",
      "Iteration 50, loss = 0.26321047\n",
      "Iteration 59, loss = 0.26201179\n",
      "Iteration 51, loss = 0.26388561\n",
      "Iteration 60, loss = 0.26147837\n",
      "Iteration 52, loss = 0.26295302\n",
      "Iteration 53, loss = 0.26316175\n",
      "Iteration 61, loss = 0.26142713\n",
      "Iteration 54, loss = 0.26284495\n",
      "Iteration 62, loss = 0.26153787\n",
      "Iteration 55, loss = 0.26268169\n",
      "Iteration 63, loss = 0.26137713\n",
      "Iteration 56, loss = 0.26255081\n",
      "Iteration 64, loss = 0.26118407\n",
      "Iteration 57, loss = 0.26269770\n",
      "Iteration 65, loss = 0.26129445\n",
      "Iteration 58, loss = 0.26255176\n",
      "Iteration 66, loss = 0.26121218\n",
      "Iteration 59, loss = 0.26260406\n",
      "Iteration 67, loss = 0.26108501\n",
      "Iteration 60, loss = 0.26248030\n",
      "Iteration 68, loss = 0.26105323\n",
      "Iteration 61, loss = 0.26224940\n",
      "Iteration 69, loss = 0.26078589\n",
      "Iteration 62, loss = 0.26247605\n",
      "Iteration 70, loss = 0.26081493\n",
      "Iteration 63, loss = 0.26209654\n",
      "Iteration 64, loss = 0.26203220\n",
      "Iteration 71, loss = 0.26077866\n",
      "Iteration 65, loss = 0.26183721\n",
      "Iteration 72, loss = 0.26096117\n",
      "Iteration 66, loss = 0.26201780\n",
      "Iteration 73, loss = 0.26084831\n",
      "Iteration 67, loss = 0.26200891\n",
      "Iteration 74, loss = 0.26100813\n",
      "Iteration 75, loss = 0.26095378\n",
      "Iteration 68, loss = 0.26175741\n",
      "Iteration 76, loss = 0.26087848\n",
      "Iteration 69, loss = 0.26184660\n",
      "Iteration 77, loss = 0.26043912\n",
      "Iteration 70, loss = 0.26155378\n",
      "Iteration 78, loss = 0.26065459\n",
      "Iteration 71, loss = 0.26162473\n",
      "Iteration 79, loss = 0.26058708\n",
      "Iteration 72, loss = 0.26195602\n",
      "Iteration 80, loss = 0.26051648\n",
      "Iteration 73, loss = 0.26159770\n",
      "Iteration 81, loss = 0.26072640\n",
      "Iteration 74, loss = 0.26159304\n",
      "Iteration 82, loss = 0.26056901\n",
      "Iteration 75, loss = 0.26169020\n",
      "Iteration 76, loss = 0.26117516\n",
      "Iteration 83, loss = 0.26069317\n",
      "Iteration 77, loss = 0.26119707\n",
      "Iteration 84, loss = 0.26052913\n",
      "Iteration 78, loss = 0.26130422\n",
      "Iteration 85, loss = 0.26021385\n",
      "Iteration 79, loss = 0.26135162\n",
      "Iteration 80, loss = 0.26109387\n",
      "Iteration 86, loss = 0.26042073\n",
      "Iteration 81, loss = 0.26126760\n",
      "Iteration 82, loss = 0.26142330\n",
      "Iteration 87, loss = 0.26037974\n",
      "Iteration 83, loss = 0.26152282\n",
      "Iteration 88, loss = 0.25997678\n",
      "Iteration 89, loss = 0.26030687\n",
      "Iteration 84, loss = 0.26161936\n",
      "Iteration 90, loss = 0.26007394\n",
      "Iteration 91, loss = 0.26011535\n",
      "Iteration 92, loss = 0.26016786\n",
      "Iteration 85, loss = 0.26091120\n",
      "Iteration 93, loss = 0.25995072\n",
      "Iteration 86, loss = 0.26114149\n",
      "Iteration 87, loss = 0.26086921\n",
      "Iteration 94, loss = 0.25998480\n",
      "Iteration 95, loss = 0.25998895\n",
      "Iteration 88, loss = 0.26068227\n",
      "Iteration 89, loss = 0.26073892\n",
      "Iteration 96, loss = 0.25998297\n",
      "Iteration 90, loss = 0.26070392\n",
      "Iteration 97, loss = 0.25987791\n",
      "Iteration 91, loss = 0.26066558\n",
      "Iteration 98, loss = 0.25996881\n",
      "Iteration 92, loss = 0.26068148\n",
      "Iteration 99, loss = 0.25970198\n",
      "Iteration 93, loss = 0.26045172\n",
      "Iteration 100, loss = 0.25968764\n",
      "Iteration 94, loss = 0.26066612\n",
      "Iteration 95, loss = 0.26070725\n",
      "Iteration 101, loss = 0.25963223\n",
      "Iteration 96, loss = 0.26077487Iteration 102, loss = 0.25994309\n",
      "\n",
      "Iteration 97, loss = 0.26066384\n",
      "Iteration 103, loss = 0.25961470\n",
      "Iteration 98, loss = 0.26069885\n",
      "Iteration 104, loss = 0.25969125\n",
      "Iteration 99, loss = 0.26047636\n",
      "Iteration 105, loss = 0.25963591\n",
      "Iteration 100, loss = 0.26038352\n",
      "Iteration 106, loss = 0.25978822\n",
      "Iteration 101, loss = 0.26018519\n",
      "Iteration 107, loss = 0.25949213\n",
      "Iteration 102, loss = 0.26074089\n",
      "Iteration 108, loss = 0.25944867\n",
      "Iteration 103, loss = 0.26051098\n",
      "Iteration 109, loss = 0.25975125\n",
      "Iteration 104, loss = 0.26038852\n",
      "Iteration 110, loss = 0.25946484\n",
      "Iteration 111, loss = 0.25940155\n",
      "Iteration 105, loss = 0.26037163\n",
      "Iteration 112, loss = 0.25941088\n",
      "Iteration 106, loss = 0.26049818\n",
      "Iteration 113, loss = 0.25937562\n",
      "Iteration 114, loss = 0.25921739\n",
      "Iteration 107, loss = 0.26037462\n",
      "Iteration 115, loss = 0.25935279\n",
      "Iteration 108, loss = 0.26034605\n",
      "Iteration 116, loss = 0.25923646\n",
      "Iteration 109, loss = 0.26047797\n",
      "Iteration 117, loss = 0.25941595\n",
      "Iteration 110, loss = 0.26024750\n",
      "Iteration 118, loss = 0.25922518\n",
      "Iteration 111, loss = 0.26016811\n",
      "Iteration 119, loss = 0.25944702\n",
      "Iteration 112, loss = 0.26020873\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 120, loss = 0.25911896\n",
      "Iteration 1, loss = 0.33605295\n",
      "Iteration 121, loss = 0.25904665\n",
      "Iteration 2, loss = 0.28988828\n",
      "Iteration 122, loss = 0.25920304\n",
      "Iteration 3, loss = 0.28101638\n",
      "Iteration 123, loss = 0.25904263\n",
      "Iteration 4, loss = 0.27851916\n",
      "Iteration 124, loss = 0.25903264\n",
      "Iteration 5, loss = 0.27703563\n",
      "Iteration 125, loss = 0.25899861\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 6, loss = 0.27695444\n",
      "Iteration 7, loss = 0.27602778\n",
      "Iteration 1, loss = 0.33646822\n",
      "Iteration 8, loss = 0.27611408\n",
      "Iteration 2, loss = 0.29104699\n",
      "Iteration 9, loss = 0.27490829\n",
      "Iteration 3, loss = 0.28216071\n",
      "Iteration 10, loss = 0.27479834\n",
      "Iteration 4, loss = 0.27965544\n",
      "Iteration 11, loss = 0.27445245\n",
      "Iteration 5, loss = 0.27850537\n",
      "Iteration 12, loss = 0.27397328\n",
      "Iteration 6, loss = 0.27830974\n",
      "Iteration 13, loss = 0.27323089\n",
      "Iteration 7, loss = 0.27720338\n",
      "Iteration 14, loss = 0.27307104\n",
      "Iteration 15, loss = 0.27326723\n",
      "Iteration 8, loss = 0.27772850\n",
      "Iteration 16, loss = 0.27218253\n",
      "Iteration 9, loss = 0.27615640\n",
      "Iteration 17, loss = 0.27193351\n",
      "Iteration 10, loss = 0.27597991\n",
      "Iteration 18, loss = 0.27136572\n",
      "Iteration 11, loss = 0.27578241\n",
      "Iteration 19, loss = 0.27057554\n",
      "Iteration 12, loss = 0.27494134\n",
      "Iteration 20, loss = 0.27021438\n",
      "Iteration 13, loss = 0.27437957\n",
      "Iteration 14, loss = 0.27424493\n",
      "Iteration 21, loss = 0.26947082\n",
      "Iteration 15, loss = 0.27429226\n",
      "Iteration 22, loss = 0.26955642\n",
      "Iteration 16, loss = 0.27320061\n",
      "Iteration 23, loss = 0.26865417\n",
      "Iteration 17, loss = 0.27279615\n",
      "Iteration 24, loss = 0.26821404\n",
      "Iteration 25, loss = 0.26778773\n",
      "Iteration 18, loss = 0.27224674\n",
      "Iteration 26, loss = 0.26745798\n",
      "Iteration 19, loss = 0.27173443\n",
      "Iteration 27, loss = 0.26728604\n",
      "Iteration 20, loss = 0.27133980\n",
      "Iteration 28, loss = 0.26697904\n",
      "Iteration 21, loss = 0.27086881\n",
      "Iteration 29, loss = 0.26631954\n",
      "Iteration 22, loss = 0.27093973\n",
      "Iteration 30, loss = 0.26649181\n",
      "Iteration 23, loss = 0.26998486\n",
      "Iteration 31, loss = 0.26593415\n",
      "Iteration 24, loss = 0.26972559\n",
      "Iteration 25, loss = 0.26961707\n",
      "Iteration 32, loss = 0.26527605\n",
      "Iteration 26, loss = 0.26926001\n",
      "Iteration 33, loss = 0.26517753\n",
      "Iteration 27, loss = 0.26939874\n",
      "Iteration 34, loss = 0.26514525\n",
      "Iteration 28, loss = 0.26879442\n",
      "Iteration 35, loss = 0.26460380\n",
      "Iteration 29, loss = 0.26859203\n",
      "Iteration 36, loss = 0.26477844\n",
      "Iteration 30, loss = 0.26872203\n",
      "Iteration 37, loss = 0.26449606\n",
      "Iteration 31, loss = 0.26832434\n",
      "Iteration 38, loss = 0.26432393\n",
      "Iteration 32, loss = 0.26786872\n",
      "Iteration 39, loss = 0.26405475\n",
      "Iteration 33, loss = 0.26784338\n",
      "Iteration 40, loss = 0.26416329\n",
      "Iteration 34, loss = 0.26804326\n",
      "Iteration 41, loss = 0.26366156\n",
      "Iteration 35, loss = 0.26741278\n",
      "Iteration 42, loss = 0.26349826\n",
      "Iteration 36, loss = 0.26748295\n",
      "Iteration 43, loss = 0.26340097\n",
      "Iteration 37, loss = 0.26717860\n",
      "Iteration 44, loss = 0.26306255\n",
      "Iteration 38, loss = 0.26714136\n",
      "Iteration 45, loss = 0.26323088\n",
      "Iteration 39, loss = 0.26675316\n",
      "Iteration 46, loss = 0.26287825\n",
      "Iteration 40, loss = 0.26704852\n",
      "Iteration 47, loss = 0.26354649\n",
      "Iteration 41, loss = 0.26658360\n",
      "Iteration 48, loss = 0.26331481\n",
      "Iteration 42, loss = 0.26634419\n",
      "Iteration 43, loss = 0.26630952Iteration 49, loss = 0.26271449\n",
      "\n",
      "Iteration 50, loss = 0.26277089\n",
      "Iteration 44, loss = 0.26590541\n",
      "Iteration 51, loss = 0.26331515\n",
      "Iteration 45, loss = 0.26584022\n",
      "Iteration 46, loss = 0.26580739\n",
      "Iteration 52, loss = 0.26248333\n",
      "Iteration 47, loss = 0.26595570\n",
      "Iteration 48, loss = 0.26602893\n",
      "Iteration 53, loss = 0.26231435\n",
      "Iteration 49, loss = 0.26566161\n",
      "Iteration 54, loss = 0.26234205\n",
      "Iteration 50, loss = 0.26562955\n",
      "Iteration 55, loss = 0.26219452\n",
      "Iteration 51, loss = 0.26570652\n",
      "Iteration 56, loss = 0.26226241\n",
      "Iteration 52, loss = 0.26527420\n",
      "Iteration 57, loss = 0.26220462\n",
      "Iteration 53, loss = 0.26494288\n",
      "Iteration 58, loss = 0.26219034\n",
      "Iteration 54, loss = 0.26490373\n",
      "Iteration 59, loss = 0.26201338\n",
      "Iteration 55, loss = 0.26479878\n",
      "Iteration 60, loss = 0.26211581\n",
      "Iteration 56, loss = 0.26463703\n",
      "Iteration 61, loss = 0.26186371\n",
      "Iteration 57, loss = 0.26493794\n",
      "Iteration 62, loss = 0.26222720\n",
      "Iteration 58, loss = 0.26459280\n",
      "Iteration 63, loss = 0.26174660\n",
      "Iteration 59, loss = 0.26446197\n",
      "Iteration 64, loss = 0.26163329\n",
      "Iteration 60, loss = 0.26462090\n",
      "Iteration 65, loss = 0.26157917\n",
      "Iteration 61, loss = 0.26438367\n",
      "Iteration 66, loss = 0.26171354\n",
      "Iteration 62, loss = 0.26456726\n",
      "Iteration 67, loss = 0.26129687\n",
      "Iteration 63, loss = 0.26407366\n",
      "Iteration 68, loss = 0.26121970\n",
      "Iteration 64, loss = 0.26407698\n",
      "Iteration 69, loss = 0.26130329\n",
      "Iteration 65, loss = 0.26393494\n",
      "Iteration 70, loss = 0.26127278\n",
      "Iteration 66, loss = 0.26423999\n",
      "Iteration 71, loss = 0.26110412\n",
      "Iteration 67, loss = 0.26377202\n",
      "Iteration 72, loss = 0.26134410\n",
      "Iteration 68, loss = 0.26378038\n",
      "Iteration 73, loss = 0.26115856\n",
      "Iteration 69, loss = 0.26354608\n",
      "Iteration 74, loss = 0.26081169\n",
      "Iteration 75, loss = 0.26113232\n",
      "Iteration 70, loss = 0.26366535\n",
      "Iteration 71, loss = 0.26363397\n",
      "Iteration 76, loss = 0.26092620\n",
      "Iteration 77, loss = 0.26094481\n",
      "Iteration 72, loss = 0.26365245\n",
      "Iteration 78, loss = 0.26094289\n",
      "Iteration 73, loss = 0.26364574\n",
      "Iteration 79, loss = 0.26098146\n",
      "Iteration 74, loss = 0.26353395\n",
      "Iteration 75, loss = 0.26358908\n",
      "Iteration 80, loss = 0.26065089\n",
      "Iteration 76, loss = 0.26294161\n",
      "Iteration 81, loss = 0.26110293\n",
      "Iteration 77, loss = 0.26318523\n",
      "Iteration 82, loss = 0.26078718\n",
      "Iteration 78, loss = 0.26309868\n",
      "Iteration 83, loss = 0.26089379\n",
      "Iteration 79, loss = 0.26333145\n",
      "Iteration 84, loss = 0.26093631\n",
      "Iteration 80, loss = 0.26303981\n",
      "Iteration 85, loss = 0.26050040\n",
      "Iteration 81, loss = 0.26280705\n",
      "Iteration 86, loss = 0.26066100\n",
      "Iteration 82, loss = 0.26295642\n",
      "Iteration 87, loss = 0.26074321\n",
      "Iteration 83, loss = 0.26308453\n",
      "Iteration 88, loss = 0.26015512\n",
      "Iteration 84, loss = 0.26303404\n",
      "Iteration 89, loss = 0.26032688\n",
      "Iteration 85, loss = 0.26260269\n",
      "Iteration 90, loss = 0.26016303\n",
      "Iteration 86, loss = 0.26274748\n",
      "Iteration 91, loss = 0.26011737\n",
      "Iteration 87, loss = 0.26268964\n",
      "Iteration 92, loss = 0.26014326\n",
      "Iteration 88, loss = 0.26242853\n",
      "Iteration 93, loss = 0.25997601\n",
      "Iteration 89, loss = 0.26236564\n",
      "Iteration 94, loss = 0.26025328\n",
      "Iteration 90, loss = 0.26230964\n",
      "Iteration 95, loss = 0.26006058\n",
      "Iteration 91, loss = 0.26219332\n",
      "Iteration 96, loss = 0.26012125\n",
      "Iteration 92, loss = 0.26229833\n",
      "Iteration 97, loss = 0.25995367\n",
      "Iteration 93, loss = 0.26216646\n",
      "Iteration 98, loss = 0.25989706\n",
      "Iteration 94, loss = 0.26223601\n",
      "Iteration 99, loss = 0.25989954\n",
      "Iteration 95, loss = 0.26202881\n",
      "Iteration 100, loss = 0.25969191\n",
      "Iteration 96, loss = 0.26206199\n",
      "Iteration 101, loss = 0.25970776\n",
      "Iteration 97, loss = 0.26189294\n",
      "Iteration 102, loss = 0.25983135\n",
      "Iteration 98, loss = 0.26185189\n",
      "Iteration 103, loss = 0.25971719\n",
      "Iteration 99, loss = 0.26175940\n",
      "Iteration 104, loss = 0.25966340\n",
      "Iteration 100, loss = 0.26171050\n",
      "Iteration 105, loss = 0.25961274\n",
      "Iteration 101, loss = 0.26151237\n",
      "Iteration 106, loss = 0.25975560\n",
      "Iteration 102, loss = 0.26172675\n",
      "Iteration 107, loss = 0.25966050\n",
      "Iteration 103, loss = 0.26172132\n",
      "Iteration 108, loss = 0.25969054\n",
      "Iteration 104, loss = 0.26155129\n",
      "Iteration 109, loss = 0.25975895\n",
      "Iteration 105, loss = 0.26151393\n",
      "Iteration 110, loss = 0.25958152\n",
      "Iteration 106, loss = 0.26146343\n",
      "Iteration 111, loss = 0.25928629\n",
      "Iteration 107, loss = 0.26169148\n",
      "Iteration 112, loss = 0.25954763\n",
      "Iteration 108, loss = 0.26136872\n",
      "Iteration 113, loss = 0.25934937\n",
      "Iteration 109, loss = 0.26176289\n",
      "Iteration 114, loss = 0.25924092\n",
      "Iteration 110, loss = 0.26154785\n",
      "Iteration 115, loss = 0.25948965\n",
      "Iteration 111, loss = 0.26093830\n",
      "Iteration 116, loss = 0.25944516\n",
      "Iteration 112, loss = 0.26141896\n",
      "Iteration 113, loss = 0.26115330\n",
      "Iteration 117, loss = 0.25966264\n",
      "Iteration 118, loss = 0.25911146\n",
      "Iteration 114, loss = 0.26124724\n",
      "Iteration 119, loss = 0.25987275\n",
      "Iteration 115, loss = 0.26097857\n",
      "Iteration 120, loss = 0.25914092\n",
      "Iteration 116, loss = 0.26146356\n",
      "Iteration 121, loss = 0.25920445\n",
      "Iteration 117, loss = 0.26113391\n",
      "Iteration 118, loss = 0.26089017\n",
      "Iteration 122, loss = 0.25904921\n",
      "Iteration 123, loss = 0.25919683\n",
      "Iteration 119, loss = 0.26156759\n",
      "Iteration 124, loss = 0.25940008\n",
      "Iteration 120, loss = 0.26089204\n",
      "Iteration 125, loss = 0.25920382\n",
      "Iteration 121, loss = 0.26095870\n",
      "Iteration 126, loss = 0.25902984\n",
      "Iteration 122, loss = 0.26101599\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 127, loss = 0.25953346\n",
      "Iteration 128, loss = 0.25891873\n",
      "Iteration 1, loss = 0.33592148\n",
      "Iteration 129, loss = 0.25907587\n",
      "Iteration 2, loss = 0.29021776\n",
      "Iteration 130, loss = 0.25949139\n",
      "Iteration 3, loss = 0.28117958\n",
      "Iteration 131, loss = 0.25887946\n",
      "Iteration 4, loss = 0.27857797\n",
      "Iteration 132, loss = 0.25898025\n",
      "Iteration 5, loss = 0.27763946\n",
      "Iteration 133, loss = 0.25915712\n",
      "Iteration 6, loss = 0.27710102\n",
      "Iteration 134, loss = 0.25894301\n",
      "Iteration 7, loss = 0.27619924\n",
      "Iteration 135, loss = 0.25876780\n",
      "Iteration 8, loss = 0.27628766\n",
      "Iteration 136, loss = 0.25884469\n",
      "Iteration 9, loss = 0.27495995\n",
      "Iteration 137, loss = 0.25897779\n",
      "Iteration 10, loss = 0.27474554\n",
      "Iteration 138, loss = 0.25881977\n",
      "Iteration 11, loss = 0.27443161\n",
      "Iteration 139, loss = 0.25880957\n",
      "Iteration 12, loss = 0.27380397\n",
      "Iteration 140, loss = 0.25867624\n",
      "Iteration 13, loss = 0.27327955\n",
      "Iteration 141, loss = 0.25893748\n",
      "Iteration 14, loss = 0.27307510\n",
      "Iteration 142, loss = 0.25894325\n",
      "Iteration 15, loss = 0.27308437\n",
      "Iteration 16, loss = 0.27204340\n",
      "Iteration 143, loss = 0.25859783\n",
      "Iteration 17, loss = 0.27176070\n",
      "Iteration 144, loss = 0.25881121\n",
      "Iteration 18, loss = 0.27123942\n",
      "Iteration 145, loss = 0.25889631\n",
      "Iteration 19, loss = 0.27080156\n",
      "Iteration 146, loss = 0.25852363\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 20, loss = 0.27036733\n",
      "Iteration 21, loss = 0.26989438\n",
      "Iteration 1, loss = 0.33614798\n",
      "Iteration 22, loss = 0.26998263\n",
      "Iteration 2, loss = 0.29000250\n",
      "Iteration 23, loss = 0.26941710\n",
      "Iteration 3, loss = 0.28080171\n",
      "Iteration 24, loss = 0.26885680\n",
      "Iteration 4, loss = 0.27813565\n",
      "Iteration 25, loss = 0.26848773\n",
      "Iteration 5, loss = 0.27727455\n",
      "Iteration 26, loss = 0.26830569\n",
      "Iteration 27, loss = 0.26799861\n",
      "Iteration 6, loss = 0.27693401\n",
      "Iteration 28, loss = 0.26763487\n",
      "Iteration 7, loss = 0.27598597\n",
      "Iteration 29, loss = 0.26718428\n",
      "Iteration 8, loss = 0.27590085\n",
      "Iteration 30, loss = 0.26706080\n",
      "Iteration 9, loss = 0.27506818\n",
      "Iteration 31, loss = 0.26700757\n",
      "Iteration 10, loss = 0.27481080\n",
      "Iteration 32, loss = 0.26627732\n",
      "Iteration 11, loss = 0.27444789\n",
      "Iteration 33, loss = 0.26617547\n",
      "Iteration 12, loss = 0.27398008\n",
      "Iteration 34, loss = 0.26620999\n",
      "Iteration 13, loss = 0.27365273\n",
      "Iteration 35, loss = 0.26587241\n",
      "Iteration 14, loss = 0.27339114\n",
      "Iteration 36, loss = 0.26560663\n",
      "Iteration 15, loss = 0.27306657\n",
      "Iteration 37, loss = 0.26538560\n",
      "Iteration 16, loss = 0.27235966\n",
      "Iteration 38, loss = 0.26540980Iteration 17, loss = 0.27203246\n",
      "\n",
      "Iteration 18, loss = 0.27146754\n",
      "Iteration 39, loss = 0.26519652\n",
      "Iteration 19, loss = 0.27094497\n",
      "Iteration 40, loss = 0.26527655\n",
      "Iteration 20, loss = 0.27047911\n",
      "Iteration 41, loss = 0.26502275\n",
      "Iteration 21, loss = 0.26995079\n",
      "Iteration 42, loss = 0.26478265\n",
      "Iteration 22, loss = 0.26998346\n",
      "Iteration 43, loss = 0.26493626\n",
      "Iteration 23, loss = 0.26941579\n",
      "Iteration 44, loss = 0.26469668\n",
      "Iteration 24, loss = 0.26876398\n",
      "Iteration 45, loss = 0.26452653\n",
      "Iteration 25, loss = 0.26832012\n",
      "Iteration 46, loss = 0.26444868\n",
      "Iteration 26, loss = 0.26791295\n",
      "Iteration 47, loss = 0.26450770\n",
      "Iteration 27, loss = 0.26806652\n",
      "Iteration 48, loss = 0.26429192\n",
      "Iteration 28, loss = 0.26759715\n",
      "Iteration 49, loss = 0.26439380\n",
      "Iteration 29, loss = 0.26699038\n",
      "Iteration 50, loss = 0.26406402\n",
      "Iteration 30, loss = 0.26689002\n",
      "Iteration 51, loss = 0.26430319\n",
      "Iteration 31, loss = 0.26685117\n",
      "Iteration 52, loss = 0.26400061\n",
      "Iteration 32, loss = 0.26592230\n",
      "Iteration 53, loss = 0.26370657\n",
      "Iteration 33, loss = 0.26583050\n",
      "Iteration 54, loss = 0.26355838\n",
      "Iteration 34, loss = 0.26578397\n",
      "Iteration 55, loss = 0.26341071\n",
      "Iteration 35, loss = 0.26566554\n",
      "Iteration 56, loss = 0.26348679\n",
      "Iteration 36, loss = 0.26534094\n",
      "Iteration 57, loss = 0.26366400\n",
      "Iteration 37, loss = 0.26515735\n",
      "Iteration 58, loss = 0.26330828\n",
      "Iteration 38, loss = 0.26513134\n",
      "Iteration 59, loss = 0.26309743\n",
      "Iteration 39, loss = 0.26505217\n",
      "Iteration 40, loss = 0.26503119\n",
      "Iteration 60, loss = 0.26334444\n",
      "Iteration 41, loss = 0.26459913\n",
      "Iteration 61, loss = 0.26311270\n",
      "Iteration 42, loss = 0.26440871\n",
      "Iteration 62, loss = 0.26324290\n",
      "Iteration 43, loss = 0.26447163\n",
      "Iteration 63, loss = 0.26267807\n",
      "Iteration 64, loss = 0.26258792\n",
      "Iteration 44, loss = 0.26422635\n",
      "Iteration 65, loss = 0.26241497\n",
      "Iteration 45, loss = 0.26415380\n",
      "Iteration 66, loss = 0.26265190\n",
      "Iteration 46, loss = 0.26393279\n",
      "Iteration 67, loss = 0.26248983\n",
      "Iteration 47, loss = 0.26406392\n",
      "Iteration 68, loss = 0.26243316\n",
      "Iteration 48, loss = 0.26367260\n",
      "Iteration 69, loss = 0.26231760\n",
      "Iteration 49, loss = 0.26398170\n",
      "Iteration 70, loss = 0.26225174\n",
      "Iteration 50, loss = 0.26348278\n",
      "Iteration 71, loss = 0.26216932\n",
      "Iteration 51, loss = 0.26348606\n",
      "Iteration 72, loss = 0.26209531\n",
      "Iteration 52, loss = 0.26363081\n",
      "Iteration 73, loss = 0.26227011\n",
      "Iteration 53, loss = 0.26315833\n",
      "Iteration 74, loss = 0.26174834\n",
      "Iteration 54, loss = 0.26325529\n",
      "Iteration 75, loss = 0.26192705\n",
      "Iteration 55, loss = 0.26287205\n",
      "Iteration 76, loss = 0.26168483\n",
      "Iteration 56, loss = 0.26305430\n",
      "Iteration 77, loss = 0.26191920\n",
      "Iteration 57, loss = 0.26295769\n",
      "Iteration 78, loss = 0.26177399\n",
      "Iteration 58, loss = 0.26284916\n",
      "Iteration 79, loss = 0.26188013\n",
      "Iteration 59, loss = 0.26303993\n",
      "Iteration 80, loss = 0.26150565\n",
      "Iteration 60, loss = 0.26297747\n",
      "Iteration 81, loss = 0.26151353\n",
      "Iteration 61, loss = 0.26264784\n",
      "Iteration 82, loss = 0.26146451\n",
      "Iteration 62, loss = 0.26263870\n",
      "Iteration 83, loss = 0.26156064\n",
      "Iteration 63, loss = 0.26251705\n",
      "Iteration 84, loss = 0.26165823\n",
      "Iteration 64, loss = 0.26231279\n",
      "Iteration 85, loss = 0.26126256\n",
      "Iteration 65, loss = 0.26219914\n",
      "Iteration 86, loss = 0.26127576\n",
      "Iteration 66, loss = 0.26254939\n",
      "Iteration 87, loss = 0.26130889\n",
      "Iteration 67, loss = 0.26229426\n",
      "Iteration 88, loss = 0.26130177\n",
      "Iteration 68, loss = 0.26227686\n",
      "Iteration 89, loss = 0.26118147\n",
      "Iteration 69, loss = 0.26220135\n",
      "Iteration 90, loss = 0.26115440\n",
      "Iteration 70, loss = 0.26204539\n",
      "Iteration 71, loss = 0.26210939\n",
      "Iteration 91, loss = 0.26088664\n",
      "Iteration 72, loss = 0.26207013\n",
      "Iteration 92, loss = 0.26118595\n",
      "Iteration 73, loss = 0.26239962\n",
      "Iteration 74, loss = 0.26178463\n",
      "Iteration 93, loss = 0.26114612\n",
      "Iteration 75, loss = 0.26209470\n",
      "Iteration 94, loss = 0.26106465\n",
      "Iteration 76, loss = 0.26160337\n",
      "Iteration 95, loss = 0.26093270\n",
      "Iteration 77, loss = 0.26193863\n",
      "Iteration 96, loss = 0.26108839\n",
      "Iteration 78, loss = 0.26176107\n",
      "Iteration 97, loss = 0.26105812\n",
      "Iteration 98, loss = 0.26077789Iteration 79, loss = 0.26200941\n",
      "\n",
      "Iteration 99, loss = 0.26081530\n",
      "Iteration 80, loss = 0.26182579\n",
      "Iteration 100, loss = 0.26079985\n",
      "Iteration 81, loss = 0.26177103\n",
      "Iteration 101, loss = 0.26056382\n",
      "Iteration 82, loss = 0.26163243\n",
      "Iteration 102, loss = 0.26051418\n",
      "Iteration 83, loss = 0.26162024\n",
      "Iteration 103, loss = 0.26083273\n",
      "Iteration 84, loss = 0.26190077\n",
      "Iteration 104, loss = 0.26060375\n",
      "Iteration 85, loss = 0.26133484\n",
      "Iteration 105, loss = 0.26074643\n",
      "Iteration 86, loss = 0.26148138\n",
      "Iteration 106, loss = 0.26071933\n",
      "Iteration 87, loss = 0.26132215\n",
      "Iteration 107, loss = 0.26065879\n",
      "Iteration 88, loss = 0.26138090\n",
      "Iteration 89, loss = 0.26135283\n",
      "Iteration 108, loss = 0.26059071\n",
      "Iteration 90, loss = 0.26129158\n",
      "Iteration 109, loss = 0.26081282\n",
      "Iteration 91, loss = 0.26118829\n",
      "Iteration 110, loss = 0.26032260\n",
      "Iteration 92, loss = 0.26121015\n",
      "Iteration 111, loss = 0.26003360\n",
      "Iteration 93, loss = 0.26112821\n",
      "Iteration 112, loss = 0.26052760\n",
      "Iteration 94, loss = 0.26130004Iteration 113, loss = 0.26026343\n",
      "\n",
      "Iteration 95, loss = 0.26087492\n",
      "Iteration 114, loss = 0.26021121\n",
      "Iteration 115, loss = 0.26023444\n",
      "Iteration 96, loss = 0.26117667\n",
      "Iteration 116, loss = 0.26058567\n",
      "Iteration 97, loss = 0.26091638\n",
      "Iteration 117, loss = 0.26042689\n",
      "Iteration 98, loss = 0.26090790\n",
      "Iteration 118, loss = 0.26001425\n",
      "Iteration 99, loss = 0.26095346\n",
      "Iteration 119, loss = 0.26046618\n",
      "Iteration 100, loss = 0.26065042\n",
      "Iteration 120, loss = 0.26016742\n",
      "Iteration 101, loss = 0.26067721\n",
      "Iteration 121, loss = 0.26023054\n",
      "Iteration 102, loss = 0.26072957\n",
      "Iteration 122, loss = 0.26023447\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 103, loss = 0.26073178\n",
      "Iteration 104, loss = 0.26042116\n",
      "Iteration 1, loss = 0.33574501\n",
      "Iteration 105, loss = 0.26085526\n",
      "Iteration 2, loss = 0.29012566\n",
      "Iteration 3, loss = 0.28060550\n",
      "Iteration 106, loss = 0.26050955\n",
      "Iteration 4, loss = 0.27812881Iteration 107, loss = 0.26071023\n",
      "\n",
      "Iteration 108, loss = 0.26042315\n",
      "Iteration 5, loss = 0.27693494\n",
      "Iteration 109, loss = 0.26060604\n",
      "Iteration 6, loss = 0.27656340\n",
      "Iteration 110, loss = 0.26036570\n",
      "Iteration 7, loss = 0.27573762\n",
      "Iteration 111, loss = 0.26008820\n",
      "Iteration 8, loss = 0.27545596\n",
      "Iteration 112, loss = 0.26029345\n",
      "Iteration 9, loss = 0.27467190\n",
      "Iteration 113, loss = 0.26009675\n",
      "Iteration 10, loss = 0.27433954\n",
      "Iteration 114, loss = 0.26016165\n",
      "Iteration 11, loss = 0.27410779\n",
      "Iteration 115, loss = 0.25997482\n",
      "Iteration 12, loss = 0.27370752\n",
      "Iteration 116, loss = 0.26039451\n",
      "Iteration 13, loss = 0.27319682\n",
      "Iteration 117, loss = 0.26011769\n",
      "Iteration 14, loss = 0.27284608\n",
      "Iteration 118, loss = 0.25996562\n",
      "Iteration 15, loss = 0.27241325\n",
      "Iteration 119, loss = 0.26049869\n",
      "Iteration 16, loss = 0.27183717\n",
      "Iteration 120, loss = 0.25993228\n",
      "Iteration 17, loss = 0.27155152\n",
      "Iteration 121, loss = 0.25988186\n",
      "Iteration 18, loss = 0.27113797\n",
      "Iteration 122, loss = 0.26006694\n",
      "Iteration 19, loss = 0.27077187\n",
      "Iteration 20, loss = 0.27035134Iteration 123, loss = 0.26018200\n",
      "\n",
      "Iteration 21, loss = 0.26998219\n",
      "Iteration 124, loss = 0.25993671\n",
      "Iteration 22, loss = 0.26999166\n",
      "Iteration 125, loss = 0.25989361\n",
      "Iteration 23, loss = 0.26937766\n",
      "Iteration 126, loss = 0.25967599\n",
      "Iteration 24, loss = 0.26917623\n",
      "Iteration 127, loss = 0.25993419\n",
      "Iteration 25, loss = 0.26871754\n",
      "Iteration 128, loss = 0.25989923\n",
      "Iteration 129, loss = 0.25961357\n",
      "Iteration 26, loss = 0.26851047\n",
      "Iteration 130, loss = 0.25977515\n",
      "Iteration 27, loss = 0.26851655\n",
      "Iteration 131, loss = 0.25954556\n",
      "Iteration 28, loss = 0.26822619\n",
      "Iteration 132, loss = 0.25950573\n",
      "Iteration 29, loss = 0.26740610\n",
      "Iteration 133, loss = 0.25977979\n",
      "Iteration 30, loss = 0.26730408\n",
      "Iteration 134, loss = 0.25961834\n",
      "Iteration 31, loss = 0.26714267\n",
      "Iteration 135, loss = 0.25978886\n",
      "Iteration 32, loss = 0.26648061\n",
      "Iteration 136, loss = 0.25948045\n",
      "Iteration 33, loss = 0.26628340\n",
      "Iteration 137, loss = 0.25941265\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 34, loss = 0.26607869\n",
      "Iteration 35, loss = 0.26585431\n",
      "Iteration 36, loss = 0.26562876\n",
      "Iteration 1, loss = 0.33545758\n",
      "Iteration 37, loss = 0.26509883\n",
      "Iteration 2, loss = 0.28954972\n",
      "Iteration 38, loss = 0.26507637\n",
      "Iteration 3, loss = 0.27999763\n",
      "Iteration 4, loss = 0.27737707\n",
      "Iteration 39, loss = 0.26503993\n",
      "Iteration 5, loss = 0.27629670\n",
      "Iteration 40, loss = 0.26473233\n",
      "Iteration 6, loss = 0.27567704\n",
      "Iteration 41, loss = 0.26408498\n",
      "Iteration 7, loss = 0.27505124\n",
      "Iteration 42, loss = 0.26394726\n",
      "Iteration 8, loss = 0.27466121\n",
      "Iteration 43, loss = 0.26378370\n",
      "Iteration 9, loss = 0.27402384\n",
      "Iteration 44, loss = 0.26356983\n",
      "Iteration 10, loss = 0.27351978\n",
      "Iteration 45, loss = 0.26348322\n",
      "Iteration 11, loss = 0.27319401\n",
      "Iteration 46, loss = 0.26330928\n",
      "Iteration 12, loss = 0.27278037\n",
      "Iteration 47, loss = 0.26316066\n",
      "Iteration 13, loss = 0.27258607\n",
      "Iteration 48, loss = 0.26292889\n",
      "Iteration 14, loss = 0.27198908\n",
      "Iteration 49, loss = 0.26320870\n",
      "Iteration 15, loss = 0.27164498\n",
      "Iteration 50, loss = 0.26277253\n",
      "Iteration 16, loss = 0.27133913\n",
      "Iteration 51, loss = 0.26297365\n",
      "Iteration 17, loss = 0.27091535\n",
      "Iteration 52, loss = 0.26344337\n",
      "Iteration 18, loss = 0.27056875\n",
      "Iteration 19, loss = 0.27047210\n",
      "Iteration 53, loss = 0.26251588\n",
      "Iteration 20, loss = 0.27007626\n",
      "Iteration 54, loss = 0.26263128\n",
      "Iteration 21, loss = 0.26973147\n",
      "Iteration 55, loss = 0.26212670\n",
      "Iteration 22, loss = 0.26960317\n",
      "Iteration 56, loss = 0.26223935\n",
      "Iteration 23, loss = 0.26911454\n",
      "Iteration 57, loss = 0.26234435\n",
      "Iteration 24, loss = 0.26901084\n",
      "Iteration 58, loss = 0.26191103\n",
      "Iteration 25, loss = 0.26871928\n",
      "Iteration 59, loss = 0.26232378\n",
      "Iteration 26, loss = 0.26844829\n",
      "Iteration 60, loss = 0.26229768\n",
      "Iteration 27, loss = 0.26825262\n",
      "Iteration 61, loss = 0.26253956\n",
      "Iteration 28, loss = 0.26796487\n",
      "Iteration 62, loss = 0.26170636\n",
      "Iteration 29, loss = 0.26751298\n",
      "Iteration 63, loss = 0.26153658\n",
      "Iteration 30, loss = 0.26730704\n",
      "Iteration 64, loss = 0.26163969\n",
      "Iteration 31, loss = 0.26701183\n",
      "Iteration 65, loss = 0.26131227\n",
      "Iteration 32, loss = 0.26652474\n",
      "Iteration 66, loss = 0.26154857\n",
      "Iteration 33, loss = 0.26645951\n",
      "Iteration 67, loss = 0.26146833\n",
      "Iteration 34, loss = 0.26612960\n",
      "Iteration 68, loss = 0.26132973\n",
      "Iteration 35, loss = 0.26609834\n",
      "Iteration 69, loss = 0.26114495\n",
      "Iteration 36, loss = 0.26597845\n",
      "Iteration 70, loss = 0.26091493\n",
      "Iteration 37, loss = 0.26555665\n",
      "Iteration 71, loss = 0.26095337\n",
      "Iteration 38, loss = 0.26535817\n",
      "Iteration 72, loss = 0.26095184\n",
      "Iteration 39, loss = 0.26531100\n",
      "Iteration 73, loss = 0.26136190\n",
      "Iteration 40, loss = 0.26505384\n",
      "Iteration 74, loss = 0.26064644\n",
      "Iteration 41, loss = 0.26455868\n",
      "Iteration 75, loss = 0.26086717\n",
      "Iteration 42, loss = 0.26448494\n",
      "Iteration 76, loss = 0.26032291\n",
      "Iteration 43, loss = 0.26440318\n",
      "Iteration 77, loss = 0.26077088\n",
      "Iteration 44, loss = 0.26397183\n",
      "Iteration 78, loss = 0.26038925\n",
      "Iteration 45, loss = 0.26400296\n",
      "Iteration 79, loss = 0.26056027\n",
      "Iteration 46, loss = 0.26363437\n",
      "Iteration 80, loss = 0.26037791\n",
      "Iteration 47, loss = 0.26347701\n",
      "Iteration 81, loss = 0.26057649\n",
      "Iteration 48, loss = 0.26339665\n",
      "Iteration 82, loss = 0.26044518\n",
      "Iteration 49, loss = 0.26347548\n",
      "Iteration 83, loss = 0.26055242\n",
      "Iteration 50, loss = 0.26310112\n",
      "Iteration 84, loss = 0.26046363\n",
      "Iteration 51, loss = 0.26305534\n",
      "Iteration 85, loss = 0.26013610\n",
      "Iteration 52, loss = 0.26319244\n",
      "Iteration 86, loss = 0.26038993\n",
      "Iteration 53, loss = 0.26268159\n",
      "Iteration 87, loss = 0.26011974\n",
      "Iteration 54, loss = 0.26283024\n",
      "Iteration 88, loss = 0.26014294\n",
      "Iteration 55, loss = 0.26215251\n",
      "Iteration 89, loss = 0.26018859\n",
      "Iteration 56, loss = 0.26242205\n",
      "Iteration 57, loss = 0.26246074\n",
      "Iteration 90, loss = 0.25998839\n",
      "Iteration 91, loss = 0.26025360\n",
      "Iteration 58, loss = 0.26174101\n",
      "Iteration 92, loss = 0.25989175\n",
      "Iteration 59, loss = 0.26211202\n",
      "Iteration 93, loss = 0.25994625\n",
      "Iteration 60, loss = 0.26226567\n",
      "Iteration 94, loss = 0.26003102\n",
      "Iteration 61, loss = 0.26253216\n",
      "Iteration 95, loss = 0.25970436\n",
      "Iteration 62, loss = 0.26151319\n",
      "Iteration 96, loss = 0.25998354\n",
      "Iteration 63, loss = 0.26139402\n",
      "Iteration 97, loss = 0.25979720\n",
      "Iteration 64, loss = 0.26141501\n",
      "Iteration 65, loss = 0.26130875\n",
      "Iteration 98, loss = 0.25975127\n",
      "Iteration 66, loss = 0.26129333\n",
      "Iteration 99, loss = 0.25979544\n",
      "Iteration 67, loss = 0.26121287\n",
      "Iteration 100, loss = 0.25987037\n",
      "Iteration 68, loss = 0.26095287\n",
      "Iteration 101, loss = 0.25968761\n",
      "Iteration 69, loss = 0.26094258\n",
      "Iteration 102, loss = 0.25990140\n",
      "Iteration 70, loss = 0.26074280\n",
      "Iteration 103, loss = 0.25979184\n",
      "Iteration 104, loss = 0.25947730\n",
      "Iteration 71, loss = 0.26067988\n",
      "Iteration 72, loss = 0.26050497\n",
      "Iteration 105, loss = 0.25999235\n",
      "Iteration 73, loss = 0.26084611\n",
      "Iteration 74, loss = 0.26023061\n",
      "Iteration 106, loss = 0.25956997\n",
      "Iteration 75, loss = 0.26028314\n",
      "Iteration 107, loss = 0.25961033\n",
      "Iteration 76, loss = 0.26028215\n",
      "Iteration 108, loss = 0.25956891\n",
      "Iteration 77, loss = 0.26038880\n",
      "Iteration 109, loss = 0.25997228\n",
      "Iteration 78, loss = 0.25984716\n",
      "Iteration 110, loss = 0.25937862\n",
      "Iteration 79, loss = 0.26018699\n",
      "Iteration 111, loss = 0.25904606\n",
      "Iteration 80, loss = 0.25997278\n",
      "Iteration 112, loss = 0.25926662\n",
      "Iteration 81, loss = 0.26007755\n",
      "Iteration 113, loss = 0.25923653\n",
      "Iteration 82, loss = 0.25987279\n",
      "Iteration 114, loss = 0.25933075\n",
      "Iteration 83, loss = 0.25999519\n",
      "Iteration 115, loss = 0.25895098\n",
      "Iteration 84, loss = 0.25983334\n",
      "Iteration 116, loss = 0.25969252\n",
      "Iteration 85, loss = 0.25962441\n",
      "Iteration 117, loss = 0.25899382\n",
      "Iteration 86, loss = 0.25965290\n",
      "Iteration 118, loss = 0.25896352\n",
      "Iteration 87, loss = 0.25959948\n",
      "Iteration 119, loss = 0.25976387\n",
      "Iteration 88, loss = 0.25963829\n",
      "Iteration 120, loss = 0.25901522\n",
      "Iteration 121, loss = 0.25876093\n",
      "Iteration 89, loss = 0.25949323\n",
      "Iteration 122, loss = 0.25919889\n",
      "Iteration 90, loss = 0.25942645\n",
      "Iteration 91, loss = 0.25964133\n",
      "Iteration 123, loss = 0.25902734\n",
      "Iteration 92, loss = 0.25938794\n",
      "Iteration 124, loss = 0.25893168\n",
      "Iteration 93, loss = 0.25935170\n",
      "Iteration 125, loss = 0.25897024\n",
      "Iteration 94, loss = 0.25944676Iteration 126, loss = 0.25900642\n",
      "\n",
      "Iteration 95, loss = 0.25906740\n",
      "Iteration 127, loss = 0.25910943\n",
      "Iteration 128, loss = 0.25892633\n",
      "Iteration 96, loss = 0.25919107\n",
      "Iteration 129, loss = 0.25870841\n",
      "Iteration 97, loss = 0.25922174\n",
      "Iteration 130, loss = 0.25878248\n",
      "Iteration 98, loss = 0.25912547\n",
      "Iteration 131, loss = 0.25882089\n",
      "Iteration 99, loss = 0.25912988\n",
      "Iteration 132, loss = 0.25859807\n",
      "Iteration 100, loss = 0.25926764\n",
      "Iteration 133, loss = 0.25868992\n",
      "Iteration 101, loss = 0.25886152\n",
      "Iteration 134, loss = 0.25877006\n",
      "Iteration 102, loss = 0.25893397\n",
      "Iteration 103, loss = 0.25917444\n",
      "Iteration 135, loss = 0.25899514\n",
      "Iteration 136, loss = 0.25876123\n",
      "Iteration 104, loss = 0.25871834\n",
      "Iteration 137, loss = 0.25862242\n",
      "Iteration 105, loss = 0.25907050\n",
      "Iteration 138, loss = 0.25852194\n",
      "Iteration 106, loss = 0.25884735\n",
      "Iteration 139, loss = 0.25885130\n",
      "Iteration 107, loss = 0.25896502\n",
      "Iteration 140, loss = 0.25856634\n",
      "Iteration 108, loss = 0.25868977\n",
      "Iteration 141, loss = 0.25841679\n",
      "Iteration 109, loss = 0.25904915\n",
      "Iteration 142, loss = 0.25879431\n",
      "Iteration 110, loss = 0.25857140\n",
      "Iteration 143, loss = 0.25872129\n",
      "Iteration 111, loss = 0.25841008\n",
      "Iteration 144, loss = 0.25879898\n",
      "Iteration 145, loss = 0.25834969\n",
      "Iteration 112, loss = 0.25841432\n",
      "Iteration 146, loss = 0.25858226\n",
      "Iteration 113, loss = 0.25825443\n",
      "Iteration 147, loss = 0.25836565\n",
      "Iteration 114, loss = 0.25855195\n",
      "Iteration 148, loss = 0.25838503\n",
      "Iteration 115, loss = 0.25858158\n",
      "Iteration 149, loss = 0.25822712\n",
      "Iteration 116, loss = 0.25879904\n",
      "Iteration 150, loss = 0.25841464\n",
      "Iteration 117, loss = 0.25852775\n",
      "Iteration 151, loss = 0.25860022\n",
      "Iteration 118, loss = 0.25827178\n",
      "Iteration 152, loss = 0.25853637\n",
      "Iteration 119, loss = 0.25881203\n",
      "Iteration 153, loss = 0.25817453\n",
      "Iteration 120, loss = 0.25834447\n",
      "Iteration 154, loss = 0.25831125\n",
      "Iteration 121, loss = 0.25825829\n",
      "Iteration 122, loss = 0.25817843\n",
      "Iteration 155, loss = 0.25834216\n",
      "Iteration 123, loss = 0.25826458\n",
      "Iteration 156, loss = 0.25815607\n",
      "Iteration 124, loss = 0.25821553\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 157, loss = 0.25823086\n",
      "Iteration 158, loss = 0.25839975\n",
      "Iteration 159, loss = 0.25808492\n",
      "Iteration 160, loss = 0.25831449\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33189925\n",
      "Iteration 1, loss = 0.33234528\n",
      "Iteration 2, loss = 0.28596726\n",
      "Iteration 2, loss = 0.28619569\n",
      "Iteration 3, loss = 0.27823468\n",
      "Iteration 3, loss = 0.27866802\n",
      "Iteration 4, loss = 0.27600513\n",
      "Iteration 4, loss = 0.27635229\n",
      "Iteration 5, loss = 0.27499148\n",
      "Iteration 5, loss = 0.27554733\n",
      "Iteration 6, loss = 0.27469276\n",
      "Iteration 6, loss = 0.27512162\n",
      "Iteration 7, loss = 0.27429728\n",
      "Iteration 7, loss = 0.27501008\n",
      "Iteration 8, loss = 0.27430723\n",
      "Iteration 8, loss = 0.27459084\n",
      "Iteration 9, loss = 0.27290765\n",
      "Iteration 9, loss = 0.27376103\n",
      "Iteration 10, loss = 0.27289022\n",
      "Iteration 10, loss = 0.27337060\n",
      "Iteration 11, loss = 0.27244703\n",
      "Iteration 11, loss = 0.27319694\n",
      "Iteration 12, loss = 0.27231367\n",
      "Iteration 12, loss = 0.27267147\n",
      "Iteration 13, loss = 0.27172366\n",
      "Iteration 13, loss = 0.27212133\n",
      "Iteration 14, loss = 0.27090635\n",
      "Iteration 14, loss = 0.27158054\n",
      "Iteration 15, loss = 0.27066616\n",
      "Iteration 15, loss = 0.27152212\n",
      "Iteration 16, loss = 0.27002583\n",
      "Iteration 16, loss = 0.27062984\n",
      "Iteration 17, loss = 0.26984067\n",
      "Iteration 17, loss = 0.27048657\n",
      "Iteration 18, loss = 0.26978164\n",
      "Iteration 18, loss = 0.27050010\n",
      "Iteration 19, loss = 0.26992777Iteration 19, loss = 0.26948654\n",
      "\n",
      "Iteration 20, loss = 0.26980604\n",
      "Iteration 20, loss = 0.26914996\n",
      "Iteration 21, loss = 0.26934413\n",
      "Iteration 21, loss = 0.26862334\n",
      "Iteration 22, loss = 0.26944516\n",
      "Iteration 22, loss = 0.26852462\n",
      "Iteration 23, loss = 0.26912913\n",
      "Iteration 24, loss = 0.26856715\n",
      "Iteration 23, loss = 0.26832857\n",
      "Iteration 25, loss = 0.26845574\n",
      "Iteration 24, loss = 0.26782539\n",
      "Iteration 26, loss = 0.26833112\n",
      "Iteration 25, loss = 0.26752622\n",
      "Iteration 27, loss = 0.26829669\n",
      "Iteration 26, loss = 0.26726097\n",
      "Iteration 28, loss = 0.26806691\n",
      "Iteration 27, loss = 0.26715462\n",
      "Iteration 29, loss = 0.26799290\n",
      "Iteration 28, loss = 0.26737979\n",
      "Iteration 30, loss = 0.26740225\n",
      "Iteration 29, loss = 0.26691834\n",
      "Iteration 31, loss = 0.26756586\n",
      "Iteration 30, loss = 0.26618660\n",
      "Iteration 32, loss = 0.26720939\n",
      "Iteration 31, loss = 0.26622899\n",
      "Iteration 33, loss = 0.26720940\n",
      "Iteration 32, loss = 0.26583719\n",
      "Iteration 34, loss = 0.26695332\n",
      "Iteration 33, loss = 0.26609436\n",
      "Iteration 35, loss = 0.26698791\n",
      "Iteration 34, loss = 0.26551436\n",
      "Iteration 36, loss = 0.26661678\n",
      "Iteration 35, loss = 0.26521231\n",
      "Iteration 37, loss = 0.26656287\n",
      "Iteration 36, loss = 0.26495523\n",
      "Iteration 38, loss = 0.26627976\n",
      "Iteration 37, loss = 0.26493420\n",
      "Iteration 39, loss = 0.26598944\n",
      "Iteration 38, loss = 0.26454601\n",
      "Iteration 40, loss = 0.26620937\n",
      "Iteration 39, loss = 0.26421175\n",
      "Iteration 41, loss = 0.26626918\n",
      "Iteration 40, loss = 0.26441706\n",
      "Iteration 42, loss = 0.26584354\n",
      "Iteration 43, loss = 0.26537999\n",
      "Iteration 41, loss = 0.26425181\n",
      "Iteration 44, loss = 0.26553831\n",
      "Iteration 42, loss = 0.26412470\n",
      "Iteration 45, loss = 0.26519418\n",
      "Iteration 43, loss = 0.26363630\n",
      "Iteration 46, loss = 0.26485130\n",
      "Iteration 44, loss = 0.26358457\n",
      "Iteration 47, loss = 0.26482266\n",
      "Iteration 45, loss = 0.26315645\n",
      "Iteration 48, loss = 0.26457985\n",
      "Iteration 46, loss = 0.26305958\n",
      "Iteration 49, loss = 0.26475939\n",
      "Iteration 47, loss = 0.26307489\n",
      "Iteration 50, loss = 0.26424780\n",
      "Iteration 48, loss = 0.26279615\n",
      "Iteration 51, loss = 0.26431536\n",
      "Iteration 49, loss = 0.26286239\n",
      "Iteration 52, loss = 0.26389231\n",
      "Iteration 50, loss = 0.26264403\n",
      "Iteration 53, loss = 0.26347194\n",
      "Iteration 51, loss = 0.26257196\n",
      "Iteration 54, loss = 0.26361825\n",
      "Iteration 52, loss = 0.26236888\n",
      "Iteration 55, loss = 0.26343954\n",
      "Iteration 53, loss = 0.26206936\n",
      "Iteration 56, loss = 0.26327630\n",
      "Iteration 54, loss = 0.26218062\n",
      "Iteration 57, loss = 0.26276815\n",
      "Iteration 55, loss = 0.26203655\n",
      "Iteration 58, loss = 0.26278878\n",
      "Iteration 59, loss = 0.26271566\n",
      "Iteration 56, loss = 0.26207356\n",
      "Iteration 57, loss = 0.26175043\n",
      "Iteration 60, loss = 0.26274360\n",
      "Iteration 61, loss = 0.26248262\n",
      "Iteration 58, loss = 0.26176319\n",
      "Iteration 62, loss = 0.26233984\n",
      "Iteration 59, loss = 0.26197358\n",
      "Iteration 63, loss = 0.26212891\n",
      "Iteration 60, loss = 0.26172095\n",
      "Iteration 64, loss = 0.26199464\n",
      "Iteration 61, loss = 0.26173803\n",
      "Iteration 65, loss = 0.26196924\n",
      "Iteration 62, loss = 0.26149430\n",
      "Iteration 66, loss = 0.26155963\n",
      "Iteration 63, loss = 0.26122631\n",
      "Iteration 67, loss = 0.26148225\n",
      "Iteration 64, loss = 0.26124818\n",
      "Iteration 68, loss = 0.26182135\n",
      "Iteration 65, loss = 0.26133781\n",
      "Iteration 69, loss = 0.26134961\n",
      "Iteration 66, loss = 0.26095275\n",
      "Iteration 70, loss = 0.26143390\n",
      "Iteration 67, loss = 0.26069915\n",
      "Iteration 71, loss = 0.26109412\n",
      "Iteration 68, loss = 0.26090852\n",
      "Iteration 72, loss = 0.26117748\n",
      "Iteration 69, loss = 0.26093542\n",
      "Iteration 73, loss = 0.26095221\n",
      "Iteration 70, loss = 0.26062771\n",
      "Iteration 74, loss = 0.26096193\n",
      "Iteration 71, loss = 0.26069437\n",
      "Iteration 75, loss = 0.26069749\n",
      "Iteration 72, loss = 0.26078549\n",
      "Iteration 76, loss = 0.26100309\n",
      "Iteration 73, loss = 0.26038398\n",
      "Iteration 77, loss = 0.26085362\n",
      "Iteration 74, loss = 0.26054378\n",
      "Iteration 78, loss = 0.26069424\n",
      "Iteration 75, loss = 0.26020355\n",
      "Iteration 79, loss = 0.26073616\n",
      "Iteration 76, loss = 0.26046245\n",
      "Iteration 80, loss = 0.26037974\n",
      "Iteration 77, loss = 0.26023776\n",
      "Iteration 81, loss = 0.26029162\n",
      "Iteration 78, loss = 0.26018378\n",
      "Iteration 82, loss = 0.26019753\n",
      "Iteration 79, loss = 0.26009180\n",
      "Iteration 83, loss = 0.26044490\n",
      "Iteration 84, loss = 0.26028997\n",
      "Iteration 80, loss = 0.25990496\n",
      "Iteration 85, loss = 0.26016223\n",
      "Iteration 81, loss = 0.25998345\n",
      "Iteration 86, loss = 0.26025498\n",
      "Iteration 82, loss = 0.25979545\n",
      "Iteration 87, loss = 0.26037886\n",
      "Iteration 88, loss = 0.26024621\n",
      "Iteration 83, loss = 0.26017043\n",
      "Iteration 89, loss = 0.26004693\n",
      "Iteration 84, loss = 0.25966269\n",
      "Iteration 90, loss = 0.26011114\n",
      "Iteration 85, loss = 0.25981629\n",
      "Iteration 91, loss = 0.25998589\n",
      "Iteration 86, loss = 0.25970122\n",
      "Iteration 92, loss = 0.25997077\n",
      "Iteration 87, loss = 0.25989696\n",
      "Iteration 93, loss = 0.26011581\n",
      "Iteration 88, loss = 0.25992332\n",
      "Iteration 94, loss = 0.26008473\n",
      "Iteration 89, loss = 0.25955411\n",
      "Iteration 95, loss = 0.25964808\n",
      "Iteration 90, loss = 0.25983194\n",
      "Iteration 91, loss = 0.25948394\n",
      "Iteration 96, loss = 0.25987558\n",
      "Iteration 97, loss = 0.25975883\n",
      "Iteration 92, loss = 0.25948696\n",
      "Iteration 98, loss = 0.25984918\n",
      "Iteration 93, loss = 0.25993365\n",
      "Iteration 94, loss = 0.25972978\n",
      "Iteration 99, loss = 0.25988721\n",
      "Iteration 95, loss = 0.25922318\n",
      "Iteration 100, loss = 0.25955733\n",
      "Iteration 96, loss = 0.25942262\n",
      "Iteration 101, loss = 0.25964330\n",
      "Iteration 97, loss = 0.25923280\n",
      "Iteration 102, loss = 0.25942142\n",
      "Iteration 98, loss = 0.25930318\n",
      "Iteration 103, loss = 0.25940673\n",
      "Iteration 99, loss = 0.25929433\n",
      "Iteration 104, loss = 0.25954648\n",
      "Iteration 100, loss = 0.25900048\n",
      "Iteration 105, loss = 0.25967408\n",
      "Iteration 101, loss = 0.25908289\n",
      "Iteration 106, loss = 0.25938729\n",
      "Iteration 102, loss = 0.25906338\n",
      "Iteration 107, loss = 0.25925437\n",
      "Iteration 103, loss = 0.25905578\n",
      "Iteration 108, loss = 0.25990692\n",
      "Iteration 104, loss = 0.25904349\n",
      "Iteration 109, loss = 0.25943510\n",
      "Iteration 105, loss = 0.25912497\n",
      "Iteration 110, loss = 0.25955979\n",
      "Iteration 106, loss = 0.25885360\n",
      "Iteration 111, loss = 0.25907187\n",
      "Iteration 107, loss = 0.25897819\n",
      "Iteration 112, loss = 0.25923167\n",
      "Iteration 108, loss = 0.25948057\n",
      "Iteration 113, loss = 0.25910313\n",
      "Iteration 109, loss = 0.25899931\n",
      "Iteration 114, loss = 0.25912999\n",
      "Iteration 110, loss = 0.25902353\n",
      "Iteration 115, loss = 0.25909980\n",
      "Iteration 111, loss = 0.25876096\n",
      "Iteration 116, loss = 0.25955187\n",
      "Iteration 112, loss = 0.25876546\n",
      "Iteration 117, loss = 0.25891771\n",
      "Iteration 113, loss = 0.25859959\n",
      "Iteration 118, loss = 0.25891095\n",
      "Iteration 114, loss = 0.25863223\n",
      "Iteration 119, loss = 0.25899412\n",
      "Iteration 115, loss = 0.25889239\n",
      "Iteration 120, loss = 0.25923530\n",
      "Iteration 116, loss = 0.25902912\n",
      "Iteration 121, loss = 0.25889293\n",
      "Iteration 117, loss = 0.25849629\n",
      "Iteration 122, loss = 0.25892504\n",
      "Iteration 118, loss = 0.25850321\n",
      "Iteration 123, loss = 0.25888733\n",
      "Iteration 119, loss = 0.25856449\n",
      "Iteration 124, loss = 0.25875866\n",
      "Iteration 120, loss = 0.25873730\n",
      "Iteration 125, loss = 0.25904424\n",
      "Iteration 121, loss = 0.25848406\n",
      "Iteration 126, loss = 0.25889261\n",
      "Iteration 122, loss = 0.25858909\n",
      "Iteration 127, loss = 0.25866938\n",
      "Iteration 123, loss = 0.25849705\n",
      "Iteration 128, loss = 0.25885831\n",
      "Iteration 124, loss = 0.25834407\n",
      "Iteration 129, loss = 0.25900062\n",
      "Iteration 125, loss = 0.25865632\n",
      "Iteration 130, loss = 0.25874748\n",
      "Iteration 126, loss = 0.25837375\n",
      "Iteration 131, loss = 0.25883348\n",
      "Iteration 127, loss = 0.25842848\n",
      "Iteration 132, loss = 0.25857557\n",
      "Iteration 128, loss = 0.25838543\n",
      "Iteration 133, loss = 0.25859635\n",
      "Iteration 129, loss = 0.25846359\n",
      "Iteration 130, loss = 0.25843565\n",
      "Iteration 134, loss = 0.25867273\n",
      "Iteration 131, loss = 0.25831772\n",
      "Iteration 135, loss = 0.25860325\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 132, loss = 0.25799787\n",
      "Iteration 133, loss = 0.25824572\n",
      "Iteration 1, loss = 0.33187773\n",
      "Iteration 134, loss = 0.25822663\n",
      "Iteration 2, loss = 0.28639609\n",
      "Iteration 135, loss = 0.25809338\n",
      "Iteration 3, loss = 0.27900297\n",
      "Iteration 136, loss = 0.25809410\n",
      "Iteration 4, loss = 0.27694893\n",
      "Iteration 137, loss = 0.25805425\n",
      "Iteration 5, loss = 0.27621606\n",
      "Iteration 138, loss = 0.25816729\n",
      "Iteration 6, loss = 0.27573009\n",
      "Iteration 139, loss = 0.25830011\n",
      "Iteration 7, loss = 0.27577178\n",
      "Iteration 140, loss = 0.25798146\n",
      "Iteration 8, loss = 0.27580079\n",
      "Iteration 141, loss = 0.25801467\n",
      "Iteration 9, loss = 0.27465158\n",
      "Iteration 142, loss = 0.25819023\n",
      "Iteration 10, loss = 0.27416539\n",
      "Iteration 143, loss = 0.25800207\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 11, loss = 0.27390155\n",
      "Iteration 12, loss = 0.27380543\n",
      "Iteration 1, loss = 0.33114800\n",
      "Iteration 13, loss = 0.27342964\n",
      "Iteration 2, loss = 0.28663135\n",
      "Iteration 14, loss = 0.27288971\n",
      "Iteration 3, loss = 0.27914705\n",
      "Iteration 15, loss = 0.27284572\n",
      "Iteration 4, loss = 0.27714570\n",
      "Iteration 16, loss = 0.27212447\n",
      "Iteration 5, loss = 0.27636720\n",
      "Iteration 17, loss = 0.27205486\n",
      "Iteration 6, loss = 0.27594113\n",
      "Iteration 18, loss = 0.27181715\n",
      "Iteration 7, loss = 0.27524569\n",
      "Iteration 19, loss = 0.27165077\n",
      "Iteration 8, loss = 0.27512425\n",
      "Iteration 20, loss = 0.27179021\n",
      "Iteration 9, loss = 0.27450110\n",
      "Iteration 21, loss = 0.27120423\n",
      "Iteration 10, loss = 0.27373952\n",
      "Iteration 22, loss = 0.27148482\n",
      "Iteration 11, loss = 0.27320441\n",
      "Iteration 23, loss = 0.27076754\n",
      "Iteration 12, loss = 0.27273088\n",
      "Iteration 24, loss = 0.27034836\n",
      "Iteration 13, loss = 0.27234387\n",
      "Iteration 25, loss = 0.27003571\n",
      "Iteration 14, loss = 0.27175482\n",
      "Iteration 26, loss = 0.27017108\n",
      "Iteration 27, loss = 0.26970664\n",
      "Iteration 15, loss = 0.27164151\n",
      "Iteration 28, loss = 0.26951121\n",
      "Iteration 16, loss = 0.27112774\n",
      "Iteration 29, loss = 0.26921985\n",
      "Iteration 17, loss = 0.27089620\n",
      "Iteration 30, loss = 0.26906343\n",
      "Iteration 18, loss = 0.27017601\n",
      "Iteration 31, loss = 0.26881078\n",
      "Iteration 19, loss = 0.27003414\n",
      "Iteration 32, loss = 0.26885210\n",
      "Iteration 20, loss = 0.26980748\n",
      "Iteration 21, loss = 0.26969747\n",
      "Iteration 33, loss = 0.26861954\n",
      "Iteration 22, loss = 0.26942530\n",
      "Iteration 23, loss = 0.26898480\n",
      "Iteration 34, loss = 0.26846912\n",
      "Iteration 24, loss = 0.26881869\n",
      "Iteration 35, loss = 0.26831743\n",
      "Iteration 25, loss = 0.26825905\n",
      "Iteration 36, loss = 0.26825940\n",
      "Iteration 26, loss = 0.26827504\n",
      "Iteration 37, loss = 0.26780311\n",
      "Iteration 27, loss = 0.26791720\n",
      "Iteration 38, loss = 0.26767738\n",
      "Iteration 28, loss = 0.26739887\n",
      "Iteration 39, loss = 0.26760807\n",
      "Iteration 29, loss = 0.26737418\n",
      "Iteration 40, loss = 0.26756323\n",
      "Iteration 30, loss = 0.26692487\n",
      "Iteration 41, loss = 0.26765052\n",
      "Iteration 31, loss = 0.26655635\n",
      "Iteration 42, loss = 0.26748374\n",
      "Iteration 32, loss = 0.26656205\n",
      "Iteration 43, loss = 0.26673280\n",
      "Iteration 33, loss = 0.26639927\n",
      "Iteration 44, loss = 0.26699849\n",
      "Iteration 34, loss = 0.26615564\n",
      "Iteration 45, loss = 0.26659200\n",
      "Iteration 35, loss = 0.26640057\n",
      "Iteration 46, loss = 0.26643970\n",
      "Iteration 36, loss = 0.26595807\n",
      "Iteration 47, loss = 0.26628943\n",
      "Iteration 37, loss = 0.26610566\n",
      "Iteration 48, loss = 0.26626413\n",
      "Iteration 49, loss = 0.26652651\n",
      "Iteration 38, loss = 0.26522063\n",
      "Iteration 50, loss = 0.26586351\n",
      "Iteration 39, loss = 0.26523828\n",
      "Iteration 51, loss = 0.26594318\n",
      "Iteration 40, loss = 0.26529628\n",
      "Iteration 52, loss = 0.26570183\n",
      "Iteration 41, loss = 0.26527638\n",
      "Iteration 53, loss = 0.26557136\n",
      "Iteration 42, loss = 0.26488406\n",
      "Iteration 54, loss = 0.26551481\n",
      "Iteration 43, loss = 0.26516917\n",
      "Iteration 55, loss = 0.26563181\n",
      "Iteration 44, loss = 0.26467101\n",
      "Iteration 56, loss = 0.26534673\n",
      "Iteration 57, loss = 0.26498127\n",
      "Iteration 45, loss = 0.26469402\n",
      "Iteration 46, loss = 0.26458893\n",
      "Iteration 58, loss = 0.26508418\n",
      "Iteration 47, loss = 0.26426659\n",
      "Iteration 59, loss = 0.26508632\n",
      "Iteration 48, loss = 0.26420068\n",
      "Iteration 60, loss = 0.26531660\n",
      "Iteration 49, loss = 0.26405976\n",
      "Iteration 61, loss = 0.26482773\n",
      "Iteration 50, loss = 0.26399618\n",
      "Iteration 62, loss = 0.26484665\n",
      "Iteration 51, loss = 0.26386000\n",
      "Iteration 63, loss = 0.26471028\n",
      "Iteration 52, loss = 0.26390492\n",
      "Iteration 64, loss = 0.26461286\n",
      "Iteration 53, loss = 0.26389642\n",
      "Iteration 65, loss = 0.26451289\n",
      "Iteration 54, loss = 0.26392758\n",
      "Iteration 66, loss = 0.26423267\n",
      "Iteration 55, loss = 0.26371205\n",
      "Iteration 67, loss = 0.26417617\n",
      "Iteration 56, loss = 0.26334290\n",
      "Iteration 68, loss = 0.26443640\n",
      "Iteration 57, loss = 0.26321072\n",
      "Iteration 69, loss = 0.26416775\n",
      "Iteration 58, loss = 0.26338521\n",
      "Iteration 70, loss = 0.26446947Iteration 59, loss = 0.26322663\n",
      "\n",
      "Iteration 71, loss = 0.26397585\n",
      "Iteration 60, loss = 0.26298531\n",
      "Iteration 61, loss = 0.26288605\n",
      "Iteration 72, loss = 0.26405115\n",
      "Iteration 73, loss = 0.26392442Iteration 62, loss = 0.26288727\n",
      "\n",
      "Iteration 74, loss = 0.26377870\n",
      "Iteration 63, loss = 0.26326507\n",
      "Iteration 75, loss = 0.26375623\n",
      "Iteration 64, loss = 0.26288502\n",
      "Iteration 76, loss = 0.26390953\n",
      "Iteration 65, loss = 0.26256062\n",
      "Iteration 77, loss = 0.26374380\n",
      "Iteration 66, loss = 0.26272382\n",
      "Iteration 78, loss = 0.26366625\n",
      "Iteration 67, loss = 0.26276091\n",
      "Iteration 68, loss = 0.26259000\n",
      "Iteration 79, loss = 0.26374990\n",
      "Iteration 69, loss = 0.26248821\n",
      "Iteration 80, loss = 0.26345176\n",
      "Iteration 70, loss = 0.26240850\n",
      "Iteration 81, loss = 0.26341360\n",
      "Iteration 82, loss = 0.26328002\n",
      "Iteration 71, loss = 0.26208164\n",
      "Iteration 72, loss = 0.26219115\n",
      "Iteration 83, loss = 0.26336612\n",
      "Iteration 73, loss = 0.26212886\n",
      "Iteration 84, loss = 0.26332122\n",
      "Iteration 85, loss = 0.26328592\n",
      "Iteration 74, loss = 0.26218859\n",
      "Iteration 86, loss = 0.26327173\n",
      "Iteration 75, loss = 0.26223964\n",
      "Iteration 87, loss = 0.26334086\n",
      "Iteration 76, loss = 0.26214782\n",
      "Iteration 88, loss = 0.26330082\n",
      "Iteration 77, loss = 0.26209432\n",
      "Iteration 89, loss = 0.26330899\n",
      "Iteration 78, loss = 0.26180588\n",
      "Iteration 90, loss = 0.26305071\n",
      "Iteration 79, loss = 0.26181950\n",
      "Iteration 91, loss = 0.26290725\n",
      "Iteration 80, loss = 0.26183977\n",
      "Iteration 92, loss = 0.26304844\n",
      "Iteration 81, loss = 0.26168555\n",
      "Iteration 93, loss = 0.26293840\n",
      "Iteration 82, loss = 0.26168008\n",
      "Iteration 94, loss = 0.26308442\n",
      "Iteration 83, loss = 0.26158566\n",
      "Iteration 95, loss = 0.26263607\n",
      "Iteration 84, loss = 0.26174626\n",
      "Iteration 96, loss = 0.26285041\n",
      "Iteration 85, loss = 0.26159553\n",
      "Iteration 97, loss = 0.26269893\n",
      "Iteration 86, loss = 0.26134210\n",
      "Iteration 98, loss = 0.26291074\n",
      "Iteration 87, loss = 0.26149051\n",
      "Iteration 99, loss = 0.26256857\n",
      "Iteration 88, loss = 0.26122206\n",
      "Iteration 100, loss = 0.26251366\n",
      "Iteration 89, loss = 0.26144113\n",
      "Iteration 101, loss = 0.26267076\n",
      "Iteration 90, loss = 0.26137286\n",
      "Iteration 102, loss = 0.26264667\n",
      "Iteration 103, loss = 0.26235726\n",
      "Iteration 91, loss = 0.26130822\n",
      "Iteration 104, loss = 0.26232157\n",
      "Iteration 105, loss = 0.26264142\n",
      "Iteration 92, loss = 0.26114942\n",
      "Iteration 93, loss = 0.26114737\n",
      "Iteration 106, loss = 0.26231133\n",
      "Iteration 94, loss = 0.26109788\n",
      "Iteration 107, loss = 0.26209651\n",
      "Iteration 95, loss = 0.26123269\n",
      "Iteration 108, loss = 0.26220836\n",
      "Iteration 96, loss = 0.26107388\n",
      "Iteration 109, loss = 0.26219216\n",
      "Iteration 110, loss = 0.26230385\n",
      "Iteration 97, loss = 0.26092486\n",
      "Iteration 111, loss = 0.26189554\n",
      "Iteration 98, loss = 0.26133796\n",
      "Iteration 112, loss = 0.26193516\n",
      "Iteration 113, loss = 0.26182385\n",
      "Iteration 99, loss = 0.26114949\n",
      "Iteration 114, loss = 0.26193739\n",
      "Iteration 100, loss = 0.26063794\n",
      "Iteration 115, loss = 0.26192160\n",
      "Iteration 101, loss = 0.26102619\n",
      "Iteration 116, loss = 0.26222030\n",
      "Iteration 102, loss = 0.26091397\n",
      "Iteration 117, loss = 0.26164817\n",
      "Iteration 103, loss = 0.26076187\n",
      "Iteration 118, loss = 0.26158347\n",
      "Iteration 104, loss = 0.26080156\n",
      "Iteration 105, loss = 0.26072286\n",
      "Iteration 119, loss = 0.26168986\n",
      "Iteration 106, loss = 0.26064383\n",
      "Iteration 120, loss = 0.26191004\n",
      "Iteration 107, loss = 0.26088637\n",
      "Iteration 121, loss = 0.26158798\n",
      "Iteration 108, loss = 0.26067339\n",
      "Iteration 122, loss = 0.26156437\n",
      "Iteration 109, loss = 0.26057821\n",
      "Iteration 123, loss = 0.26148228\n",
      "Iteration 110, loss = 0.26071498\n",
      "Iteration 124, loss = 0.26148338\n",
      "Iteration 111, loss = 0.26047947\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 125, loss = 0.26153382\n",
      "Iteration 126, loss = 0.26143497\n",
      "Iteration 1, loss = 0.33129023\n",
      "Iteration 127, loss = 0.26146359\n",
      "Iteration 2, loss = 0.28736712\n",
      "Iteration 128, loss = 0.26132056\n",
      "Iteration 3, loss = 0.28049680\n",
      "Iteration 129, loss = 0.26155243\n",
      "Iteration 4, loss = 0.27855734\n",
      "Iteration 130, loss = 0.26126268\n",
      "Iteration 5, loss = 0.27796356\n",
      "Iteration 131, loss = 0.26131463\n",
      "Iteration 6, loss = 0.27773523\n",
      "Iteration 132, loss = 0.26118955\n",
      "Iteration 7, loss = 0.27697945\n",
      "Iteration 133, loss = 0.26102720\n",
      "Iteration 8, loss = 0.27690934\n",
      "Iteration 134, loss = 0.26116363\n",
      "Iteration 9, loss = 0.27613059\n",
      "Iteration 135, loss = 0.26118507\n",
      "Iteration 10, loss = 0.27561696\n",
      "Iteration 136, loss = 0.26099136\n",
      "Iteration 11, loss = 0.27512282\n",
      "Iteration 137, loss = 0.26108597\n",
      "Iteration 12, loss = 0.27468666\n",
      "Iteration 138, loss = 0.26121429\n",
      "Iteration 13, loss = 0.27448163\n",
      "Iteration 139, loss = 0.26100096\n",
      "Iteration 14, loss = 0.27401236\n",
      "Iteration 140, loss = 0.26104771\n",
      "Iteration 15, loss = 0.27373318\n",
      "Iteration 141, loss = 0.26106524\n",
      "Iteration 16, loss = 0.27334635\n",
      "Iteration 142, loss = 0.26128166\n",
      "Iteration 17, loss = 0.27305432\n",
      "Iteration 143, loss = 0.26090410\n",
      "Iteration 18, loss = 0.27266828\n",
      "Iteration 144, loss = 0.26093727\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 19, loss = 0.27245293\n",
      "Iteration 20, loss = 0.27229234\n",
      "Iteration 1, loss = 0.32993702\n",
      "Iteration 21, loss = 0.27238981\n",
      "Iteration 2, loss = 0.28545836\n",
      "Iteration 22, loss = 0.27178952\n",
      "Iteration 3, loss = 0.27848891\n",
      "Iteration 23, loss = 0.27126687\n",
      "Iteration 4, loss = 0.27652791\n",
      "Iteration 5, loss = 0.27571029\n",
      "Iteration 24, loss = 0.27119757\n",
      "Iteration 25, loss = 0.27040949\n",
      "Iteration 6, loss = 0.27539533\n",
      "Iteration 7, loss = 0.27546407\n",
      "Iteration 26, loss = 0.27042066\n",
      "Iteration 8, loss = 0.27494710\n",
      "Iteration 27, loss = 0.26999470\n",
      "Iteration 9, loss = 0.27436623\n",
      "Iteration 28, loss = 0.26962768\n",
      "Iteration 10, loss = 0.27403786\n",
      "Iteration 29, loss = 0.26921805\n",
      "Iteration 11, loss = 0.27346057\n",
      "Iteration 30, loss = 0.26888002\n",
      "Iteration 12, loss = 0.27298776\n",
      "Iteration 31, loss = 0.26851539\n",
      "Iteration 13, loss = 0.27319080\n",
      "Iteration 32, loss = 0.26834053\n",
      "Iteration 14, loss = 0.27284733\n",
      "Iteration 33, loss = 0.26832703\n",
      "Iteration 15, loss = 0.27225590\n",
      "Iteration 34, loss = 0.26774683\n",
      "Iteration 16, loss = 0.27196024\n",
      "Iteration 35, loss = 0.26781422\n",
      "Iteration 17, loss = 0.27147232\n",
      "Iteration 36, loss = 0.26739610\n",
      "Iteration 18, loss = 0.27106947\n",
      "Iteration 19, loss = 0.27083318\n",
      "Iteration 37, loss = 0.26735780\n",
      "Iteration 20, loss = 0.27044121\n",
      "Iteration 38, loss = 0.26653120\n",
      "Iteration 21, loss = 0.27056260\n",
      "Iteration 39, loss = 0.26651257\n",
      "Iteration 22, loss = 0.26981388\n",
      "Iteration 40, loss = 0.26653020\n",
      "Iteration 23, loss = 0.26943782\n",
      "Iteration 41, loss = 0.26610666\n",
      "Iteration 42, loss = 0.26582268\n",
      "Iteration 24, loss = 0.26921203\n",
      "Iteration 43, loss = 0.26603645\n",
      "Iteration 25, loss = 0.26862749\n",
      "Iteration 44, loss = 0.26551623\n",
      "Iteration 26, loss = 0.26864144\n",
      "Iteration 45, loss = 0.26551466\n",
      "Iteration 27, loss = 0.26839931\n",
      "Iteration 46, loss = 0.26536276\n",
      "Iteration 28, loss = 0.26817887\n",
      "Iteration 47, loss = 0.26486916\n",
      "Iteration 29, loss = 0.26767305\n",
      "Iteration 48, loss = 0.26477150\n",
      "Iteration 30, loss = 0.26757516\n",
      "Iteration 49, loss = 0.26468790\n",
      "Iteration 31, loss = 0.26718956\n",
      "Iteration 32, loss = 0.26711954\n",
      "Iteration 50, loss = 0.26462336\n",
      "Iteration 33, loss = 0.26743319\n",
      "Iteration 51, loss = 0.26445810\n",
      "Iteration 34, loss = 0.26677060\n",
      "Iteration 52, loss = 0.26447010\n",
      "Iteration 35, loss = 0.26716528\n",
      "Iteration 53, loss = 0.26426216\n",
      "Iteration 36, loss = 0.26641118\n",
      "Iteration 54, loss = 0.26435864\n",
      "Iteration 37, loss = 0.26683734\n",
      "Iteration 55, loss = 0.26415948\n",
      "Iteration 38, loss = 0.26591676\n",
      "Iteration 56, loss = 0.26414552\n",
      "Iteration 39, loss = 0.26592452\n",
      "Iteration 57, loss = 0.26414469\n",
      "Iteration 40, loss = 0.26574882\n",
      "Iteration 58, loss = 0.26396610\n",
      "Iteration 41, loss = 0.26540252\n",
      "Iteration 59, loss = 0.26390821\n",
      "Iteration 60, loss = 0.26368273\n",
      "Iteration 42, loss = 0.26514300\n",
      "Iteration 43, loss = 0.26521670\n",
      "Iteration 44, loss = 0.26469428\n",
      "Iteration 61, loss = 0.26370175\n",
      "Iteration 62, loss = 0.26366335\n",
      "Iteration 45, loss = 0.26492055\n",
      "Iteration 63, loss = 0.26398121\n",
      "Iteration 46, loss = 0.26474383\n",
      "Iteration 64, loss = 0.26353070\n",
      "Iteration 47, loss = 0.26431326\n",
      "Iteration 65, loss = 0.26352974\n",
      "Iteration 48, loss = 0.26458442\n",
      "Iteration 66, loss = 0.26335521\n",
      "Iteration 49, loss = 0.26394148\n",
      "Iteration 67, loss = 0.26342412\n",
      "Iteration 50, loss = 0.26425604\n",
      "Iteration 68, loss = 0.26308701\n",
      "Iteration 51, loss = 0.26379069\n",
      "Iteration 69, loss = 0.26330854\n",
      "Iteration 52, loss = 0.26377637\n",
      "Iteration 53, loss = 0.26365010\n",
      "Iteration 70, loss = 0.26323637\n",
      "Iteration 71, loss = 0.26319403\n",
      "Iteration 54, loss = 0.26366487\n",
      "Iteration 72, loss = 0.26309806\n",
      "Iteration 55, loss = 0.26325298\n",
      "Iteration 73, loss = 0.26299645\n",
      "Iteration 56, loss = 0.26334743\n",
      "Iteration 57, loss = 0.26309630\n",
      "Iteration 74, loss = 0.26299226\n",
      "Iteration 58, loss = 0.26322531\n",
      "Iteration 75, loss = 0.26292408\n",
      "Iteration 59, loss = 0.26306196\n",
      "Iteration 76, loss = 0.26302157\n",
      "Iteration 60, loss = 0.26271817\n",
      "Iteration 77, loss = 0.26310291\n",
      "Iteration 61, loss = 0.26274975\n",
      "Iteration 78, loss = 0.26272820\n",
      "Iteration 62, loss = 0.26277910\n",
      "Iteration 79, loss = 0.26286413\n",
      "Iteration 80, loss = 0.26269882\n",
      "Iteration 63, loss = 0.26285608\n",
      "Iteration 81, loss = 0.26265747\n",
      "Iteration 64, loss = 0.26263803\n",
      "Iteration 82, loss = 0.26265107\n",
      "Iteration 65, loss = 0.26236864\n",
      "Iteration 83, loss = 0.26259046\n",
      "Iteration 66, loss = 0.26227289\n",
      "Iteration 84, loss = 0.26270602\n",
      "Iteration 67, loss = 0.26225010\n",
      "Iteration 85, loss = 0.26229795\n",
      "Iteration 68, loss = 0.26204706\n",
      "Iteration 86, loss = 0.26229800\n",
      "Iteration 69, loss = 0.26205501\n",
      "Iteration 70, loss = 0.26168589\n",
      "Iteration 87, loss = 0.26226126\n",
      "Iteration 71, loss = 0.26194980\n",
      "Iteration 88, loss = 0.26226197\n",
      "Iteration 72, loss = 0.26173980\n",
      "Iteration 89, loss = 0.26225023\n",
      "Iteration 73, loss = 0.26176693\n",
      "Iteration 90, loss = 0.26218786\n",
      "Iteration 74, loss = 0.26167626\n",
      "Iteration 91, loss = 0.26208140\n",
      "Iteration 75, loss = 0.26155669\n",
      "Iteration 92, loss = 0.26180575\n",
      "Iteration 76, loss = 0.26158040\n",
      "Iteration 93, loss = 0.26195159\n",
      "Iteration 77, loss = 0.26170140\n",
      "Iteration 94, loss = 0.26198102\n",
      "Iteration 78, loss = 0.26145180\n",
      "Iteration 95, loss = 0.26186316\n",
      "Iteration 79, loss = 0.26147007\n",
      "Iteration 80, loss = 0.26134933\n",
      "Iteration 96, loss = 0.26168624\n",
      "Iteration 81, loss = 0.26132923\n",
      "Iteration 97, loss = 0.26167177\n",
      "Iteration 82, loss = 0.26115800\n",
      "Iteration 98, loss = 0.26212722\n",
      "Iteration 99, loss = 0.26180111\n",
      "Iteration 83, loss = 0.26125555\n",
      "Iteration 100, loss = 0.26157298\n",
      "Iteration 84, loss = 0.26126244\n",
      "Iteration 85, loss = 0.26085315\n",
      "Iteration 101, loss = 0.26181763\n",
      "Iteration 86, loss = 0.26115990\n",
      "Iteration 102, loss = 0.26177600\n",
      "Iteration 87, loss = 0.26096552\n",
      "Iteration 103, loss = 0.26141961\n",
      "Iteration 88, loss = 0.26108897\n",
      "Iteration 104, loss = 0.26152242\n",
      "Iteration 89, loss = 0.26117717\n",
      "Iteration 90, loss = 0.26076706\n",
      "Iteration 105, loss = 0.26140275\n",
      "Iteration 91, loss = 0.26092427\n",
      "Iteration 106, loss = 0.26131856\n",
      "Iteration 92, loss = 0.26083985\n",
      "Iteration 107, loss = 0.26160010\n",
      "Iteration 93, loss = 0.26084317\n",
      "Iteration 108, loss = 0.26115752\n",
      "Iteration 94, loss = 0.26077470\n",
      "Iteration 109, loss = 0.26140286\n",
      "Iteration 95, loss = 0.26077041\n",
      "Iteration 110, loss = 0.26146475\n",
      "Iteration 96, loss = 0.26075168\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 111, loss = 0.26122345\n",
      "Iteration 112, loss = 0.26127331\n",
      "Iteration 1, loss = 0.33177921\n",
      "Iteration 113, loss = 0.26119466\n",
      "Iteration 2, loss = 0.28766156\n",
      "Iteration 114, loss = 0.26113466\n",
      "Iteration 3, loss = 0.28062302\n",
      "Iteration 115, loss = 0.26111261\n",
      "Iteration 4, loss = 0.27857695\n",
      "Iteration 116, loss = 0.26120935\n",
      "Iteration 5, loss = 0.27756583\n",
      "Iteration 117, loss = 0.26133976\n",
      "Iteration 6, loss = 0.27721285\n",
      "Iteration 118, loss = 0.26089504\n",
      "Iteration 7, loss = 0.27703127\n",
      "Iteration 119, loss = 0.26140019\n",
      "Iteration 8, loss = 0.27643919\n",
      "Iteration 120, loss = 0.26106522\n",
      "Iteration 9, loss = 0.27591278\n",
      "Iteration 121, loss = 0.26103168\n",
      "Iteration 10, loss = 0.27581136\n",
      "Iteration 122, loss = 0.26094075\n",
      "Iteration 11, loss = 0.27502523\n",
      "Iteration 123, loss = 0.26065952\n",
      "Iteration 12, loss = 0.27459221\n",
      "Iteration 124, loss = 0.26127037\n",
      "Iteration 13, loss = 0.27478836\n",
      "Iteration 125, loss = 0.26120676\n",
      "Iteration 14, loss = 0.27416903\n",
      "Iteration 126, loss = 0.26089362\n",
      "Iteration 15, loss = 0.27352799\n",
      "Iteration 127, loss = 0.26083416\n",
      "Iteration 16, loss = 0.27366893\n",
      "Iteration 128, loss = 0.26082522\n",
      "Iteration 17, loss = 0.27311953\n",
      "Iteration 129, loss = 0.26083867\n",
      "Iteration 18, loss = 0.27263235\n",
      "Iteration 19, loss = 0.27233043\n",
      "Iteration 130, loss = 0.26087815\n",
      "Iteration 20, loss = 0.27203106\n",
      "Iteration 131, loss = 0.26087764\n",
      "Iteration 132, loss = 0.26065371\n",
      "Iteration 21, loss = 0.27244713\n",
      "Iteration 22, loss = 0.27141993\n",
      "Iteration 133, loss = 0.26079450\n",
      "Iteration 134, loss = 0.26059055\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 23, loss = 0.27107648\n",
      "Iteration 24, loss = 0.27063503\n",
      "Iteration 25, loss = 0.27026806\n",
      "Iteration 1, loss = 0.33164205\n",
      "Iteration 26, loss = 0.26991355\n",
      "Iteration 2, loss = 0.28672278\n",
      "Iteration 27, loss = 0.26984390\n",
      "Iteration 3, loss = 0.27984909\n",
      "Iteration 28, loss = 0.26970362\n",
      "Iteration 4, loss = 0.27776742\n",
      "Iteration 29, loss = 0.26901902\n",
      "Iteration 5, loss = 0.27684282\n",
      "Iteration 30, loss = 0.26883798\n",
      "Iteration 6, loss = 0.27642670\n",
      "Iteration 31, loss = 0.26865048\n",
      "Iteration 7, loss = 0.27637396\n",
      "Iteration 32, loss = 0.26848146\n",
      "Iteration 8, loss = 0.27578088\n",
      "Iteration 33, loss = 0.26845147\n",
      "Iteration 9, loss = 0.27499313\n",
      "Iteration 34, loss = 0.26799897\n",
      "Iteration 10, loss = 0.27525200\n",
      "Iteration 35, loss = 0.26832655\n",
      "Iteration 11, loss = 0.27437031\n",
      "Iteration 36, loss = 0.26784129\n",
      "Iteration 12, loss = 0.27364767\n",
      "Iteration 37, loss = 0.26803896\n",
      "Iteration 13, loss = 0.27386870\n",
      "Iteration 38, loss = 0.26709871\n",
      "Iteration 14, loss = 0.27293639\n",
      "Iteration 39, loss = 0.26707925\n",
      "Iteration 15, loss = 0.27259191\n",
      "Iteration 40, loss = 0.26694036\n",
      "Iteration 16, loss = 0.27250617\n",
      "Iteration 41, loss = 0.26676174\n",
      "Iteration 17, loss = 0.27202076\n",
      "Iteration 42, loss = 0.26657222\n",
      "Iteration 18, loss = 0.27143158\n",
      "Iteration 43, loss = 0.26637463\n",
      "Iteration 44, loss = 0.26621897\n",
      "Iteration 19, loss = 0.27108602\n",
      "Iteration 45, loss = 0.26632639\n",
      "Iteration 20, loss = 0.27071495\n",
      "Iteration 46, loss = 0.26593584\n",
      "Iteration 21, loss = 0.27113327\n",
      "Iteration 47, loss = 0.26594888\n",
      "Iteration 22, loss = 0.26998661\n",
      "Iteration 48, loss = 0.26586430\n",
      "Iteration 23, loss = 0.26980173\n",
      "Iteration 24, loss = 0.26943960\n",
      "Iteration 49, loss = 0.26531400\n",
      "Iteration 25, loss = 0.26898365\n",
      "Iteration 50, loss = 0.26562660\n",
      "Iteration 26, loss = 0.26845202\n",
      "Iteration 51, loss = 0.26514932\n",
      "Iteration 27, loss = 0.26855374\n",
      "Iteration 52, loss = 0.26506452\n",
      "Iteration 53, loss = 0.26505405\n",
      "Iteration 28, loss = 0.26847330\n",
      "Iteration 54, loss = 0.26491471\n",
      "Iteration 29, loss = 0.26761255\n",
      "Iteration 55, loss = 0.26475693\n",
      "Iteration 30, loss = 0.26767627\n",
      "Iteration 31, loss = 0.26713993\n",
      "Iteration 56, loss = 0.26503650\n",
      "Iteration 57, loss = 0.26463867\n",
      "Iteration 32, loss = 0.26709911\n",
      "Iteration 58, loss = 0.26474395\n",
      "Iteration 33, loss = 0.26734380\n",
      "Iteration 59, loss = 0.26465192\n",
      "Iteration 34, loss = 0.26687819\n",
      "Iteration 35, loss = 0.26691337\n",
      "Iteration 60, loss = 0.26416142\n",
      "Iteration 36, loss = 0.26642876\n",
      "Iteration 37, loss = 0.26673703\n",
      "Iteration 61, loss = 0.26413505\n",
      "Iteration 38, loss = 0.26604876\n",
      "Iteration 62, loss = 0.26422030\n",
      "Iteration 39, loss = 0.26582911\n",
      "Iteration 63, loss = 0.26412183\n",
      "Iteration 40, loss = 0.26575028\n",
      "Iteration 64, loss = 0.26396198\n",
      "Iteration 41, loss = 0.26577318\n",
      "Iteration 65, loss = 0.26394123\n",
      "Iteration 42, loss = 0.26547249\n",
      "Iteration 66, loss = 0.26361728\n",
      "Iteration 43, loss = 0.26535615\n",
      "Iteration 67, loss = 0.26379063\n",
      "Iteration 44, loss = 0.26523948\n",
      "Iteration 68, loss = 0.26371906\n",
      "Iteration 45, loss = 0.26521951\n",
      "Iteration 69, loss = 0.26347670\n",
      "Iteration 46, loss = 0.26486187\n",
      "Iteration 70, loss = 0.26323256\n",
      "Iteration 47, loss = 0.26492824\n",
      "Iteration 71, loss = 0.26351209\n",
      "Iteration 48, loss = 0.26496701\n",
      "Iteration 49, loss = 0.26447900\n",
      "Iteration 72, loss = 0.26308224\n",
      "Iteration 50, loss = 0.26476366\n",
      "Iteration 73, loss = 0.26348610\n",
      "Iteration 51, loss = 0.26441139\n",
      "Iteration 74, loss = 0.26328177\n",
      "Iteration 75, loss = 0.26311018\n",
      "Iteration 52, loss = 0.26427528\n",
      "Iteration 53, loss = 0.26430355\n",
      "Iteration 76, loss = 0.26296062\n",
      "Iteration 54, loss = 0.26417502\n",
      "Iteration 77, loss = 0.26357312\n",
      "Iteration 55, loss = 0.26402582\n",
      "Iteration 78, loss = 0.26288604\n",
      "Iteration 56, loss = 0.26443108\n",
      "Iteration 79, loss = 0.26331449\n",
      "Iteration 57, loss = 0.26406482\n",
      "Iteration 80, loss = 0.26302293\n",
      "Iteration 58, loss = 0.26422093\n",
      "Iteration 81, loss = 0.26293775\n",
      "Iteration 59, loss = 0.26414044\n",
      "Iteration 82, loss = 0.26299405\n",
      "Iteration 60, loss = 0.26359894\n",
      "Iteration 83, loss = 0.26268509\n",
      "Iteration 61, loss = 0.26370902\n",
      "Iteration 84, loss = 0.26295756\n",
      "Iteration 62, loss = 0.26369077\n",
      "Iteration 85, loss = 0.26256199\n",
      "Iteration 63, loss = 0.26359213\n",
      "Iteration 86, loss = 0.26261340\n",
      "Iteration 64, loss = 0.26333272\n",
      "Iteration 87, loss = 0.26265863\n",
      "Iteration 65, loss = 0.26348861\n",
      "Iteration 88, loss = 0.26254180\n",
      "Iteration 66, loss = 0.26320912\n",
      "Iteration 89, loss = 0.26254861\n",
      "Iteration 67, loss = 0.26327171\n",
      "Iteration 90, loss = 0.26248902\n",
      "Iteration 68, loss = 0.26327653\n",
      "Iteration 91, loss = 0.26254830\n",
      "Iteration 69, loss = 0.26308713\n",
      "Iteration 92, loss = 0.26229879\n",
      "Iteration 70, loss = 0.26293157\n",
      "Iteration 93, loss = 0.26235647\n",
      "Iteration 71, loss = 0.26314918\n",
      "Iteration 94, loss = 0.26250739\n",
      "Iteration 72, loss = 0.26276780\n",
      "Iteration 95, loss = 0.26237644Iteration 73, loss = 0.26301606\n",
      "\n",
      "Iteration 96, loss = 0.26225527\n",
      "Iteration 74, loss = 0.26301211\n",
      "Iteration 97, loss = 0.26217499\n",
      "Iteration 75, loss = 0.26287231\n",
      "Iteration 98, loss = 0.26241710\n",
      "Iteration 76, loss = 0.26256145\n",
      "Iteration 99, loss = 0.26213693\n",
      "Iteration 77, loss = 0.26286829\n",
      "Iteration 100, loss = 0.26209642\n",
      "Iteration 78, loss = 0.26256542\n",
      "Iteration 101, loss = 0.26198275\n",
      "Iteration 79, loss = 0.26280617\n",
      "Iteration 102, loss = 0.26231538\n",
      "Iteration 80, loss = 0.26247857\n",
      "Iteration 103, loss = 0.26213072\n",
      "Iteration 81, loss = 0.26261860\n",
      "Iteration 82, loss = 0.26240744\n",
      "Iteration 104, loss = 0.26208883\n",
      "Iteration 83, loss = 0.26249756\n",
      "Iteration 105, loss = 0.26225205\n",
      "Iteration 84, loss = 0.26238356\n",
      "Iteration 106, loss = 0.26178694\n",
      "Iteration 85, loss = 0.26206718\n",
      "Iteration 107, loss = 0.26206605\n",
      "Iteration 86, loss = 0.26224977\n",
      "Iteration 108, loss = 0.26182105\n",
      "Iteration 87, loss = 0.26210176\n",
      "Iteration 109, loss = 0.26161623\n",
      "Iteration 88, loss = 0.26199214\n",
      "Iteration 110, loss = 0.26189765\n",
      "Iteration 89, loss = 0.26198670\n",
      "Iteration 111, loss = 0.26182195\n",
      "Iteration 90, loss = 0.26188665\n",
      "Iteration 112, loss = 0.26181243\n",
      "Iteration 91, loss = 0.26204280\n",
      "Iteration 92, loss = 0.26177416\n",
      "Iteration 113, loss = 0.26166660\n",
      "Iteration 93, loss = 0.26181676\n",
      "Iteration 114, loss = 0.26153030\n",
      "Iteration 94, loss = 0.26200867\n",
      "Iteration 115, loss = 0.26140627\n",
      "Iteration 95, loss = 0.26184958\n",
      "Iteration 116, loss = 0.26150288\n",
      "Iteration 96, loss = 0.26165112\n",
      "Iteration 117, loss = 0.26181335\n",
      "Iteration 97, loss = 0.26160850\n",
      "Iteration 118, loss = 0.26138975\n",
      "Iteration 98, loss = 0.26164899\n",
      "Iteration 119, loss = 0.26149292\n",
      "Iteration 99, loss = 0.26156112\n",
      "Iteration 120, loss = 0.26129347\n",
      "Iteration 100, loss = 0.26160302\n",
      "Iteration 121, loss = 0.26143408\n",
      "Iteration 101, loss = 0.26131177\n",
      "Iteration 122, loss = 0.26141190\n",
      "Iteration 102, loss = 0.26168905\n",
      "Iteration 123, loss = 0.26129371\n",
      "Iteration 103, loss = 0.26165124\n",
      "Iteration 124, loss = 0.26142427\n",
      "Iteration 104, loss = 0.26162222\n",
      "Iteration 125, loss = 0.26184526\n",
      "Iteration 105, loss = 0.26149343\n",
      "Iteration 106, loss = 0.26121076\n",
      "Iteration 126, loss = 0.26133890\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 107, loss = 0.26133495\n",
      "Iteration 108, loss = 0.26109327\n",
      "Iteration 1, loss = 0.33180788\n",
      "Iteration 109, loss = 0.26111476\n",
      "Iteration 2, loss = 0.28726732\n",
      "Iteration 110, loss = 0.26139417\n",
      "Iteration 3, loss = 0.28019545\n",
      "Iteration 111, loss = 0.26112180\n",
      "Iteration 112, loss = 0.26122926\n",
      "Iteration 4, loss = 0.27804479\n",
      "Iteration 5, loss = 0.27717219\n",
      "Iteration 113, loss = 0.26103067\n",
      "Iteration 6, loss = 0.27677237\n",
      "Iteration 7, loss = 0.27641730\n",
      "Iteration 114, loss = 0.26093224\n",
      "Iteration 115, loss = 0.26075932\n",
      "Iteration 8, loss = 0.27605655\n",
      "Iteration 9, loss = 0.27548707\n",
      "Iteration 116, loss = 0.26083752\n",
      "Iteration 10, loss = 0.27564452\n",
      "Iteration 117, loss = 0.26110368\n",
      "Iteration 11, loss = 0.27478299\n",
      "Iteration 118, loss = 0.26074330\n",
      "Iteration 12, loss = 0.27422818\n",
      "Iteration 119, loss = 0.26078973\n",
      "Iteration 13, loss = 0.27441023\n",
      "Iteration 120, loss = 0.26061389\n",
      "Iteration 14, loss = 0.27330915\n",
      "Iteration 121, loss = 0.26067792\n",
      "Iteration 15, loss = 0.27285213\n",
      "Iteration 122, loss = 0.26068333\n",
      "Iteration 16, loss = 0.27287527\n",
      "Iteration 123, loss = 0.26069197\n",
      "Iteration 17, loss = 0.27289928\n",
      "Iteration 124, loss = 0.26081785\n",
      "Iteration 18, loss = 0.27199653\n",
      "Iteration 19, loss = 0.27166443\n",
      "Iteration 125, loss = 0.26087234\n",
      "Iteration 126, loss = 0.26058030\n",
      "Iteration 20, loss = 0.27147574\n",
      "Iteration 127, loss = 0.26049280\n",
      "Iteration 21, loss = 0.27151375\n",
      "Iteration 128, loss = 0.26052702\n",
      "Iteration 22, loss = 0.27049032\n",
      "Iteration 129, loss = 0.26072802\n",
      "Iteration 23, loss = 0.27042757\n",
      "Iteration 130, loss = 0.26052452\n",
      "Iteration 24, loss = 0.26991858\n",
      "Iteration 131, loss = 0.26054467\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 25, loss = 0.26962669\n",
      "Iteration 26, loss = 0.26935682\n",
      "Iteration 27, loss = 0.26957926\n",
      "Iteration 1, loss = 0.33154267\n",
      "Iteration 28, loss = 0.26917827\n",
      "Iteration 2, loss = 0.28671822\n",
      "Iteration 29, loss = 0.26874245\n",
      "Iteration 3, loss = 0.27981573\n",
      "Iteration 30, loss = 0.26847458\n",
      "Iteration 4, loss = 0.27756600\n",
      "Iteration 31, loss = 0.26805336\n",
      "Iteration 32, loss = 0.26798777\n",
      "Iteration 5, loss = 0.27682690\n",
      "Iteration 33, loss = 0.26814785\n",
      "Iteration 34, loss = 0.26801634\n",
      "Iteration 6, loss = 0.27622886\n",
      "Iteration 35, loss = 0.26755566\n",
      "Iteration 7, loss = 0.27570839\n",
      "Iteration 36, loss = 0.26716333\n",
      "Iteration 37, loss = 0.26771894\n",
      "Iteration 8, loss = 0.27529506\n",
      "Iteration 38, loss = 0.26691697\n",
      "Iteration 9, loss = 0.27500041\n",
      "Iteration 39, loss = 0.26656133\n",
      "Iteration 10, loss = 0.27473747\n",
      "Iteration 40, loss = 0.26648066\n",
      "Iteration 11, loss = 0.27396548\n",
      "Iteration 41, loss = 0.26643124\n",
      "Iteration 42, loss = 0.26625454\n",
      "Iteration 12, loss = 0.27370451\n",
      "Iteration 43, loss = 0.26605101\n",
      "Iteration 13, loss = 0.27374554\n",
      "Iteration 44, loss = 0.26600263\n",
      "Iteration 14, loss = 0.27273947\n",
      "Iteration 15, loss = 0.27232584\n",
      "Iteration 45, loss = 0.26582220\n",
      "Iteration 16, loss = 0.27214632\n",
      "Iteration 46, loss = 0.26543635\n",
      "Iteration 17, loss = 0.27214774\n",
      "Iteration 47, loss = 0.26542017\n",
      "Iteration 18, loss = 0.27133240\n",
      "Iteration 19, loss = 0.27107676\n",
      "Iteration 48, loss = 0.26560687\n",
      "Iteration 20, loss = 0.27081091\n",
      "Iteration 49, loss = 0.26491646\n",
      "Iteration 21, loss = 0.27101855\n",
      "Iteration 50, loss = 0.26537501\n",
      "Iteration 22, loss = 0.26975298\n",
      "Iteration 51, loss = 0.26511050\n",
      "Iteration 23, loss = 0.26961473\n",
      "Iteration 52, loss = 0.26499997\n",
      "Iteration 24, loss = 0.26921721\n",
      "Iteration 53, loss = 0.26479856\n",
      "Iteration 25, loss = 0.26892318\n",
      "Iteration 54, loss = 0.26451830\n",
      "Iteration 26, loss = 0.26860322\n",
      "Iteration 55, loss = 0.26426832\n",
      "Iteration 27, loss = 0.26864181\n",
      "Iteration 28, loss = 0.26829527\n",
      "Iteration 56, loss = 0.26456806\n",
      "Iteration 29, loss = 0.26784961\n",
      "Iteration 57, loss = 0.26406958\n",
      "Iteration 30, loss = 0.26746047\n",
      "Iteration 58, loss = 0.26429328\n",
      "Iteration 31, loss = 0.26712157\n",
      "Iteration 59, loss = 0.26408474\n",
      "Iteration 32, loss = 0.26704517\n",
      "Iteration 60, loss = 0.26376717\n",
      "Iteration 61, loss = 0.26366574\n",
      "Iteration 33, loss = 0.26753319\n",
      "Iteration 62, loss = 0.26358193\n",
      "Iteration 34, loss = 0.26684062\n",
      "Iteration 63, loss = 0.26357734\n",
      "Iteration 35, loss = 0.26672753\n",
      "Iteration 64, loss = 0.26348685\n",
      "Iteration 36, loss = 0.26639947\n",
      "Iteration 65, loss = 0.26336377\n",
      "Iteration 37, loss = 0.26675284\n",
      "Iteration 66, loss = 0.26314522\n",
      "Iteration 38, loss = 0.26594148\n",
      "Iteration 67, loss = 0.26306039\n",
      "Iteration 39, loss = 0.26569675\n",
      "Iteration 68, loss = 0.26307359\n",
      "Iteration 40, loss = 0.26566191\n",
      "Iteration 69, loss = 0.26302856\n",
      "Iteration 41, loss = 0.26541336\n",
      "Iteration 70, loss = 0.26282312\n",
      "Iteration 42, loss = 0.26564969\n",
      "Iteration 71, loss = 0.26275668\n",
      "Iteration 43, loss = 0.26520817\n",
      "Iteration 72, loss = 0.26258659\n",
      "Iteration 44, loss = 0.26495717\n",
      "Iteration 73, loss = 0.26280466\n",
      "Iteration 45, loss = 0.26510147\n",
      "Iteration 74, loss = 0.26274417\n",
      "Iteration 46, loss = 0.26480719\n",
      "Iteration 75, loss = 0.26281343\n",
      "Iteration 47, loss = 0.26477139\n",
      "Iteration 76, loss = 0.26239785\n",
      "Iteration 48, loss = 0.26480001\n",
      "Iteration 77, loss = 0.26250036\n",
      "Iteration 78, loss = 0.26239196\n",
      "Iteration 49, loss = 0.26449694\n",
      "Iteration 79, loss = 0.26243978\n",
      "Iteration 50, loss = 0.26465947\n",
      "Iteration 51, loss = 0.26414841\n",
      "Iteration 80, loss = 0.26217533\n",
      "Iteration 81, loss = 0.26225832\n",
      "Iteration 52, loss = 0.26429243\n",
      "Iteration 82, loss = 0.26211605\n",
      "Iteration 53, loss = 0.26395270\n",
      "Iteration 83, loss = 0.26205637\n",
      "Iteration 54, loss = 0.26390049\n",
      "Iteration 84, loss = 0.26217422\n",
      "Iteration 55, loss = 0.26369592\n",
      "Iteration 85, loss = 0.26179770\n",
      "Iteration 56, loss = 0.26390086\n",
      "Iteration 86, loss = 0.26185663\n",
      "Iteration 57, loss = 0.26349398\n",
      "Iteration 87, loss = 0.26175725\n",
      "Iteration 58, loss = 0.26391560\n",
      "Iteration 88, loss = 0.26166526\n",
      "Iteration 59, loss = 0.26356387\n",
      "Iteration 89, loss = 0.26198815\n",
      "Iteration 60, loss = 0.26341344\n",
      "Iteration 90, loss = 0.26173683\n",
      "Iteration 61, loss = 0.26319535\n",
      "Iteration 91, loss = 0.26173943\n",
      "Iteration 62, loss = 0.26331737\n",
      "Iteration 92, loss = 0.26162721\n",
      "Iteration 63, loss = 0.26317363\n",
      "Iteration 93, loss = 0.26177959Iteration 64, loss = 0.26318143\n",
      "\n",
      "Iteration 65, loss = 0.26320170\n",
      "Iteration 94, loss = 0.26170289\n",
      "Iteration 66, loss = 0.26305686\n",
      "Iteration 95, loss = 0.26160915\n",
      "Iteration 67, loss = 0.26314118\n",
      "Iteration 96, loss = 0.26158013\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 68, loss = 0.26296190\n",
      "Iteration 69, loss = 0.26292322\n",
      "Iteration 70, loss = 0.26261620\n",
      "Iteration 71, loss = 0.26259742\n",
      "Iteration 72, loss = 0.26262361\n",
      "Iteration 73, loss = 0.26279990\n",
      "Iteration 74, loss = 0.26268615\n",
      "Iteration 75, loss = 0.26263468\n",
      "Iteration 76, loss = 0.26256713\n",
      "Iteration 77, loss = 0.26238644\n",
      "Iteration 78, loss = 0.26250677\n",
      "Iteration 79, loss = 0.26246552\n",
      "Iteration 80, loss = 0.26220690\n",
      "Iteration 81, loss = 0.26237951\n",
      "Iteration 82, loss = 0.26225857\n",
      "Iteration 83, loss = 0.26227968\n",
      "Iteration 84, loss = 0.26238607\n",
      "Iteration 85, loss = 0.26210615\n",
      "Iteration 86, loss = 0.26204192\n",
      "Iteration 87, loss = 0.26197311\n",
      "Iteration 88, loss = 0.26198939\n",
      "Iteration 89, loss = 0.26221042\n",
      "Iteration 90, loss = 0.26191305\n",
      "Iteration 91, loss = 0.26195561\n",
      "Iteration 92, loss = 0.26190522\n",
      "Iteration 93, loss = 0.26187750\n",
      "Iteration 94, loss = 0.26180756\n",
      "Iteration 95, loss = 0.26177087\n",
      "Iteration 96, loss = 0.26180909\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33125225\n",
      "Iteration 1, loss = 0.33048840\n",
      "Iteration 2, loss = 0.28995020\n",
      "Iteration 2, loss = 0.28934463\n",
      "Iteration 3, loss = 0.28330371\n",
      "Iteration 3, loss = 0.28279558\n",
      "Iteration 4, loss = 0.28215250\n",
      "Iteration 4, loss = 0.28171658\n",
      "Iteration 5, loss = 0.28127187\n",
      "Iteration 5, loss = 0.28061858\n",
      "Iteration 6, loss = 0.28118295\n",
      "Iteration 6, loss = 0.28069397\n",
      "Iteration 7, loss = 0.28024879\n",
      "Iteration 7, loss = 0.27954690\n",
      "Iteration 8, loss = 0.27945086\n",
      "Iteration 8, loss = 0.27885742\n",
      "Iteration 9, loss = 0.27967639\n",
      "Iteration 9, loss = 0.27916197\n",
      "Iteration 10, loss = 0.27883931\n",
      "Iteration 10, loss = 0.27805842\n",
      "Iteration 11, loss = 0.27845230\n",
      "Iteration 12, loss = 0.27825685\n",
      "Iteration 11, loss = 0.27800594\n",
      "Iteration 13, loss = 0.27757492\n",
      "Iteration 12, loss = 0.27758633\n",
      "Iteration 14, loss = 0.27690591\n",
      "Iteration 13, loss = 0.27708407\n",
      "Iteration 15, loss = 0.27672225\n",
      "Iteration 14, loss = 0.27602458\n",
      "Iteration 16, loss = 0.27681060\n",
      "Iteration 15, loss = 0.27582022\n",
      "Iteration 17, loss = 0.27608953\n",
      "Iteration 16, loss = 0.27609531\n",
      "Iteration 18, loss = 0.27566216\n",
      "Iteration 17, loss = 0.27534107\n",
      "Iteration 19, loss = 0.27570379\n",
      "Iteration 18, loss = 0.27489960\n",
      "Iteration 20, loss = 0.27538348\n",
      "Iteration 19, loss = 0.27485672\n",
      "Iteration 21, loss = 0.27497973\n",
      "Iteration 20, loss = 0.27449765\n",
      "Iteration 22, loss = 0.27461077\n",
      "Iteration 21, loss = 0.27396117\n",
      "Iteration 23, loss = 0.27429928\n",
      "Iteration 22, loss = 0.27365704\n",
      "Iteration 24, loss = 0.27408161\n",
      "Iteration 23, loss = 0.27352988\n",
      "Iteration 25, loss = 0.27364003\n",
      "Iteration 24, loss = 0.27340919\n",
      "Iteration 26, loss = 0.27307091\n",
      "Iteration 25, loss = 0.27307211\n",
      "Iteration 27, loss = 0.27304781\n",
      "Iteration 26, loss = 0.27249238\n",
      "Iteration 28, loss = 0.27249613\n",
      "Iteration 27, loss = 0.27276940\n",
      "Iteration 29, loss = 0.27242381\n",
      "Iteration 28, loss = 0.27217823\n",
      "Iteration 29, loss = 0.27221751\n",
      "Iteration 30, loss = 0.27186767\n",
      "Iteration 31, loss = 0.27189957\n",
      "Iteration 30, loss = 0.27161971\n",
      "Iteration 32, loss = 0.27132601\n",
      "Iteration 31, loss = 0.27171825\n",
      "Iteration 33, loss = 0.27157256\n",
      "Iteration 32, loss = 0.27132317\n",
      "Iteration 34, loss = 0.27116988\n",
      "Iteration 33, loss = 0.27163603\n",
      "Iteration 35, loss = 0.27120563\n",
      "Iteration 34, loss = 0.27111386\n",
      "Iteration 36, loss = 0.27054602\n",
      "Iteration 35, loss = 0.27117470\n",
      "Iteration 36, loss = 0.27067991\n",
      "Iteration 37, loss = 0.27069606\n",
      "Iteration 37, loss = 0.27096510\n",
      "Iteration 38, loss = 0.27060536\n",
      "Iteration 38, loss = 0.27082317\n",
      "Iteration 39, loss = 0.27013560\n",
      "Iteration 39, loss = 0.27046032\n",
      "Iteration 40, loss = 0.26996952\n",
      "Iteration 40, loss = 0.27026346\n",
      "Iteration 41, loss = 0.26989683\n",
      "Iteration 41, loss = 0.27019917\n",
      "Iteration 42, loss = 0.26970991\n",
      "Iteration 42, loss = 0.27000553\n",
      "Iteration 43, loss = 0.26969698\n",
      "Iteration 43, loss = 0.27013339\n",
      "Iteration 44, loss = 0.26956849\n",
      "Iteration 44, loss = 0.27003919\n",
      "Iteration 45, loss = 0.26939848\n",
      "Iteration 45, loss = 0.26985943\n",
      "Iteration 46, loss = 0.26925677\n",
      "Iteration 46, loss = 0.26955624\n",
      "Iteration 47, loss = 0.26898718\n",
      "Iteration 47, loss = 0.26944338\n",
      "Iteration 48, loss = 0.26904404\n",
      "Iteration 48, loss = 0.26948213\n",
      "Iteration 49, loss = 0.26955950\n",
      "Iteration 49, loss = 0.26974609\n",
      "Iteration 50, loss = 0.26890429\n",
      "Iteration 50, loss = 0.26946048\n",
      "Iteration 51, loss = 0.26886739\n",
      "Iteration 51, loss = 0.26932651\n",
      "Iteration 52, loss = 0.26858575\n",
      "Iteration 52, loss = 0.26895978\n",
      "Iteration 53, loss = 0.26864016\n",
      "Iteration 53, loss = 0.26906757\n",
      "Iteration 54, loss = 0.26841996\n",
      "Iteration 54, loss = 0.26890077\n",
      "Iteration 55, loss = 0.26866640\n",
      "Iteration 55, loss = 0.26887164\n",
      "Iteration 56, loss = 0.26849779\n",
      "Iteration 56, loss = 0.26868004\n",
      "Iteration 57, loss = 0.26822102\n",
      "Iteration 57, loss = 0.26889605\n",
      "Iteration 58, loss = 0.26791709\n",
      "Iteration 58, loss = 0.26836416\n",
      "Iteration 59, loss = 0.26796653\n",
      "Iteration 59, loss = 0.26863301\n",
      "Iteration 60, loss = 0.26849326\n",
      "Iteration 60, loss = 0.26809683\n",
      "Iteration 61, loss = 0.26838612\n",
      "Iteration 61, loss = 0.26768362\n",
      "Iteration 62, loss = 0.26833120\n",
      "Iteration 62, loss = 0.26772271\n",
      "Iteration 63, loss = 0.26862209\n",
      "Iteration 63, loss = 0.26801166\n",
      "Iteration 64, loss = 0.26837048\n",
      "Iteration 64, loss = 0.26766403\n",
      "Iteration 65, loss = 0.26823791\n",
      "Iteration 65, loss = 0.26750130\n",
      "Iteration 66, loss = 0.26811844\n",
      "Iteration 66, loss = 0.26742752\n",
      "Iteration 67, loss = 0.26743879\n",
      "Iteration 67, loss = 0.26793910\n",
      "Iteration 68, loss = 0.26743101\n",
      "Iteration 68, loss = 0.26805453\n",
      "Iteration 69, loss = 0.26728752\n",
      "Iteration 69, loss = 0.26799418\n",
      "Iteration 70, loss = 0.26704561\n",
      "Iteration 70, loss = 0.26793555\n",
      "Iteration 71, loss = 0.26760398\n",
      "Iteration 71, loss = 0.26710077\n",
      "Iteration 72, loss = 0.26771069\n",
      "Iteration 72, loss = 0.26720519\n",
      "Iteration 73, loss = 0.26766707\n",
      "Iteration 73, loss = 0.26688905\n",
      "Iteration 74, loss = 0.26791119\n",
      "Iteration 74, loss = 0.26719400\n",
      "Iteration 75, loss = 0.26774938\n",
      "Iteration 75, loss = 0.26693609\n",
      "Iteration 76, loss = 0.26744354\n",
      "Iteration 76, loss = 0.26683305\n",
      "Iteration 77, loss = 0.26759533\n",
      "Iteration 77, loss = 0.26687190\n",
      "Iteration 78, loss = 0.26775135\n",
      "Iteration 78, loss = 0.26702829\n",
      "Iteration 79, loss = 0.26763243\n",
      "Iteration 79, loss = 0.26685169\n",
      "Iteration 80, loss = 0.26743953\n",
      "Iteration 80, loss = 0.26656458\n",
      "Iteration 81, loss = 0.26730326\n",
      "Iteration 81, loss = 0.26655814\n",
      "Iteration 82, loss = 0.26740096\n",
      "Iteration 82, loss = 0.26643867\n",
      "Iteration 83, loss = 0.26735915\n",
      "Iteration 83, loss = 0.26666736\n",
      "Iteration 84, loss = 0.26717356\n",
      "Iteration 84, loss = 0.26643701\n",
      "Iteration 85, loss = 0.26710988\n",
      "Iteration 85, loss = 0.26658322\n",
      "Iteration 86, loss = 0.26721672\n",
      "Iteration 86, loss = 0.26645315\n",
      "Iteration 87, loss = 0.26702756\n",
      "Iteration 87, loss = 0.26632539\n",
      "Iteration 88, loss = 0.26695859\n",
      "Iteration 88, loss = 0.26602972\n",
      "Iteration 89, loss = 0.26683898\n",
      "Iteration 89, loss = 0.26624001\n",
      "Iteration 90, loss = 0.26707441\n",
      "Iteration 90, loss = 0.26613539\n",
      "Iteration 91, loss = 0.26675097\n",
      "Iteration 91, loss = 0.26608569\n",
      "Iteration 92, loss = 0.26686066\n",
      "Iteration 92, loss = 0.26608250\n",
      "Iteration 93, loss = 0.26665338\n",
      "Iteration 93, loss = 0.26625481\n",
      "Iteration 94, loss = 0.26649922\n",
      "Iteration 94, loss = 0.26591905\n",
      "Iteration 95, loss = 0.26669627\n",
      "Iteration 95, loss = 0.26598016\n",
      "Iteration 96, loss = 0.26684473\n",
      "Iteration 96, loss = 0.26605097\n",
      "Iteration 97, loss = 0.26658878\n",
      "Iteration 97, loss = 0.26592109\n",
      "Iteration 98, loss = 0.26653350\n",
      "Iteration 98, loss = 0.26603741\n",
      "Iteration 99, loss = 0.26633204\n",
      "Iteration 99, loss = 0.26587921\n",
      "Iteration 100, loss = 0.26557427\n",
      "Iteration 100, loss = 0.26623476\n",
      "Iteration 101, loss = 0.26556788\n",
      "Iteration 101, loss = 0.26646921\n",
      "Iteration 102, loss = 0.26645818\n",
      "Iteration 102, loss = 0.26595371\n",
      "Iteration 103, loss = 0.26667021\n",
      "Iteration 104, loss = 0.26636825\n",
      "Iteration 103, loss = 0.26608292\n",
      "Iteration 105, loss = 0.26657286\n",
      "Iteration 104, loss = 0.26576258\n",
      "Iteration 106, loss = 0.26641413\n",
      "Iteration 105, loss = 0.26584018\n",
      "Iteration 107, loss = 0.26604470\n",
      "Iteration 106, loss = 0.26570708\n",
      "Iteration 108, loss = 0.26597568\n",
      "Iteration 107, loss = 0.26542262\n",
      "Iteration 109, loss = 0.26613408\n",
      "Iteration 108, loss = 0.26543232\n",
      "Iteration 110, loss = 0.26579677\n",
      "Iteration 109, loss = 0.26542639\n",
      "Iteration 111, loss = 0.26591661\n",
      "Iteration 110, loss = 0.26531292\n",
      "Iteration 112, loss = 0.26622488\n",
      "Iteration 111, loss = 0.26542761\n",
      "Iteration 113, loss = 0.26605526\n",
      "Iteration 112, loss = 0.26553450\n",
      "Iteration 114, loss = 0.26589715\n",
      "Iteration 113, loss = 0.26541991\n",
      "Iteration 115, loss = 0.26585859\n",
      "Iteration 114, loss = 0.26538439\n",
      "Iteration 116, loss = 0.26618498\n",
      "Iteration 115, loss = 0.26539239\n",
      "Iteration 117, loss = 0.26575979\n",
      "Iteration 116, loss = 0.26570364\n",
      "Iteration 118, loss = 0.26571998\n",
      "Iteration 117, loss = 0.26524710\n",
      "Iteration 119, loss = 0.26558372\n",
      "Iteration 118, loss = 0.26526645\n",
      "Iteration 120, loss = 0.26577181\n",
      "Iteration 119, loss = 0.26520758\n",
      "Iteration 121, loss = 0.26566556\n",
      "Iteration 120, loss = 0.26520878\n",
      "Iteration 122, loss = 0.26558845\n",
      "Iteration 121, loss = 0.26549065\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 123, loss = 0.26560308\n",
      "Iteration 124, loss = 0.26554856\n",
      "Iteration 1, loss = 0.33028846\n",
      "Iteration 125, loss = 0.26552014\n",
      "Iteration 2, loss = 0.28922924\n",
      "Iteration 126, loss = 0.26558110\n",
      "Iteration 3, loss = 0.28248424\n",
      "Iteration 127, loss = 0.26562105\n",
      "Iteration 4, loss = 0.28119784\n",
      "Iteration 128, loss = 0.26555406\n",
      "Iteration 5, loss = 0.28027935\n",
      "Iteration 129, loss = 0.26528122\n",
      "Iteration 6, loss = 0.28016641\n",
      "Iteration 130, loss = 0.26540251\n",
      "Iteration 7, loss = 0.27943809\n",
      "Iteration 131, loss = 0.26542293\n",
      "Iteration 8, loss = 0.27852859\n",
      "Iteration 132, loss = 0.26557636\n",
      "Iteration 9, loss = 0.27845173\n",
      "Iteration 133, loss = 0.26547557\n",
      "Iteration 10, loss = 0.27776900\n",
      "Iteration 134, loss = 0.26523798\n",
      "Iteration 11, loss = 0.27724802\n",
      "Iteration 135, loss = 0.26528660\n",
      "Iteration 12, loss = 0.27707756\n",
      "Iteration 136, loss = 0.26520356\n",
      "Iteration 13, loss = 0.27637743\n",
      "Iteration 137, loss = 0.26540985\n",
      "Iteration 14, loss = 0.27569522\n",
      "Iteration 138, loss = 0.26529452\n",
      "Iteration 15, loss = 0.27571662\n",
      "Iteration 139, loss = 0.26507975\n",
      "Iteration 16, loss = 0.27559920\n",
      "Iteration 140, loss = 0.26519993\n",
      "Iteration 17, loss = 0.27503057\n",
      "Iteration 18, loss = 0.27427048\n",
      "Iteration 141, loss = 0.26516142\n",
      "Iteration 19, loss = 0.27432927\n",
      "Iteration 142, loss = 0.26530443\n",
      "Iteration 20, loss = 0.27379599\n",
      "Iteration 143, loss = 0.26509066\n",
      "Iteration 144, loss = 0.26545368\n",
      "Iteration 21, loss = 0.27360279\n",
      "Iteration 145, loss = 0.26505912\n",
      "Iteration 22, loss = 0.27296397\n",
      "Iteration 146, loss = 0.26507198\n",
      "Iteration 23, loss = 0.27273466\n",
      "Iteration 147, loss = 0.26506530\n",
      "Iteration 24, loss = 0.27261729\n",
      "Iteration 148, loss = 0.26513543\n",
      "Iteration 25, loss = 0.27231453\n",
      "Iteration 149, loss = 0.26527080\n",
      "Iteration 26, loss = 0.27175135\n",
      "Iteration 150, loss = 0.26512628\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 27, loss = 0.27160733\n",
      "Iteration 1, loss = 0.33033591\n",
      "Iteration 28, loss = 0.27131544\n",
      "Iteration 2, loss = 0.28917601\n",
      "Iteration 29, loss = 0.27145574\n",
      "Iteration 3, loss = 0.28294842\n",
      "Iteration 30, loss = 0.27085992\n",
      "Iteration 4, loss = 0.28125467\n",
      "Iteration 5, loss = 0.28049622\n",
      "Iteration 31, loss = 0.27058613\n",
      "Iteration 6, loss = 0.28028249\n",
      "Iteration 32, loss = 0.27018513\n",
      "Iteration 7, loss = 0.27959533\n",
      "Iteration 33, loss = 0.27012595\n",
      "Iteration 8, loss = 0.27882358\n",
      "Iteration 34, loss = 0.26992695\n",
      "Iteration 9, loss = 0.27880047\n",
      "Iteration 35, loss = 0.27005409\n",
      "Iteration 10, loss = 0.27799589\n",
      "Iteration 36, loss = 0.26942630\n",
      "Iteration 11, loss = 0.27745779\n",
      "Iteration 37, loss = 0.26952711\n",
      "Iteration 12, loss = 0.27713665\n",
      "Iteration 38, loss = 0.26940399\n",
      "Iteration 13, loss = 0.27656043\n",
      "Iteration 39, loss = 0.26907281\n",
      "Iteration 14, loss = 0.27605995\n",
      "Iteration 40, loss = 0.26908416\n",
      "Iteration 15, loss = 0.27595633\n",
      "Iteration 41, loss = 0.26894686\n",
      "Iteration 16, loss = 0.27556421\n",
      "Iteration 42, loss = 0.26885853\n",
      "Iteration 43, loss = 0.26888436\n",
      "Iteration 17, loss = 0.27517786\n",
      "Iteration 44, loss = 0.26890950\n",
      "Iteration 18, loss = 0.27457257\n",
      "Iteration 19, loss = 0.27457778\n",
      "Iteration 45, loss = 0.26848634\n",
      "Iteration 46, loss = 0.26826850\n",
      "Iteration 20, loss = 0.27421557\n",
      "Iteration 47, loss = 0.26800788\n",
      "Iteration 21, loss = 0.27400266\n",
      "Iteration 48, loss = 0.26828463\n",
      "Iteration 22, loss = 0.27324170\n",
      "Iteration 23, loss = 0.27326327\n",
      "Iteration 49, loss = 0.26835104\n",
      "Iteration 50, loss = 0.26793927\n",
      "Iteration 24, loss = 0.27304214\n",
      "Iteration 51, loss = 0.26798425\n",
      "Iteration 25, loss = 0.27309908\n",
      "Iteration 52, loss = 0.26765494\n",
      "Iteration 26, loss = 0.27236828\n",
      "Iteration 53, loss = 0.26779572\n",
      "Iteration 27, loss = 0.27229032\n",
      "Iteration 28, loss = 0.27209645\n",
      "Iteration 54, loss = 0.26761909\n",
      "Iteration 55, loss = 0.26756556\n",
      "Iteration 29, loss = 0.27200165\n",
      "Iteration 30, loss = 0.27144416\n",
      "Iteration 56, loss = 0.26740119\n",
      "Iteration 31, loss = 0.27119787\n",
      "Iteration 57, loss = 0.26734327\n",
      "Iteration 32, loss = 0.27091427\n",
      "Iteration 58, loss = 0.26701841\n",
      "Iteration 33, loss = 0.27083902\n",
      "Iteration 59, loss = 0.26721611\n",
      "Iteration 34, loss = 0.27071160\n",
      "Iteration 60, loss = 0.26710645\n",
      "Iteration 35, loss = 0.27063472\n",
      "Iteration 61, loss = 0.26698625\n",
      "Iteration 36, loss = 0.27031114\n",
      "Iteration 62, loss = 0.26671252\n",
      "Iteration 37, loss = 0.27035455\n",
      "Iteration 63, loss = 0.26730466\n",
      "Iteration 38, loss = 0.27003976\n",
      "Iteration 64, loss = 0.26707076\n",
      "Iteration 39, loss = 0.26991508\n",
      "Iteration 65, loss = 0.26669419\n",
      "Iteration 40, loss = 0.26968245\n",
      "Iteration 66, loss = 0.26674632\n",
      "Iteration 41, loss = 0.26967016\n",
      "Iteration 67, loss = 0.26661649\n",
      "Iteration 42, loss = 0.26938891\n",
      "Iteration 68, loss = 0.26662546\n",
      "Iteration 43, loss = 0.26951283\n",
      "Iteration 69, loss = 0.26654658\n",
      "Iteration 70, loss = 0.26651345\n",
      "Iteration 44, loss = 0.26942056\n",
      "Iteration 71, loss = 0.26634523\n",
      "Iteration 45, loss = 0.26914143\n",
      "Iteration 72, loss = 0.26659191\n",
      "Iteration 46, loss = 0.26877938\n",
      "Iteration 73, loss = 0.26623509\n",
      "Iteration 47, loss = 0.26851963\n",
      "Iteration 74, loss = 0.26638941\n",
      "Iteration 48, loss = 0.26874670\n",
      "Iteration 49, loss = 0.26860302\n",
      "Iteration 75, loss = 0.26611892\n",
      "Iteration 50, loss = 0.26834050\n",
      "Iteration 76, loss = 0.26606345\n",
      "Iteration 77, loss = 0.26606545\n",
      "Iteration 51, loss = 0.26821047\n",
      "Iteration 78, loss = 0.26626379\n",
      "Iteration 52, loss = 0.26785831\n",
      "Iteration 79, loss = 0.26595093\n",
      "Iteration 53, loss = 0.26817493\n",
      "Iteration 80, loss = 0.26565986\n",
      "Iteration 81, loss = 0.26579359\n",
      "Iteration 54, loss = 0.26784213\n",
      "Iteration 55, loss = 0.26769142\n",
      "Iteration 82, loss = 0.26572991\n",
      "Iteration 56, loss = 0.26744916\n",
      "Iteration 83, loss = 0.26571036\n",
      "Iteration 57, loss = 0.26750786\n",
      "Iteration 84, loss = 0.26547854\n",
      "Iteration 58, loss = 0.26709062\n",
      "Iteration 85, loss = 0.26571832\n",
      "Iteration 59, loss = 0.26714696\n",
      "Iteration 86, loss = 0.26564412\n",
      "Iteration 60, loss = 0.26693715\n",
      "Iteration 87, loss = 0.26553846\n",
      "Iteration 61, loss = 0.26687482\n",
      "Iteration 88, loss = 0.26537915\n",
      "Iteration 62, loss = 0.26663200\n",
      "Iteration 89, loss = 0.26531465\n",
      "Iteration 63, loss = 0.26712790\n",
      "Iteration 90, loss = 0.26548066\n",
      "Iteration 64, loss = 0.26673727\n",
      "Iteration 91, loss = 0.26539905\n",
      "Iteration 65, loss = 0.26631012\n",
      "Iteration 92, loss = 0.26541061\n",
      "Iteration 66, loss = 0.26628862\n",
      "Iteration 67, loss = 0.26633208\n",
      "Iteration 93, loss = 0.26544318\n",
      "Iteration 68, loss = 0.26622736\n",
      "Iteration 69, loss = 0.26594596\n",
      "Iteration 94, loss = 0.26519953\n",
      "Iteration 70, loss = 0.26579395\n",
      "Iteration 95, loss = 0.26515198\n",
      "Iteration 71, loss = 0.26589420\n",
      "Iteration 96, loss = 0.26530027\n",
      "Iteration 72, loss = 0.26609662\n",
      "Iteration 97, loss = 0.26498120\n",
      "Iteration 73, loss = 0.26578022\n",
      "Iteration 98, loss = 0.26516892\n",
      "Iteration 74, loss = 0.26569029\n",
      "Iteration 99, loss = 0.26504742\n",
      "Iteration 100, loss = 0.26500020\n",
      "Iteration 75, loss = 0.26549627\n",
      "Iteration 101, loss = 0.26494806\n",
      "Iteration 76, loss = 0.26560810\n",
      "Iteration 102, loss = 0.26499080\n",
      "Iteration 77, loss = 0.26541396\n",
      "Iteration 103, loss = 0.26525906\n",
      "Iteration 78, loss = 0.26559540\n",
      "Iteration 104, loss = 0.26500407\n",
      "Iteration 79, loss = 0.26522273\n",
      "Iteration 105, loss = 0.26490130\n",
      "Iteration 80, loss = 0.26509808\n",
      "Iteration 106, loss = 0.26486644\n",
      "Iteration 81, loss = 0.26524246\n",
      "Iteration 107, loss = 0.26472580\n",
      "Iteration 82, loss = 0.26506714\n",
      "Iteration 108, loss = 0.26468417\n",
      "Iteration 83, loss = 0.26505299\n",
      "Iteration 109, loss = 0.26472710\n",
      "Iteration 84, loss = 0.26504987\n",
      "Iteration 110, loss = 0.26443807\n",
      "Iteration 85, loss = 0.26466705\n",
      "Iteration 111, loss = 0.26465635\n",
      "Iteration 86, loss = 0.26505687\n",
      "Iteration 112, loss = 0.26451706\n",
      "Iteration 87, loss = 0.26487365\n",
      "Iteration 113, loss = 0.26463278\n",
      "Iteration 88, loss = 0.26479585\n",
      "Iteration 114, loss = 0.26448269\n",
      "Iteration 89, loss = 0.26454442\n",
      "Iteration 115, loss = 0.26441214\n",
      "Iteration 90, loss = 0.26490230\n",
      "Iteration 116, loss = 0.26468923\n",
      "Iteration 91, loss = 0.26457411\n",
      "Iteration 117, loss = 0.26436769\n",
      "Iteration 92, loss = 0.26452322\n",
      "Iteration 118, loss = 0.26441243\n",
      "Iteration 93, loss = 0.26453569\n",
      "Iteration 119, loss = 0.26441487\n",
      "Iteration 94, loss = 0.26436149\n",
      "Iteration 120, loss = 0.26460753\n",
      "Iteration 95, loss = 0.26426080\n",
      "Iteration 121, loss = 0.26431134\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 96, loss = 0.26462704\n",
      "Iteration 97, loss = 0.26423594\n",
      "Iteration 1, loss = 0.33090185\n",
      "Iteration 98, loss = 0.26430215\n",
      "Iteration 2, loss = 0.28833840\n",
      "Iteration 99, loss = 0.26424983\n",
      "Iteration 3, loss = 0.28203199\n",
      "Iteration 100, loss = 0.26434072\n",
      "Iteration 4, loss = 0.28062523\n",
      "Iteration 101, loss = 0.26415249\n",
      "Iteration 5, loss = 0.27972286\n",
      "Iteration 102, loss = 0.26414287\n",
      "Iteration 6, loss = 0.27951078\n",
      "Iteration 103, loss = 0.26425091\n",
      "Iteration 7, loss = 0.27908300\n",
      "Iteration 104, loss = 0.26417177\n",
      "Iteration 8, loss = 0.27834309\n",
      "Iteration 105, loss = 0.26398823\n",
      "Iteration 9, loss = 0.27785250\n",
      "Iteration 106, loss = 0.26401492\n",
      "Iteration 10, loss = 0.27717198\n",
      "Iteration 107, loss = 0.26386755\n",
      "Iteration 11, loss = 0.27662685\n",
      "Iteration 108, loss = 0.26382228\n",
      "Iteration 12, loss = 0.27608369\n",
      "Iteration 109, loss = 0.26393690\n",
      "Iteration 13, loss = 0.27573292\n",
      "Iteration 110, loss = 0.26373319\n",
      "Iteration 14, loss = 0.27523298\n",
      "Iteration 111, loss = 0.26403779\n",
      "Iteration 15, loss = 0.27510455\n",
      "Iteration 112, loss = 0.26388870\n",
      "Iteration 16, loss = 0.27460937\n",
      "Iteration 113, loss = 0.26377843\n",
      "Iteration 17, loss = 0.27448589\n",
      "Iteration 114, loss = 0.26374015\n",
      "Iteration 115, loss = 0.26360968\n",
      "Iteration 18, loss = 0.27385789\n",
      "Iteration 116, loss = 0.26393154\n",
      "Iteration 19, loss = 0.27383973\n",
      "Iteration 117, loss = 0.26374005\n",
      "Iteration 20, loss = 0.27340762\n",
      "Iteration 118, loss = 0.26353532\n",
      "Iteration 21, loss = 0.27315511\n",
      "Iteration 119, loss = 0.26375215\n",
      "Iteration 120, loss = 0.26380786\n",
      "Iteration 22, loss = 0.27260644\n",
      "Iteration 121, loss = 0.26374553\n",
      "Iteration 23, loss = 0.27297361\n",
      "Iteration 122, loss = 0.26340266\n",
      "Iteration 24, loss = 0.27247530\n",
      "Iteration 123, loss = 0.26359756\n",
      "Iteration 25, loss = 0.27232085\n",
      "Iteration 124, loss = 0.26348190\n",
      "Iteration 26, loss = 0.27185792\n",
      "Iteration 125, loss = 0.26346703\n",
      "Iteration 27, loss = 0.27174400\n",
      "Iteration 28, loss = 0.27162409\n",
      "Iteration 126, loss = 0.26378431\n",
      "Iteration 29, loss = 0.27153038\n",
      "Iteration 127, loss = 0.26351029\n",
      "Iteration 30, loss = 0.27104977\n",
      "Iteration 128, loss = 0.26355052\n",
      "Iteration 129, loss = 0.26342598\n",
      "Iteration 31, loss = 0.27075671\n",
      "Iteration 130, loss = 0.26337741\n",
      "Iteration 32, loss = 0.27049505\n",
      "Iteration 131, loss = 0.26340337\n",
      "Iteration 33, loss = 0.27033066\n",
      "Iteration 132, loss = 0.26330277\n",
      "Iteration 34, loss = 0.27019666\n",
      "Iteration 133, loss = 0.26352902\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 35, loss = 0.27007834\n",
      "Iteration 36, loss = 0.27005029\n",
      "Iteration 1, loss = 0.33156251\n",
      "Iteration 37, loss = 0.26992712\n",
      "Iteration 2, loss = 0.28794598\n",
      "Iteration 38, loss = 0.26977801\n",
      "Iteration 3, loss = 0.28137117\n",
      "Iteration 39, loss = 0.26955728\n",
      "Iteration 4, loss = 0.27952392\n",
      "Iteration 40, loss = 0.26928757\n",
      "Iteration 5, loss = 0.27908348\n",
      "Iteration 41, loss = 0.26923831\n",
      "Iteration 6, loss = 0.27814150\n",
      "Iteration 7, loss = 0.27768651\n",
      "Iteration 42, loss = 0.26895132\n",
      "Iteration 43, loss = 0.26891623\n",
      "Iteration 8, loss = 0.27715217\n",
      "Iteration 44, loss = 0.26886345\n",
      "Iteration 9, loss = 0.27637769\n",
      "Iteration 45, loss = 0.26851431\n",
      "Iteration 10, loss = 0.27580076\n",
      "Iteration 46, loss = 0.26833922\n",
      "Iteration 11, loss = 0.27560718\n",
      "Iteration 12, loss = 0.27518472\n",
      "Iteration 47, loss = 0.26790970\n",
      "Iteration 13, loss = 0.27446217\n",
      "Iteration 48, loss = 0.26790679\n",
      "Iteration 14, loss = 0.27409880\n",
      "Iteration 49, loss = 0.26789955\n",
      "Iteration 15, loss = 0.27368773\n",
      "Iteration 50, loss = 0.26773328\n",
      "Iteration 16, loss = 0.27321168\n",
      "Iteration 51, loss = 0.26756928\n",
      "Iteration 17, loss = 0.27280453\n",
      "Iteration 52, loss = 0.26716356\n",
      "Iteration 18, loss = 0.27271234\n",
      "Iteration 53, loss = 0.26724575\n",
      "Iteration 54, loss = 0.26710255\n",
      "Iteration 19, loss = 0.27239818\n",
      "Iteration 55, loss = 0.26695566\n",
      "Iteration 20, loss = 0.27172964\n",
      "Iteration 56, loss = 0.26642674\n",
      "Iteration 21, loss = 0.27109184\n",
      "Iteration 57, loss = 0.26673872\n",
      "Iteration 22, loss = 0.27118602\n",
      "Iteration 58, loss = 0.26634661\n",
      "Iteration 23, loss = 0.27080587\n",
      "Iteration 59, loss = 0.26636269\n",
      "Iteration 24, loss = 0.27091330\n",
      "Iteration 60, loss = 0.26614722\n",
      "Iteration 25, loss = 0.27009382\n",
      "Iteration 61, loss = 0.26627067\n",
      "Iteration 26, loss = 0.27011349\n",
      "Iteration 62, loss = 0.26601521\n",
      "Iteration 27, loss = 0.26977324\n",
      "Iteration 63, loss = 0.26638126\n",
      "Iteration 28, loss = 0.26938131\n",
      "Iteration 64, loss = 0.26605257\n",
      "Iteration 29, loss = 0.26932192\n",
      "Iteration 65, loss = 0.26574929\n",
      "Iteration 30, loss = 0.26893104\n",
      "Iteration 31, loss = 0.26946157Iteration 66, loss = 0.26575589\n",
      "\n",
      "Iteration 67, loss = 0.26554331\n",
      "Iteration 32, loss = 0.26886780\n",
      "Iteration 68, loss = 0.26585023\n",
      "Iteration 33, loss = 0.26838049\n",
      "Iteration 69, loss = 0.26549084\n",
      "Iteration 34, loss = 0.26843457\n",
      "Iteration 70, loss = 0.26533302\n",
      "Iteration 35, loss = 0.26818475\n",
      "Iteration 71, loss = 0.26563425\n",
      "Iteration 36, loss = 0.26780594\n",
      "Iteration 72, loss = 0.26546901\n",
      "Iteration 37, loss = 0.26794090\n",
      "Iteration 73, loss = 0.26542507\n",
      "Iteration 38, loss = 0.26781516\n",
      "Iteration 74, loss = 0.26559839\n",
      "Iteration 39, loss = 0.26740268\n",
      "Iteration 75, loss = 0.26516469\n",
      "Iteration 76, loss = 0.26534316\n",
      "Iteration 40, loss = 0.26704681\n",
      "Iteration 77, loss = 0.26532547\n",
      "Iteration 41, loss = 0.26709753\n",
      "Iteration 78, loss = 0.26550467\n",
      "Iteration 42, loss = 0.26702371\n",
      "Iteration 79, loss = 0.26495988\n",
      "Iteration 43, loss = 0.26679685\n",
      "Iteration 80, loss = 0.26496774\n",
      "Iteration 44, loss = 0.26657572\n",
      "Iteration 81, loss = 0.26500070\n",
      "Iteration 45, loss = 0.26626411\n",
      "Iteration 82, loss = 0.26490389\n",
      "Iteration 46, loss = 0.26583067\n",
      "Iteration 83, loss = 0.26461922\n",
      "Iteration 47, loss = 0.26581287\n",
      "Iteration 84, loss = 0.26491205\n",
      "Iteration 48, loss = 0.26570665\n",
      "Iteration 85, loss = 0.26467862\n",
      "Iteration 49, loss = 0.26535061\n",
      "Iteration 86, loss = 0.26536508\n",
      "Iteration 50, loss = 0.26522123\n",
      "Iteration 51, loss = 0.26524304\n",
      "Iteration 87, loss = 0.26470186\n",
      "Iteration 52, loss = 0.26510300\n",
      "Iteration 88, loss = 0.26452102\n",
      "Iteration 53, loss = 0.26469145\n",
      "Iteration 89, loss = 0.26453287\n",
      "Iteration 90, loss = 0.26479494\n",
      "Iteration 54, loss = 0.26474149\n",
      "Iteration 91, loss = 0.26433252\n",
      "Iteration 55, loss = 0.26442427\n",
      "Iteration 92, loss = 0.26448756\n",
      "Iteration 56, loss = 0.26454926\n",
      "Iteration 93, loss = 0.26444426\n",
      "Iteration 57, loss = 0.26442176\n",
      "Iteration 94, loss = 0.26438323\n",
      "Iteration 58, loss = 0.26413128\n",
      "Iteration 95, loss = 0.26427272\n",
      "Iteration 59, loss = 0.26395780\n",
      "Iteration 96, loss = 0.26461024\n",
      "Iteration 60, loss = 0.26388636\n",
      "Iteration 97, loss = 0.26443701\n",
      "Iteration 61, loss = 0.26379316\n",
      "Iteration 98, loss = 0.26430600\n",
      "Iteration 62, loss = 0.26387999\n",
      "Iteration 99, loss = 0.26425975\n",
      "Iteration 63, loss = 0.26386682\n",
      "Iteration 100, loss = 0.26430026\n",
      "Iteration 64, loss = 0.26370290\n",
      "Iteration 101, loss = 0.26408608\n",
      "Iteration 65, loss = 0.26360516\n",
      "Iteration 102, loss = 0.26417522\n",
      "Iteration 66, loss = 0.26356760\n",
      "Iteration 103, loss = 0.26436143\n",
      "Iteration 67, loss = 0.26373667\n",
      "Iteration 104, loss = 0.26431650\n",
      "Iteration 68, loss = 0.26344728\n",
      "Iteration 105, loss = 0.26413736\n",
      "Iteration 69, loss = 0.26309784\n",
      "Iteration 106, loss = 0.26394401\n",
      "Iteration 70, loss = 0.26296521\n",
      "Iteration 107, loss = 0.26370388\n",
      "Iteration 71, loss = 0.26314204\n",
      "Iteration 108, loss = 0.26391131\n",
      "Iteration 72, loss = 0.26306395\n",
      "Iteration 109, loss = 0.26412138\n",
      "Iteration 73, loss = 0.26292218\n",
      "Iteration 110, loss = 0.26391091\n",
      "Iteration 74, loss = 0.26291230\n",
      "Iteration 111, loss = 0.26382488\n",
      "Iteration 75, loss = 0.26305263\n",
      "Iteration 112, loss = 0.26398989\n",
      "Iteration 76, loss = 0.26283971\n",
      "Iteration 113, loss = 0.26382024\n",
      "Iteration 77, loss = 0.26290394\n",
      "Iteration 114, loss = 0.26388780\n",
      "Iteration 78, loss = 0.26265306\n",
      "Iteration 115, loss = 0.26364922\n",
      "Iteration 79, loss = 0.26285322\n",
      "Iteration 116, loss = 0.26380320\n",
      "Iteration 80, loss = 0.26263534\n",
      "Iteration 117, loss = 0.26394016\n",
      "Iteration 81, loss = 0.26276561\n",
      "Iteration 118, loss = 0.26365803\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 82, loss = 0.26253556\n",
      "Iteration 83, loss = 0.26275598\n",
      "Iteration 1, loss = 0.33190686\n",
      "Iteration 84, loss = 0.26244130\n",
      "Iteration 2, loss = 0.28972821\n",
      "Iteration 85, loss = 0.26233539\n",
      "Iteration 86, loss = 0.26226724\n",
      "Iteration 3, loss = 0.28324517\n",
      "Iteration 87, loss = 0.26264096\n",
      "Iteration 4, loss = 0.28130351\n",
      "Iteration 5, loss = 0.28090291\n",
      "Iteration 88, loss = 0.26211101\n",
      "Iteration 6, loss = 0.28016678\n",
      "Iteration 89, loss = 0.26212472\n",
      "Iteration 7, loss = 0.27987711\n",
      "Iteration 90, loss = 0.26225506\n",
      "Iteration 8, loss = 0.27925172\n",
      "Iteration 91, loss = 0.26216492\n",
      "Iteration 9, loss = 0.27863405\n",
      "Iteration 92, loss = 0.26193772\n",
      "Iteration 93, loss = 0.26224452\n",
      "Iteration 10, loss = 0.27816908\n",
      "Iteration 94, loss = 0.26201603\n",
      "Iteration 11, loss = 0.27785653\n",
      "Iteration 95, loss = 0.26181343\n",
      "Iteration 12, loss = 0.27743618\n",
      "Iteration 96, loss = 0.26161640\n",
      "Iteration 13, loss = 0.27678737\n",
      "Iteration 97, loss = 0.26183566\n",
      "Iteration 14, loss = 0.27676964\n",
      "Iteration 98, loss = 0.26200061\n",
      "Iteration 99, loss = 0.26168829\n",
      "Iteration 15, loss = 0.27633167\n",
      "Iteration 100, loss = 0.26167991\n",
      "Iteration 16, loss = 0.27574904\n",
      "Iteration 101, loss = 0.26155762\n",
      "Iteration 17, loss = 0.27563233\n",
      "Iteration 102, loss = 0.26134115\n",
      "Iteration 18, loss = 0.27532854\n",
      "Iteration 103, loss = 0.26178604\n",
      "Iteration 19, loss = 0.27521835\n",
      "Iteration 104, loss = 0.26176339\n",
      "Iteration 20, loss = 0.27407356\n",
      "Iteration 105, loss = 0.26151786\n",
      "Iteration 21, loss = 0.27338420\n",
      "Iteration 106, loss = 0.26138456\n",
      "Iteration 22, loss = 0.27341465\n",
      "Iteration 107, loss = 0.26153769\n",
      "Iteration 23, loss = 0.27286897\n",
      "Iteration 108, loss = 0.26162328\n",
      "Iteration 109, loss = 0.26130284\n",
      "Iteration 24, loss = 0.27291224\n",
      "Iteration 110, loss = 0.26132000\n",
      "Iteration 25, loss = 0.27214595\n",
      "Iteration 111, loss = 0.26124297\n",
      "Iteration 26, loss = 0.27219989\n",
      "Iteration 112, loss = 0.26118903\n",
      "Iteration 27, loss = 0.27186119\n",
      "Iteration 113, loss = 0.26126234\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 28, loss = 0.27133628\n",
      "Iteration 29, loss = 0.27121308\n",
      "Iteration 1, loss = 0.33120489\n",
      "Iteration 30, loss = 0.27073413\n",
      "Iteration 2, loss = 0.28848119\n",
      "Iteration 31, loss = 0.27095342\n",
      "Iteration 3, loss = 0.28188474\n",
      "Iteration 32, loss = 0.27057533\n",
      "Iteration 4, loss = 0.28025103\n",
      "Iteration 33, loss = 0.27009624\n",
      "Iteration 5, loss = 0.27937796\n",
      "Iteration 34, loss = 0.27022970\n",
      "Iteration 6, loss = 0.27881299\n",
      "Iteration 35, loss = 0.27021321\n",
      "Iteration 7, loss = 0.27839124\n",
      "Iteration 36, loss = 0.26948932\n",
      "Iteration 8, loss = 0.27733191\n",
      "Iteration 37, loss = 0.26974740\n",
      "Iteration 38, loss = 0.26938293\n",
      "Iteration 9, loss = 0.27687911\n",
      "Iteration 39, loss = 0.26938606\n",
      "Iteration 10, loss = 0.27623039\n",
      "Iteration 40, loss = 0.26898989\n",
      "Iteration 11, loss = 0.27632146\n",
      "Iteration 41, loss = 0.26908533\n",
      "Iteration 12, loss = 0.27561068\n",
      "Iteration 42, loss = 0.26883464\n",
      "Iteration 13, loss = 0.27505321\n",
      "Iteration 43, loss = 0.26901891\n",
      "Iteration 14, loss = 0.27501113\n",
      "Iteration 44, loss = 0.26885807\n",
      "Iteration 15, loss = 0.27461444\n",
      "Iteration 45, loss = 0.26893437\n",
      "Iteration 16, loss = 0.27424958\n",
      "Iteration 46, loss = 0.26841835\n",
      "Iteration 17, loss = 0.27393495\n",
      "Iteration 47, loss = 0.26838196\n",
      "Iteration 18, loss = 0.27368590Iteration 48, loss = 0.26836622\n",
      "\n",
      "Iteration 19, loss = 0.27388816\n",
      "Iteration 49, loss = 0.26816195\n",
      "Iteration 20, loss = 0.27289865\n",
      "Iteration 50, loss = 0.26797068\n",
      "Iteration 21, loss = 0.27272782\n",
      "Iteration 51, loss = 0.26820715\n",
      "Iteration 22, loss = 0.27251113\n",
      "Iteration 52, loss = 0.26813315\n",
      "Iteration 23, loss = 0.27195293\n",
      "Iteration 53, loss = 0.26778201\n",
      "Iteration 24, loss = 0.27208217\n",
      "Iteration 54, loss = 0.26772302\n",
      "Iteration 25, loss = 0.27149481\n",
      "Iteration 55, loss = 0.26768706\n",
      "Iteration 26, loss = 0.27157126\n",
      "Iteration 56, loss = 0.26783616\n",
      "Iteration 27, loss = 0.27135533\n",
      "Iteration 57, loss = 0.26751148\n",
      "Iteration 58, loss = 0.26732239\n",
      "Iteration 28, loss = 0.27079680\n",
      "Iteration 59, loss = 0.26724442\n",
      "Iteration 29, loss = 0.27066839\n",
      "Iteration 60, loss = 0.26713244\n",
      "Iteration 30, loss = 0.27050108\n",
      "Iteration 61, loss = 0.26725118\n",
      "Iteration 31, loss = 0.27063029\n",
      "Iteration 62, loss = 0.26713612\n",
      "Iteration 32, loss = 0.27007628\n",
      "Iteration 63, loss = 0.26704262\n",
      "Iteration 33, loss = 0.26975780\n",
      "Iteration 64, loss = 0.26708958\n",
      "Iteration 34, loss = 0.26968408\n",
      "Iteration 65, loss = 0.26687600\n",
      "Iteration 35, loss = 0.26947216\n",
      "Iteration 66, loss = 0.26712777\n",
      "Iteration 36, loss = 0.26914517\n",
      "Iteration 67, loss = 0.26692004\n",
      "Iteration 37, loss = 0.26922491\n",
      "Iteration 68, loss = 0.26663865\n",
      "Iteration 38, loss = 0.26890164\n",
      "Iteration 69, loss = 0.26649497\n",
      "Iteration 70, loss = 0.26640695\n",
      "Iteration 39, loss = 0.26856252\n",
      "Iteration 71, loss = 0.26633594\n",
      "Iteration 40, loss = 0.26835745\n",
      "Iteration 72, loss = 0.26622047\n",
      "Iteration 41, loss = 0.26836418\n",
      "Iteration 73, loss = 0.26610514\n",
      "Iteration 42, loss = 0.26790860\n",
      "Iteration 74, loss = 0.26615036\n",
      "Iteration 43, loss = 0.26788939\n",
      "Iteration 75, loss = 0.26620666\n",
      "Iteration 44, loss = 0.26781450\n",
      "Iteration 76, loss = 0.26613782\n",
      "Iteration 45, loss = 0.26770999\n",
      "Iteration 77, loss = 0.26602100\n",
      "Iteration 46, loss = 0.26753495\n",
      "Iteration 78, loss = 0.26566914\n",
      "Iteration 47, loss = 0.26733784\n",
      "Iteration 79, loss = 0.26594012\n",
      "Iteration 48, loss = 0.26717093\n",
      "Iteration 80, loss = 0.26562404\n",
      "Iteration 49, loss = 0.26685720\n",
      "Iteration 81, loss = 0.26590297\n",
      "Iteration 50, loss = 0.26676899\n",
      "Iteration 82, loss = 0.26543540\n",
      "Iteration 83, loss = 0.26561695\n",
      "Iteration 51, loss = 0.26672854\n",
      "Iteration 84, loss = 0.26540537\n",
      "Iteration 52, loss = 0.26646301\n",
      "Iteration 85, loss = 0.26524872\n",
      "Iteration 53, loss = 0.26628238\n",
      "Iteration 86, loss = 0.26525565\n",
      "Iteration 54, loss = 0.26623001\n",
      "Iteration 87, loss = 0.26531641\n",
      "Iteration 55, loss = 0.26607793\n",
      "Iteration 88, loss = 0.26507206\n",
      "Iteration 56, loss = 0.26636866\n",
      "Iteration 89, loss = 0.26507138\n",
      "Iteration 57, loss = 0.26595134\n",
      "Iteration 90, loss = 0.26503588\n",
      "Iteration 58, loss = 0.26582473\n",
      "Iteration 91, loss = 0.26495871\n",
      "Iteration 59, loss = 0.26551681\n",
      "Iteration 92, loss = 0.26472141\n",
      "Iteration 60, loss = 0.26572234\n",
      "Iteration 93, loss = 0.26527847\n",
      "Iteration 61, loss = 0.26545669\n",
      "Iteration 94, loss = 0.26490962\n",
      "Iteration 62, loss = 0.26543688\n",
      "Iteration 95, loss = 0.26454760\n",
      "Iteration 63, loss = 0.26560215\n",
      "Iteration 96, loss = 0.26429837\n",
      "Iteration 64, loss = 0.26565400\n",
      "Iteration 97, loss = 0.26464648\n",
      "Iteration 65, loss = 0.26533858\n",
      "Iteration 98, loss = 0.26481791\n",
      "Iteration 66, loss = 0.26547057\n",
      "Iteration 99, loss = 0.26424664\n",
      "Iteration 67, loss = 0.26520470\n",
      "Iteration 100, loss = 0.26428581\n",
      "Iteration 68, loss = 0.26501566\n",
      "Iteration 101, loss = 0.26405844\n",
      "Iteration 69, loss = 0.26488300\n",
      "Iteration 102, loss = 0.26408629\n",
      "Iteration 70, loss = 0.26478562\n",
      "Iteration 103, loss = 0.26444712\n",
      "Iteration 71, loss = 0.26492771\n",
      "Iteration 104, loss = 0.26404653\n",
      "Iteration 72, loss = 0.26483119\n",
      "Iteration 105, loss = 0.26408004\n",
      "Iteration 73, loss = 0.26478265\n",
      "Iteration 106, loss = 0.26381349\n",
      "Iteration 74, loss = 0.26473849\n",
      "Iteration 107, loss = 0.26415159\n",
      "Iteration 75, loss = 0.26498409\n",
      "Iteration 108, loss = 0.26430761\n",
      "Iteration 76, loss = 0.26476699\n",
      "Iteration 77, loss = 0.26454446\n",
      "Iteration 109, loss = 0.26404983\n",
      "Iteration 110, loss = 0.26383250\n",
      "Iteration 78, loss = 0.26445344\n",
      "Iteration 79, loss = 0.26470536\n",
      "Iteration 111, loss = 0.26379743\n",
      "Iteration 80, loss = 0.26451468\n",
      "Iteration 112, loss = 0.26371660\n",
      "Iteration 81, loss = 0.26464598\n",
      "Iteration 113, loss = 0.26380044\n",
      "Iteration 82, loss = 0.26412330\n",
      "Iteration 114, loss = 0.26350959\n",
      "Iteration 83, loss = 0.26460604\n",
      "Iteration 115, loss = 0.26353148\n",
      "Iteration 84, loss = 0.26430271\n",
      "Iteration 116, loss = 0.26371071\n",
      "Iteration 85, loss = 0.26455745\n",
      "Iteration 117, loss = 0.26359796\n",
      "Iteration 86, loss = 0.26413935\n",
      "Iteration 118, loss = 0.26343681\n",
      "Iteration 87, loss = 0.26406339\n",
      "Iteration 119, loss = 0.26335953\n",
      "Iteration 88, loss = 0.26425241\n",
      "Iteration 89, loss = 0.26415388\n",
      "Iteration 120, loss = 0.26322052\n",
      "Iteration 90, loss = 0.26409445\n",
      "Iteration 121, loss = 0.26327823\n",
      "Iteration 91, loss = 0.26410977\n",
      "Iteration 122, loss = 0.26348136\n",
      "Iteration 92, loss = 0.26381399\n",
      "Iteration 123, loss = 0.26318035\n",
      "Iteration 93, loss = 0.26380577\n",
      "Iteration 124, loss = 0.26314645\n",
      "Iteration 94, loss = 0.26382706\n",
      "Iteration 125, loss = 0.26300242\n",
      "Iteration 95, loss = 0.26370323\n",
      "Iteration 126, loss = 0.26299111\n",
      "Iteration 96, loss = 0.26362605\n",
      "Iteration 127, loss = 0.26298082\n",
      "Iteration 97, loss = 0.26368304\n",
      "Iteration 128, loss = 0.26300572\n",
      "Iteration 98, loss = 0.26391992\n",
      "Iteration 129, loss = 0.26303483\n",
      "Iteration 99, loss = 0.26353650\n",
      "Iteration 130, loss = 0.26308858\n",
      "Iteration 100, loss = 0.26367870\n",
      "Iteration 131, loss = 0.26316301\n",
      "Iteration 101, loss = 0.26342266\n",
      "Iteration 132, loss = 0.26289370\n",
      "Iteration 102, loss = 0.26321429\n",
      "Iteration 133, loss = 0.26286263\n",
      "Iteration 134, loss = 0.26296179\n",
      "Iteration 103, loss = 0.26382636\n",
      "Iteration 135, loss = 0.26288081\n",
      "Iteration 104, loss = 0.26348559\n",
      "Iteration 136, loss = 0.26281288\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 105, loss = 0.26327131\n",
      "Iteration 106, loss = 0.26303136\n",
      "Iteration 1, loss = 0.33129192\n",
      "Iteration 107, loss = 0.26340972\n",
      "Iteration 2, loss = 0.28924625\n",
      "Iteration 108, loss = 0.26323582\n",
      "Iteration 3, loss = 0.28277223\n",
      "Iteration 109, loss = 0.26333909\n",
      "Iteration 4, loss = 0.28137617\n",
      "Iteration 110, loss = 0.26308590\n",
      "Iteration 5, loss = 0.28055053\n",
      "Iteration 111, loss = 0.26316532\n",
      "Iteration 6, loss = 0.28048838\n",
      "Iteration 112, loss = 0.26287079\n",
      "Iteration 7, loss = 0.27972205\n",
      "Iteration 113, loss = 0.26328045\n",
      "Iteration 8, loss = 0.27900126\n",
      "Iteration 114, loss = 0.26290750\n",
      "Iteration 9, loss = 0.27865326\n",
      "Iteration 115, loss = 0.26275661\n",
      "Iteration 116, loss = 0.26279419\n",
      "Iteration 10, loss = 0.27804775\n",
      "Iteration 117, loss = 0.26291547\n",
      "Iteration 11, loss = 0.27836164\n",
      "Iteration 12, loss = 0.27751058\n",
      "Iteration 118, loss = 0.26259654\n",
      "Iteration 13, loss = 0.27662041\n",
      "Iteration 119, loss = 0.26287884\n",
      "Iteration 14, loss = 0.27680394\n",
      "Iteration 120, loss = 0.26252306\n",
      "Iteration 15, loss = 0.27607173\n",
      "Iteration 121, loss = 0.26278341\n",
      "Iteration 16, loss = 0.27564147\n",
      "Iteration 122, loss = 0.26264621\n",
      "Iteration 17, loss = 0.27522747\n",
      "Iteration 123, loss = 0.26241263\n",
      "Iteration 18, loss = 0.27487750\n",
      "Iteration 124, loss = 0.26256110\n",
      "Iteration 19, loss = 0.27499121\n",
      "Iteration 125, loss = 0.26225275\n",
      "Iteration 20, loss = 0.27378961\n",
      "Iteration 126, loss = 0.26246302\n",
      "Iteration 21, loss = 0.27363237\n",
      "Iteration 127, loss = 0.26240919\n",
      "Iteration 128, loss = 0.26223656\n",
      "Iteration 22, loss = 0.27334282\n",
      "Iteration 129, loss = 0.26211523\n",
      "Iteration 23, loss = 0.27292793\n",
      "Iteration 130, loss = 0.26242603\n",
      "Iteration 131, loss = 0.26220142\n",
      "Iteration 24, loss = 0.27264420\n",
      "Iteration 132, loss = 0.26219861\n",
      "Iteration 25, loss = 0.27234041\n",
      "Iteration 133, loss = 0.26198798\n",
      "Iteration 26, loss = 0.27186638\n",
      "Iteration 134, loss = 0.26222831\n",
      "Iteration 27, loss = 0.27178334\n",
      "Iteration 135, loss = 0.26207990\n",
      "Iteration 28, loss = 0.27131307\n",
      "Iteration 136, loss = 0.26218188\n",
      "Iteration 29, loss = 0.27102423\n",
      "Iteration 30, loss = 0.27091068Iteration 137, loss = 0.26196125\n",
      "\n",
      "Iteration 138, loss = 0.26203671\n",
      "Iteration 31, loss = 0.27117088\n",
      "Iteration 139, loss = 0.26178950\n",
      "Iteration 32, loss = 0.27046098\n",
      "Iteration 140, loss = 0.26181331\n",
      "Iteration 33, loss = 0.27016534\n",
      "Iteration 141, loss = 0.26185433\n",
      "Iteration 34, loss = 0.27020293\n",
      "Iteration 142, loss = 0.26182398\n",
      "Iteration 35, loss = 0.26984300\n",
      "Iteration 143, loss = 0.26174774\n",
      "Iteration 36, loss = 0.26985437\n",
      "Iteration 144, loss = 0.26183820\n",
      "Iteration 37, loss = 0.26990284\n",
      "Iteration 145, loss = 0.26182716\n",
      "Iteration 38, loss = 0.26963003\n",
      "Iteration 146, loss = 0.26178551\n",
      "Iteration 39, loss = 0.26940105\n",
      "Iteration 147, loss = 0.26199449\n",
      "Iteration 40, loss = 0.26896048\n",
      "Iteration 148, loss = 0.26166232\n",
      "Iteration 41, loss = 0.26929398\n",
      "Iteration 149, loss = 0.26176819\n",
      "Iteration 42, loss = 0.26876347\n",
      "Iteration 150, loss = 0.26169459\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 43, loss = 0.26895628\n",
      "Iteration 44, loss = 0.26878941\n",
      "Iteration 1, loss = 0.33149270\n",
      "Iteration 45, loss = 0.26887486\n",
      "Iteration 2, loss = 0.28963008\n",
      "Iteration 46, loss = 0.26875334\n",
      "Iteration 3, loss = 0.28294904\n",
      "Iteration 47, loss = 0.26859103\n",
      "Iteration 4, loss = 0.28114265\n",
      "Iteration 48, loss = 0.26834126\n",
      "Iteration 5, loss = 0.28046229\n",
      "Iteration 49, loss = 0.26811236\n",
      "Iteration 6, loss = 0.28002692\n",
      "Iteration 50, loss = 0.26785175\n",
      "Iteration 7, loss = 0.27944742\n",
      "Iteration 51, loss = 0.26800618\n",
      "Iteration 8, loss = 0.27886595\n",
      "Iteration 9, loss = 0.27844544\n",
      "Iteration 52, loss = 0.26795283\n",
      "Iteration 10, loss = 0.27779749\n",
      "Iteration 53, loss = 0.26761053\n",
      "Iteration 11, loss = 0.27790219\n",
      "Iteration 54, loss = 0.26761259\n",
      "Iteration 12, loss = 0.27722492\n",
      "Iteration 55, loss = 0.26761167\n",
      "Iteration 13, loss = 0.27624764\n",
      "Iteration 56, loss = 0.26759818\n",
      "Iteration 14, loss = 0.27663251\n",
      "Iteration 15, loss = 0.27592458\n",
      "Iteration 57, loss = 0.26738751\n",
      "Iteration 16, loss = 0.27518263\n",
      "Iteration 58, loss = 0.26731502\n",
      "Iteration 17, loss = 0.27490484\n",
      "Iteration 18, loss = 0.27440111\n",
      "Iteration 59, loss = 0.26709479\n",
      "Iteration 19, loss = 0.27490097\n",
      "Iteration 60, loss = 0.26722156\n",
      "Iteration 20, loss = 0.27350336\n",
      "Iteration 61, loss = 0.26703393\n",
      "Iteration 21, loss = 0.27362624\n",
      "Iteration 62, loss = 0.26679432\n",
      "Iteration 22, loss = 0.27327722\n",
      "Iteration 63, loss = 0.26700743\n",
      "Iteration 23, loss = 0.27313028\n",
      "Iteration 64, loss = 0.26678704\n",
      "Iteration 24, loss = 0.27281589\n",
      "Iteration 65, loss = 0.26684868\n",
      "Iteration 25, loss = 0.27275554\n",
      "Iteration 66, loss = 0.26713465\n",
      "Iteration 26, loss = 0.27221000\n",
      "Iteration 67, loss = 0.26644325\n",
      "Iteration 27, loss = 0.27209948\n",
      "Iteration 28, loss = 0.27177492\n",
      "Iteration 68, loss = 0.26645778Iteration 29, loss = 0.27155592\n",
      "\n",
      "Iteration 30, loss = 0.27144553\n",
      "Iteration 69, loss = 0.26637559\n",
      "Iteration 31, loss = 0.27153966\n",
      "Iteration 70, loss = 0.26639654\n",
      "Iteration 32, loss = 0.27113315\n",
      "Iteration 71, loss = 0.26638427\n",
      "Iteration 33, loss = 0.27091591\n",
      "Iteration 72, loss = 0.26628009\n",
      "Iteration 34, loss = 0.27087772\n",
      "Iteration 73, loss = 0.26613287\n",
      "Iteration 35, loss = 0.27066852\n",
      "Iteration 74, loss = 0.26605714\n",
      "Iteration 36, loss = 0.27047640\n",
      "Iteration 75, loss = 0.26614616\n",
      "Iteration 37, loss = 0.27044451\n",
      "Iteration 76, loss = 0.26607011\n",
      "Iteration 38, loss = 0.27033607\n",
      "Iteration 77, loss = 0.26605772\n",
      "Iteration 39, loss = 0.27010002\n",
      "Iteration 78, loss = 0.26578447\n",
      "Iteration 40, loss = 0.26979448\n",
      "Iteration 79, loss = 0.26593201\n",
      "Iteration 41, loss = 0.26974355\n",
      "Iteration 80, loss = 0.26559080\n",
      "Iteration 42, loss = 0.26981716\n",
      "Iteration 43, loss = 0.26959098\n",
      "Iteration 81, loss = 0.26591322\n",
      "Iteration 44, loss = 0.26943443\n",
      "Iteration 82, loss = 0.26535737\n",
      "Iteration 45, loss = 0.26955791\n",
      "Iteration 83, loss = 0.26617047\n",
      "Iteration 46, loss = 0.26930712\n",
      "Iteration 84, loss = 0.26560471\n",
      "Iteration 47, loss = 0.26939073\n",
      "Iteration 85, loss = 0.26566010\n",
      "Iteration 48, loss = 0.26883647\n",
      "Iteration 86, loss = 0.26536565\n",
      "Iteration 49, loss = 0.26874174\n",
      "Iteration 87, loss = 0.26550727\n",
      "Iteration 50, loss = 0.26848678\n",
      "Iteration 88, loss = 0.26530680\n",
      "Iteration 51, loss = 0.26850719\n",
      "Iteration 89, loss = 0.26529063\n",
      "Iteration 52, loss = 0.26840900\n",
      "Iteration 53, loss = 0.26816167\n",
      "Iteration 90, loss = 0.26539043\n",
      "Iteration 54, loss = 0.26842876Iteration 91, loss = 0.26520359\n",
      "\n",
      "Iteration 55, loss = 0.26818140\n",
      "Iteration 92, loss = 0.26517238\n",
      "Iteration 56, loss = 0.26789683\n",
      "Iteration 93, loss = 0.26507195\n",
      "Iteration 57, loss = 0.26779926\n",
      "Iteration 94, loss = 0.26516061\n",
      "Iteration 95, loss = 0.26511187\n",
      "Iteration 58, loss = 0.26768883\n",
      "Iteration 59, loss = 0.26748617\n",
      "Iteration 96, loss = 0.26493678\n",
      "Iteration 60, loss = 0.26754862\n",
      "Iteration 97, loss = 0.26491295\n",
      "Iteration 61, loss = 0.26730083\n",
      "Iteration 98, loss = 0.26510901\n",
      "Iteration 62, loss = 0.26706809\n",
      "Iteration 99, loss = 0.26494855\n",
      "Iteration 63, loss = 0.26693926\n",
      "Iteration 100, loss = 0.26488825\n",
      "Iteration 64, loss = 0.26714689\n",
      "Iteration 101, loss = 0.26479535\n",
      "Iteration 65, loss = 0.26696606\n",
      "Iteration 102, loss = 0.26478346\n",
      "Iteration 66, loss = 0.26696259\n",
      "Iteration 103, loss = 0.26477941\n",
      "Iteration 67, loss = 0.26642063\n",
      "Iteration 104, loss = 0.26469842\n",
      "Iteration 68, loss = 0.26650244\n",
      "Iteration 105, loss = 0.26477776\n",
      "Iteration 69, loss = 0.26621210\n",
      "Iteration 106, loss = 0.26457564\n",
      "Iteration 70, loss = 0.26637868\n",
      "Iteration 107, loss = 0.26497154\n",
      "Iteration 71, loss = 0.26614345\n",
      "Iteration 108, loss = 0.26501507\n",
      "Iteration 72, loss = 0.26616007\n",
      "Iteration 109, loss = 0.26492073\n",
      "Iteration 73, loss = 0.26595572\n",
      "Iteration 110, loss = 0.26465889\n",
      "Iteration 74, loss = 0.26590821\n",
      "Iteration 111, loss = 0.26462305\n",
      "Iteration 75, loss = 0.26583634\n",
      "Iteration 112, loss = 0.26470529\n",
      "Iteration 76, loss = 0.26594974\n",
      "Iteration 113, loss = 0.26472128\n",
      "Iteration 77, loss = 0.26550113\n",
      "Iteration 114, loss = 0.26470285\n",
      "Iteration 78, loss = 0.26546523\n",
      "Iteration 115, loss = 0.26447539\n",
      "Iteration 79, loss = 0.26552693\n",
      "Iteration 116, loss = 0.26461579\n",
      "Iteration 80, loss = 0.26546791\n",
      "Iteration 117, loss = 0.26478701\n",
      "Iteration 81, loss = 0.26560379\n",
      "Iteration 118, loss = 0.26433864\n",
      "Iteration 82, loss = 0.26521871\n",
      "Iteration 119, loss = 0.26448481\n",
      "Iteration 83, loss = 0.26567550\n",
      "Iteration 120, loss = 0.26432670\n",
      "Iteration 84, loss = 0.26511440\n",
      "Iteration 121, loss = 0.26435238\n",
      "Iteration 85, loss = 0.26528492\n",
      "Iteration 122, loss = 0.26432542Iteration 86, loss = 0.26505320\n",
      "\n",
      "Iteration 87, loss = 0.26500298\n",
      "Iteration 123, loss = 0.26426173\n",
      "Iteration 88, loss = 0.26494423\n",
      "Iteration 124, loss = 0.26437354\n",
      "Iteration 89, loss = 0.26498753\n",
      "Iteration 125, loss = 0.26405751\n",
      "Iteration 90, loss = 0.26483053\n",
      "Iteration 126, loss = 0.26406549\n",
      "Iteration 91, loss = 0.26481067\n",
      "Iteration 127, loss = 0.26409293\n",
      "Iteration 92, loss = 0.26479298\n",
      "Iteration 128, loss = 0.26388326\n",
      "Iteration 93, loss = 0.26449341\n",
      "Iteration 129, loss = 0.26404210\n",
      "Iteration 94, loss = 0.26482502\n",
      "Iteration 130, loss = 0.26421326\n",
      "Iteration 95, loss = 0.26476546\n",
      "Iteration 96, loss = 0.26443331\n",
      "Iteration 131, loss = 0.26412278\n",
      "Iteration 132, loss = 0.26418940\n",
      "Iteration 97, loss = 0.26452260\n",
      "Iteration 133, loss = 0.26398321\n",
      "Iteration 98, loss = 0.26442870\n",
      "Iteration 134, loss = 0.26411704\n",
      "Iteration 99, loss = 0.26436720\n",
      "Iteration 135, loss = 0.26385129\n",
      "Iteration 100, loss = 0.26422924\n",
      "Iteration 136, loss = 0.26385338\n",
      "Iteration 101, loss = 0.26426611\n",
      "Iteration 137, loss = 0.26399855\n",
      "Iteration 102, loss = 0.26441503\n",
      "Iteration 138, loss = 0.26401287\n",
      "Iteration 139, loss = 0.26379915\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 103, loss = 0.26419000\n",
      "Iteration 104, loss = 0.26408340\n",
      "Iteration 105, loss = 0.26412427\n",
      "Iteration 106, loss = 0.26409380\n",
      "Iteration 107, loss = 0.26438498\n",
      "Iteration 108, loss = 0.26421584\n",
      "Iteration 109, loss = 0.26417222\n",
      "Iteration 110, loss = 0.26392377\n",
      "Iteration 111, loss = 0.26390597\n",
      "Iteration 112, loss = 0.26386848\n",
      "Iteration 113, loss = 0.26392580\n",
      "Iteration 114, loss = 0.26368677\n",
      "Iteration 115, loss = 0.26377062\n",
      "Iteration 116, loss = 0.26396197\n",
      "Iteration 117, loss = 0.26375662\n",
      "Iteration 118, loss = 0.26353558\n",
      "Iteration 119, loss = 0.26382832\n",
      "Iteration 120, loss = 0.26338551\n",
      "Iteration 121, loss = 0.26336634\n",
      "Iteration 122, loss = 0.26359415\n",
      "Iteration 123, loss = 0.26366871\n",
      "Iteration 124, loss = 0.26365298\n",
      "Iteration 125, loss = 0.26325179\n",
      "Iteration 126, loss = 0.26332165\n",
      "Iteration 127, loss = 0.26332178\n",
      "Iteration 128, loss = 0.26324110\n",
      "Iteration 129, loss = 0.26309083\n",
      "Iteration 130, loss = 0.26330359\n",
      "Iteration 131, loss = 0.26343273\n",
      "Iteration 132, loss = 0.26337497\n",
      "Iteration 133, loss = 0.26326405\n",
      "Iteration 134, loss = 0.26313489\n",
      "Iteration 135, loss = 0.26314926\n",
      "Iteration 136, loss = 0.26306306\n",
      "Iteration 137, loss = 0.26319605\n",
      "Iteration 138, loss = 0.26301285\n",
      "Iteration 139, loss = 0.26286813\n",
      "Iteration 140, loss = 0.26283009\n",
      "Iteration 141, loss = 0.26304107\n",
      "Iteration 142, loss = 0.26294450\n",
      "Iteration 143, loss = 0.26266381\n",
      "Iteration 144, loss = 0.26285634\n",
      "Iteration 145, loss = 0.26272657\n",
      "Iteration 146, loss = 0.26264294\n",
      "Iteration 147, loss = 0.26267931\n",
      "Iteration 148, loss = 0.26282946\n",
      "Iteration 149, loss = 0.26245814\n",
      "Iteration 150, loss = 0.26248119\n",
      "Iteration 151, loss = 0.26312735\n",
      "Iteration 152, loss = 0.26249861\n",
      "Iteration 153, loss = 0.26265604\n",
      "Iteration 154, loss = 0.26248831\n",
      "Iteration 155, loss = 0.26259637\n",
      "Iteration 156, loss = 0.26246007\n",
      "Iteration 157, loss = 0.26245606\n",
      "Iteration 158, loss = 0.26228957\n",
      "Iteration 159, loss = 0.26240063\n",
      "Iteration 160, loss = 0.26231532\n",
      "Iteration 161, loss = 0.26238612\n",
      "Iteration 162, loss = 0.26257471\n",
      "Iteration 163, loss = 0.26242405\n",
      "Iteration 164, loss = 0.26234164\n",
      "Iteration 165, loss = 0.26224167\n",
      "Iteration 166, loss = 0.26225811\n",
      "Iteration 167, loss = 0.26222708\n",
      "Iteration 168, loss = 0.26262294\n",
      "Iteration 169, loss = 0.26233152\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.32933191\n",
      "Iteration 1, loss = 0.32868857\n",
      "Iteration 2, loss = 0.28414072\n",
      "Iteration 2, loss = 0.28516368\n",
      "Iteration 3, loss = 0.27780825\n",
      "Iteration 3, loss = 0.27886037\n",
      "Iteration 4, loss = 0.27628053\n",
      "Iteration 4, loss = 0.27695893\n",
      "Iteration 5, loss = 0.27564871\n",
      "Iteration 5, loss = 0.27630014\n",
      "Iteration 6, loss = 0.27473679\n",
      "Iteration 6, loss = 0.27560781\n",
      "Iteration 7, loss = 0.27425567\n",
      "Iteration 7, loss = 0.27509756\n",
      "Iteration 8, loss = 0.27379643\n",
      "Iteration 8, loss = 0.27458305\n",
      "Iteration 9, loss = 0.27321029\n",
      "Iteration 9, loss = 0.27382013\n",
      "Iteration 10, loss = 0.27246196\n",
      "Iteration 10, loss = 0.27319870\n",
      "Iteration 11, loss = 0.27227162\n",
      "Iteration 11, loss = 0.27297560\n",
      "Iteration 12, loss = 0.27195568\n",
      "Iteration 12, loss = 0.27293139\n",
      "Iteration 13, loss = 0.27091348\n",
      "Iteration 13, loss = 0.27189136\n",
      "Iteration 14, loss = 0.27060403\n",
      "Iteration 14, loss = 0.27148363\n",
      "Iteration 15, loss = 0.27044572\n",
      "Iteration 15, loss = 0.27143097\n",
      "Iteration 16, loss = 0.26993637\n",
      "Iteration 16, loss = 0.27100486\n",
      "Iteration 17, loss = 0.26975474\n",
      "Iteration 17, loss = 0.27067598\n",
      "Iteration 18, loss = 0.26924988\n",
      "Iteration 18, loss = 0.27043442\n",
      "Iteration 19, loss = 0.26888286\n",
      "Iteration 19, loss = 0.27018521\n",
      "Iteration 20, loss = 0.26837360\n",
      "Iteration 20, loss = 0.26953784\n",
      "Iteration 21, loss = 0.26838334\n",
      "Iteration 21, loss = 0.26965969\n",
      "Iteration 22, loss = 0.26769221\n",
      "Iteration 22, loss = 0.26888524\n",
      "Iteration 23, loss = 0.26876316\n",
      "Iteration 23, loss = 0.26747241\n",
      "Iteration 24, loss = 0.26837643\n",
      "Iteration 24, loss = 0.26727076\n",
      "Iteration 25, loss = 0.26781433\n",
      "Iteration 25, loss = 0.26650556\n",
      "Iteration 26, loss = 0.26787017\n",
      "Iteration 26, loss = 0.26654949\n",
      "Iteration 27, loss = 0.26748432\n",
      "Iteration 27, loss = 0.26629879\n",
      "Iteration 28, loss = 0.26731592\n",
      "Iteration 28, loss = 0.26612822\n",
      "Iteration 29, loss = 0.26700507\n",
      "Iteration 29, loss = 0.26582158\n",
      "Iteration 30, loss = 0.26678392\n",
      "Iteration 30, loss = 0.26549091\n",
      "Iteration 31, loss = 0.26648742\n",
      "Iteration 31, loss = 0.26547237\n",
      "Iteration 32, loss = 0.26626822\n",
      "Iteration 32, loss = 0.26549244\n",
      "Iteration 33, loss = 0.26593493\n",
      "Iteration 33, loss = 0.26522165\n",
      "Iteration 34, loss = 0.26570612\n",
      "Iteration 34, loss = 0.26492217\n",
      "Iteration 35, loss = 0.26575260\n",
      "Iteration 35, loss = 0.26466831\n",
      "Iteration 36, loss = 0.26523111\n",
      "Iteration 36, loss = 0.26477173\n",
      "Iteration 37, loss = 0.26474364\n",
      "Iteration 37, loss = 0.26439340\n",
      "Iteration 38, loss = 0.26463399\n",
      "Iteration 38, loss = 0.26426286\n",
      "Iteration 39, loss = 0.26445961\n",
      "Iteration 39, loss = 0.26405188\n",
      "Iteration 40, loss = 0.26409430\n",
      "Iteration 40, loss = 0.26396302\n",
      "Iteration 41, loss = 0.26393442\n",
      "Iteration 41, loss = 0.26383001\n",
      "Iteration 42, loss = 0.26380540\n",
      "Iteration 42, loss = 0.26397876\n",
      "Iteration 43, loss = 0.26361932\n",
      "Iteration 43, loss = 0.26351747\n",
      "Iteration 44, loss = 0.26330650\n",
      "Iteration 44, loss = 0.26327294\n",
      "Iteration 45, loss = 0.26314113\n",
      "Iteration 45, loss = 0.26340143\n",
      "Iteration 46, loss = 0.26315990Iteration 46, loss = 0.26305854\n",
      "\n",
      "Iteration 47, loss = 0.26319333\n",
      "Iteration 47, loss = 0.26282108\n",
      "Iteration 48, loss = 0.26307208\n",
      "Iteration 48, loss = 0.26277634\n",
      "Iteration 49, loss = 0.26300053\n",
      "Iteration 49, loss = 0.26270758\n",
      "Iteration 50, loss = 0.26292013\n",
      "Iteration 50, loss = 0.26261763\n",
      "Iteration 51, loss = 0.26307590\n",
      "Iteration 51, loss = 0.26298822\n",
      "Iteration 52, loss = 0.26275583\n",
      "Iteration 52, loss = 0.26246203\n",
      "Iteration 53, loss = 0.26315422\n",
      "Iteration 53, loss = 0.26287104\n",
      "Iteration 54, loss = 0.26275883\n",
      "Iteration 54, loss = 0.26229115\n",
      "Iteration 55, loss = 0.26255990\n",
      "Iteration 55, loss = 0.26227698\n",
      "Iteration 56, loss = 0.26240775\n",
      "Iteration 56, loss = 0.26212394\n",
      "Iteration 57, loss = 0.26237754\n",
      "Iteration 58, loss = 0.26217627\n",
      "Iteration 57, loss = 0.26191013\n",
      "Iteration 59, loss = 0.26193423\n",
      "Iteration 58, loss = 0.26165507\n",
      "Iteration 60, loss = 0.26214225\n",
      "Iteration 59, loss = 0.26188661\n",
      "Iteration 61, loss = 0.26184682\n",
      "Iteration 60, loss = 0.26198380\n",
      "Iteration 62, loss = 0.26217096\n",
      "Iteration 61, loss = 0.26175349\n",
      "Iteration 63, loss = 0.26178632\n",
      "Iteration 62, loss = 0.26213840\n",
      "Iteration 63, loss = 0.26180902\n",
      "Iteration 64, loss = 0.26169841\n",
      "Iteration 64, loss = 0.26157720\n",
      "Iteration 65, loss = 0.26199817\n",
      "Iteration 65, loss = 0.26149950\n",
      "Iteration 66, loss = 0.26151069\n",
      "Iteration 66, loss = 0.26163909\n",
      "Iteration 67, loss = 0.26136010\n",
      "Iteration 67, loss = 0.26119408\n",
      "Iteration 68, loss = 0.26142932\n",
      "Iteration 68, loss = 0.26151255\n",
      "Iteration 69, loss = 0.26186649\n",
      "Iteration 69, loss = 0.26180613\n",
      "Iteration 70, loss = 0.26143864\n",
      "Iteration 70, loss = 0.26122299\n",
      "Iteration 71, loss = 0.26166400\n",
      "Iteration 71, loss = 0.26187500\n",
      "Iteration 72, loss = 0.26104246\n",
      "Iteration 73, loss = 0.26101157\n",
      "Iteration 72, loss = 0.26110583\n",
      "Iteration 74, loss = 0.26085505\n",
      "Iteration 75, loss = 0.26089551\n",
      "Iteration 73, loss = 0.26130271\n",
      "Iteration 76, loss = 0.26076597\n",
      "Iteration 74, loss = 0.26089752\n",
      "Iteration 77, loss = 0.26097378\n",
      "Iteration 75, loss = 0.26109712\n",
      "Iteration 78, loss = 0.26065230\n",
      "Iteration 76, loss = 0.26092570\n",
      "Iteration 79, loss = 0.26062024\n",
      "Iteration 77, loss = 0.26099891\n",
      "Iteration 80, loss = 0.26057909\n",
      "Iteration 78, loss = 0.26092563\n",
      "Iteration 81, loss = 0.26065787\n",
      "Iteration 79, loss = 0.26084401\n",
      "Iteration 82, loss = 0.26065577Iteration 80, loss = 0.26081123\n",
      "\n",
      "Iteration 81, loss = 0.26090293\n",
      "Iteration 83, loss = 0.26047266\n",
      "Iteration 84, loss = 0.26058167\n",
      "Iteration 82, loss = 0.26088605\n",
      "Iteration 83, loss = 0.26062903\n",
      "Iteration 85, loss = 0.26032860\n",
      "Iteration 84, loss = 0.26081141\n",
      "Iteration 86, loss = 0.26043258\n",
      "Iteration 85, loss = 0.26040297\n",
      "Iteration 87, loss = 0.26008284\n",
      "Iteration 86, loss = 0.26056548\n",
      "Iteration 87, loss = 0.26052722\n",
      "Iteration 88, loss = 0.26008962\n",
      "Iteration 89, loss = 0.26022389\n",
      "Iteration 88, loss = 0.26045069\n",
      "Iteration 90, loss = 0.26014483\n",
      "Iteration 91, loss = 0.26031119\n",
      "Iteration 89, loss = 0.26055343\n",
      "Iteration 90, loss = 0.26041025\n",
      "Iteration 92, loss = 0.26003975\n",
      "Iteration 91, loss = 0.26061583\n",
      "Iteration 93, loss = 0.25996433\n",
      "Iteration 92, loss = 0.26054756\n",
      "Iteration 94, loss = 0.25991264\n",
      "Iteration 93, loss = 0.26043977\n",
      "Iteration 95, loss = 0.25979598\n",
      "Iteration 94, loss = 0.26050712\n",
      "Iteration 96, loss = 0.25977997\n",
      "Iteration 95, loss = 0.26028095\n",
      "Iteration 97, loss = 0.25958965\n",
      "Iteration 96, loss = 0.26000283\n",
      "Iteration 98, loss = 0.25979069\n",
      "Iteration 97, loss = 0.26001431\n",
      "Iteration 99, loss = 0.25944345\n",
      "Iteration 98, loss = 0.26019706\n",
      "Iteration 100, loss = 0.25962107\n",
      "Iteration 99, loss = 0.26009291\n",
      "Iteration 101, loss = 0.25942348\n",
      "Iteration 100, loss = 0.26004693\n",
      "Iteration 102, loss = 0.25930672\n",
      "Iteration 101, loss = 0.26010337\n",
      "Iteration 103, loss = 0.25932824\n",
      "Iteration 102, loss = 0.26001796\n",
      "Iteration 104, loss = 0.25948399\n",
      "Iteration 103, loss = 0.25987239\n",
      "Iteration 105, loss = 0.25936897\n",
      "Iteration 104, loss = 0.26011320\n",
      "Iteration 106, loss = 0.25918615\n",
      "Iteration 105, loss = 0.25993398\n",
      "Iteration 107, loss = 0.25921654\n",
      "Iteration 106, loss = 0.26007610\n",
      "Iteration 108, loss = 0.25937801\n",
      "Iteration 107, loss = 0.26007286\n",
      "Iteration 109, loss = 0.25927916\n",
      "Iteration 108, loss = 0.25990900\n",
      "Iteration 110, loss = 0.25911515\n",
      "Iteration 109, loss = 0.25999882\n",
      "Iteration 111, loss = 0.25902142\n",
      "Iteration 110, loss = 0.25982906\n",
      "Iteration 112, loss = 0.25927722\n",
      "Iteration 111, loss = 0.25984172\n",
      "Iteration 113, loss = 0.25893839\n",
      "Iteration 112, loss = 0.25987970\n",
      "Iteration 114, loss = 0.25897341\n",
      "Iteration 113, loss = 0.25977431\n",
      "Iteration 115, loss = 0.25906810\n",
      "Iteration 114, loss = 0.25975184\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 116, loss = 0.25881516\n",
      "Iteration 117, loss = 0.25893074\n",
      "Iteration 1, loss = 0.32933181\n",
      "Iteration 118, loss = 0.25883560\n",
      "Iteration 2, loss = 0.28468115\n",
      "Iteration 119, loss = 0.25867174\n",
      "Iteration 3, loss = 0.27826097\n",
      "Iteration 120, loss = 0.25883634\n",
      "Iteration 4, loss = 0.27693113\n",
      "Iteration 121, loss = 0.25843105\n",
      "Iteration 5, loss = 0.27596101\n",
      "Iteration 122, loss = 0.25858426\n",
      "Iteration 6, loss = 0.27539175\n",
      "Iteration 123, loss = 0.25863622\n",
      "Iteration 7, loss = 0.27476544\n",
      "Iteration 124, loss = 0.25836906\n",
      "Iteration 8, loss = 0.27399453\n",
      "Iteration 9, loss = 0.27355270\n",
      "Iteration 125, loss = 0.25819803\n",
      "Iteration 10, loss = 0.27283067\n",
      "Iteration 126, loss = 0.25834143\n",
      "Iteration 11, loss = 0.27252749\n",
      "Iteration 127, loss = 0.25814931\n",
      "Iteration 12, loss = 0.27223653\n",
      "Iteration 128, loss = 0.25803975\n",
      "Iteration 13, loss = 0.27126664\n",
      "Iteration 129, loss = 0.25811160\n",
      "Iteration 14, loss = 0.27109161\n",
      "Iteration 130, loss = 0.25826093Iteration 15, loss = 0.27082412\n",
      "\n",
      "Iteration 16, loss = 0.27045287\n",
      "Iteration 131, loss = 0.25837197\n",
      "Iteration 17, loss = 0.27014997\n",
      "Iteration 132, loss = 0.25801122\n",
      "Iteration 18, loss = 0.26966065\n",
      "Iteration 19, loss = 0.26939894\n",
      "Iteration 133, loss = 0.25791507\n",
      "Iteration 20, loss = 0.26869932\n",
      "Iteration 134, loss = 0.25816175\n",
      "Iteration 21, loss = 0.26870540\n",
      "Iteration 135, loss = 0.25797780\n",
      "Iteration 22, loss = 0.26797836\n",
      "Iteration 136, loss = 0.25803706\n",
      "Iteration 23, loss = 0.26795114\n",
      "Iteration 137, loss = 0.25817710\n",
      "Iteration 24, loss = 0.26765100\n",
      "Iteration 138, loss = 0.25811530\n",
      "Iteration 25, loss = 0.26684157\n",
      "Iteration 139, loss = 0.25786528\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 26, loss = 0.26691297\n",
      "Iteration 27, loss = 0.26632522\n",
      "Iteration 1, loss = 0.32949569\n",
      "Iteration 28, loss = 0.26630913\n",
      "Iteration 2, loss = 0.28518340\n",
      "Iteration 29, loss = 0.26600401\n",
      "Iteration 3, loss = 0.27874032\n",
      "Iteration 30, loss = 0.26566463\n",
      "Iteration 4, loss = 0.27743307\n",
      "Iteration 5, loss = 0.27649749\n",
      "Iteration 31, loss = 0.26528066\n",
      "Iteration 32, loss = 0.26549294\n",
      "Iteration 6, loss = 0.27583105\n",
      "Iteration 33, loss = 0.26498859\n",
      "Iteration 7, loss = 0.27530298\n",
      "Iteration 34, loss = 0.26493360\n",
      "Iteration 8, loss = 0.27450846\n",
      "Iteration 35, loss = 0.26483872\n",
      "Iteration 9, loss = 0.27391224\n",
      "Iteration 36, loss = 0.26449065\n",
      "Iteration 10, loss = 0.27322037\n",
      "Iteration 37, loss = 0.26435520\n",
      "Iteration 11, loss = 0.27298288\n",
      "Iteration 38, loss = 0.26388636\n",
      "Iteration 12, loss = 0.27253272\n",
      "Iteration 39, loss = 0.26396730\n",
      "Iteration 13, loss = 0.27187008\n",
      "Iteration 14, loss = 0.27158694\n",
      "Iteration 40, loss = 0.26390201\n",
      "Iteration 41, loss = 0.26368638\n",
      "Iteration 15, loss = 0.27115171\n",
      "Iteration 16, loss = 0.27086647\n",
      "Iteration 42, loss = 0.26377176\n",
      "Iteration 43, loss = 0.26323076\n",
      "Iteration 17, loss = 0.27037129\n",
      "Iteration 44, loss = 0.26310944\n",
      "Iteration 18, loss = 0.26987418\n",
      "Iteration 45, loss = 0.26307375\n",
      "Iteration 19, loss = 0.26941296\n",
      "Iteration 46, loss = 0.26301458\n",
      "Iteration 20, loss = 0.26878069\n",
      "Iteration 47, loss = 0.26281337\n",
      "Iteration 21, loss = 0.26873170\n",
      "Iteration 48, loss = 0.26288679\n",
      "Iteration 22, loss = 0.26814093\n",
      "Iteration 49, loss = 0.26289949\n",
      "Iteration 23, loss = 0.26767713\n",
      "Iteration 50, loss = 0.26259709\n",
      "Iteration 24, loss = 0.26793689\n",
      "Iteration 51, loss = 0.26263691\n",
      "Iteration 25, loss = 0.26705692\n",
      "Iteration 52, loss = 0.26253602\n",
      "Iteration 26, loss = 0.26702295\n",
      "Iteration 53, loss = 0.26312757\n",
      "Iteration 54, loss = 0.26231956\n",
      "Iteration 27, loss = 0.26682284\n",
      "Iteration 28, loss = 0.26650690\n",
      "Iteration 55, loss = 0.26232603\n",
      "Iteration 29, loss = 0.26631509\n",
      "Iteration 56, loss = 0.26226416\n",
      "Iteration 30, loss = 0.26592194\n",
      "Iteration 57, loss = 0.26217641\n",
      "Iteration 31, loss = 0.26560720\n",
      "Iteration 58, loss = 0.26184161\n",
      "Iteration 32, loss = 0.26590733\n",
      "Iteration 59, loss = 0.26191029\n",
      "Iteration 33, loss = 0.26532717\n",
      "Iteration 60, loss = 0.26211066\n",
      "Iteration 34, loss = 0.26529376\n",
      "Iteration 61, loss = 0.26177854\n",
      "Iteration 35, loss = 0.26527458\n",
      "Iteration 62, loss = 0.26196564\n",
      "Iteration 36, loss = 0.26490248\n",
      "Iteration 63, loss = 0.26184992\n",
      "Iteration 37, loss = 0.26482871\n",
      "Iteration 64, loss = 0.26173812\n",
      "Iteration 38, loss = 0.26435495\n",
      "Iteration 65, loss = 0.26194552\n",
      "Iteration 66, loss = 0.26156353\n",
      "Iteration 39, loss = 0.26437586\n",
      "Iteration 40, loss = 0.26455065\n",
      "Iteration 67, loss = 0.26148416\n",
      "Iteration 41, loss = 0.26417428\n",
      "Iteration 68, loss = 0.26149912\n",
      "Iteration 42, loss = 0.26405088\n",
      "Iteration 69, loss = 0.26183182\n",
      "Iteration 43, loss = 0.26386017\n",
      "Iteration 70, loss = 0.26125110\n",
      "Iteration 44, loss = 0.26359552\n",
      "Iteration 71, loss = 0.26181841\n",
      "Iteration 45, loss = 0.26358013\n",
      "Iteration 72, loss = 0.26103702\n",
      "Iteration 46, loss = 0.26351644\n",
      "Iteration 73, loss = 0.26135831\n",
      "Iteration 47, loss = 0.26352964\n",
      "Iteration 74, loss = 0.26100382\n",
      "Iteration 48, loss = 0.26332950\n",
      "Iteration 75, loss = 0.26118584\n",
      "Iteration 49, loss = 0.26338504\n",
      "Iteration 76, loss = 0.26110739\n",
      "Iteration 50, loss = 0.26317277Iteration 77, loss = 0.26100988\n",
      "\n",
      "Iteration 78, loss = 0.26092660\n",
      "Iteration 51, loss = 0.26291228\n",
      "Iteration 52, loss = 0.26285288\n",
      "Iteration 79, loss = 0.26089849\n",
      "Iteration 53, loss = 0.26298661\n",
      "Iteration 80, loss = 0.26086464\n",
      "Iteration 54, loss = 0.26254238\n",
      "Iteration 81, loss = 0.26087757\n",
      "Iteration 55, loss = 0.26268937\n",
      "Iteration 82, loss = 0.26097399\n",
      "Iteration 56, loss = 0.26244151\n",
      "Iteration 83, loss = 0.26084001\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 57, loss = 0.26232886\n",
      "Iteration 58, loss = 0.26199858\n",
      "Iteration 1, loss = 0.32910572\n",
      "Iteration 59, loss = 0.26209806\n",
      "Iteration 2, loss = 0.28526578\n",
      "Iteration 60, loss = 0.26237268\n",
      "Iteration 3, loss = 0.27875290\n",
      "Iteration 61, loss = 0.26191793\n",
      "Iteration 4, loss = 0.27749052\n",
      "Iteration 62, loss = 0.26174034\n",
      "Iteration 5, loss = 0.27655896\n",
      "Iteration 63, loss = 0.26181386\n",
      "Iteration 6, loss = 0.27617209\n",
      "Iteration 64, loss = 0.26153964\n",
      "Iteration 7, loss = 0.27534553\n",
      "Iteration 65, loss = 0.26178848\n",
      "Iteration 8, loss = 0.27467351\n",
      "Iteration 66, loss = 0.26141315\n",
      "Iteration 9, loss = 0.27419438\n",
      "Iteration 67, loss = 0.26122320\n",
      "Iteration 10, loss = 0.27366842\n",
      "Iteration 68, loss = 0.26127132\n",
      "Iteration 11, loss = 0.27322420\n",
      "Iteration 69, loss = 0.26139448\n",
      "Iteration 12, loss = 0.27310184\n",
      "Iteration 70, loss = 0.26125030\n",
      "Iteration 13, loss = 0.27232201\n",
      "Iteration 71, loss = 0.26181887\n",
      "Iteration 14, loss = 0.27209936\n",
      "Iteration 15, loss = 0.27172196\n",
      "Iteration 72, loss = 0.26053701\n",
      "Iteration 73, loss = 0.26094581\n",
      "Iteration 16, loss = 0.27150101\n",
      "Iteration 74, loss = 0.26046726\n",
      "Iteration 17, loss = 0.27127481\n",
      "Iteration 75, loss = 0.26057643\n",
      "Iteration 18, loss = 0.27075652\n",
      "Iteration 76, loss = 0.26035406\n",
      "Iteration 19, loss = 0.27040710\n",
      "Iteration 77, loss = 0.26045832\n",
      "Iteration 20, loss = 0.26998592\n",
      "Iteration 78, loss = 0.26019743\n",
      "Iteration 21, loss = 0.27015637\n",
      "Iteration 79, loss = 0.26027778\n",
      "Iteration 80, loss = 0.26017696\n",
      "Iteration 22, loss = 0.26969123\n",
      "Iteration 81, loss = 0.26040300\n",
      "Iteration 23, loss = 0.26919045\n",
      "Iteration 82, loss = 0.26013640\n",
      "Iteration 24, loss = 0.26918040\n",
      "Iteration 83, loss = 0.26042937\n",
      "Iteration 25, loss = 0.26847692\n",
      "Iteration 84, loss = 0.26031058\n",
      "Iteration 26, loss = 0.26818308\n",
      "Iteration 27, loss = 0.26802578\n",
      "Iteration 85, loss = 0.25986380\n",
      "Iteration 86, loss = 0.25977305\n",
      "Iteration 28, loss = 0.26771100\n",
      "Iteration 87, loss = 0.25973942\n",
      "Iteration 29, loss = 0.26741654\n",
      "Iteration 88, loss = 0.26007599\n",
      "Iteration 30, loss = 0.26714565\n",
      "Iteration 89, loss = 0.25991577\n",
      "Iteration 31, loss = 0.26689467\n",
      "Iteration 90, loss = 0.25996343\n",
      "Iteration 32, loss = 0.26697088\n",
      "Iteration 91, loss = 0.25988323\n",
      "Iteration 33, loss = 0.26632869\n",
      "Iteration 92, loss = 0.26008368\n",
      "Iteration 34, loss = 0.26667281\n",
      "Iteration 93, loss = 0.25993028\n",
      "Iteration 35, loss = 0.26640038\n",
      "Iteration 94, loss = 0.25963906\n",
      "Iteration 36, loss = 0.26602626\n",
      "Iteration 95, loss = 0.25948271\n",
      "Iteration 37, loss = 0.26585525\n",
      "Iteration 96, loss = 0.25961640\n",
      "Iteration 38, loss = 0.26539504\n",
      "Iteration 97, loss = 0.25936309\n",
      "Iteration 39, loss = 0.26537467\n",
      "Iteration 98, loss = 0.25953092\n",
      "Iteration 40, loss = 0.26543286\n",
      "Iteration 99, loss = 0.25938611\n",
      "Iteration 41, loss = 0.26510226\n",
      "Iteration 100, loss = 0.25949990\n",
      "Iteration 42, loss = 0.26490182\n",
      "Iteration 101, loss = 0.25944202\n",
      "Iteration 43, loss = 0.26487106\n",
      "Iteration 102, loss = 0.25935665\n",
      "Iteration 44, loss = 0.26453578\n",
      "Iteration 103, loss = 0.25931108\n",
      "Iteration 45, loss = 0.26427134\n",
      "Iteration 104, loss = 0.25962562\n",
      "Iteration 46, loss = 0.26444816\n",
      "Iteration 105, loss = 0.25940101\n",
      "Iteration 47, loss = 0.26425918\n",
      "Iteration 106, loss = 0.25915824\n",
      "Iteration 48, loss = 0.26408717\n",
      "Iteration 107, loss = 0.25922370\n",
      "Iteration 49, loss = 0.26399042\n",
      "Iteration 108, loss = 0.25930750\n",
      "Iteration 50, loss = 0.26380906\n",
      "Iteration 109, loss = 0.25923488\n",
      "Iteration 51, loss = 0.26345687\n",
      "Iteration 110, loss = 0.25914334\n",
      "Iteration 52, loss = 0.26348610\n",
      "Iteration 111, loss = 0.25946177\n",
      "Iteration 112, loss = 0.25932841\n",
      "Iteration 53, loss = 0.26340458\n",
      "Iteration 113, loss = 0.25910689\n",
      "Iteration 54, loss = 0.26309998\n",
      "Iteration 114, loss = 0.25904065\n",
      "Iteration 55, loss = 0.26313015\n",
      "Iteration 115, loss = 0.25923110\n",
      "Iteration 56, loss = 0.26307044\n",
      "Iteration 116, loss = 0.25902463\n",
      "Iteration 57, loss = 0.26277849\n",
      "Iteration 117, loss = 0.25907425\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 58, loss = 0.26259426\n",
      "Iteration 1, loss = 0.32878667\n",
      "Iteration 59, loss = 0.26251476\n",
      "Iteration 60, loss = 0.26271569\n",
      "Iteration 2, loss = 0.28415550\n",
      "Iteration 61, loss = 0.26223006\n",
      "Iteration 3, loss = 0.27752294\n",
      "Iteration 4, loss = 0.27610762\n",
      "Iteration 62, loss = 0.26228763\n",
      "Iteration 63, loss = 0.26218987\n",
      "Iteration 5, loss = 0.27508292\n",
      "Iteration 6, loss = 0.27473393Iteration 64, loss = 0.26206008\n",
      "\n",
      "Iteration 65, loss = 0.26231848\n",
      "Iteration 7, loss = 0.27379985\n",
      "Iteration 66, loss = 0.26201950\n",
      "Iteration 8, loss = 0.27336564\n",
      "Iteration 67, loss = 0.26168957\n",
      "Iteration 9, loss = 0.27278050\n",
      "Iteration 68, loss = 0.26196432\n",
      "Iteration 10, loss = 0.27245194\n",
      "Iteration 69, loss = 0.26181080\n",
      "Iteration 11, loss = 0.27181630\n",
      "Iteration 70, loss = 0.26204708\n",
      "Iteration 12, loss = 0.27171411\n",
      "Iteration 13, loss = 0.27101510\n",
      "Iteration 71, loss = 0.26197790\n",
      "Iteration 14, loss = 0.27069171\n",
      "Iteration 72, loss = 0.26123937\n",
      "Iteration 15, loss = 0.27025542\n",
      "Iteration 73, loss = 0.26139829\n",
      "Iteration 16, loss = 0.27011516\n",
      "Iteration 74, loss = 0.26121117\n",
      "Iteration 17, loss = 0.26968932\n",
      "Iteration 75, loss = 0.26111955\n",
      "Iteration 18, loss = 0.26893701\n",
      "Iteration 76, loss = 0.26111821\n",
      "Iteration 19, loss = 0.26858198\n",
      "Iteration 77, loss = 0.26098833\n",
      "Iteration 20, loss = 0.26830366\n",
      "Iteration 78, loss = 0.26104923\n",
      "Iteration 21, loss = 0.26817861\n",
      "Iteration 79, loss = 0.26107324\n",
      "Iteration 22, loss = 0.26752922\n",
      "Iteration 80, loss = 0.26108430\n",
      "Iteration 23, loss = 0.26730790\n",
      "Iteration 81, loss = 0.26109287\n",
      "Iteration 82, loss = 0.26092832\n",
      "Iteration 24, loss = 0.26687805\n",
      "Iteration 25, loss = 0.26643562\n",
      "Iteration 83, loss = 0.26084842\n",
      "Iteration 84, loss = 0.26138161\n",
      "Iteration 26, loss = 0.26613137\n",
      "Iteration 85, loss = 0.26061212\n",
      "Iteration 27, loss = 0.26604670\n",
      "Iteration 86, loss = 0.26067960\n",
      "Iteration 28, loss = 0.26561242\n",
      "Iteration 87, loss = 0.26065931\n",
      "Iteration 29, loss = 0.26536224\n",
      "Iteration 88, loss = 0.26115078\n",
      "Iteration 30, loss = 0.26511381\n",
      "Iteration 89, loss = 0.26084164\n",
      "Iteration 31, loss = 0.26481260\n",
      "Iteration 90, loss = 0.26087157\n",
      "Iteration 32, loss = 0.26476115\n",
      "Iteration 91, loss = 0.26072443\n",
      "Iteration 33, loss = 0.26431736\n",
      "Iteration 92, loss = 0.26073262\n",
      "Iteration 34, loss = 0.26466619\n",
      "Iteration 93, loss = 0.26045615\n",
      "Iteration 35, loss = 0.26437205\n",
      "Iteration 94, loss = 0.26039763\n",
      "Iteration 36, loss = 0.26402058\n",
      "Iteration 95, loss = 0.26032486\n",
      "Iteration 37, loss = 0.26378056\n",
      "Iteration 96, loss = 0.26040636\n",
      "Iteration 38, loss = 0.26327733\n",
      "Iteration 97, loss = 0.26035035\n",
      "Iteration 39, loss = 0.26324567\n",
      "Iteration 98, loss = 0.26010347\n",
      "Iteration 40, loss = 0.26303142\n",
      "Iteration 99, loss = 0.26042295\n",
      "Iteration 41, loss = 0.26305745\n",
      "Iteration 100, loss = 0.26034279\n",
      "Iteration 42, loss = 0.26275070\n",
      "Iteration 101, loss = 0.26032965\n",
      "Iteration 43, loss = 0.26251235\n",
      "Iteration 102, loss = 0.26007725\n",
      "Iteration 44, loss = 0.26220089\n",
      "Iteration 103, loss = 0.26001435\n",
      "Iteration 45, loss = 0.26202349\n",
      "Iteration 104, loss = 0.26025504\n",
      "Iteration 46, loss = 0.26195532\n",
      "Iteration 105, loss = 0.26000659\n",
      "Iteration 47, loss = 0.26209719\n",
      "Iteration 106, loss = 0.26000539\n",
      "Iteration 48, loss = 0.26181538\n",
      "Iteration 107, loss = 0.26011482\n",
      "Iteration 49, loss = 0.26175408\n",
      "Iteration 108, loss = 0.26007666\n",
      "Iteration 50, loss = 0.26150957\n",
      "Iteration 109, loss = 0.26010532\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 51, loss = 0.26103749\n",
      "Iteration 52, loss = 0.26097149\n",
      "Iteration 1, loss = 0.32981658Iteration 53, loss = 0.26087783\n",
      "\n",
      "Iteration 54, loss = 0.26058230\n",
      "Iteration 2, loss = 0.28643744\n",
      "Iteration 55, loss = 0.26081650\n",
      "Iteration 3, loss = 0.27981131\n",
      "Iteration 56, loss = 0.26052991\n",
      "Iteration 4, loss = 0.27858117\n",
      "Iteration 57, loss = 0.26033352\n",
      "Iteration 5, loss = 0.27740002\n",
      "Iteration 58, loss = 0.26016666\n",
      "Iteration 6, loss = 0.27706237\n",
      "Iteration 59, loss = 0.26006595\n",
      "Iteration 7, loss = 0.27627582\n",
      "Iteration 60, loss = 0.26018593\n",
      "Iteration 8, loss = 0.27585735\n",
      "Iteration 61, loss = 0.25987949\n",
      "Iteration 62, loss = 0.26000307\n",
      "Iteration 9, loss = 0.27511828\n",
      "Iteration 63, loss = 0.25974974\n",
      "Iteration 10, loss = 0.27451702\n",
      "Iteration 64, loss = 0.25964588\n",
      "Iteration 11, loss = 0.27416165\n",
      "Iteration 65, loss = 0.26013937\n",
      "Iteration 12, loss = 0.27361793\n",
      "Iteration 66, loss = 0.25971631\n",
      "Iteration 13, loss = 0.27315163\n",
      "Iteration 67, loss = 0.25943432\n",
      "Iteration 14, loss = 0.27292983\n",
      "Iteration 68, loss = 0.25932026\n",
      "Iteration 69, loss = 0.25932476\n",
      "Iteration 15, loss = 0.27241472\n",
      "Iteration 70, loss = 0.25955827\n",
      "Iteration 16, loss = 0.27201424\n",
      "Iteration 71, loss = 0.25934709\n",
      "Iteration 17, loss = 0.27187185\n",
      "Iteration 72, loss = 0.25880840\n",
      "Iteration 18, loss = 0.27083689\n",
      "Iteration 73, loss = 0.25870429\n",
      "Iteration 19, loss = 0.27040932\n",
      "Iteration 74, loss = 0.25873459\n",
      "Iteration 20, loss = 0.27024212\n",
      "Iteration 75, loss = 0.25849062\n",
      "Iteration 21, loss = 0.27006022\n",
      "Iteration 76, loss = 0.25857263\n",
      "Iteration 22, loss = 0.26967243\n",
      "Iteration 77, loss = 0.25854935\n",
      "Iteration 78, loss = 0.25828983\n",
      "Iteration 23, loss = 0.26975493\n",
      "Iteration 79, loss = 0.25849754\n",
      "Iteration 24, loss = 0.26898396\n",
      "Iteration 80, loss = 0.25828208\n",
      "Iteration 25, loss = 0.26848318\n",
      "Iteration 81, loss = 0.25845709\n",
      "Iteration 26, loss = 0.26818438\n",
      "Iteration 82, loss = 0.25788971\n",
      "Iteration 27, loss = 0.26814903\n",
      "Iteration 83, loss = 0.25851779\n",
      "Iteration 28, loss = 0.26759558\n",
      "Iteration 84, loss = 0.25843904\n",
      "Iteration 29, loss = 0.26733024\n",
      "Iteration 85, loss = 0.25805116\n",
      "Iteration 30, loss = 0.26711970\n",
      "Iteration 86, loss = 0.25795235\n",
      "Iteration 31, loss = 0.26691518\n",
      "Iteration 87, loss = 0.25788394\n",
      "Iteration 32, loss = 0.26663538\n",
      "Iteration 88, loss = 0.25797124\n",
      "Iteration 89, loss = 0.25777098\n",
      "Iteration 33, loss = 0.26615614\n",
      "Iteration 90, loss = 0.25837533\n",
      "Iteration 34, loss = 0.26647763\n",
      "Iteration 91, loss = 0.25782679\n",
      "Iteration 35, loss = 0.26613056\n",
      "Iteration 92, loss = 0.25779255\n",
      "Iteration 36, loss = 0.26590276\n",
      "Iteration 93, loss = 0.25768693\n",
      "Iteration 37, loss = 0.26553793\n",
      "Iteration 94, loss = 0.25748336\n",
      "Iteration 38, loss = 0.26540193\n",
      "Iteration 95, loss = 0.25748952\n",
      "Iteration 39, loss = 0.26521102\n",
      "Iteration 96, loss = 0.25750052\n",
      "Iteration 40, loss = 0.26509550\n",
      "Iteration 97, loss = 0.25738260\n",
      "Iteration 41, loss = 0.26492144\n",
      "Iteration 98, loss = 0.25736752\n",
      "Iteration 42, loss = 0.26482021\n",
      "Iteration 99, loss = 0.25758463\n",
      "Iteration 43, loss = 0.26448948\n",
      "Iteration 100, loss = 0.25746558\n",
      "Iteration 44, loss = 0.26421006\n",
      "Iteration 101, loss = 0.25728446\n",
      "Iteration 45, loss = 0.26422464\n",
      "Iteration 102, loss = 0.25728236\n",
      "Iteration 103, loss = 0.25722106\n",
      "Iteration 46, loss = 0.26413871\n",
      "Iteration 104, loss = 0.25744974\n",
      "Iteration 47, loss = 0.26419722\n",
      "Iteration 105, loss = 0.25733257\n",
      "Iteration 48, loss = 0.26416529\n",
      "Iteration 106, loss = 0.25727535\n",
      "Iteration 49, loss = 0.26396564\n",
      "Iteration 50, loss = 0.26376280Iteration 107, loss = 0.25713641\n",
      "\n",
      "Iteration 51, loss = 0.26337898\n",
      "Iteration 108, loss = 0.25720297\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 52, loss = 0.26341659\n",
      "Iteration 53, loss = 0.26330493\n",
      "Iteration 1, loss = 0.32764999\n",
      "Iteration 2, loss = 0.28329292\n",
      "Iteration 54, loss = 0.26327997\n",
      "Iteration 3, loss = 0.27660154\n",
      "Iteration 55, loss = 0.26327581\n",
      "Iteration 4, loss = 0.27511253\n",
      "Iteration 56, loss = 0.26314164\n",
      "Iteration 5, loss = 0.27405281\n",
      "Iteration 57, loss = 0.26312810\n",
      "Iteration 6, loss = 0.27394466\n",
      "Iteration 58, loss = 0.26291880\n",
      "Iteration 7, loss = 0.27307294\n",
      "Iteration 59, loss = 0.26296774\n",
      "Iteration 8, loss = 0.27263359\n",
      "Iteration 60, loss = 0.26291253\n",
      "Iteration 9, loss = 0.27201366\n",
      "Iteration 10, loss = 0.27141296\n",
      "Iteration 61, loss = 0.26273107\n",
      "Iteration 11, loss = 0.27094189\n",
      "Iteration 62, loss = 0.26283872\n",
      "Iteration 12, loss = 0.27039460\n",
      "Iteration 63, loss = 0.26244044\n",
      "Iteration 13, loss = 0.27012936\n",
      "Iteration 64, loss = 0.26245380\n",
      "Iteration 14, loss = 0.26955493\n",
      "Iteration 65, loss = 0.26269895\n",
      "Iteration 15, loss = 0.26884678\n",
      "Iteration 66, loss = 0.26265048\n",
      "Iteration 16, loss = 0.26845191\n",
      "Iteration 67, loss = 0.26266608\n",
      "Iteration 17, loss = 0.26810773\n",
      "Iteration 68, loss = 0.26257453\n",
      "Iteration 18, loss = 0.26710992\n",
      "Iteration 69, loss = 0.26221943\n",
      "Iteration 19, loss = 0.26660531\n",
      "Iteration 70, loss = 0.26261671\n",
      "Iteration 20, loss = 0.26640734\n",
      "Iteration 71, loss = 0.26234281\n",
      "Iteration 21, loss = 0.26597857\n",
      "Iteration 72, loss = 0.26199797\n",
      "Iteration 22, loss = 0.26547427\n",
      "Iteration 73, loss = 0.26189824\n",
      "Iteration 23, loss = 0.26541208\n",
      "Iteration 74, loss = 0.26189804\n",
      "Iteration 24, loss = 0.26455696\n",
      "Iteration 75, loss = 0.26200414\n",
      "Iteration 25, loss = 0.26444902\n",
      "Iteration 76, loss = 0.26174322\n",
      "Iteration 26, loss = 0.26400793\n",
      "Iteration 77, loss = 0.26163774\n",
      "Iteration 27, loss = 0.26398455\n",
      "Iteration 78, loss = 0.26174119\n",
      "Iteration 28, loss = 0.26340265\n",
      "Iteration 79, loss = 0.26174544\n",
      "Iteration 29, loss = 0.26330653\n",
      "Iteration 80, loss = 0.26146408\n",
      "Iteration 30, loss = 0.26300981\n",
      "Iteration 31, loss = 0.26290390\n",
      "Iteration 81, loss = 0.26172606\n",
      "Iteration 82, loss = 0.26164048Iteration 32, loss = 0.26255746\n",
      "\n",
      "Iteration 83, loss = 0.26167079\n",
      "Iteration 33, loss = 0.26214398\n",
      "Iteration 84, loss = 0.26173007\n",
      "Iteration 34, loss = 0.26227093\n",
      "Iteration 85, loss = 0.26154143\n",
      "Iteration 35, loss = 0.26216485\n",
      "Iteration 86, loss = 0.26118612\n",
      "Iteration 36, loss = 0.26199485\n",
      "Iteration 87, loss = 0.26125162\n",
      "Iteration 37, loss = 0.26152478\n",
      "Iteration 88, loss = 0.26157098\n",
      "Iteration 38, loss = 0.26162196\n",
      "Iteration 89, loss = 0.26136641\n",
      "Iteration 39, loss = 0.26150195\n",
      "Iteration 90, loss = 0.26154881\n",
      "Iteration 40, loss = 0.26133235\n",
      "Iteration 91, loss = 0.26166371\n",
      "Iteration 41, loss = 0.26121635\n",
      "Iteration 92, loss = 0.26160173\n",
      "Iteration 42, loss = 0.26119454\n",
      "Iteration 93, loss = 0.26116658\n",
      "Iteration 43, loss = 0.26075145\n",
      "Iteration 94, loss = 0.26118314\n",
      "Iteration 44, loss = 0.26059951\n",
      "Iteration 95, loss = 0.26118762\n",
      "Iteration 45, loss = 0.26052860\n",
      "Iteration 96, loss = 0.26133710\n",
      "Iteration 46, loss = 0.26073491\n",
      "Iteration 97, loss = 0.26144500\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 47, loss = 0.26038878\n",
      "Iteration 48, loss = 0.26060543\n",
      "Iteration 1, loss = 0.32864312\n",
      "Iteration 49, loss = 0.26034581\n",
      "Iteration 2, loss = 0.28458247\n",
      "Iteration 50, loss = 0.26018654\n",
      "Iteration 3, loss = 0.27812934\n",
      "Iteration 51, loss = 0.25972390\n",
      "Iteration 4, loss = 0.27663351\n",
      "Iteration 52, loss = 0.25985630\n",
      "Iteration 5, loss = 0.27569768\n",
      "Iteration 53, loss = 0.25986184\n",
      "Iteration 6, loss = 0.27512615\n",
      "Iteration 54, loss = 0.25982737\n",
      "Iteration 7, loss = 0.27471387\n",
      "Iteration 55, loss = 0.25972727\n",
      "Iteration 8, loss = 0.27395590\n",
      "Iteration 56, loss = 0.25977149\n",
      "Iteration 9, loss = 0.27366243\n",
      "Iteration 57, loss = 0.25985176\n",
      "Iteration 10, loss = 0.27292559\n",
      "Iteration 58, loss = 0.25944101\n",
      "Iteration 11, loss = 0.27266132\n",
      "Iteration 59, loss = 0.25944360\n",
      "Iteration 12, loss = 0.27218594\n",
      "Iteration 60, loss = 0.25970847\n",
      "Iteration 13, loss = 0.27161035\n",
      "Iteration 61, loss = 0.25946930\n",
      "Iteration 14, loss = 0.27134686\n",
      "Iteration 62, loss = 0.25930171\n",
      "Iteration 15, loss = 0.27127317\n",
      "Iteration 63, loss = 0.25927632\n",
      "Iteration 16, loss = 0.27069500\n",
      "Iteration 64, loss = 0.25922768\n",
      "Iteration 65, loss = 0.25942460\n",
      "Iteration 17, loss = 0.27035102\n",
      "Iteration 18, loss = 0.26990063\n",
      "Iteration 66, loss = 0.25937408\n",
      "Iteration 67, loss = 0.25961537\n",
      "Iteration 19, loss = 0.26955638\n",
      "Iteration 68, loss = 0.25932031\n",
      "Iteration 20, loss = 0.26935179\n",
      "Iteration 69, loss = 0.25911570\n",
      "Iteration 21, loss = 0.26902354\n",
      "Iteration 22, loss = 0.26815678\n",
      "Iteration 70, loss = 0.25925698\n",
      "Iteration 23, loss = 0.26818976\n",
      "Iteration 71, loss = 0.25907566\n",
      "Iteration 72, loss = 0.25888521\n",
      "Iteration 24, loss = 0.26753569\n",
      "Iteration 25, loss = 0.26746709\n",
      "Iteration 73, loss = 0.25876996\n",
      "Iteration 26, loss = 0.26670344\n",
      "Iteration 74, loss = 0.25908852\n",
      "Iteration 27, loss = 0.26648910\n",
      "Iteration 75, loss = 0.25890281\n",
      "Iteration 28, loss = 0.26629104\n",
      "Iteration 76, loss = 0.25876007\n",
      "Iteration 29, loss = 0.26602766\n",
      "Iteration 77, loss = 0.25861629\n",
      "Iteration 30, loss = 0.26582518\n",
      "Iteration 78, loss = 0.25885265\n",
      "Iteration 31, loss = 0.26538987\n",
      "Iteration 79, loss = 0.25875791\n",
      "Iteration 32, loss = 0.26514923\n",
      "Iteration 80, loss = 0.25845324\n",
      "Iteration 33, loss = 0.26503460\n",
      "Iteration 81, loss = 0.25863139\n",
      "Iteration 34, loss = 0.26492426\n",
      "Iteration 82, loss = 0.25861965\n",
      "Iteration 35, loss = 0.26465404\n",
      "Iteration 83, loss = 0.25881725\n",
      "Iteration 36, loss = 0.26427709\n",
      "Iteration 84, loss = 0.25866675\n",
      "Iteration 37, loss = 0.26429912\n",
      "Iteration 85, loss = 0.25855279\n",
      "Iteration 38, loss = 0.26428782\n",
      "Iteration 86, loss = 0.25846061\n",
      "Iteration 39, loss = 0.26406430\n",
      "Iteration 87, loss = 0.25839379\n",
      "Iteration 40, loss = 0.26379259\n",
      "Iteration 88, loss = 0.25868611\n",
      "Iteration 41, loss = 0.26417410\n",
      "Iteration 89, loss = 0.25837084\n",
      "Iteration 42, loss = 0.26354940\n",
      "Iteration 90, loss = 0.25849037\n",
      "Iteration 43, loss = 0.26335399\n",
      "Iteration 91, loss = 0.25868055\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 44, loss = 0.26304967\n",
      "Iteration 45, loss = 0.26306634\n",
      "Iteration 1, loss = 0.32810474\n",
      "Iteration 46, loss = 0.26297445\n",
      "Iteration 2, loss = 0.28313193Iteration 47, loss = 0.26289751\n",
      "\n",
      "Iteration 3, loss = 0.27672396\n",
      "Iteration 48, loss = 0.26264953\n",
      "Iteration 4, loss = 0.27515133\n",
      "Iteration 5, loss = 0.27411945\n",
      "Iteration 49, loss = 0.26316957\n",
      "Iteration 6, loss = 0.27340814\n",
      "Iteration 50, loss = 0.26294829\n",
      "Iteration 7, loss = 0.27265701\n",
      "Iteration 51, loss = 0.26265709\n",
      "Iteration 8, loss = 0.27188527\n",
      "Iteration 52, loss = 0.26231665\n",
      "Iteration 9, loss = 0.27167164\n",
      "Iteration 53, loss = 0.26227742\n",
      "Iteration 10, loss = 0.27087386\n",
      "Iteration 54, loss = 0.26281978\n",
      "Iteration 11, loss = 0.27082095\n",
      "Iteration 55, loss = 0.26224675\n",
      "Iteration 12, loss = 0.27045627\n",
      "Iteration 56, loss = 0.26204438\n",
      "Iteration 13, loss = 0.26947330\n",
      "Iteration 57, loss = 0.26195411\n",
      "Iteration 14, loss = 0.26920064\n",
      "Iteration 58, loss = 0.26187974\n",
      "Iteration 15, loss = 0.26898093\n",
      "Iteration 59, loss = 0.26195387\n",
      "Iteration 16, loss = 0.26840738\n",
      "Iteration 60, loss = 0.26204707\n",
      "Iteration 17, loss = 0.26816017\n",
      "Iteration 61, loss = 0.26198929\n",
      "Iteration 18, loss = 0.26777052\n",
      "Iteration 62, loss = 0.26164883\n",
      "Iteration 19, loss = 0.26753589\n",
      "Iteration 63, loss = 0.26168577\n",
      "Iteration 20, loss = 0.26690089\n",
      "Iteration 64, loss = 0.26160131\n",
      "Iteration 21, loss = 0.26653496\n",
      "Iteration 65, loss = 0.26160554\n",
      "Iteration 22, loss = 0.26585414\n",
      "Iteration 66, loss = 0.26160612\n",
      "Iteration 23, loss = 0.26581512\n",
      "Iteration 67, loss = 0.26150479\n",
      "Iteration 24, loss = 0.26522168\n",
      "Iteration 68, loss = 0.26142848\n",
      "Iteration 25, loss = 0.26504731\n",
      "Iteration 26, loss = 0.26467531\n",
      "Iteration 69, loss = 0.26165026\n",
      "Iteration 27, loss = 0.26445693\n",
      "Iteration 70, loss = 0.26160728\n",
      "Iteration 28, loss = 0.26400784\n",
      "Iteration 71, loss = 0.26155704\n",
      "Iteration 29, loss = 0.26424131\n",
      "Iteration 72, loss = 0.26145492\n",
      "Iteration 30, loss = 0.26347146\n",
      "Iteration 73, loss = 0.26144164\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 31, loss = 0.26339349\n",
      "Iteration 32, loss = 0.26319473\n",
      "Iteration 33, loss = 0.26312936\n",
      "Iteration 34, loss = 0.26288289\n",
      "Iteration 35, loss = 0.26266634\n",
      "Iteration 36, loss = 0.26226321\n",
      "Iteration 37, loss = 0.26223193\n",
      "Iteration 38, loss = 0.26203195\n",
      "Iteration 39, loss = 0.26169619\n",
      "Iteration 40, loss = 0.26154610\n",
      "Iteration 41, loss = 0.26199291\n",
      "Iteration 42, loss = 0.26132099\n",
      "Iteration 43, loss = 0.26094121\n",
      "Iteration 44, loss = 0.26087972\n",
      "Iteration 45, loss = 0.26073162\n",
      "Iteration 46, loss = 0.26055504\n",
      "Iteration 47, loss = 0.26072075\n",
      "Iteration 48, loss = 0.26029449\n",
      "Iteration 49, loss = 0.26056914\n",
      "Iteration 50, loss = 0.26027967\n",
      "Iteration 51, loss = 0.26007385\n",
      "Iteration 52, loss = 0.25989012\n",
      "Iteration 53, loss = 0.25971560\n",
      "Iteration 54, loss = 0.26047288\n",
      "Iteration 55, loss = 0.25959911\n",
      "Iteration 56, loss = 0.25946706\n",
      "Iteration 57, loss = 0.25935159\n",
      "Iteration 58, loss = 0.25941333\n",
      "Iteration 59, loss = 0.25928149\n",
      "Iteration 60, loss = 0.25924436\n",
      "Iteration 61, loss = 0.25907427\n",
      "Iteration 62, loss = 0.25877528\n",
      "Iteration 63, loss = 0.25889812\n",
      "Iteration 64, loss = 0.25890588\n",
      "Iteration 65, loss = 0.25855659\n",
      "Iteration 66, loss = 0.25877925\n",
      "Iteration 67, loss = 0.25861652\n",
      "Iteration 68, loss = 0.25846174\n",
      "Iteration 69, loss = 0.25848146\n",
      "Iteration 70, loss = 0.25845556\n",
      "Iteration 71, loss = 0.25836621\n",
      "Iteration 72, loss = 0.25833525\n",
      "Iteration 73, loss = 0.25823083\n",
      "Iteration 74, loss = 0.25813904\n",
      "Iteration 75, loss = 0.25779920\n",
      "Iteration 76, loss = 0.25789347\n",
      "Iteration 77, loss = 0.25787141\n",
      "Iteration 78, loss = 0.25794094\n",
      "Iteration 79, loss = 0.25765547\n",
      "Iteration 80, loss = 0.25780827\n",
      "Iteration 81, loss = 0.25765049\n",
      "Iteration 82, loss = 0.25770964\n",
      "Iteration 83, loss = 0.25785371\n",
      "Iteration 84, loss = 0.25762706\n",
      "Iteration 85, loss = 0.25747984\n",
      "Iteration 86, loss = 0.25787699\n",
      "Iteration 87, loss = 0.25762829\n",
      "Iteration 88, loss = 0.25724469\n",
      "Iteration 89, loss = 0.25788493\n",
      "Iteration 90, loss = 0.25728115\n",
      "Iteration 91, loss = 0.25737737\n",
      "Iteration 92, loss = 0.25735281\n",
      "Iteration 93, loss = 0.25713494\n",
      "Iteration 94, loss = 0.25754065\n",
      "Iteration 95, loss = 0.25734836\n",
      "Iteration 96, loss = 0.25733161\n",
      "Iteration 97, loss = 0.25729230\n",
      "Iteration 98, loss = 0.25708331\n",
      "Iteration 99, loss = 0.25707407\n",
      "Iteration 100, loss = 0.25693845\n",
      "Iteration 101, loss = 0.25744053\n",
      "Iteration 102, loss = 0.25700685\n",
      "Iteration 103, loss = 0.25714528\n",
      "Iteration 104, loss = 0.25687979\n",
      "Iteration 105, loss = 0.25687604\n",
      "Iteration 106, loss = 0.25663675\n",
      "Iteration 107, loss = 0.25688529\n",
      "Iteration 108, loss = 0.25686073\n",
      "Iteration 109, loss = 0.25698563\n",
      "Iteration 110, loss = 0.25674689\n",
      "Iteration 111, loss = 0.25707362\n",
      "Iteration 112, loss = 0.25664641\n",
      "Iteration 113, loss = 0.25657169\n",
      "Iteration 114, loss = 0.25694601\n",
      "Iteration 115, loss = 0.25675175\n",
      "Iteration 116, loss = 0.25682815\n",
      "Iteration 117, loss = 0.25670071\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.31954522\n",
      "Iteration 2, loss = 0.28168422\n",
      "Iteration 1, loss = 0.31928602\n",
      "Iteration 3, loss = 0.27694391\n",
      "Iteration 2, loss = 0.28173787\n",
      "Iteration 4, loss = 0.27531510\n",
      "Iteration 5, loss = 0.27475046Iteration 3, loss = 0.27696056\n",
      "\n",
      "Iteration 4, loss = 0.27535918\n",
      "Iteration 6, loss = 0.27394726\n",
      "Iteration 7, loss = 0.27324976\n",
      "Iteration 5, loss = 0.27466613\n",
      "Iteration 8, loss = 0.27270986\n",
      "Iteration 6, loss = 0.27397174\n",
      "Iteration 9, loss = 0.27238941\n",
      "Iteration 7, loss = 0.27342788\n",
      "Iteration 10, loss = 0.27120252\n",
      "Iteration 8, loss = 0.27300679\n",
      "Iteration 11, loss = 0.27087671\n",
      "Iteration 9, loss = 0.27291601\n",
      "Iteration 12, loss = 0.27029488\n",
      "Iteration 10, loss = 0.27177742\n",
      "Iteration 13, loss = 0.27022656\n",
      "Iteration 11, loss = 0.27115045\n",
      "Iteration 14, loss = 0.26960292\n",
      "Iteration 12, loss = 0.27062908\n",
      "Iteration 15, loss = 0.26891735\n",
      "Iteration 13, loss = 0.27067467\n",
      "Iteration 16, loss = 0.26834080\n",
      "Iteration 14, loss = 0.26972359\n",
      "Iteration 15, loss = 0.26946148\n",
      "Iteration 17, loss = 0.26801538\n",
      "Iteration 16, loss = 0.26901293\n",
      "Iteration 18, loss = 0.26815213\n",
      "Iteration 17, loss = 0.26849942\n",
      "Iteration 18, loss = 0.26855552\n",
      "Iteration 19, loss = 0.26720783\n",
      "Iteration 19, loss = 0.26779743\n",
      "Iteration 20, loss = 0.26723395\n",
      "Iteration 20, loss = 0.26758197\n",
      "Iteration 21, loss = 0.26713473\n",
      "Iteration 21, loss = 0.26746473\n",
      "Iteration 22, loss = 0.26637870\n",
      "Iteration 23, loss = 0.26609198\n",
      "Iteration 22, loss = 0.26674155\n",
      "Iteration 24, loss = 0.26570775\n",
      "Iteration 25, loss = 0.26560480\n",
      "Iteration 23, loss = 0.26658693\n",
      "Iteration 26, loss = 0.26516170\n",
      "Iteration 27, loss = 0.26513831\n",
      "Iteration 24, loss = 0.26596396\n",
      "Iteration 28, loss = 0.26489851\n",
      "Iteration 25, loss = 0.26604320\n",
      "Iteration 26, loss = 0.26557873\n",
      "Iteration 29, loss = 0.26507024\n",
      "Iteration 27, loss = 0.26537920\n",
      "Iteration 30, loss = 0.26449179\n",
      "Iteration 28, loss = 0.26511513\n",
      "Iteration 31, loss = 0.26455073\n",
      "Iteration 29, loss = 0.26516914\n",
      "Iteration 30, loss = 0.26484737\n",
      "Iteration 32, loss = 0.26414536\n",
      "Iteration 33, loss = 0.26406713\n",
      "Iteration 31, loss = 0.26486163\n",
      "Iteration 34, loss = 0.26389777\n",
      "Iteration 32, loss = 0.26427227\n",
      "Iteration 35, loss = 0.26366692\n",
      "Iteration 33, loss = 0.26427481\n",
      "Iteration 36, loss = 0.26370427\n",
      "Iteration 34, loss = 0.26416251\n",
      "Iteration 37, loss = 0.26363104\n",
      "Iteration 38, loss = 0.26359457\n",
      "Iteration 35, loss = 0.26381496\n",
      "Iteration 39, loss = 0.26356190\n",
      "Iteration 36, loss = 0.26387081\n",
      "Iteration 40, loss = 0.26322070\n",
      "Iteration 37, loss = 0.26348956\n",
      "Iteration 41, loss = 0.26328155\n",
      "Iteration 38, loss = 0.26333963\n",
      "Iteration 42, loss = 0.26312708\n",
      "Iteration 39, loss = 0.26360020\n",
      "Iteration 43, loss = 0.26273770\n",
      "Iteration 44, loss = 0.26301451\n",
      "Iteration 40, loss = 0.26310016\n",
      "Iteration 41, loss = 0.26321136\n",
      "Iteration 45, loss = 0.26293727\n",
      "Iteration 42, loss = 0.26309144\n",
      "Iteration 46, loss = 0.26292759\n",
      "Iteration 43, loss = 0.26264693\n",
      "Iteration 47, loss = 0.26242492\n",
      "Iteration 44, loss = 0.26307005\n",
      "Iteration 48, loss = 0.26243427\n",
      "Iteration 49, loss = 0.26219958\n",
      "Iteration 45, loss = 0.26286642\n",
      "Iteration 50, loss = 0.26218392\n",
      "Iteration 46, loss = 0.26287315\n",
      "Iteration 51, loss = 0.26220388\n",
      "Iteration 52, loss = 0.26242521\n",
      "Iteration 47, loss = 0.26247823\n",
      "Iteration 48, loss = 0.26241080\n",
      "Iteration 53, loss = 0.26201199\n",
      "Iteration 49, loss = 0.26244231\n",
      "Iteration 54, loss = 0.26185194\n",
      "Iteration 50, loss = 0.26217187\n",
      "Iteration 55, loss = 0.26185647\n",
      "Iteration 51, loss = 0.26228240\n",
      "Iteration 56, loss = 0.26189263\n",
      "Iteration 52, loss = 0.26223931\n",
      "Iteration 57, loss = 0.26206388\n",
      "Iteration 53, loss = 0.26219480\n",
      "Iteration 58, loss = 0.26199168\n",
      "Iteration 54, loss = 0.26194792\n",
      "Iteration 59, loss = 0.26166478\n",
      "Iteration 55, loss = 0.26199706\n",
      "Iteration 60, loss = 0.26186868\n",
      "Iteration 56, loss = 0.26199710\n",
      "Iteration 61, loss = 0.26151317\n",
      "Iteration 57, loss = 0.26220514\n",
      "Iteration 62, loss = 0.26178257\n",
      "Iteration 58, loss = 0.26176222\n",
      "Iteration 63, loss = 0.26176765\n",
      "Iteration 59, loss = 0.26188460\n",
      "Iteration 60, loss = 0.26173682Iteration 64, loss = 0.26150434\n",
      "\n",
      "Iteration 65, loss = 0.26143531\n",
      "Iteration 61, loss = 0.26149170\n",
      "Iteration 66, loss = 0.26121428\n",
      "Iteration 67, loss = 0.26127329\n",
      "Iteration 62, loss = 0.26163328\n",
      "Iteration 68, loss = 0.26131915\n",
      "Iteration 69, loss = 0.26108563\n",
      "Iteration 63, loss = 0.26162359\n",
      "Iteration 64, loss = 0.26124366\n",
      "Iteration 70, loss = 0.26116852\n",
      "Iteration 65, loss = 0.26125481\n",
      "Iteration 66, loss = 0.26117428\n",
      "Iteration 71, loss = 0.26105318\n",
      "Iteration 67, loss = 0.26099016\n",
      "Iteration 72, loss = 0.26112307\n",
      "Iteration 68, loss = 0.26111317\n",
      "Iteration 69, loss = 0.26089250\n",
      "Iteration 73, loss = 0.26111860\n",
      "Iteration 70, loss = 0.26093328\n",
      "Iteration 74, loss = 0.26120079\n",
      "Iteration 71, loss = 0.26083350\n",
      "Iteration 72, loss = 0.26078414\n",
      "Iteration 75, loss = 0.26099057\n",
      "Iteration 73, loss = 0.26056081\n",
      "Iteration 76, loss = 0.26084694\n",
      "Iteration 77, loss = 0.26100331\n",
      "Iteration 74, loss = 0.26096964\n",
      "Iteration 78, loss = 0.26101129\n",
      "Iteration 75, loss = 0.26051201\n",
      "Iteration 79, loss = 0.26063463\n",
      "Iteration 76, loss = 0.26043919\n",
      "Iteration 80, loss = 0.26088955\n",
      "Iteration 77, loss = 0.26059156\n",
      "Iteration 81, loss = 0.26076881Iteration 78, loss = 0.26051164\n",
      "\n",
      "Iteration 79, loss = 0.26026358\n",
      "Iteration 82, loss = 0.26055475\n",
      "Iteration 83, loss = 0.26072269\n",
      "Iteration 80, loss = 0.26037131\n",
      "Iteration 84, loss = 0.26057798\n",
      "Iteration 85, loss = 0.26068814\n",
      "Iteration 81, loss = 0.26007277\n",
      "Iteration 82, loss = 0.26016251\n",
      "Iteration 86, loss = 0.26055217\n",
      "Iteration 83, loss = 0.26015465\n",
      "Iteration 84, loss = 0.26021089\n",
      "Iteration 87, loss = 0.26074497\n",
      "Iteration 85, loss = 0.26006080\n",
      "Iteration 88, loss = 0.26056582\n",
      "Iteration 86, loss = 0.26009116\n",
      "Iteration 89, loss = 0.26072588\n",
      "Iteration 87, loss = 0.26009575\n",
      "Iteration 90, loss = 0.26057939\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 88, loss = 0.25977788\n",
      "Iteration 89, loss = 0.26000644\n",
      "Iteration 1, loss = 0.31927925\n",
      "Iteration 90, loss = 0.25991569\n",
      "Iteration 2, loss = 0.28080210\n",
      "Iteration 91, loss = 0.26013421\n",
      "Iteration 3, loss = 0.27591737\n",
      "Iteration 92, loss = 0.25999520\n",
      "Iteration 4, loss = 0.27445384\n",
      "Iteration 93, loss = 0.25995372\n",
      "Iteration 5, loss = 0.27374979\n",
      "Iteration 94, loss = 0.26023016\n",
      "Iteration 6, loss = 0.27302048\n",
      "Iteration 95, loss = 0.25978249\n",
      "Iteration 7, loss = 0.27258342\n",
      "Iteration 96, loss = 0.25967869\n",
      "Iteration 8, loss = 0.27180854\n",
      "Iteration 97, loss = 0.25969446\n",
      "Iteration 9, loss = 0.27150218\n",
      "Iteration 98, loss = 0.25967225\n",
      "Iteration 10, loss = 0.27066371\n",
      "Iteration 11, loss = 0.26984052\n",
      "Iteration 99, loss = 0.25957783\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 12, loss = 0.26946960\n",
      "Iteration 1, loss = 0.32033428\n",
      "Iteration 13, loss = 0.26932777\n",
      "Iteration 14, loss = 0.26852890\n",
      "Iteration 2, loss = 0.28233499\n",
      "Iteration 15, loss = 0.26841483\n",
      "Iteration 3, loss = 0.27754763\n",
      "Iteration 4, loss = 0.27623535\n",
      "Iteration 5, loss = 0.27523208\n",
      "Iteration 16, loss = 0.26783663\n",
      "Iteration 6, loss = 0.27437579\n",
      "Iteration 17, loss = 0.26729201\n",
      "Iteration 7, loss = 0.27398393\n",
      "Iteration 18, loss = 0.26693622\n",
      "Iteration 8, loss = 0.27350611\n",
      "Iteration 19, loss = 0.26672827\n",
      "Iteration 9, loss = 0.27316333\n",
      "Iteration 20, loss = 0.26640537\n",
      "Iteration 10, loss = 0.27224543\n",
      "Iteration 21, loss = 0.26610603\n",
      "Iteration 11, loss = 0.27151686\n",
      "Iteration 22, loss = 0.26559704\n",
      "Iteration 23, loss = 0.26572198\n",
      "Iteration 12, loss = 0.27135420\n",
      "Iteration 13, loss = 0.27106368\n",
      "Iteration 24, loss = 0.26507113\n",
      "Iteration 14, loss = 0.27041484\n",
      "Iteration 25, loss = 0.26522519\n",
      "Iteration 15, loss = 0.27005014\n",
      "Iteration 26, loss = 0.26470234\n",
      "Iteration 16, loss = 0.26964577\n",
      "Iteration 27, loss = 0.26464923\n",
      "Iteration 17, loss = 0.26904722\n",
      "Iteration 28, loss = 0.26440703\n",
      "Iteration 18, loss = 0.26873411\n",
      "Iteration 29, loss = 0.26442824\n",
      "Iteration 19, loss = 0.26859217\n",
      "Iteration 30, loss = 0.26405985\n",
      "Iteration 20, loss = 0.26810822\n",
      "Iteration 31, loss = 0.26397184\n",
      "Iteration 21, loss = 0.26805037\n",
      "Iteration 32, loss = 0.26373283\n",
      "Iteration 22, loss = 0.26739987\n",
      "Iteration 33, loss = 0.26359028\n",
      "Iteration 23, loss = 0.26758057\n",
      "Iteration 34, loss = 0.26342883\n",
      "Iteration 24, loss = 0.26707878\n",
      "Iteration 35, loss = 0.26334519\n",
      "Iteration 25, loss = 0.26686845\n",
      "Iteration 36, loss = 0.26333287\n",
      "Iteration 26, loss = 0.26635731\n",
      "Iteration 37, loss = 0.26309637\n",
      "Iteration 27, loss = 0.26643192\n",
      "Iteration 28, loss = 0.26609347\n",
      "Iteration 38, loss = 0.26316676\n",
      "Iteration 29, loss = 0.26575486\n",
      "Iteration 39, loss = 0.26306230\n",
      "Iteration 30, loss = 0.26568474\n",
      "Iteration 40, loss = 0.26263189\n",
      "Iteration 31, loss = 0.26549169\n",
      "Iteration 41, loss = 0.26272952\n",
      "Iteration 32, loss = 0.26528473\n",
      "Iteration 42, loss = 0.26251384\n",
      "Iteration 33, loss = 0.26522700\n",
      "Iteration 43, loss = 0.26217946\n",
      "Iteration 34, loss = 0.26507893\n",
      "Iteration 44, loss = 0.26232581\n",
      "Iteration 35, loss = 0.26479324\n",
      "Iteration 45, loss = 0.26218402\n",
      "Iteration 36, loss = 0.26515269\n",
      "Iteration 46, loss = 0.26227757\n",
      "Iteration 37, loss = 0.26476291\n",
      "Iteration 47, loss = 0.26203212\n",
      "Iteration 38, loss = 0.26478266\n",
      "Iteration 48, loss = 0.26178231\n",
      "Iteration 39, loss = 0.26449620\n",
      "Iteration 40, loss = 0.26437047\n",
      "Iteration 49, loss = 0.26199768\n",
      "Iteration 41, loss = 0.26457420\n",
      "Iteration 50, loss = 0.26156349\n",
      "Iteration 42, loss = 0.26399451Iteration 51, loss = 0.26156503\n",
      "\n",
      "Iteration 52, loss = 0.26148774\n",
      "Iteration 43, loss = 0.26370175\n",
      "Iteration 53, loss = 0.26139645\n",
      "Iteration 44, loss = 0.26391764\n",
      "Iteration 54, loss = 0.26142533\n",
      "Iteration 45, loss = 0.26359083\n",
      "Iteration 55, loss = 0.26124165\n",
      "Iteration 46, loss = 0.26374453\n",
      "Iteration 56, loss = 0.26115642\n",
      "Iteration 47, loss = 0.26352064\n",
      "Iteration 57, loss = 0.26137052\n",
      "Iteration 58, loss = 0.26109310\n",
      "Iteration 48, loss = 0.26324037\n",
      "Iteration 59, loss = 0.26114810\n",
      "Iteration 49, loss = 0.26342867\n",
      "Iteration 60, loss = 0.26112554\n",
      "Iteration 50, loss = 0.26296215\n",
      "Iteration 61, loss = 0.26069629\n",
      "Iteration 51, loss = 0.26311251\n",
      "Iteration 62, loss = 0.26082311\n",
      "Iteration 52, loss = 0.26292219\n",
      "Iteration 63, loss = 0.26072581\n",
      "Iteration 53, loss = 0.26295864\n",
      "Iteration 64, loss = 0.26051035\n",
      "Iteration 54, loss = 0.26290332\n",
      "Iteration 65, loss = 0.26048871\n",
      "Iteration 66, loss = 0.26039485\n",
      "Iteration 55, loss = 0.26288725\n",
      "Iteration 67, loss = 0.26046195\n",
      "Iteration 56, loss = 0.26289031\n",
      "Iteration 68, loss = 0.26056524\n",
      "Iteration 57, loss = 0.26302247\n",
      "Iteration 69, loss = 0.26020825\n",
      "Iteration 58, loss = 0.26266246\n",
      "Iteration 70, loss = 0.26021793\n",
      "Iteration 71, loss = 0.26017247\n",
      "Iteration 59, loss = 0.26276097\n",
      "Iteration 72, loss = 0.26024194\n",
      "Iteration 60, loss = 0.26292192\n",
      "Iteration 73, loss = 0.26012301\n",
      "Iteration 61, loss = 0.26249592\n",
      "Iteration 74, loss = 0.26054409\n",
      "Iteration 62, loss = 0.26263465\n",
      "Iteration 75, loss = 0.26009933\n",
      "Iteration 63, loss = 0.26274282\n",
      "Iteration 76, loss = 0.25995838\n",
      "Iteration 77, loss = 0.26031865\n",
      "Iteration 64, loss = 0.26238637\n",
      "Iteration 65, loss = 0.26230264\n",
      "Iteration 78, loss = 0.25996831\n",
      "Iteration 66, loss = 0.26220452\n",
      "Iteration 79, loss = 0.25991732\n",
      "Iteration 80, loss = 0.25997198\n",
      "Iteration 67, loss = 0.26222776\n",
      "Iteration 81, loss = 0.25974745\n",
      "Iteration 68, loss = 0.26238936\n",
      "Iteration 82, loss = 0.25984139\n",
      "Iteration 69, loss = 0.26215743\n",
      "Iteration 70, loss = 0.26214657\n",
      "Iteration 83, loss = 0.25948282\n",
      "Iteration 71, loss = 0.26198297\n",
      "Iteration 84, loss = 0.25959700\n",
      "Iteration 72, loss = 0.26201332\n",
      "Iteration 85, loss = 0.25963293\n",
      "Iteration 73, loss = 0.26176788\n",
      "Iteration 86, loss = 0.25968205\n",
      "Iteration 87, loss = 0.25955503\n",
      "Iteration 74, loss = 0.26261285\n",
      "Iteration 88, loss = 0.25959731\n",
      "Iteration 75, loss = 0.26186564\n",
      "Iteration 89, loss = 0.25964353\n",
      "Iteration 76, loss = 0.26194025\n",
      "Iteration 90, loss = 0.25948888\n",
      "Iteration 77, loss = 0.26192159\n",
      "Iteration 91, loss = 0.25956180\n",
      "Iteration 78, loss = 0.26189868\n",
      "Iteration 92, loss = 0.25936429\n",
      "Iteration 79, loss = 0.26175153\n",
      "Iteration 93, loss = 0.25948285\n",
      "Iteration 80, loss = 0.26197730\n",
      "Iteration 81, loss = 0.26161128\n",
      "Iteration 94, loss = 0.25944987\n",
      "Iteration 82, loss = 0.26150660\n",
      "Iteration 95, loss = 0.25924021\n",
      "Iteration 96, loss = 0.25909333Iteration 83, loss = 0.26150192\n",
      "\n",
      "Iteration 84, loss = 0.26144070\n",
      "Iteration 97, loss = 0.25939546\n",
      "Iteration 98, loss = 0.25905368Iteration 85, loss = 0.26163429\n",
      "\n",
      "Iteration 99, loss = 0.25926576\n",
      "Iteration 86, loss = 0.26141875\n",
      "Iteration 100, loss = 0.25928551Iteration 87, loss = 0.26141916\n",
      "\n",
      "Iteration 101, loss = 0.25913670\n",
      "Iteration 88, loss = 0.26140245\n",
      "Iteration 102, loss = 0.25939453\n",
      "Iteration 89, loss = 0.26143813\n",
      "Iteration 90, loss = 0.26148132\n",
      "Iteration 103, loss = 0.25946346\n",
      "Iteration 91, loss = 0.26162182\n",
      "Iteration 104, loss = 0.25902179\n",
      "Iteration 92, loss = 0.26134414\n",
      "Iteration 105, loss = 0.25912299\n",
      "Iteration 93, loss = 0.26125162\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 106, loss = 0.25896773\n",
      "Iteration 107, loss = 0.25911187\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.32081308\n",
      "Iteration 1, loss = 0.32100248\n",
      "Iteration 2, loss = 0.28175055\n",
      "Iteration 3, loss = 0.27696875\n",
      "Iteration 2, loss = 0.28158098\n",
      "Iteration 4, loss = 0.27552315\n",
      "Iteration 3, loss = 0.27658970\n",
      "Iteration 5, loss = 0.27464321\n",
      "Iteration 4, loss = 0.27504223\n",
      "Iteration 6, loss = 0.27386227\n",
      "Iteration 5, loss = 0.27410660\n",
      "Iteration 7, loss = 0.27351939\n",
      "Iteration 6, loss = 0.27318893\n",
      "Iteration 8, loss = 0.27288245\n",
      "Iteration 7, loss = 0.27250637\n",
      "Iteration 8, loss = 0.27212857\n",
      "Iteration 9, loss = 0.27241034\n",
      "Iteration 9, loss = 0.27135517\n",
      "Iteration 10, loss = 0.27147641\n",
      "Iteration 10, loss = 0.27052188\n",
      "Iteration 11, loss = 0.27081305\n",
      "Iteration 11, loss = 0.26982348\n",
      "Iteration 12, loss = 0.26957450\n",
      "Iteration 12, loss = 0.27052746\n",
      "Iteration 13, loss = 0.26878930\n",
      "Iteration 13, loss = 0.27016156\n",
      "Iteration 14, loss = 0.26985982\n",
      "Iteration 14, loss = 0.26859812\n",
      "Iteration 15, loss = 0.26810018\n",
      "Iteration 15, loss = 0.26918401\n",
      "Iteration 16, loss = 0.26768325\n",
      "Iteration 16, loss = 0.26898445\n",
      "Iteration 17, loss = 0.26713148\n",
      "Iteration 17, loss = 0.26826111\n",
      "Iteration 18, loss = 0.26673896\n",
      "Iteration 18, loss = 0.26808959\n",
      "Iteration 19, loss = 0.26663223\n",
      "Iteration 19, loss = 0.26785275\n",
      "Iteration 20, loss = 0.26627613\n",
      "Iteration 20, loss = 0.26743406\n",
      "Iteration 21, loss = 0.26735346\n",
      "Iteration 21, loss = 0.26609587\n",
      "Iteration 22, loss = 0.26687971\n",
      "Iteration 22, loss = 0.26572467\n",
      "Iteration 23, loss = 0.26681409\n",
      "Iteration 24, loss = 0.26649454\n",
      "Iteration 23, loss = 0.26569545\n",
      "Iteration 25, loss = 0.26637308\n",
      "Iteration 24, loss = 0.26525699\n",
      "Iteration 25, loss = 0.26522858\n",
      "Iteration 26, loss = 0.26601695\n",
      "Iteration 26, loss = 0.26492635\n",
      "Iteration 27, loss = 0.26592127\n",
      "Iteration 27, loss = 0.26504818\n",
      "Iteration 28, loss = 0.26555908\n",
      "Iteration 28, loss = 0.26466289\n",
      "Iteration 29, loss = 0.26522953\n",
      "Iteration 29, loss = 0.26441391\n",
      "Iteration 30, loss = 0.26510913\n",
      "Iteration 30, loss = 0.26415342\n",
      "Iteration 31, loss = 0.26509345\n",
      "Iteration 31, loss = 0.26394955\n",
      "Iteration 32, loss = 0.26479850\n",
      "Iteration 32, loss = 0.26391926\n",
      "Iteration 33, loss = 0.26485685\n",
      "Iteration 33, loss = 0.26402395\n",
      "Iteration 34, loss = 0.26468592\n",
      "Iteration 34, loss = 0.26366067\n",
      "Iteration 35, loss = 0.26441297\n",
      "Iteration 35, loss = 0.26342300\n",
      "Iteration 36, loss = 0.26478095\n",
      "Iteration 36, loss = 0.26382696\n",
      "Iteration 37, loss = 0.26426261\n",
      "Iteration 37, loss = 0.26348403\n",
      "Iteration 38, loss = 0.26436970\n",
      "Iteration 38, loss = 0.26343132\n",
      "Iteration 39, loss = 0.26401729\n",
      "Iteration 39, loss = 0.26299882\n",
      "Iteration 40, loss = 0.26328176\n",
      "Iteration 40, loss = 0.26420512\n",
      "Iteration 41, loss = 0.26282080\n",
      "Iteration 42, loss = 0.26266335\n",
      "Iteration 41, loss = 0.26375033\n",
      "Iteration 42, loss = 0.26343574\n",
      "Iteration 43, loss = 0.26256548\n",
      "Iteration 43, loss = 0.26359990\n",
      "Iteration 44, loss = 0.26265322\n",
      "Iteration 45, loss = 0.26251983\n",
      "Iteration 44, loss = 0.26344092\n",
      "Iteration 46, loss = 0.26242190\n",
      "Iteration 45, loss = 0.26337041\n",
      "Iteration 47, loss = 0.26238893\n",
      "Iteration 46, loss = 0.26334601\n",
      "Iteration 48, loss = 0.26209843\n",
      "Iteration 47, loss = 0.26360181\n",
      "Iteration 48, loss = 0.26292755\n",
      "Iteration 49, loss = 0.26236824\n",
      "Iteration 49, loss = 0.26323272\n",
      "Iteration 50, loss = 0.26182432\n",
      "Iteration 50, loss = 0.26272282\n",
      "Iteration 51, loss = 0.26181563\n",
      "Iteration 51, loss = 0.26273225\n",
      "Iteration 52, loss = 0.26177368\n",
      "Iteration 52, loss = 0.26266275\n",
      "Iteration 53, loss = 0.26181770\n",
      "Iteration 53, loss = 0.26265377\n",
      "Iteration 54, loss = 0.26157988\n",
      "Iteration 54, loss = 0.26248917\n",
      "Iteration 55, loss = 0.26181374\n",
      "Iteration 55, loss = 0.26279770\n",
      "Iteration 56, loss = 0.26147738\n",
      "Iteration 56, loss = 0.26261634\n",
      "Iteration 57, loss = 0.26151718\n",
      "Iteration 57, loss = 0.26228953\n",
      "Iteration 58, loss = 0.26139480\n",
      "Iteration 58, loss = 0.26228403\n",
      "Iteration 59, loss = 0.26148509\n",
      "Iteration 59, loss = 0.26234118\n",
      "Iteration 60, loss = 0.26140368\n",
      "Iteration 60, loss = 0.26245183\n",
      "Iteration 61, loss = 0.26132177\n",
      "Iteration 61, loss = 0.26206245\n",
      "Iteration 62, loss = 0.26132672\n",
      "Iteration 62, loss = 0.26216782\n",
      "Iteration 63, loss = 0.26118326\n",
      "Iteration 63, loss = 0.26212744\n",
      "Iteration 64, loss = 0.26099318\n",
      "Iteration 64, loss = 0.26174688\n",
      "Iteration 65, loss = 0.26101986\n",
      "Iteration 66, loss = 0.26070064\n",
      "Iteration 65, loss = 0.26190743\n",
      "Iteration 66, loss = 0.26165470\n",
      "Iteration 67, loss = 0.26080751\n",
      "Iteration 67, loss = 0.26161283\n",
      "Iteration 68, loss = 0.26099943\n",
      "Iteration 68, loss = 0.26171387\n",
      "Iteration 69, loss = 0.26093179\n",
      "Iteration 69, loss = 0.26173941\n",
      "Iteration 70, loss = 0.26169049\n",
      "Iteration 70, loss = 0.26101244\n",
      "Iteration 71, loss = 0.26089433\n",
      "Iteration 71, loss = 0.26143707\n",
      "Iteration 72, loss = 0.26065416\n",
      "Iteration 72, loss = 0.26146364\n",
      "Iteration 73, loss = 0.26078641\n",
      "Iteration 73, loss = 0.26149301\n",
      "Iteration 74, loss = 0.26104921\n",
      "Iteration 74, loss = 0.26184445\n",
      "Iteration 75, loss = 0.26038179\n",
      "Iteration 75, loss = 0.26123432\n",
      "Iteration 76, loss = 0.26057868\n",
      "Iteration 76, loss = 0.26128392\n",
      "Iteration 77, loss = 0.26053665\n",
      "Iteration 77, loss = 0.26143181\n",
      "Iteration 78, loss = 0.26042014\n",
      "Iteration 78, loss = 0.26126313\n",
      "Iteration 79, loss = 0.26124342\n",
      "Iteration 79, loss = 0.26038019\n",
      "Iteration 80, loss = 0.26143199\n",
      "Iteration 80, loss = 0.26093316\n",
      "Iteration 81, loss = 0.26101890\n",
      "Iteration 81, loss = 0.26044078\n",
      "Iteration 82, loss = 0.26081279\n",
      "Iteration 82, loss = 0.26019125\n",
      "Iteration 83, loss = 0.26077490\n",
      "Iteration 83, loss = 0.26004278\n",
      "Iteration 84, loss = 0.26086150\n",
      "Iteration 84, loss = 0.26025202\n",
      "Iteration 85, loss = 0.26106125\n",
      "Iteration 85, loss = 0.26046876\n",
      "Iteration 86, loss = 0.26092351\n",
      "Iteration 86, loss = 0.26040712\n",
      "Iteration 87, loss = 0.26080535\n",
      "Iteration 87, loss = 0.26001698\n",
      "Iteration 88, loss = 0.26074202\n",
      "Iteration 88, loss = 0.25999963\n",
      "Iteration 89, loss = 0.26096515\n",
      "Iteration 89, loss = 0.26021387\n",
      "Iteration 90, loss = 0.26083792\n",
      "Iteration 90, loss = 0.26017402\n",
      "Iteration 91, loss = 0.26101654\n",
      "Iteration 91, loss = 0.26026878\n",
      "Iteration 92, loss = 0.26088545\n",
      "Iteration 92, loss = 0.26019453\n",
      "Iteration 93, loss = 0.26066716\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 93, loss = 0.25992848\n",
      "Iteration 94, loss = 0.25979277\n",
      "Iteration 1, loss = 0.32163222\n",
      "Iteration 95, loss = 0.26000610\n",
      "Iteration 2, loss = 0.28231339\n",
      "Iteration 96, loss = 0.25984224\n",
      "Iteration 3, loss = 0.27751673\n",
      "Iteration 97, loss = 0.25982021\n",
      "Iteration 4, loss = 0.27589616\n",
      "Iteration 98, loss = 0.25972978\n",
      "Iteration 99, loss = 0.25975846\n",
      "Iteration 5, loss = 0.27516710\n",
      "Iteration 100, loss = 0.25983269\n",
      "Iteration 6, loss = 0.27434738\n",
      "Iteration 101, loss = 0.25968054\n",
      "Iteration 7, loss = 0.27353527\n",
      "Iteration 8, loss = 0.27314184\n",
      "Iteration 102, loss = 0.25995583\n",
      "Iteration 9, loss = 0.27231052\n",
      "Iteration 103, loss = 0.26005597\n",
      "Iteration 10, loss = 0.27150243\n",
      "Iteration 104, loss = 0.25976212\n",
      "Iteration 11, loss = 0.27093629\n",
      "Iteration 105, loss = 0.25968820\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 12, loss = 0.27058110\n",
      "Iteration 13, loss = 0.26984422\n",
      "Iteration 14, loss = 0.26957672\n",
      "Iteration 1, loss = 0.32120790\n",
      "Iteration 15, loss = 0.26915690\n",
      "Iteration 2, loss = 0.28166026\n",
      "Iteration 16, loss = 0.26870554\n",
      "Iteration 17, loss = 0.26839976\n",
      "Iteration 3, loss = 0.27665988\n",
      "Iteration 18, loss = 0.26805843\n",
      "Iteration 4, loss = 0.27505395\n",
      "Iteration 19, loss = 0.26804871\n",
      "Iteration 5, loss = 0.27457378\n",
      "Iteration 20, loss = 0.26752682\n",
      "Iteration 6, loss = 0.27384032\n",
      "Iteration 21, loss = 0.26718450\n",
      "Iteration 7, loss = 0.27308882\n",
      "Iteration 22, loss = 0.26684493\n",
      "Iteration 8, loss = 0.27265604\n",
      "Iteration 23, loss = 0.26692679\n",
      "Iteration 24, loss = 0.26636515\n",
      "Iteration 9, loss = 0.27228506\n",
      "Iteration 25, loss = 0.26645320\n",
      "Iteration 10, loss = 0.27127286\n",
      "Iteration 26, loss = 0.26608337\n",
      "Iteration 11, loss = 0.27061341\n",
      "Iteration 27, loss = 0.26607867\n",
      "Iteration 12, loss = 0.27032549\n",
      "Iteration 13, loss = 0.26954338\n",
      "Iteration 28, loss = 0.26583524\n",
      "Iteration 14, loss = 0.26934161\n",
      "Iteration 29, loss = 0.26548293\n",
      "Iteration 15, loss = 0.26896477\n",
      "Iteration 30, loss = 0.26539858\n",
      "Iteration 31, loss = 0.26501927\n",
      "Iteration 16, loss = 0.26827033\n",
      "Iteration 32, loss = 0.26503119\n",
      "Iteration 17, loss = 0.26811622\n",
      "Iteration 33, loss = 0.26483933\n",
      "Iteration 18, loss = 0.26751737\n",
      "Iteration 19, loss = 0.26747056\n",
      "Iteration 34, loss = 0.26466738\n",
      "Iteration 20, loss = 0.26694505\n",
      "Iteration 35, loss = 0.26466866\n",
      "Iteration 21, loss = 0.26677193\n",
      "Iteration 36, loss = 0.26454545\n",
      "Iteration 22, loss = 0.26659980Iteration 37, loss = 0.26430084\n",
      "\n",
      "Iteration 23, loss = 0.26622517\n",
      "Iteration 38, loss = 0.26440715\n",
      "Iteration 24, loss = 0.26574533\n",
      "Iteration 39, loss = 0.26394393\n",
      "Iteration 25, loss = 0.26562524\n",
      "Iteration 40, loss = 0.26424474\n",
      "Iteration 26, loss = 0.26554648\n",
      "Iteration 41, loss = 0.26379224\n",
      "Iteration 27, loss = 0.26541420\n",
      "Iteration 28, loss = 0.26519112\n",
      "Iteration 42, loss = 0.26379177\n",
      "Iteration 43, loss = 0.26354346\n",
      "Iteration 29, loss = 0.26502171\n",
      "Iteration 44, loss = 0.26367365\n",
      "Iteration 30, loss = 0.26472660\n",
      "Iteration 45, loss = 0.26347648\n",
      "Iteration 31, loss = 0.26446876\n",
      "Iteration 46, loss = 0.26353466\n",
      "Iteration 32, loss = 0.26440570\n",
      "Iteration 47, loss = 0.26333448\n",
      "Iteration 33, loss = 0.26421850\n",
      "Iteration 48, loss = 0.26298377\n",
      "Iteration 34, loss = 0.26423986\n",
      "Iteration 49, loss = 0.26317332\n",
      "Iteration 35, loss = 0.26379201\n",
      "Iteration 50, loss = 0.26298305\n",
      "Iteration 36, loss = 0.26388527\n",
      "Iteration 51, loss = 0.26305332\n",
      "Iteration 37, loss = 0.26375309\n",
      "Iteration 52, loss = 0.26273221\n",
      "Iteration 38, loss = 0.26367786\n",
      "Iteration 53, loss = 0.26272710\n",
      "Iteration 39, loss = 0.26316662\n",
      "Iteration 54, loss = 0.26269212\n",
      "Iteration 40, loss = 0.26359107\n",
      "Iteration 55, loss = 0.26273698\n",
      "Iteration 41, loss = 0.26309097\n",
      "Iteration 56, loss = 0.26269028\n",
      "Iteration 42, loss = 0.26340152\n",
      "Iteration 57, loss = 0.26255934\n",
      "Iteration 43, loss = 0.26303966\n",
      "Iteration 58, loss = 0.26239499\n",
      "Iteration 44, loss = 0.26302414\n",
      "Iteration 59, loss = 0.26246460\n",
      "Iteration 45, loss = 0.26275189\n",
      "Iteration 60, loss = 0.26227248\n",
      "Iteration 46, loss = 0.26288471\n",
      "Iteration 47, loss = 0.26290696\n",
      "Iteration 61, loss = 0.26238643\n",
      "Iteration 62, loss = 0.26194740\n",
      "Iteration 48, loss = 0.26217321\n",
      "Iteration 63, loss = 0.26225477\n",
      "Iteration 49, loss = 0.26241073\n",
      "Iteration 64, loss = 0.26212063\n",
      "Iteration 50, loss = 0.26224158\n",
      "Iteration 65, loss = 0.26179710\n",
      "Iteration 51, loss = 0.26223412\n",
      "Iteration 66, loss = 0.26170304\n",
      "Iteration 52, loss = 0.26200725\n",
      "Iteration 67, loss = 0.26182671\n",
      "Iteration 53, loss = 0.26192909\n",
      "Iteration 68, loss = 0.26182936\n",
      "Iteration 54, loss = 0.26179997\n",
      "Iteration 69, loss = 0.26172159\n",
      "Iteration 55, loss = 0.26212186\n",
      "Iteration 70, loss = 0.26162324\n",
      "Iteration 56, loss = 0.26185168\n",
      "Iteration 71, loss = 0.26189266\n",
      "Iteration 57, loss = 0.26151511\n",
      "Iteration 72, loss = 0.26147544\n",
      "Iteration 58, loss = 0.26150546\n",
      "Iteration 73, loss = 0.26140580\n",
      "Iteration 74, loss = 0.26199393\n",
      "Iteration 59, loss = 0.26153836\n",
      "Iteration 60, loss = 0.26138769\n",
      "Iteration 75, loss = 0.26131682\n",
      "Iteration 61, loss = 0.26159627\n",
      "Iteration 76, loss = 0.26151645\n",
      "Iteration 62, loss = 0.26128933\n",
      "Iteration 77, loss = 0.26150730\n",
      "Iteration 63, loss = 0.26134989\n",
      "Iteration 78, loss = 0.26127551\n",
      "Iteration 64, loss = 0.26108418\n",
      "Iteration 79, loss = 0.26112570\n",
      "Iteration 65, loss = 0.26111114\n",
      "Iteration 80, loss = 0.26143764\n",
      "Iteration 66, loss = 0.26092240\n",
      "Iteration 81, loss = 0.26120144\n",
      "Iteration 67, loss = 0.26114085\n",
      "Iteration 82, loss = 0.26115291\n",
      "Iteration 68, loss = 0.26118832\n",
      "Iteration 83, loss = 0.26093912\n",
      "Iteration 69, loss = 0.26111275\n",
      "Iteration 84, loss = 0.26112923\n",
      "Iteration 70, loss = 0.26103052\n",
      "Iteration 85, loss = 0.26132841\n",
      "Iteration 71, loss = 0.26103173\n",
      "Iteration 86, loss = 0.26100213\n",
      "Iteration 87, loss = 0.26089515\n",
      "Iteration 72, loss = 0.26071535\n",
      "Iteration 88, loss = 0.26083237\n",
      "Iteration 73, loss = 0.26075593\n",
      "Iteration 89, loss = 0.26091448\n",
      "Iteration 74, loss = 0.26077021\n",
      "Iteration 90, loss = 0.26085566\n",
      "Iteration 75, loss = 0.26061896\n",
      "Iteration 76, loss = 0.26087882\n",
      "Iteration 91, loss = 0.26093079\n",
      "Iteration 77, loss = 0.26063743\n",
      "Iteration 92, loss = 0.26076674\n",
      "Iteration 78, loss = 0.26051427\n",
      "Iteration 93, loss = 0.26061324\n",
      "Iteration 94, loss = 0.26052996\n",
      "Iteration 79, loss = 0.26066346\n",
      "Iteration 95, loss = 0.26089060\n",
      "Iteration 80, loss = 0.26051153\n",
      "Iteration 96, loss = 0.26070287\n",
      "Iteration 81, loss = 0.26061765\n",
      "Iteration 97, loss = 0.26051589\n",
      "Iteration 82, loss = 0.26038702\n",
      "Iteration 98, loss = 0.26041680\n",
      "Iteration 83, loss = 0.26041345\n",
      "Iteration 99, loss = 0.26069440\n",
      "Iteration 84, loss = 0.26031586\n",
      "Iteration 100, loss = 0.26065403\n",
      "Iteration 101, loss = 0.26038965\n",
      "Iteration 85, loss = 0.26059461\n",
      "Iteration 102, loss = 0.26044175\n",
      "Iteration 86, loss = 0.26051086\n",
      "Iteration 103, loss = 0.26076890\n",
      "Iteration 87, loss = 0.26038135\n",
      "Iteration 104, loss = 0.26034030\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 88, loss = 0.26020582\n",
      "Iteration 89, loss = 0.26021934\n",
      "Iteration 1, loss = 0.32146993\n",
      "Iteration 90, loss = 0.26035529\n",
      "Iteration 2, loss = 0.28127434\n",
      "Iteration 91, loss = 0.26033890\n",
      "Iteration 3, loss = 0.27609062\n",
      "Iteration 92, loss = 0.26024786\n",
      "Iteration 4, loss = 0.27478578\n",
      "Iteration 93, loss = 0.26018224\n",
      "Iteration 5, loss = 0.27425063\n",
      "Iteration 6, loss = 0.27375004\n",
      "Iteration 94, loss = 0.26004791\n",
      "Iteration 7, loss = 0.27313647\n",
      "Iteration 95, loss = 0.26016487\n",
      "Iteration 8, loss = 0.27260119\n",
      "Iteration 96, loss = 0.26022692\n",
      "Iteration 9, loss = 0.27225751\n",
      "Iteration 97, loss = 0.26020697\n",
      "Iteration 10, loss = 0.27150307\n",
      "Iteration 98, loss = 0.25993232\n",
      "Iteration 11, loss = 0.27088080\n",
      "Iteration 12, loss = 0.27045741\n",
      "Iteration 99, loss = 0.26029345\n",
      "Iteration 13, loss = 0.26981202\n",
      "Iteration 100, loss = 0.26012737\n",
      "Iteration 101, loss = 0.26015273\n",
      "Iteration 14, loss = 0.26934806\n",
      "Iteration 102, loss = 0.26008885\n",
      "Iteration 15, loss = 0.26930790\n",
      "Iteration 103, loss = 0.26030087\n",
      "Iteration 16, loss = 0.26827743\n",
      "Iteration 104, loss = 0.25982450\n",
      "Iteration 17, loss = 0.26811589\n",
      "Iteration 18, loss = 0.26769599\n",
      "Iteration 105, loss = 0.26007992\n",
      "Iteration 106, loss = 0.26011467\n",
      "Iteration 19, loss = 0.26748833\n",
      "Iteration 107, loss = 0.25997486\n",
      "Iteration 20, loss = 0.26707188\n",
      "Iteration 108, loss = 0.25992667\n",
      "Iteration 21, loss = 0.26683104\n",
      "Iteration 109, loss = 0.26022449\n",
      "Iteration 22, loss = 0.26651014\n",
      "Iteration 110, loss = 0.25993535\n",
      "Iteration 23, loss = 0.26613440\n",
      "Iteration 111, loss = 0.26039852\n",
      "Iteration 112, loss = 0.25970972\n",
      "Iteration 24, loss = 0.26569507\n",
      "Iteration 113, loss = 0.25976067\n",
      "Iteration 114, loss = 0.25987796\n",
      "Iteration 25, loss = 0.26555321\n",
      "Iteration 115, loss = 0.25981139\n",
      "Iteration 26, loss = 0.26541072\n",
      "Iteration 116, loss = 0.25974811\n",
      "Iteration 27, loss = 0.26509246\n",
      "Iteration 117, loss = 0.25962571\n",
      "Iteration 28, loss = 0.26478545\n",
      "Iteration 29, loss = 0.26455746\n",
      "Iteration 118, loss = 0.25966382\n",
      "Iteration 30, loss = 0.26440883\n",
      "Iteration 119, loss = 0.25966358\n",
      "Iteration 31, loss = 0.26418394\n",
      "Iteration 120, loss = 0.25983489\n",
      "Iteration 121, loss = 0.25977194\n",
      "Iteration 32, loss = 0.26395219\n",
      "Iteration 122, loss = 0.25972994\n",
      "Iteration 33, loss = 0.26390540\n",
      "Iteration 123, loss = 0.25971009\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 34, loss = 0.26368552\n",
      "Iteration 35, loss = 0.26333285\n",
      "Iteration 36, loss = 0.26326189\n",
      "Iteration 1, loss = 0.32211072\n",
      "Iteration 37, loss = 0.26308609\n",
      "Iteration 2, loss = 0.28214864\n",
      "Iteration 38, loss = 0.26302063\n",
      "Iteration 39, loss = 0.26257816\n",
      "Iteration 3, loss = 0.27702432\n",
      "Iteration 4, loss = 0.27527997\n",
      "Iteration 40, loss = 0.26304524\n",
      "Iteration 5, loss = 0.27474233\n",
      "Iteration 41, loss = 0.26229343\n",
      "Iteration 6, loss = 0.27416943\n",
      "Iteration 42, loss = 0.26267676\n",
      "Iteration 7, loss = 0.27342243\n",
      "Iteration 8, loss = 0.27266978\n",
      "Iteration 43, loss = 0.26227712\n",
      "Iteration 9, loss = 0.27190363\n",
      "Iteration 44, loss = 0.26231126\n",
      "Iteration 10, loss = 0.27126699\n",
      "Iteration 45, loss = 0.26204965\n",
      "Iteration 11, loss = 0.27058089\n",
      "Iteration 46, loss = 0.26205956\n",
      "Iteration 12, loss = 0.27018063\n",
      "Iteration 47, loss = 0.26194087\n",
      "Iteration 13, loss = 0.26968051\n",
      "Iteration 48, loss = 0.26162384\n",
      "Iteration 14, loss = 0.26924518\n",
      "Iteration 49, loss = 0.26182612\n",
      "Iteration 15, loss = 0.26906141\n",
      "Iteration 50, loss = 0.26147203\n",
      "Iteration 16, loss = 0.26825974\n",
      "Iteration 51, loss = 0.26146911\n",
      "Iteration 17, loss = 0.26804008\n",
      "Iteration 52, loss = 0.26121850\n",
      "Iteration 18, loss = 0.26808290\n",
      "Iteration 53, loss = 0.26128109\n",
      "Iteration 19, loss = 0.26751285\n",
      "Iteration 54, loss = 0.26113440\n",
      "Iteration 20, loss = 0.26710621\n",
      "Iteration 55, loss = 0.26150477\n",
      "Iteration 21, loss = 0.26691558\n",
      "Iteration 56, loss = 0.26125990\n",
      "Iteration 22, loss = 0.26692850\n",
      "Iteration 57, loss = 0.26080863\n",
      "Iteration 23, loss = 0.26655105\n",
      "Iteration 58, loss = 0.26099687\n",
      "Iteration 24, loss = 0.26620796\n",
      "Iteration 59, loss = 0.26103047\n",
      "Iteration 25, loss = 0.26621608\n",
      "Iteration 60, loss = 0.26100358\n",
      "Iteration 61, loss = 0.26088939\n",
      "Iteration 26, loss = 0.26593954\n",
      "Iteration 62, loss = 0.26079904\n",
      "Iteration 27, loss = 0.26579019\n",
      "Iteration 63, loss = 0.26055231\n",
      "Iteration 28, loss = 0.26573347\n",
      "Iteration 64, loss = 0.26059117\n",
      "Iteration 29, loss = 0.26550934Iteration 65, loss = 0.26046079\n",
      "\n",
      "Iteration 30, loss = 0.26517291\n",
      "Iteration 66, loss = 0.26026908\n",
      "Iteration 31, loss = 0.26518166\n",
      "Iteration 67, loss = 0.26023932\n",
      "Iteration 32, loss = 0.26468757\n",
      "Iteration 68, loss = 0.26043823\n",
      "Iteration 33, loss = 0.26483240\n",
      "Iteration 34, loss = 0.26474818\n",
      "Iteration 69, loss = 0.26046752\n",
      "Iteration 35, loss = 0.26423413\n",
      "Iteration 70, loss = 0.26028803\n",
      "Iteration 36, loss = 0.26426029\n",
      "Iteration 71, loss = 0.25998683\n",
      "Iteration 72, loss = 0.26007485\n",
      "Iteration 37, loss = 0.26432659\n",
      "Iteration 73, loss = 0.25999192\n",
      "Iteration 38, loss = 0.26397417\n",
      "Iteration 74, loss = 0.26001873\n",
      "Iteration 39, loss = 0.26375945\n",
      "Iteration 75, loss = 0.25991791\n",
      "Iteration 76, loss = 0.26008178\n",
      "Iteration 40, loss = 0.26413402\n",
      "Iteration 77, loss = 0.25990611\n",
      "Iteration 78, loss = 0.25967667\n",
      "Iteration 41, loss = 0.26356930\n",
      "Iteration 79, loss = 0.25990261Iteration 42, loss = 0.26366774\n",
      "\n",
      "Iteration 80, loss = 0.25986341\n",
      "Iteration 43, loss = 0.26352154\n",
      "Iteration 44, loss = 0.26348518\n",
      "Iteration 81, loss = 0.25981470\n",
      "Iteration 45, loss = 0.26313803\n",
      "Iteration 82, loss = 0.25977338\n",
      "Iteration 46, loss = 0.26344926\n",
      "Iteration 83, loss = 0.25975537\n",
      "Iteration 47, loss = 0.26326744\n",
      "Iteration 84, loss = 0.25955790\n",
      "Iteration 48, loss = 0.26288552\n",
      "Iteration 85, loss = 0.25990181\n",
      "Iteration 49, loss = 0.26314114\n",
      "Iteration 86, loss = 0.25971903\n",
      "Iteration 50, loss = 0.26291087\n",
      "Iteration 87, loss = 0.25935808\n",
      "Iteration 51, loss = 0.26299598\n",
      "Iteration 88, loss = 0.25952164\n",
      "Iteration 52, loss = 0.26268545\n",
      "Iteration 89, loss = 0.25947288\n",
      "Iteration 53, loss = 0.26276284\n",
      "Iteration 90, loss = 0.25959884\n",
      "Iteration 54, loss = 0.26256560\n",
      "Iteration 91, loss = 0.25938517\n",
      "Iteration 55, loss = 0.26279217\n",
      "Iteration 92, loss = 0.25951827\n",
      "Iteration 93, loss = 0.25931964\n",
      "Iteration 56, loss = 0.26278054\n",
      "Iteration 94, loss = 0.25924011\n",
      "Iteration 57, loss = 0.26245272\n",
      "Iteration 95, loss = 0.25974686\n",
      "Iteration 58, loss = 0.26250950\n",
      "Iteration 96, loss = 0.25929197\n",
      "Iteration 59, loss = 0.26258694\n",
      "Iteration 97, loss = 0.25929083\n",
      "Iteration 60, loss = 0.26246285\n",
      "Iteration 98, loss = 0.25916962\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 61, loss = 0.26258835\n",
      "Iteration 62, loss = 0.26249006\n",
      "Iteration 63, loss = 0.26222973\n",
      "Iteration 64, loss = 0.26241391\n",
      "Iteration 65, loss = 0.26205743\n",
      "Iteration 66, loss = 0.26205146\n",
      "Iteration 67, loss = 0.26208956\n",
      "Iteration 68, loss = 0.26211933\n",
      "Iteration 69, loss = 0.26238075\n",
      "Iteration 70, loss = 0.26220446\n",
      "Iteration 71, loss = 0.26190988\n",
      "Iteration 72, loss = 0.26192064\n",
      "Iteration 73, loss = 0.26192722\n",
      "Iteration 74, loss = 0.26188892\n",
      "Iteration 75, loss = 0.26174243\n",
      "Iteration 76, loss = 0.26181191\n",
      "Iteration 77, loss = 0.26171635\n",
      "Iteration 78, loss = 0.26167595\n",
      "Iteration 79, loss = 0.26154517\n",
      "Iteration 80, loss = 0.26168626\n",
      "Iteration 81, loss = 0.26164176\n",
      "Iteration 82, loss = 0.26159834\n",
      "Iteration 83, loss = 0.26175758\n",
      "Iteration 84, loss = 0.26161635\n",
      "Iteration 85, loss = 0.26179401\n",
      "Iteration 86, loss = 0.26144627\n",
      "Iteration 87, loss = 0.26145015\n",
      "Iteration 88, loss = 0.26150381\n",
      "Iteration 89, loss = 0.26135131\n",
      "Iteration 90, loss = 0.26143202\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.32690832\n",
      "Iteration 1, loss = 0.32771212\n",
      "Iteration 2, loss = 0.28866580\n",
      "Iteration 2, loss = 0.28924699\n",
      "Iteration 3, loss = 0.28412473\n",
      "Iteration 3, loss = 0.28412055\n",
      "Iteration 4, loss = 0.28254279\n",
      "Iteration 4, loss = 0.28234022\n",
      "Iteration 5, loss = 0.28187201\n",
      "Iteration 5, loss = 0.28187215\n",
      "Iteration 6, loss = 0.28113134\n",
      "Iteration 6, loss = 0.28105089\n",
      "Iteration 7, loss = 0.28085982\n",
      "Iteration 7, loss = 0.28075479\n",
      "Iteration 8, loss = 0.27996855\n",
      "Iteration 8, loss = 0.27995421\n",
      "Iteration 9, loss = 0.27930207\n",
      "Iteration 10, loss = 0.27893968\n",
      "Iteration 9, loss = 0.27940223\n",
      "Iteration 11, loss = 0.27844555\n",
      "Iteration 12, loss = 0.27788023\n",
      "Iteration 10, loss = 0.27914560\n",
      "Iteration 13, loss = 0.27711040\n",
      "Iteration 11, loss = 0.27892355\n",
      "Iteration 14, loss = 0.27703273\n",
      "Iteration 12, loss = 0.27841447\n",
      "Iteration 15, loss = 0.27629660\n",
      "Iteration 13, loss = 0.27759472\n",
      "Iteration 14, loss = 0.27771753\n",
      "Iteration 16, loss = 0.27687978\n",
      "Iteration 15, loss = 0.27693974\n",
      "Iteration 17, loss = 0.27527351\n",
      "Iteration 16, loss = 0.27751672\n",
      "Iteration 18, loss = 0.27495690\n",
      "Iteration 19, loss = 0.27467567\n",
      "Iteration 17, loss = 0.27631356\n",
      "Iteration 20, loss = 0.27425732\n",
      "Iteration 18, loss = 0.27589536\n",
      "Iteration 21, loss = 0.27386337\n",
      "Iteration 19, loss = 0.27576446\n",
      "Iteration 22, loss = 0.27379843\n",
      "Iteration 20, loss = 0.27548690\n",
      "Iteration 23, loss = 0.27339976\n",
      "Iteration 21, loss = 0.27501957\n",
      "Iteration 24, loss = 0.27301498\n",
      "Iteration 22, loss = 0.27488893\n",
      "Iteration 25, loss = 0.27269912\n",
      "Iteration 23, loss = 0.27455378\n",
      "Iteration 26, loss = 0.27252473\n",
      "Iteration 24, loss = 0.27404436\n",
      "Iteration 27, loss = 0.27221161\n",
      "Iteration 25, loss = 0.27392635\n",
      "Iteration 28, loss = 0.27195066\n",
      "Iteration 26, loss = 0.27358816\n",
      "Iteration 29, loss = 0.27211324\n",
      "Iteration 27, loss = 0.27323138\n",
      "Iteration 30, loss = 0.27181151\n",
      "Iteration 28, loss = 0.27288569\n",
      "Iteration 29, loss = 0.27317080\n",
      "Iteration 31, loss = 0.27151521\n",
      "Iteration 30, loss = 0.27253333\n",
      "Iteration 32, loss = 0.27110124\n",
      "Iteration 31, loss = 0.27225667\n",
      "Iteration 33, loss = 0.27168195\n",
      "Iteration 32, loss = 0.27201400\n",
      "Iteration 34, loss = 0.27113076\n",
      "Iteration 33, loss = 0.27213428\n",
      "Iteration 35, loss = 0.27075565\n",
      "Iteration 34, loss = 0.27168758\n",
      "Iteration 36, loss = 0.27057483\n",
      "Iteration 35, loss = 0.27136474\n",
      "Iteration 37, loss = 0.27080848\n",
      "Iteration 38, loss = 0.27046724\n",
      "Iteration 36, loss = 0.27127035\n",
      "Iteration 39, loss = 0.27039914\n",
      "Iteration 37, loss = 0.27127989\n",
      "Iteration 38, loss = 0.27103228\n",
      "Iteration 40, loss = 0.27054656\n",
      "Iteration 39, loss = 0.27070396\n",
      "Iteration 41, loss = 0.27026575\n",
      "Iteration 42, loss = 0.27007715\n",
      "Iteration 40, loss = 0.27104515\n",
      "Iteration 43, loss = 0.27013736\n",
      "Iteration 41, loss = 0.27060691\n",
      "Iteration 44, loss = 0.27012022\n",
      "Iteration 42, loss = 0.27032957\n",
      "Iteration 45, loss = 0.26980956\n",
      "Iteration 43, loss = 0.27024904\n",
      "Iteration 46, loss = 0.27021447\n",
      "Iteration 44, loss = 0.27017555\n",
      "Iteration 47, loss = 0.26966189\n",
      "Iteration 45, loss = 0.27000291\n",
      "Iteration 48, loss = 0.26966340\n",
      "Iteration 46, loss = 0.27020271\n",
      "Iteration 49, loss = 0.26977037\n",
      "Iteration 47, loss = 0.26964601\n",
      "Iteration 50, loss = 0.26995382\n",
      "Iteration 48, loss = 0.26944826\n",
      "Iteration 49, loss = 0.26958231\n",
      "Iteration 51, loss = 0.26948567\n",
      "Iteration 50, loss = 0.26967046\n",
      "Iteration 52, loss = 0.26947318\n",
      "Iteration 51, loss = 0.26932801\n",
      "Iteration 53, loss = 0.26965617\n",
      "Iteration 52, loss = 0.26910233\n",
      "Iteration 54, loss = 0.26986755\n",
      "Iteration 55, loss = 0.26978705\n",
      "Iteration 53, loss = 0.26912376\n",
      "Iteration 56, loss = 0.26932598\n",
      "Iteration 54, loss = 0.26940446\n",
      "Iteration 57, loss = 0.26943096\n",
      "Iteration 55, loss = 0.26924374\n",
      "Iteration 56, loss = 0.26859042\n",
      "Iteration 58, loss = 0.26920479\n",
      "Iteration 57, loss = 0.26833971\n",
      "Iteration 59, loss = 0.26926297\n",
      "Iteration 58, loss = 0.26843048\n",
      "Iteration 60, loss = 0.26902815\n",
      "Iteration 59, loss = 0.26840015\n",
      "Iteration 61, loss = 0.26902707\n",
      "Iteration 60, loss = 0.26821463\n",
      "Iteration 62, loss = 0.26942069\n",
      "Iteration 61, loss = 0.26802557\n",
      "Iteration 63, loss = 0.26889884\n",
      "Iteration 62, loss = 0.26838362\n",
      "Iteration 64, loss = 0.26885983\n",
      "Iteration 63, loss = 0.26808114\n",
      "Iteration 65, loss = 0.26906312\n",
      "Iteration 64, loss = 0.26800499\n",
      "Iteration 66, loss = 0.26888585\n",
      "Iteration 65, loss = 0.26804271\n",
      "Iteration 67, loss = 0.26891086\n",
      "Iteration 66, loss = 0.26790370\n",
      "Iteration 68, loss = 0.26872062\n",
      "Iteration 69, loss = 0.26880508\n",
      "Iteration 67, loss = 0.26774504\n",
      "Iteration 70, loss = 0.26854654\n",
      "Iteration 68, loss = 0.26779062\n",
      "Iteration 71, loss = 0.26867518\n",
      "Iteration 69, loss = 0.26763497\n",
      "Iteration 72, loss = 0.26858140\n",
      "Iteration 70, loss = 0.26754150\n",
      "Iteration 73, loss = 0.26861744\n",
      "Iteration 71, loss = 0.26755836\n",
      "Iteration 74, loss = 0.26851122\n",
      "Iteration 72, loss = 0.26767965\n",
      "Iteration 75, loss = 0.26847808\n",
      "Iteration 76, loss = 0.26832511\n",
      "Iteration 73, loss = 0.26765155\n",
      "Iteration 77, loss = 0.26856549\n",
      "Iteration 74, loss = 0.26762628\n",
      "Iteration 78, loss = 0.26819369\n",
      "Iteration 79, loss = 0.26840067\n",
      "Iteration 75, loss = 0.26729838\n",
      "Iteration 80, loss = 0.26820921\n",
      "Iteration 76, loss = 0.26714490\n",
      "Iteration 81, loss = 0.26809027\n",
      "Iteration 77, loss = 0.26733443\n",
      "Iteration 82, loss = 0.26853400\n",
      "Iteration 78, loss = 0.26712056\n",
      "Iteration 83, loss = 0.26807703\n",
      "Iteration 79, loss = 0.26731391\n",
      "Iteration 84, loss = 0.26814461\n",
      "Iteration 80, loss = 0.26731168\n",
      "Iteration 85, loss = 0.26799815\n",
      "Iteration 86, loss = 0.26775305\n",
      "Iteration 81, loss = 0.26723075\n",
      "Iteration 87, loss = 0.26804566\n",
      "Iteration 82, loss = 0.26715914\n",
      "Iteration 88, loss = 0.26797856\n",
      "Iteration 83, loss = 0.26712178\n",
      "Iteration 89, loss = 0.26795954\n",
      "Iteration 84, loss = 0.26705769\n",
      "Iteration 90, loss = 0.26797819\n",
      "Iteration 91, loss = 0.26775226\n",
      "Iteration 85, loss = 0.26697971\n",
      "Iteration 86, loss = 0.26680167\n",
      "Iteration 92, loss = 0.26763485\n",
      "Iteration 87, loss = 0.26685867\n",
      "Iteration 93, loss = 0.26782349\n",
      "Iteration 94, loss = 0.26776778\n",
      "Iteration 88, loss = 0.26693755\n",
      "Iteration 95, loss = 0.26780100\n",
      "Iteration 89, loss = 0.26714026\n",
      "Iteration 96, loss = 0.26756742\n",
      "Iteration 90, loss = 0.26695056\n",
      "Iteration 97, loss = 0.26748862\n",
      "Iteration 91, loss = 0.26675430\n",
      "Iteration 92, loss = 0.26665042\n",
      "Iteration 98, loss = 0.26748676\n",
      "Iteration 99, loss = 0.26757793\n",
      "Iteration 93, loss = 0.26663978\n",
      "Iteration 94, loss = 0.26699531\n",
      "Iteration 100, loss = 0.26749972\n",
      "Iteration 101, loss = 0.26754473\n",
      "Iteration 95, loss = 0.26703592\n",
      "Iteration 96, loss = 0.26668131\n",
      "Iteration 102, loss = 0.26734804\n",
      "Iteration 103, loss = 0.26723698\n",
      "Iteration 97, loss = 0.26674035\n",
      "Iteration 104, loss = 0.26722048\n",
      "Iteration 98, loss = 0.26683209\n",
      "Iteration 105, loss = 0.26724278\n",
      "Iteration 99, loss = 0.26668945\n",
      "Iteration 100, loss = 0.26667272Iteration 106, loss = 0.26710755\n",
      "\n",
      "Iteration 107, loss = 0.26726776\n",
      "Iteration 101, loss = 0.26689698\n",
      "Iteration 108, loss = 0.26712001\n",
      "Iteration 102, loss = 0.26660445\n",
      "Iteration 109, loss = 0.26716501\n",
      "Iteration 103, loss = 0.26669310\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 110, loss = 0.26723564\n",
      "Iteration 111, loss = 0.26712410\n",
      "Iteration 1, loss = 0.32720785\n",
      "Iteration 112, loss = 0.26708850\n",
      "Iteration 2, loss = 0.28812409\n",
      "Iteration 113, loss = 0.26692289\n",
      "Iteration 3, loss = 0.28317356\n",
      "Iteration 114, loss = 0.26717730\n",
      "Iteration 4, loss = 0.28195652\n",
      "Iteration 115, loss = 0.26687056\n",
      "Iteration 5, loss = 0.28120416\n",
      "Iteration 116, loss = 0.26677640\n",
      "Iteration 6, loss = 0.28064082\n",
      "Iteration 117, loss = 0.26684649\n",
      "Iteration 7, loss = 0.28000574\n",
      "Iteration 118, loss = 0.26677661\n",
      "Iteration 8, loss = 0.27914858\n",
      "Iteration 119, loss = 0.26703472\n",
      "Iteration 9, loss = 0.27889201\n",
      "Iteration 120, loss = 0.26691295\n",
      "Iteration 10, loss = 0.27808218\n",
      "Iteration 121, loss = 0.26673181\n",
      "Iteration 11, loss = 0.27761431\n",
      "Iteration 122, loss = 0.26686824\n",
      "Iteration 12, loss = 0.27758309\n",
      "Iteration 123, loss = 0.26667314\n",
      "Iteration 124, loss = 0.26678660\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 13, loss = 0.27675013\n",
      "Iteration 14, loss = 0.27639452\n",
      "Iteration 1, loss = 0.32700162\n",
      "Iteration 2, loss = 0.28762533\n",
      "Iteration 15, loss = 0.27621137\n",
      "Iteration 3, loss = 0.28227369\n",
      "Iteration 16, loss = 0.27572716\n",
      "Iteration 4, loss = 0.28106060\n",
      "Iteration 17, loss = 0.27520991\n",
      "Iteration 5, loss = 0.28020124\n",
      "Iteration 18, loss = 0.27502218\n",
      "Iteration 19, loss = 0.27444343\n",
      "Iteration 6, loss = 0.27977288\n",
      "Iteration 7, loss = 0.27939733\n",
      "Iteration 20, loss = 0.27412228\n",
      "Iteration 8, loss = 0.27849463\n",
      "Iteration 21, loss = 0.27389608\n",
      "Iteration 9, loss = 0.27832553\n",
      "Iteration 22, loss = 0.27331861\n",
      "Iteration 10, loss = 0.27773813\n",
      "Iteration 23, loss = 0.27329511\n",
      "Iteration 11, loss = 0.27719956\n",
      "Iteration 24, loss = 0.27283262\n",
      "Iteration 12, loss = 0.27703154\n",
      "Iteration 13, loss = 0.27643447\n",
      "Iteration 25, loss = 0.27256776\n",
      "Iteration 14, loss = 0.27590077\n",
      "Iteration 26, loss = 0.27223948\n",
      "Iteration 15, loss = 0.27583868\n",
      "Iteration 27, loss = 0.27208438\n",
      "Iteration 28, loss = 0.27169617\n",
      "Iteration 16, loss = 0.27514474\n",
      "Iteration 17, loss = 0.27496166Iteration 29, loss = 0.27183372\n",
      "\n",
      "Iteration 18, loss = 0.27460463\n",
      "Iteration 30, loss = 0.27124293\n",
      "Iteration 19, loss = 0.27418630\n",
      "Iteration 31, loss = 0.27140554\n",
      "Iteration 20, loss = 0.27376492\n",
      "Iteration 32, loss = 0.27112531\n",
      "Iteration 21, loss = 0.27346450\n",
      "Iteration 33, loss = 0.27059818\n",
      "Iteration 22, loss = 0.27291197\n",
      "Iteration 34, loss = 0.27068864\n",
      "Iteration 23, loss = 0.27268778\n",
      "Iteration 35, loss = 0.27029710\n",
      "Iteration 24, loss = 0.27237398\n",
      "Iteration 36, loss = 0.27036423\n",
      "Iteration 25, loss = 0.27205779\n",
      "Iteration 37, loss = 0.27005423\n",
      "Iteration 26, loss = 0.27191783\n",
      "Iteration 38, loss = 0.27003295\n",
      "Iteration 27, loss = 0.27175363\n",
      "Iteration 39, loss = 0.26984325\n",
      "Iteration 28, loss = 0.27137562\n",
      "Iteration 40, loss = 0.26985592\n",
      "Iteration 29, loss = 0.27119098\n",
      "Iteration 41, loss = 0.26958356\n",
      "Iteration 30, loss = 0.27103196\n",
      "Iteration 42, loss = 0.26947439\n",
      "Iteration 31, loss = 0.27095906\n",
      "Iteration 43, loss = 0.26949585\n",
      "Iteration 32, loss = 0.27076570\n",
      "Iteration 44, loss = 0.26915095\n",
      "Iteration 45, loss = 0.26909626\n",
      "Iteration 46, loss = 0.26914220\n",
      "Iteration 33, loss = 0.27028079\n",
      "Iteration 47, loss = 0.26896570\n",
      "Iteration 34, loss = 0.27011465\n",
      "Iteration 48, loss = 0.26877869\n",
      "Iteration 35, loss = 0.27013972\n",
      "Iteration 49, loss = 0.26870547\n",
      "Iteration 36, loss = 0.26997797\n",
      "Iteration 50, loss = 0.26844755\n",
      "Iteration 37, loss = 0.26979781\n",
      "Iteration 51, loss = 0.26845984\n",
      "Iteration 38, loss = 0.26954817\n",
      "Iteration 52, loss = 0.26825939\n",
      "Iteration 39, loss = 0.26945563\n",
      "Iteration 53, loss = 0.26869176\n",
      "Iteration 54, loss = 0.26811563\n",
      "Iteration 40, loss = 0.26959669\n",
      "Iteration 55, loss = 0.26803416\n",
      "Iteration 41, loss = 0.26937958\n",
      "Iteration 56, loss = 0.26794775\n",
      "Iteration 42, loss = 0.26904919\n",
      "Iteration 57, loss = 0.26781734\n",
      "Iteration 58, loss = 0.26790372\n",
      "Iteration 43, loss = 0.26918883\n",
      "Iteration 59, loss = 0.26796993\n",
      "Iteration 44, loss = 0.26898116\n",
      "Iteration 60, loss = 0.26757683\n",
      "Iteration 45, loss = 0.26889669\n",
      "Iteration 61, loss = 0.26749556\n",
      "Iteration 46, loss = 0.26907707\n",
      "Iteration 47, loss = 0.26875671\n",
      "Iteration 62, loss = 0.26749630\n",
      "Iteration 48, loss = 0.26875859\n",
      "Iteration 63, loss = 0.26738001\n",
      "Iteration 49, loss = 0.26854032\n",
      "Iteration 64, loss = 0.26730858\n",
      "Iteration 50, loss = 0.26835961\n",
      "Iteration 65, loss = 0.26722991\n",
      "Iteration 51, loss = 0.26850434\n",
      "Iteration 66, loss = 0.26715239\n",
      "Iteration 52, loss = 0.26832848\n",
      "Iteration 67, loss = 0.26706556\n",
      "Iteration 53, loss = 0.26852924\n",
      "Iteration 68, loss = 0.26713806\n",
      "Iteration 54, loss = 0.26830418\n",
      "Iteration 69, loss = 0.26700813\n",
      "Iteration 55, loss = 0.26814479\n",
      "Iteration 70, loss = 0.26698861\n",
      "Iteration 71, loss = 0.26704920\n",
      "Iteration 56, loss = 0.26813065\n",
      "Iteration 57, loss = 0.26828053\n",
      "Iteration 72, loss = 0.26680762\n",
      "Iteration 58, loss = 0.26831824\n",
      "Iteration 73, loss = 0.26673464\n",
      "Iteration 59, loss = 0.26825368\n",
      "Iteration 74, loss = 0.26680623\n",
      "Iteration 60, loss = 0.26789057\n",
      "Iteration 75, loss = 0.26664698\n",
      "Iteration 61, loss = 0.26776822\n",
      "Iteration 76, loss = 0.26672629\n",
      "Iteration 62, loss = 0.26794675\n",
      "Iteration 77, loss = 0.26656275\n",
      "Iteration 63, loss = 0.26780643\n",
      "Iteration 64, loss = 0.26768556\n",
      "Iteration 78, loss = 0.26629192\n",
      "Iteration 65, loss = 0.26744320\n",
      "Iteration 66, loss = 0.26774555Iteration 79, loss = 0.26620178\n",
      "\n",
      "Iteration 80, loss = 0.26645637\n",
      "Iteration 67, loss = 0.26726238\n",
      "Iteration 81, loss = 0.26625996\n",
      "Iteration 68, loss = 0.26751316\n",
      "Iteration 82, loss = 0.26611400\n",
      "Iteration 69, loss = 0.26736477\n",
      "Iteration 83, loss = 0.26630935\n",
      "Iteration 70, loss = 0.26749373\n",
      "Iteration 84, loss = 0.26604937\n",
      "Iteration 71, loss = 0.26759889\n",
      "Iteration 85, loss = 0.26658002\n",
      "Iteration 72, loss = 0.26718161\n",
      "Iteration 86, loss = 0.26621345\n",
      "Iteration 73, loss = 0.26711215\n",
      "Iteration 87, loss = 0.26601560\n",
      "Iteration 74, loss = 0.26733709\n",
      "Iteration 88, loss = 0.26597997\n",
      "Iteration 75, loss = 0.26725389\n",
      "Iteration 89, loss = 0.26586715\n",
      "Iteration 76, loss = 0.26702525\n",
      "Iteration 77, loss = 0.26713702\n",
      "Iteration 90, loss = 0.26576606\n",
      "Iteration 91, loss = 0.26580766\n",
      "Iteration 78, loss = 0.26710704\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 92, loss = 0.26583956\n",
      "Iteration 93, loss = 0.26570911\n",
      "Iteration 1, loss = 0.32791702\n",
      "Iteration 94, loss = 0.26570999\n",
      "Iteration 2, loss = 0.28869661\n",
      "Iteration 95, loss = 0.26575730\n",
      "Iteration 3, loss = 0.28334330\n",
      "Iteration 96, loss = 0.26582670\n",
      "Iteration 4, loss = 0.28208310\n",
      "Iteration 5, loss = 0.28121759\n",
      "Iteration 97, loss = 0.26547110\n",
      "Iteration 6, loss = 0.28055054\n",
      "Iteration 98, loss = 0.26555771\n",
      "Iteration 7, loss = 0.28021777\n",
      "Iteration 8, loss = 0.27939539\n",
      "Iteration 99, loss = 0.26560757\n",
      "Iteration 100, loss = 0.26548065\n",
      "Iteration 9, loss = 0.27906558\n",
      "Iteration 101, loss = 0.26553369\n",
      "Iteration 10, loss = 0.27867324\n",
      "Iteration 102, loss = 0.26537765\n",
      "Iteration 11, loss = 0.27814412\n",
      "Iteration 103, loss = 0.26572783\n",
      "Iteration 12, loss = 0.27786269\n",
      "Iteration 104, loss = 0.26525942\n",
      "Iteration 13, loss = 0.27741074\n",
      "Iteration 105, loss = 0.26524247\n",
      "Iteration 14, loss = 0.27703407\n",
      "Iteration 106, loss = 0.26541051\n",
      "Iteration 15, loss = 0.27716995\n",
      "Iteration 107, loss = 0.26568354\n",
      "Iteration 16, loss = 0.27662919\n",
      "Iteration 108, loss = 0.26500679\n",
      "Iteration 17, loss = 0.27630329\n",
      "Iteration 109, loss = 0.26510958\n",
      "Iteration 18, loss = 0.27610987\n",
      "Iteration 110, loss = 0.26509220\n",
      "Iteration 19, loss = 0.27586365\n",
      "Iteration 111, loss = 0.26512339\n",
      "Iteration 20, loss = 0.27544139\n",
      "Iteration 21, loss = 0.27520553\n",
      "Iteration 112, loss = 0.26491148\n",
      "Iteration 22, loss = 0.27499318\n",
      "Iteration 113, loss = 0.26482186\n",
      "Iteration 23, loss = 0.27472738\n",
      "Iteration 114, loss = 0.26492248\n",
      "Iteration 24, loss = 0.27427877\n",
      "Iteration 115, loss = 0.26504515\n",
      "Iteration 25, loss = 0.27426888\n",
      "Iteration 116, loss = 0.26504004\n",
      "Iteration 26, loss = 0.27403131\n",
      "Iteration 117, loss = 0.26498254\n",
      "Iteration 27, loss = 0.27372483\n",
      "Iteration 118, loss = 0.26490886\n",
      "Iteration 28, loss = 0.27344691\n",
      "Iteration 119, loss = 0.26501676\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 29, loss = 0.27315532\n",
      "Iteration 30, loss = 0.27318185\n",
      "Iteration 1, loss = 0.32817413\n",
      "Iteration 31, loss = 0.27306139\n",
      "Iteration 2, loss = 0.28958651\n",
      "Iteration 32, loss = 0.27294137\n",
      "Iteration 3, loss = 0.28420400\n",
      "Iteration 33, loss = 0.27234129\n",
      "Iteration 4, loss = 0.28301880\n",
      "Iteration 34, loss = 0.27224934\n",
      "Iteration 5, loss = 0.28231636\n",
      "Iteration 6, loss = 0.28198366\n",
      "Iteration 35, loss = 0.27209409\n",
      "Iteration 36, loss = 0.27194271\n",
      "Iteration 7, loss = 0.28143972\n",
      "Iteration 37, loss = 0.27193542\n",
      "Iteration 8, loss = 0.28062140\n",
      "Iteration 38, loss = 0.27139407\n",
      "Iteration 9, loss = 0.28021868\n",
      "Iteration 39, loss = 0.27136978\n",
      "Iteration 10, loss = 0.27954857\n",
      "Iteration 11, loss = 0.27911384\n",
      "Iteration 40, loss = 0.27132417\n",
      "Iteration 12, loss = 0.27861324\n",
      "Iteration 41, loss = 0.27114650\n",
      "Iteration 13, loss = 0.27813218\n",
      "Iteration 42, loss = 0.27092164\n",
      "Iteration 14, loss = 0.27782263\n",
      "Iteration 43, loss = 0.27080038\n",
      "Iteration 15, loss = 0.27760926\n",
      "Iteration 44, loss = 0.27063124\n",
      "Iteration 16, loss = 0.27701812\n",
      "Iteration 17, loss = 0.27649933\n",
      "Iteration 45, loss = 0.27049240\n",
      "Iteration 18, loss = 0.27622506\n",
      "Iteration 46, loss = 0.27066711\n",
      "Iteration 19, loss = 0.27583371\n",
      "Iteration 47, loss = 0.27027172\n",
      "Iteration 20, loss = 0.27537332\n",
      "Iteration 48, loss = 0.27015598\n",
      "Iteration 21, loss = 0.27516641\n",
      "Iteration 49, loss = 0.26996725\n",
      "Iteration 22, loss = 0.27489413\n",
      "Iteration 50, loss = 0.26978823\n",
      "Iteration 23, loss = 0.27449356\n",
      "Iteration 51, loss = 0.26985573\n",
      "Iteration 24, loss = 0.27380674\n",
      "Iteration 52, loss = 0.26951312\n",
      "Iteration 25, loss = 0.27419518\n",
      "Iteration 26, loss = 0.27361734\n",
      "Iteration 53, loss = 0.26971796\n",
      "Iteration 27, loss = 0.27356978\n",
      "Iteration 54, loss = 0.26937348\n",
      "Iteration 28, loss = 0.27312110\n",
      "Iteration 55, loss = 0.26931942\n",
      "Iteration 29, loss = 0.27299547\n",
      "Iteration 56, loss = 0.26931220\n",
      "Iteration 30, loss = 0.27292433\n",
      "Iteration 57, loss = 0.26943090\n",
      "Iteration 31, loss = 0.27303545\n",
      "Iteration 58, loss = 0.26944830\n",
      "Iteration 32, loss = 0.27293629\n",
      "Iteration 59, loss = 0.26940301\n",
      "Iteration 33, loss = 0.27221930\n",
      "Iteration 60, loss = 0.26888307\n",
      "Iteration 34, loss = 0.27219924\n",
      "Iteration 61, loss = 0.26898043\n",
      "Iteration 35, loss = 0.27207274\n",
      "Iteration 62, loss = 0.26901002\n",
      "Iteration 36, loss = 0.27194030\n",
      "Iteration 63, loss = 0.26902159\n",
      "Iteration 37, loss = 0.27186962\n",
      "Iteration 64, loss = 0.26889380\n",
      "Iteration 38, loss = 0.27167351\n",
      "Iteration 65, loss = 0.26874557\n",
      "Iteration 39, loss = 0.27150903\n",
      "Iteration 66, loss = 0.26889301\n",
      "Iteration 40, loss = 0.27158999\n",
      "Iteration 67, loss = 0.26851081\n",
      "Iteration 41, loss = 0.27149570\n",
      "Iteration 68, loss = 0.26861531\n",
      "Iteration 42, loss = 0.27133706\n",
      "Iteration 69, loss = 0.26857978\n",
      "Iteration 43, loss = 0.27130627\n",
      "Iteration 44, loss = 0.27132343\n",
      "Iteration 70, loss = 0.26868765\n",
      "Iteration 71, loss = 0.26863948\n",
      "Iteration 45, loss = 0.27106955\n",
      "Iteration 46, loss = 0.27120283\n",
      "Iteration 72, loss = 0.26836719\n",
      "Iteration 47, loss = 0.27089813\n",
      "Iteration 73, loss = 0.26838853\n",
      "Iteration 74, loss = 0.26856122\n",
      "Iteration 48, loss = 0.27083158\n",
      "Iteration 75, loss = 0.26840612\n",
      "Iteration 49, loss = 0.27066544\n",
      "Iteration 76, loss = 0.26820365\n",
      "Iteration 50, loss = 0.27043509\n",
      "Iteration 77, loss = 0.26838301\n",
      "Iteration 51, loss = 0.27070587\n",
      "Iteration 78, loss = 0.26829524\n",
      "Iteration 52, loss = 0.27036395\n",
      "Iteration 79, loss = 0.26815699\n",
      "Iteration 53, loss = 0.27044945\n",
      "Iteration 80, loss = 0.26824238\n",
      "Iteration 54, loss = 0.27015962\n",
      "Iteration 81, loss = 0.26807839\n",
      "Iteration 55, loss = 0.27001490\n",
      "Iteration 82, loss = 0.26839673\n",
      "Iteration 56, loss = 0.27020753\n",
      "Iteration 83, loss = 0.26805137\n",
      "Iteration 57, loss = 0.27026349\n",
      "Iteration 84, loss = 0.26838598\n",
      "Iteration 58, loss = 0.27019580\n",
      "Iteration 85, loss = 0.26841601\n",
      "Iteration 59, loss = 0.26980071\n",
      "Iteration 86, loss = 0.26825900\n",
      "Iteration 60, loss = 0.26960839\n",
      "Iteration 87, loss = 0.26793892\n",
      "Iteration 61, loss = 0.26976907\n",
      "Iteration 88, loss = 0.26812627\n",
      "Iteration 62, loss = 0.26978094\n",
      "Iteration 89, loss = 0.26784400\n",
      "Iteration 63, loss = 0.26969761\n",
      "Iteration 90, loss = 0.26791064\n",
      "Iteration 64, loss = 0.26938156\n",
      "Iteration 91, loss = 0.26790598\n",
      "Iteration 65, loss = 0.26935011\n",
      "Iteration 92, loss = 0.26800413\n",
      "Iteration 66, loss = 0.26948492\n",
      "Iteration 93, loss = 0.26771918\n",
      "Iteration 67, loss = 0.26919107\n",
      "Iteration 94, loss = 0.26772397\n",
      "Iteration 68, loss = 0.26905241\n",
      "Iteration 69, loss = 0.26893676\n",
      "Iteration 95, loss = 0.26787830\n",
      "Iteration 96, loss = 0.26798654\n",
      "Iteration 70, loss = 0.26958588\n",
      "Iteration 71, loss = 0.26920718\n",
      "Iteration 97, loss = 0.26760925\n",
      "Iteration 72, loss = 0.26885781\n",
      "Iteration 98, loss = 0.26752739\n",
      "Iteration 73, loss = 0.26896799\n",
      "Iteration 99, loss = 0.26761714\n",
      "Iteration 74, loss = 0.26887879\n",
      "Iteration 75, loss = 0.26879927\n",
      "Iteration 100, loss = 0.26751675\n",
      "Iteration 76, loss = 0.26866854\n",
      "Iteration 101, loss = 0.26758353\n",
      "Iteration 77, loss = 0.26875080\n",
      "Iteration 102, loss = 0.26748105\n",
      "Iteration 78, loss = 0.26883759\n",
      "Iteration 103, loss = 0.26782763\n",
      "Iteration 79, loss = 0.26852775\n",
      "Iteration 104, loss = 0.26765719\n",
      "Iteration 80, loss = 0.26869849\n",
      "Iteration 105, loss = 0.26743996\n",
      "Iteration 81, loss = 0.26845887\n",
      "Iteration 82, loss = 0.26873391\n",
      "Iteration 106, loss = 0.26763397\n",
      "Iteration 83, loss = 0.26829679\n",
      "Iteration 107, loss = 0.26784201\n",
      "Iteration 84, loss = 0.26865510\n",
      "Iteration 108, loss = 0.26763466\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 85, loss = 0.26886736\n",
      "Iteration 86, loss = 0.26867384\n",
      "Iteration 1, loss = 0.32702016\n",
      "Iteration 87, loss = 0.26814244\n",
      "Iteration 2, loss = 0.28750625\n",
      "Iteration 88, loss = 0.26838806\n",
      "Iteration 3, loss = 0.28204209\n",
      "Iteration 4, loss = 0.28067837\n",
      "Iteration 89, loss = 0.26818748\n",
      "Iteration 5, loss = 0.28012234\n",
      "Iteration 90, loss = 0.26821084\n",
      "Iteration 6, loss = 0.27988377\n",
      "Iteration 91, loss = 0.26844236\n",
      "Iteration 7, loss = 0.27909491\n",
      "Iteration 92, loss = 0.26827128\n",
      "Iteration 8, loss = 0.27832747\n",
      "Iteration 93, loss = 0.26812300\n",
      "Iteration 94, loss = 0.26809386\n",
      "Iteration 9, loss = 0.27798811\n",
      "Iteration 10, loss = 0.27755992\n",
      "Iteration 95, loss = 0.26820967\n",
      "Iteration 11, loss = 0.27694329\n",
      "Iteration 12, loss = 0.27663323\n",
      "Iteration 96, loss = 0.26830253\n",
      "Iteration 13, loss = 0.27628551\n",
      "Iteration 97, loss = 0.26785765\n",
      "Iteration 14, loss = 0.27592565\n",
      "Iteration 98, loss = 0.26790468\n",
      "Iteration 15, loss = 0.27557093\n",
      "Iteration 99, loss = 0.26790259\n",
      "Iteration 100, loss = 0.26789349Iteration 16, loss = 0.27516731\n",
      "\n",
      "Iteration 101, loss = 0.26784636\n",
      "Iteration 17, loss = 0.27472351\n",
      "Iteration 102, loss = 0.26767056\n",
      "Iteration 18, loss = 0.27443130\n",
      "Iteration 103, loss = 0.26794825\n",
      "Iteration 19, loss = 0.27403507\n",
      "Iteration 104, loss = 0.26796550\n",
      "Iteration 20, loss = 0.27382167\n",
      "Iteration 105, loss = 0.26761787\n",
      "Iteration 21, loss = 0.27350416\n",
      "Iteration 106, loss = 0.26778246\n",
      "Iteration 22, loss = 0.27317133\n",
      "Iteration 23, loss = 0.27295066\n",
      "Iteration 107, loss = 0.26780474\n",
      "Iteration 24, loss = 0.27248219\n",
      "Iteration 108, loss = 0.26756740\n",
      "Iteration 25, loss = 0.27259617\n",
      "Iteration 109, loss = 0.26754474\n",
      "Iteration 26, loss = 0.27219742\n",
      "Iteration 110, loss = 0.26752794\n",
      "Iteration 27, loss = 0.27193700\n",
      "Iteration 111, loss = 0.26743487\n",
      "Iteration 28, loss = 0.27169150\n",
      "Iteration 112, loss = 0.26770475\n",
      "Iteration 29, loss = 0.27155245\n",
      "Iteration 113, loss = 0.26747900\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 30, loss = 0.27143565\n",
      "Iteration 31, loss = 0.27135364\n",
      "Iteration 1, loss = 0.32734673\n",
      "Iteration 32, loss = 0.27118935\n",
      "Iteration 2, loss = 0.28924002\n",
      "Iteration 33, loss = 0.27080882\n",
      "Iteration 3, loss = 0.28381378\n",
      "Iteration 4, loss = 0.28252330\n",
      "Iteration 34, loss = 0.27076096\n",
      "Iteration 35, loss = 0.27068193\n",
      "Iteration 5, loss = 0.28224626\n",
      "Iteration 36, loss = 0.27038255\n",
      "Iteration 6, loss = 0.28164229\n",
      "Iteration 37, loss = 0.27020068\n",
      "Iteration 7, loss = 0.28101313\n",
      "Iteration 38, loss = 0.27006436\n",
      "Iteration 8, loss = 0.27991317\n",
      "Iteration 39, loss = 0.26999342\n",
      "Iteration 40, loss = 0.27016685\n",
      "Iteration 9, loss = 0.27950234\n",
      "Iteration 10, loss = 0.27913243\n",
      "Iteration 11, loss = 0.27839839\n",
      "Iteration 41, loss = 0.26980815\n",
      "Iteration 12, loss = 0.27799211\n",
      "Iteration 42, loss = 0.26957696\n",
      "Iteration 13, loss = 0.27749987\n",
      "Iteration 43, loss = 0.26944547\n",
      "Iteration 14, loss = 0.27718516\n",
      "Iteration 44, loss = 0.26965709\n",
      "Iteration 15, loss = 0.27695585\n",
      "Iteration 45, loss = 0.26948613\n",
      "Iteration 16, loss = 0.27636721\n",
      "Iteration 46, loss = 0.26937668\n",
      "Iteration 17, loss = 0.27604535\n",
      "Iteration 47, loss = 0.26908622\n",
      "Iteration 18, loss = 0.27563799\n",
      "Iteration 48, loss = 0.26906364\n",
      "Iteration 19, loss = 0.27531898\n",
      "Iteration 49, loss = 0.26888732\n",
      "Iteration 20, loss = 0.27509922\n",
      "Iteration 50, loss = 0.26879531\n",
      "Iteration 21, loss = 0.27475195\n",
      "Iteration 51, loss = 0.26898491\n",
      "Iteration 22, loss = 0.27441774\n",
      "Iteration 52, loss = 0.26867279\n",
      "Iteration 23, loss = 0.27418144\n",
      "Iteration 53, loss = 0.26864687\n",
      "Iteration 24, loss = 0.27364228\n",
      "Iteration 25, loss = 0.27353718\n",
      "Iteration 54, loss = 0.26836466\n",
      "Iteration 26, loss = 0.27325818\n",
      "Iteration 55, loss = 0.26835393\n",
      "Iteration 27, loss = 0.27322773\n",
      "Iteration 56, loss = 0.26846553\n",
      "Iteration 28, loss = 0.27280018\n",
      "Iteration 57, loss = 0.26830285\n",
      "Iteration 29, loss = 0.27268394\n",
      "Iteration 58, loss = 0.26842339\n",
      "Iteration 30, loss = 0.27235635\n",
      "Iteration 59, loss = 0.26810176\n",
      "Iteration 31, loss = 0.27234294\n",
      "Iteration 60, loss = 0.26798880\n",
      "Iteration 32, loss = 0.27233089\n",
      "Iteration 61, loss = 0.26825892\n",
      "Iteration 33, loss = 0.27200499\n",
      "Iteration 62, loss = 0.26813793\n",
      "Iteration 34, loss = 0.27188639\n",
      "Iteration 63, loss = 0.26797119\n",
      "Iteration 35, loss = 0.27182503\n",
      "Iteration 64, loss = 0.26761909\n",
      "Iteration 36, loss = 0.27163275\n",
      "Iteration 65, loss = 0.26780896\n",
      "Iteration 37, loss = 0.27142907\n",
      "Iteration 66, loss = 0.26775752\n",
      "Iteration 38, loss = 0.27135534\n",
      "Iteration 67, loss = 0.26764033\n",
      "Iteration 39, loss = 0.27120784\n",
      "Iteration 68, loss = 0.26724411\n",
      "Iteration 40, loss = 0.27127507\n",
      "Iteration 69, loss = 0.26698982\n",
      "Iteration 41, loss = 0.27109458\n",
      "Iteration 70, loss = 0.26753170\n",
      "Iteration 42, loss = 0.27107263\n",
      "Iteration 71, loss = 0.26728024\n",
      "Iteration 43, loss = 0.27102979\n",
      "Iteration 72, loss = 0.26707254\n",
      "Iteration 44, loss = 0.27099488\n",
      "Iteration 73, loss = 0.26717887\n",
      "Iteration 45, loss = 0.27095146\n",
      "Iteration 74, loss = 0.26709304\n",
      "Iteration 46, loss = 0.27083724\n",
      "Iteration 75, loss = 0.26702959\n",
      "Iteration 47, loss = 0.27059575\n",
      "Iteration 76, loss = 0.26677885\n",
      "Iteration 48, loss = 0.27064858\n",
      "Iteration 77, loss = 0.26700454\n",
      "Iteration 49, loss = 0.27041176\n",
      "Iteration 78, loss = 0.26690947\n",
      "Iteration 50, loss = 0.27045930\n",
      "Iteration 79, loss = 0.26686069\n",
      "Iteration 51, loss = 0.27064069\n",
      "Iteration 80, loss = 0.26685558\n",
      "Iteration 81, loss = 0.26671459\n",
      "Iteration 52, loss = 0.27031644\n",
      "Iteration 82, loss = 0.26678403\n",
      "Iteration 53, loss = 0.27029577\n",
      "Iteration 83, loss = 0.26654143\n",
      "Iteration 54, loss = 0.27003657\n",
      "Iteration 84, loss = 0.26711134\n",
      "Iteration 55, loss = 0.27013225\n",
      "Iteration 85, loss = 0.26697596\n",
      "Iteration 56, loss = 0.27016621\n",
      "Iteration 86, loss = 0.26662855\n",
      "Iteration 57, loss = 0.27035065\n",
      "Iteration 87, loss = 0.26645352\n",
      "Iteration 58, loss = 0.27005902\n",
      "Iteration 88, loss = 0.26653320\n",
      "Iteration 59, loss = 0.27007644\n",
      "Iteration 89, loss = 0.26648972\n",
      "Iteration 60, loss = 0.26984712\n",
      "Iteration 90, loss = 0.26635808\n",
      "Iteration 61, loss = 0.27000003\n",
      "Iteration 91, loss = 0.26673943\n",
      "Iteration 62, loss = 0.26985326\n",
      "Iteration 92, loss = 0.26655018\n",
      "Iteration 63, loss = 0.26999924\n",
      "Iteration 93, loss = 0.26629828\n",
      "Iteration 64, loss = 0.26965992\n",
      "Iteration 94, loss = 0.26628667\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 65, loss = 0.26981248\n",
      "Iteration 66, loss = 0.26963426\n",
      "Iteration 1, loss = 0.32653268\n",
      "Iteration 67, loss = 0.26957889\n",
      "Iteration 2, loss = 0.28813215\n",
      "Iteration 68, loss = 0.26937495\n",
      "Iteration 3, loss = 0.28255956\n",
      "Iteration 4, loss = 0.28132921\n",
      "Iteration 69, loss = 0.26943527\n",
      "Iteration 5, loss = 0.28135047\n",
      "Iteration 70, loss = 0.26952831\n",
      "Iteration 6, loss = 0.28056825\n",
      "Iteration 71, loss = 0.26947959\n",
      "Iteration 7, loss = 0.28008495\n",
      "Iteration 72, loss = 0.26924577\n",
      "Iteration 8, loss = 0.27927857\n",
      "Iteration 73, loss = 0.26952867\n",
      "Iteration 9, loss = 0.27890721\n",
      "Iteration 74, loss = 0.26931486\n",
      "Iteration 10, loss = 0.27862193\n",
      "Iteration 75, loss = 0.26926441\n",
      "Iteration 11, loss = 0.27795516\n",
      "Iteration 12, loss = 0.27760709\n",
      "Iteration 76, loss = 0.26908433\n",
      "Iteration 13, loss = 0.27700325\n",
      "Iteration 77, loss = 0.26916585\n",
      "Iteration 14, loss = 0.27663972\n",
      "Iteration 78, loss = 0.26912426\n",
      "Iteration 15, loss = 0.27631228\n",
      "Iteration 79, loss = 0.26897670\n",
      "Iteration 16, loss = 0.27591150\n",
      "Iteration 80, loss = 0.26909010\n",
      "Iteration 17, loss = 0.27524435\n",
      "Iteration 81, loss = 0.26899801\n",
      "Iteration 18, loss = 0.27477051\n",
      "Iteration 19, loss = 0.27422058\n",
      "Iteration 82, loss = 0.26900397\n",
      "Iteration 20, loss = 0.27429524\n",
      "Iteration 83, loss = 0.26886488\n",
      "Iteration 21, loss = 0.27375928\n",
      "Iteration 84, loss = 0.26918107\n",
      "Iteration 22, loss = 0.27350694\n",
      "Iteration 85, loss = 0.26903800\n",
      "Iteration 23, loss = 0.27340354\n",
      "Iteration 86, loss = 0.26894212\n",
      "Iteration 24, loss = 0.27273095\n",
      "Iteration 87, loss = 0.26859414\n",
      "Iteration 25, loss = 0.27272006\n",
      "Iteration 88, loss = 0.26883110\n",
      "Iteration 26, loss = 0.27269600\n",
      "Iteration 89, loss = 0.26867555\n",
      "Iteration 27, loss = 0.27233084\n",
      "Iteration 90, loss = 0.26869847\n",
      "Iteration 28, loss = 0.27203309\n",
      "Iteration 29, loss = 0.27206541\n",
      "Iteration 91, loss = 0.26856853\n",
      "Iteration 30, loss = 0.27180777\n",
      "Iteration 92, loss = 0.26866391\n",
      "Iteration 31, loss = 0.27153283\n",
      "Iteration 93, loss = 0.26864764\n",
      "Iteration 32, loss = 0.27140200\n",
      "Iteration 94, loss = 0.26838938\n",
      "Iteration 33, loss = 0.27119817\n",
      "Iteration 95, loss = 0.26880950\n",
      "Iteration 34, loss = 0.27131568\n",
      "Iteration 96, loss = 0.26863757\n",
      "Iteration 35, loss = 0.27100061\n",
      "Iteration 97, loss = 0.26838273\n",
      "Iteration 36, loss = 0.27095131\n",
      "Iteration 98, loss = 0.26824437\n",
      "Iteration 37, loss = 0.27064773\n",
      "Iteration 99, loss = 0.26827354\n",
      "Iteration 38, loss = 0.27051647\n",
      "Iteration 100, loss = 0.26846141\n",
      "Iteration 39, loss = 0.27047052\n",
      "Iteration 40, loss = 0.27034208\n",
      "Iteration 101, loss = 0.26840951\n",
      "Iteration 41, loss = 0.27046091\n",
      "Iteration 102, loss = 0.26833243\n",
      "Iteration 42, loss = 0.27030002\n",
      "Iteration 103, loss = 0.26844467\n",
      "Iteration 43, loss = 0.26988709\n",
      "Iteration 104, loss = 0.26854336\n",
      "Iteration 44, loss = 0.27007520\n",
      "Iteration 105, loss = 0.26820305\n",
      "Iteration 45, loss = 0.27007967\n",
      "Iteration 106, loss = 0.26820646\n",
      "Iteration 46, loss = 0.26985645\n",
      "Iteration 107, loss = 0.26822200\n",
      "Iteration 47, loss = 0.26966906\n",
      "Iteration 108, loss = 0.26799132\n",
      "Iteration 48, loss = 0.26978525\n",
      "Iteration 109, loss = 0.26796856\n",
      "Iteration 49, loss = 0.26941437\n",
      "Iteration 110, loss = 0.26795969\n",
      "Iteration 50, loss = 0.26969713\n",
      "Iteration 111, loss = 0.26786823\n",
      "Iteration 51, loss = 0.26983192\n",
      "Iteration 112, loss = 0.26793956\n",
      "Iteration 52, loss = 0.26942350\n",
      "Iteration 113, loss = 0.26804209\n",
      "Iteration 53, loss = 0.26912951\n",
      "Iteration 114, loss = 0.26769202\n",
      "Iteration 54, loss = 0.26915181\n",
      "Iteration 115, loss = 0.26781673\n",
      "Iteration 55, loss = 0.26907938\n",
      "Iteration 116, loss = 0.26791530\n",
      "Iteration 117, loss = 0.26750975\n",
      "Iteration 56, loss = 0.26904653\n",
      "Iteration 118, loss = 0.26772971\n",
      "Iteration 57, loss = 0.26906508\n",
      "Iteration 119, loss = 0.26743190\n",
      "Iteration 58, loss = 0.26894286\n",
      "Iteration 120, loss = 0.26736456\n",
      "Iteration 59, loss = 0.26912899\n",
      "Iteration 121, loss = 0.26734282\n",
      "Iteration 60, loss = 0.26884651\n",
      "Iteration 122, loss = 0.26732733\n",
      "Iteration 61, loss = 0.26878524\n",
      "Iteration 123, loss = 0.26755557\n",
      "Iteration 62, loss = 0.26864168\n",
      "Iteration 124, loss = 0.26719194\n",
      "Iteration 125, loss = 0.26742396\n",
      "Iteration 63, loss = 0.26880269\n",
      "Iteration 126, loss = 0.26711818\n",
      "Iteration 64, loss = 0.26835959\n",
      "Iteration 65, loss = 0.26830166\n",
      "Iteration 127, loss = 0.26740756\n",
      "Iteration 66, loss = 0.26844779\n",
      "Iteration 128, loss = 0.26724211\n",
      "Iteration 67, loss = 0.26830226\n",
      "Iteration 129, loss = 0.26713521\n",
      "Iteration 68, loss = 0.26817783\n",
      "Iteration 130, loss = 0.26697046\n",
      "Iteration 69, loss = 0.26813251\n",
      "Iteration 131, loss = 0.26727267\n",
      "Iteration 132, loss = 0.26689742\n",
      "Iteration 70, loss = 0.26827148\n",
      "Iteration 133, loss = 0.26690097\n",
      "Iteration 71, loss = 0.26810934\n",
      "Iteration 134, loss = 0.26697504\n",
      "Iteration 72, loss = 0.26787311\n",
      "Iteration 135, loss = 0.26719843\n",
      "Iteration 73, loss = 0.26793548\n",
      "Iteration 136, loss = 0.26688373\n",
      "Iteration 74, loss = 0.26780674\n",
      "Iteration 137, loss = 0.26696453\n",
      "Iteration 75, loss = 0.26768798\n",
      "Iteration 138, loss = 0.26660909\n",
      "Iteration 76, loss = 0.26750277\n",
      "Iteration 139, loss = 0.26682808\n",
      "Iteration 77, loss = 0.26759441\n",
      "Iteration 140, loss = 0.26682228\n",
      "Iteration 78, loss = 0.26761635\n",
      "Iteration 141, loss = 0.26684934\n",
      "Iteration 142, loss = 0.26680010\n",
      "Iteration 79, loss = 0.26751519\n",
      "Iteration 143, loss = 0.26669575\n",
      "Iteration 80, loss = 0.26774179\n",
      "Iteration 144, loss = 0.26663423\n",
      "Iteration 81, loss = 0.26749437\n",
      "Iteration 145, loss = 0.26666406\n",
      "Iteration 82, loss = 0.26760503\n",
      "Iteration 146, loss = 0.26665447\n",
      "Iteration 83, loss = 0.26732855\n",
      "Iteration 147, loss = 0.26664862\n",
      "Iteration 84, loss = 0.26769651\n",
      "Iteration 148, loss = 0.26661977\n",
      "Iteration 85, loss = 0.26739698\n",
      "Iteration 149, loss = 0.26661140\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 86, loss = 0.26731838\n",
      "Iteration 1, loss = 0.32677585\n",
      "Iteration 87, loss = 0.26710265\n",
      "Iteration 2, loss = 0.28850873\n",
      "Iteration 88, loss = 0.26706845\n",
      "Iteration 3, loss = 0.28299651\n",
      "Iteration 4, loss = 0.28167932\n",
      "Iteration 89, loss = 0.26721489\n",
      "Iteration 5, loss = 0.28141274\n",
      "Iteration 90, loss = 0.26703268\n",
      "Iteration 6, loss = 0.28094148\n",
      "Iteration 91, loss = 0.26700014\n",
      "Iteration 7, loss = 0.28017557\n",
      "Iteration 92, loss = 0.26718267\n",
      "Iteration 8, loss = 0.27937597\n",
      "Iteration 93, loss = 0.26697417\n",
      "Iteration 9, loss = 0.27883808\n",
      "Iteration 94, loss = 0.26680685\n",
      "Iteration 10, loss = 0.27852783\n",
      "Iteration 95, loss = 0.26711457\n",
      "Iteration 96, loss = 0.26701435\n",
      "Iteration 11, loss = 0.27793910\n",
      "Iteration 97, loss = 0.26681638\n",
      "Iteration 12, loss = 0.27763883\n",
      "Iteration 13, loss = 0.27703706Iteration 98, loss = 0.26652775\n",
      "\n",
      "Iteration 99, loss = 0.26683572\n",
      "Iteration 14, loss = 0.27668118\n",
      "Iteration 100, loss = 0.26656605\n",
      "Iteration 15, loss = 0.27638634\n",
      "Iteration 101, loss = 0.26663733\n",
      "Iteration 16, loss = 0.27610755\n",
      "Iteration 102, loss = 0.26677043\n",
      "Iteration 17, loss = 0.27560820\n",
      "Iteration 103, loss = 0.26672228\n",
      "Iteration 18, loss = 0.27515800\n",
      "Iteration 104, loss = 0.26679468\n",
      "Iteration 19, loss = 0.27460046\n",
      "Iteration 105, loss = 0.26647539\n",
      "Iteration 20, loss = 0.27480501\n",
      "Iteration 106, loss = 0.26651489\n",
      "Iteration 21, loss = 0.27424668\n",
      "Iteration 107, loss = 0.26650082\n",
      "Iteration 22, loss = 0.27412507\n",
      "Iteration 108, loss = 0.26649491\n",
      "Iteration 23, loss = 0.27377608\n",
      "Iteration 109, loss = 0.26643823\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 24, loss = 0.27342755\n",
      "Iteration 25, loss = 0.27319108\n",
      "Iteration 26, loss = 0.27302267\n",
      "Iteration 27, loss = 0.27315574\n",
      "Iteration 28, loss = 0.27246446\n",
      "Iteration 29, loss = 0.27256581\n",
      "Iteration 30, loss = 0.27247405\n",
      "Iteration 31, loss = 0.27226160\n",
      "Iteration 32, loss = 0.27186805\n",
      "Iteration 33, loss = 0.27172496\n",
      "Iteration 34, loss = 0.27189945\n",
      "Iteration 35, loss = 0.27146554\n",
      "Iteration 36, loss = 0.27138444\n",
      "Iteration 37, loss = 0.27101062\n",
      "Iteration 38, loss = 0.27098891\n",
      "Iteration 39, loss = 0.27100586\n",
      "Iteration 40, loss = 0.27065691\n",
      "Iteration 41, loss = 0.27066395\n",
      "Iteration 42, loss = 0.27053798\n",
      "Iteration 43, loss = 0.27011607\n",
      "Iteration 44, loss = 0.27063205\n",
      "Iteration 45, loss = 0.27034072\n",
      "Iteration 46, loss = 0.27010802\n",
      "Iteration 47, loss = 0.26986901\n",
      "Iteration 48, loss = 0.27017330\n",
      "Iteration 49, loss = 0.26965662\n",
      "Iteration 50, loss = 0.26981711\n",
      "Iteration 51, loss = 0.27000108\n",
      "Iteration 52, loss = 0.26955209\n",
      "Iteration 53, loss = 0.26957645\n",
      "Iteration 54, loss = 0.26934456\n",
      "Iteration 55, loss = 0.26922696\n",
      "Iteration 56, loss = 0.26931903\n",
      "Iteration 57, loss = 0.26916594\n",
      "Iteration 58, loss = 0.26925652\n",
      "Iteration 59, loss = 0.26937686\n",
      "Iteration 60, loss = 0.26904749\n",
      "Iteration 61, loss = 0.26907359\n",
      "Iteration 62, loss = 0.26880400\n",
      "Iteration 63, loss = 0.26904453\n",
      "Iteration 64, loss = 0.26862253\n",
      "Iteration 65, loss = 0.26846090\n",
      "Iteration 66, loss = 0.26868897\n",
      "Iteration 67, loss = 0.26854139\n",
      "Iteration 68, loss = 0.26833094\n",
      "Iteration 69, loss = 0.26839942\n",
      "Iteration 70, loss = 0.26846104\n",
      "Iteration 71, loss = 0.26836589\n",
      "Iteration 72, loss = 0.26812065\n",
      "Iteration 73, loss = 0.26812763\n",
      "Iteration 74, loss = 0.26802584\n",
      "Iteration 75, loss = 0.26796737\n",
      "Iteration 76, loss = 0.26767347\n",
      "Iteration 77, loss = 0.26799924\n",
      "Iteration 78, loss = 0.26789502\n",
      "Iteration 79, loss = 0.26800611\n",
      "Iteration 80, loss = 0.26804683\n",
      "Iteration 81, loss = 0.26788471\n",
      "Iteration 82, loss = 0.26786049\n",
      "Iteration 83, loss = 0.26754696\n",
      "Iteration 84, loss = 0.26798791\n",
      "Iteration 85, loss = 0.26758127\n",
      "Iteration 86, loss = 0.26759072\n",
      "Iteration 87, loss = 0.26763616\n",
      "Iteration 88, loss = 0.26776012\n",
      "Iteration 89, loss = 0.26746406\n",
      "Iteration 90, loss = 0.26739609\n",
      "Iteration 91, loss = 0.26735775\n",
      "Iteration 92, loss = 0.26776269\n",
      "Iteration 93, loss = 0.26736501\n",
      "Iteration 94, loss = 0.26730012\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.32783983\n",
      "Iteration 1, loss = 0.32756999\n",
      "Iteration 2, loss = 0.28955256\n",
      "Iteration 2, loss = 0.28831846\n",
      "Iteration 3, loss = 0.28587535\n",
      "Iteration 3, loss = 0.28464814\n",
      "Iteration 4, loss = 0.28475593\n",
      "Iteration 4, loss = 0.28370956\n",
      "Iteration 5, loss = 0.28417227\n",
      "Iteration 5, loss = 0.28322815\n",
      "Iteration 6, loss = 0.28348394\n",
      "Iteration 6, loss = 0.28258002\n",
      "Iteration 7, loss = 0.28265816\n",
      "Iteration 7, loss = 0.28180533\n",
      "Iteration 8, loss = 0.28238778\n",
      "Iteration 8, loss = 0.28141474\n",
      "Iteration 9, loss = 0.28171827\n",
      "Iteration 9, loss = 0.28099100\n",
      "Iteration 10, loss = 0.28121997\n",
      "Iteration 10, loss = 0.28046159\n",
      "Iteration 11, loss = 0.28105124\n",
      "Iteration 11, loss = 0.28043070\n",
      "Iteration 12, loss = 0.28043668\n",
      "Iteration 12, loss = 0.27972192\n",
      "Iteration 13, loss = 0.28036940\n",
      "Iteration 13, loss = 0.27947305\n",
      "Iteration 14, loss = 0.27977860\n",
      "Iteration 14, loss = 0.27889300\n",
      "Iteration 15, loss = 0.27948002\n",
      "Iteration 15, loss = 0.27884232\n",
      "Iteration 16, loss = 0.27914597\n",
      "Iteration 16, loss = 0.27823959\n",
      "Iteration 17, loss = 0.27957939\n",
      "Iteration 18, loss = 0.27860463\n",
      "Iteration 17, loss = 0.27854688\n",
      "Iteration 19, loss = 0.27839535\n",
      "Iteration 18, loss = 0.27772492\n",
      "Iteration 20, loss = 0.27793124\n",
      "Iteration 19, loss = 0.27745354\n",
      "Iteration 21, loss = 0.27781073\n",
      "Iteration 20, loss = 0.27705838\n",
      "Iteration 22, loss = 0.27771648\n",
      "Iteration 21, loss = 0.27694775\n",
      "Iteration 23, loss = 0.27747883\n",
      "Iteration 22, loss = 0.27678379\n",
      "Iteration 24, loss = 0.27707128\n",
      "Iteration 23, loss = 0.27669143\n",
      "Iteration 25, loss = 0.27692824\n",
      "Iteration 24, loss = 0.27620248\n",
      "Iteration 26, loss = 0.27676897\n",
      "Iteration 25, loss = 0.27609831\n",
      "Iteration 27, loss = 0.27651001\n",
      "Iteration 26, loss = 0.27596307\n",
      "Iteration 28, loss = 0.27630125\n",
      "Iteration 27, loss = 0.27549851\n",
      "Iteration 29, loss = 0.27612907\n",
      "Iteration 28, loss = 0.27544465\n",
      "Iteration 30, loss = 0.27592431\n",
      "Iteration 29, loss = 0.27497571\n",
      "Iteration 31, loss = 0.27573091\n",
      "Iteration 30, loss = 0.27504542\n",
      "Iteration 32, loss = 0.27578863\n",
      "Iteration 31, loss = 0.27465170\n",
      "Iteration 33, loss = 0.27531719\n",
      "Iteration 32, loss = 0.27495060\n",
      "Iteration 34, loss = 0.27503212\n",
      "Iteration 33, loss = 0.27455976\n",
      "Iteration 35, loss = 0.27546648\n",
      "Iteration 34, loss = 0.27425150\n",
      "Iteration 36, loss = 0.27484753\n",
      "Iteration 35, loss = 0.27448846\n",
      "Iteration 37, loss = 0.27478120\n",
      "Iteration 36, loss = 0.27411486\n",
      "Iteration 38, loss = 0.27467358\n",
      "Iteration 37, loss = 0.27385297\n",
      "Iteration 39, loss = 0.27470689\n",
      "Iteration 38, loss = 0.27375827\n",
      "Iteration 40, loss = 0.27449344\n",
      "Iteration 39, loss = 0.27363567\n",
      "Iteration 41, loss = 0.27415082\n",
      "Iteration 40, loss = 0.27347762\n",
      "Iteration 42, loss = 0.27423066\n",
      "Iteration 41, loss = 0.27334262\n",
      "Iteration 43, loss = 0.27396940\n",
      "Iteration 42, loss = 0.27331352\n",
      "Iteration 43, loss = 0.27312144\n",
      "Iteration 44, loss = 0.27401387\n",
      "Iteration 44, loss = 0.27300185\n",
      "Iteration 45, loss = 0.27397074\n",
      "Iteration 45, loss = 0.27296546\n",
      "Iteration 46, loss = 0.27385964\n",
      "Iteration 46, loss = 0.27279932\n",
      "Iteration 47, loss = 0.27369828\n",
      "Iteration 47, loss = 0.27264417\n",
      "Iteration 48, loss = 0.27361922\n",
      "Iteration 48, loss = 0.27255079\n",
      "Iteration 49, loss = 0.27404356\n",
      "Iteration 50, loss = 0.27345715\n",
      "Iteration 49, loss = 0.27269002\n",
      "Iteration 51, loss = 0.27359750\n",
      "Iteration 50, loss = 0.27234371\n",
      "Iteration 52, loss = 0.27353114\n",
      "Iteration 51, loss = 0.27240404\n",
      "Iteration 52, loss = 0.27240602\n",
      "Iteration 53, loss = 0.27313512\n",
      "Iteration 53, loss = 0.27203476\n",
      "Iteration 54, loss = 0.27322839\n",
      "Iteration 54, loss = 0.27204252\n",
      "Iteration 55, loss = 0.27344817\n",
      "Iteration 55, loss = 0.27190647\n",
      "Iteration 56, loss = 0.27303308\n",
      "Iteration 56, loss = 0.27176236\n",
      "Iteration 57, loss = 0.27316041\n",
      "Iteration 57, loss = 0.27198774\n",
      "Iteration 58, loss = 0.27280831\n",
      "Iteration 58, loss = 0.27161145\n",
      "Iteration 59, loss = 0.27285422\n",
      "Iteration 59, loss = 0.27157814\n",
      "Iteration 60, loss = 0.27286562\n",
      "Iteration 61, loss = 0.27269777\n",
      "Iteration 60, loss = 0.27156752\n",
      "Iteration 61, loss = 0.27151526\n",
      "Iteration 62, loss = 0.27259408\n",
      "Iteration 63, loss = 0.27263310\n",
      "Iteration 62, loss = 0.27136957\n",
      "Iteration 64, loss = 0.27259953\n",
      "Iteration 63, loss = 0.27134160\n",
      "Iteration 65, loss = 0.27236580\n",
      "Iteration 64, loss = 0.27134149\n",
      "Iteration 66, loss = 0.27264146\n",
      "Iteration 65, loss = 0.27107319\n",
      "Iteration 67, loss = 0.27257271\n",
      "Iteration 66, loss = 0.27131249\n",
      "Iteration 67, loss = 0.27126715\n",
      "Iteration 68, loss = 0.27234813\n",
      "Iteration 68, loss = 0.27109869\n",
      "Iteration 69, loss = 0.27212012\n",
      "Iteration 69, loss = 0.27075885\n",
      "Iteration 70, loss = 0.27236297\n",
      "Iteration 70, loss = 0.27089468\n",
      "Iteration 71, loss = 0.27252068\n",
      "Iteration 71, loss = 0.27111012\n",
      "Iteration 72, loss = 0.27212830\n",
      "Iteration 72, loss = 0.27070054\n",
      "Iteration 73, loss = 0.27219187\n",
      "Iteration 73, loss = 0.27075381\n",
      "Iteration 74, loss = 0.27213401\n",
      "Iteration 74, loss = 0.27080184\n",
      "Iteration 75, loss = 0.27215306\n",
      "Iteration 75, loss = 0.27092203\n",
      "Iteration 76, loss = 0.27233584\n",
      "Iteration 76, loss = 0.27080205\n",
      "Iteration 77, loss = 0.27220455\n",
      "Iteration 77, loss = 0.27069909\n",
      "Iteration 78, loss = 0.27203734\n",
      "Iteration 78, loss = 0.27055046\n",
      "Iteration 79, loss = 0.27174709\n",
      "Iteration 79, loss = 0.27048037\n",
      "Iteration 80, loss = 0.27062351\n",
      "Iteration 80, loss = 0.27191081\n",
      "Iteration 81, loss = 0.27035398\n",
      "Iteration 81, loss = 0.27170832\n",
      "Iteration 82, loss = 0.27040263\n",
      "Iteration 82, loss = 0.27179516\n",
      "Iteration 83, loss = 0.27074559\n",
      "Iteration 83, loss = 0.27186926\n",
      "Iteration 84, loss = 0.27030086\n",
      "Iteration 84, loss = 0.27159582\n",
      "Iteration 85, loss = 0.27042805\n",
      "Iteration 85, loss = 0.27176493\n",
      "Iteration 86, loss = 0.27021729\n",
      "Iteration 86, loss = 0.27170289\n",
      "Iteration 87, loss = 0.27025308\n",
      "Iteration 87, loss = 0.27157835\n",
      "Iteration 88, loss = 0.27024564\n",
      "Iteration 88, loss = 0.27165511\n",
      "Iteration 89, loss = 0.27039176\n",
      "Iteration 89, loss = 0.27171103\n",
      "Iteration 90, loss = 0.27160152\n",
      "Iteration 90, loss = 0.27051960\n",
      "Iteration 91, loss = 0.26991109\n",
      "Iteration 91, loss = 0.27131983\n",
      "Iteration 92, loss = 0.26999124\n",
      "Iteration 92, loss = 0.27127301\n",
      "Iteration 93, loss = 0.27002247Iteration 93, loss = 0.27151801\n",
      "\n",
      "Iteration 94, loss = 0.27136787Iteration 94, loss = 0.27010103\n",
      "\n",
      "Iteration 95, loss = 0.27123155Iteration 95, loss = 0.26999697\n",
      "\n",
      "Iteration 96, loss = 0.26997780\n",
      "Iteration 96, loss = 0.27127643\n",
      "Iteration 97, loss = 0.27115302\n",
      "Iteration 97, loss = 0.26989320\n",
      "Iteration 98, loss = 0.26999942\n",
      "Iteration 98, loss = 0.27120044\n",
      "Iteration 99, loss = 0.26992386\n",
      "Iteration 99, loss = 0.27127742\n",
      "Iteration 100, loss = 0.27000895\n",
      "Iteration 100, loss = 0.27107881\n",
      "Iteration 101, loss = 0.26974014\n",
      "Iteration 101, loss = 0.27085053\n",
      "Iteration 102, loss = 0.26958645\n",
      "Iteration 102, loss = 0.27093124\n",
      "Iteration 103, loss = 0.26976639\n",
      "Iteration 103, loss = 0.27095731\n",
      "Iteration 104, loss = 0.26983035\n",
      "Iteration 104, loss = 0.27094837\n",
      "Iteration 105, loss = 0.26971936\n",
      "Iteration 105, loss = 0.27085854\n",
      "Iteration 106, loss = 0.26957921\n",
      "Iteration 107, loss = 0.26962899\n",
      "Iteration 106, loss = 0.27075973\n",
      "Iteration 107, loss = 0.27073198\n",
      "Iteration 108, loss = 0.26967418\n",
      "Iteration 108, loss = 0.27093631\n",
      "Iteration 109, loss = 0.26956536\n",
      "Iteration 109, loss = 0.27069916\n",
      "Iteration 110, loss = 0.26956151\n",
      "Iteration 110, loss = 0.27050798\n",
      "Iteration 111, loss = 0.26972168\n",
      "Iteration 111, loss = 0.27063033\n",
      "Iteration 112, loss = 0.26950764\n",
      "Iteration 112, loss = 0.27053193\n",
      "Iteration 113, loss = 0.26950869\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 113, loss = 0.27056505\n",
      "Iteration 114, loss = 0.27053919\n",
      "Iteration 1, loss = 0.32859474\n",
      "Iteration 115, loss = 0.27050929\n",
      "Iteration 2, loss = 0.28962957Iteration 116, loss = 0.27041164\n",
      "\n",
      "Iteration 117, loss = 0.27057537\n",
      "Iteration 3, loss = 0.28580189\n",
      "Iteration 118, loss = 0.27039596\n",
      "Iteration 4, loss = 0.28472454\n",
      "Iteration 119, loss = 0.27031099\n",
      "Iteration 5, loss = 0.28387215\n",
      "Iteration 120, loss = 0.27044156\n",
      "Iteration 6, loss = 0.28356240\n",
      "Iteration 7, loss = 0.28280920\n",
      "Iteration 121, loss = 0.27042166\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 8, loss = 0.28232413\n",
      "Iteration 9, loss = 0.28187192\n",
      "Iteration 1, loss = 0.32828083\n",
      "Iteration 10, loss = 0.28130945\n",
      "Iteration 2, loss = 0.28969587\n",
      "Iteration 11, loss = 0.28103396\n",
      "Iteration 3, loss = 0.28581368\n",
      "Iteration 12, loss = 0.28057076\n",
      "Iteration 4, loss = 0.28461567\n",
      "Iteration 13, loss = 0.28033022\n",
      "Iteration 5, loss = 0.28403908\n",
      "Iteration 14, loss = 0.27991609\n",
      "Iteration 6, loss = 0.28362981\n",
      "Iteration 15, loss = 0.27977516\n",
      "Iteration 7, loss = 0.28316784\n",
      "Iteration 16, loss = 0.27922831\n",
      "Iteration 8, loss = 0.28238465\n",
      "Iteration 17, loss = 0.27964437\n",
      "Iteration 9, loss = 0.28220640\n",
      "Iteration 18, loss = 0.27866302\n",
      "Iteration 10, loss = 0.28160115\n",
      "Iteration 19, loss = 0.27835637\n",
      "Iteration 11, loss = 0.28158234\n",
      "Iteration 20, loss = 0.27812522\n",
      "Iteration 12, loss = 0.28073414\n",
      "Iteration 21, loss = 0.27766150\n",
      "Iteration 13, loss = 0.28066635\n",
      "Iteration 22, loss = 0.27755185\n",
      "Iteration 14, loss = 0.28021283\n",
      "Iteration 23, loss = 0.27701595\n",
      "Iteration 15, loss = 0.28007824\n",
      "Iteration 24, loss = 0.27665145\n",
      "Iteration 16, loss = 0.27956601\n",
      "Iteration 25, loss = 0.27653712\n",
      "Iteration 17, loss = 0.28006536\n",
      "Iteration 26, loss = 0.27618060\n",
      "Iteration 18, loss = 0.27881852\n",
      "Iteration 27, loss = 0.27589939\n",
      "Iteration 19, loss = 0.27870772\n",
      "Iteration 28, loss = 0.27590931\n",
      "Iteration 20, loss = 0.27862976\n",
      "Iteration 29, loss = 0.27548877\n",
      "Iteration 21, loss = 0.27832281\n",
      "Iteration 22, loss = 0.27788714\n",
      "Iteration 30, loss = 0.27526916\n",
      "Iteration 23, loss = 0.27743848\n",
      "Iteration 31, loss = 0.27515019\n",
      "Iteration 24, loss = 0.27728467\n",
      "Iteration 32, loss = 0.27517123\n",
      "Iteration 25, loss = 0.27716236\n",
      "Iteration 33, loss = 0.27482675\n",
      "Iteration 26, loss = 0.27676056\n",
      "Iteration 34, loss = 0.27451816\n",
      "Iteration 27, loss = 0.27655780\n",
      "Iteration 35, loss = 0.27481526\n",
      "Iteration 28, loss = 0.27669261\n",
      "Iteration 36, loss = 0.27436838\n",
      "Iteration 29, loss = 0.27612032\n",
      "Iteration 37, loss = 0.27417292\n",
      "Iteration 30, loss = 0.27586148\n",
      "Iteration 38, loss = 0.27407269\n",
      "Iteration 31, loss = 0.27574194\n",
      "Iteration 39, loss = 0.27404238\n",
      "Iteration 32, loss = 0.27569074\n",
      "Iteration 40, loss = 0.27393359\n",
      "Iteration 33, loss = 0.27540966\n",
      "Iteration 41, loss = 0.27372071\n",
      "Iteration 42, loss = 0.27393468\n",
      "Iteration 34, loss = 0.27514638\n",
      "Iteration 35, loss = 0.27544644\n",
      "Iteration 43, loss = 0.27344423\n",
      "Iteration 44, loss = 0.27382322\n",
      "Iteration 36, loss = 0.27479861\n",
      "Iteration 37, loss = 0.27470870\n",
      "Iteration 45, loss = 0.27362965\n",
      "Iteration 38, loss = 0.27466494\n",
      "Iteration 46, loss = 0.27346874\n",
      "Iteration 39, loss = 0.27450241\n",
      "Iteration 47, loss = 0.27326921\n",
      "Iteration 40, loss = 0.27432971\n",
      "Iteration 48, loss = 0.27307312\n",
      "Iteration 41, loss = 0.27407946\n",
      "Iteration 49, loss = 0.27339559\n",
      "Iteration 42, loss = 0.27395514\n",
      "Iteration 50, loss = 0.27309293\n",
      "Iteration 43, loss = 0.27374588\n",
      "Iteration 51, loss = 0.27295197\n",
      "Iteration 44, loss = 0.27376933\n",
      "Iteration 52, loss = 0.27301517\n",
      "Iteration 45, loss = 0.27372510\n",
      "Iteration 53, loss = 0.27274681\n",
      "Iteration 46, loss = 0.27352631\n",
      "Iteration 54, loss = 0.27260341\n",
      "Iteration 47, loss = 0.27332304\n",
      "Iteration 48, loss = 0.27316850\n",
      "Iteration 55, loss = 0.27307920\n",
      "Iteration 49, loss = 0.27356664\n",
      "Iteration 56, loss = 0.27265657\n",
      "Iteration 50, loss = 0.27325713\n",
      "Iteration 57, loss = 0.27270944\n",
      "Iteration 51, loss = 0.27309836\n",
      "Iteration 58, loss = 0.27228734\n",
      "Iteration 52, loss = 0.27306517\n",
      "Iteration 59, loss = 0.27246208\n",
      "Iteration 53, loss = 0.27277099\n",
      "Iteration 60, loss = 0.27232435\n",
      "Iteration 54, loss = 0.27271155\n",
      "Iteration 61, loss = 0.27221235\n",
      "Iteration 55, loss = 0.27339567\n",
      "Iteration 62, loss = 0.27201158\n",
      "Iteration 56, loss = 0.27262214\n",
      "Iteration 63, loss = 0.27208110\n",
      "Iteration 57, loss = 0.27273515\n",
      "Iteration 64, loss = 0.27190648\n",
      "Iteration 58, loss = 0.27251885\n",
      "Iteration 65, loss = 0.27216057\n",
      "Iteration 59, loss = 0.27238612\n",
      "Iteration 66, loss = 0.27194113\n",
      "Iteration 60, loss = 0.27252088\n",
      "Iteration 67, loss = 0.27215776\n",
      "Iteration 61, loss = 0.27246496\n",
      "Iteration 68, loss = 0.27210839\n",
      "Iteration 62, loss = 0.27203249\n",
      "Iteration 69, loss = 0.27161472\n",
      "Iteration 63, loss = 0.27214795\n",
      "Iteration 70, loss = 0.27169239\n",
      "Iteration 64, loss = 0.27223718\n",
      "Iteration 71, loss = 0.27182675\n",
      "Iteration 65, loss = 0.27207459\n",
      "Iteration 72, loss = 0.27158203\n",
      "Iteration 66, loss = 0.27221680\n",
      "Iteration 73, loss = 0.27158695\n",
      "Iteration 67, loss = 0.27217603\n",
      "Iteration 74, loss = 0.27139697\n",
      "Iteration 68, loss = 0.27210761\n",
      "Iteration 75, loss = 0.27143948\n",
      "Iteration 69, loss = 0.27174294\n",
      "Iteration 76, loss = 0.27161932\n",
      "Iteration 70, loss = 0.27181719\n",
      "Iteration 77, loss = 0.27152447\n",
      "Iteration 71, loss = 0.27203996\n",
      "Iteration 78, loss = 0.27141659\n",
      "Iteration 72, loss = 0.27183329\n",
      "Iteration 79, loss = 0.27108702\n",
      "Iteration 73, loss = 0.27190705\n",
      "Iteration 80, loss = 0.27113858\n",
      "Iteration 74, loss = 0.27177665\n",
      "Iteration 81, loss = 0.27113506\n",
      "Iteration 75, loss = 0.27180262\n",
      "Iteration 82, loss = 0.27109827\n",
      "Iteration 76, loss = 0.27171193\n",
      "Iteration 83, loss = 0.27124484\n",
      "Iteration 77, loss = 0.27174830\n",
      "Iteration 84, loss = 0.27095882\n",
      "Iteration 78, loss = 0.27185184\n",
      "Iteration 85, loss = 0.27118511\n",
      "Iteration 79, loss = 0.27146924\n",
      "Iteration 86, loss = 0.27101856\n",
      "Iteration 80, loss = 0.27148982\n",
      "Iteration 87, loss = 0.27098593\n",
      "Iteration 81, loss = 0.27135710\n",
      "Iteration 88, loss = 0.27106681\n",
      "Iteration 82, loss = 0.27132702\n",
      "Iteration 89, loss = 0.27128888\n",
      "Iteration 83, loss = 0.27154481\n",
      "Iteration 90, loss = 0.27132815\n",
      "Iteration 84, loss = 0.27147439\n",
      "Iteration 91, loss = 0.27066386\n",
      "Iteration 85, loss = 0.27146210\n",
      "Iteration 92, loss = 0.27078526\n",
      "Iteration 86, loss = 0.27148589\n",
      "Iteration 93, loss = 0.27055490\n",
      "Iteration 87, loss = 0.27118012\n",
      "Iteration 94, loss = 0.27080716\n",
      "Iteration 88, loss = 0.27118333\n",
      "Iteration 95, loss = 0.27068615\n",
      "Iteration 89, loss = 0.27158964\n",
      "Iteration 96, loss = 0.27074826\n",
      "Iteration 90, loss = 0.27141476\n",
      "Iteration 97, loss = 0.27044296\n",
      "Iteration 91, loss = 0.27088142\n",
      "Iteration 98, loss = 0.27068840\n",
      "Iteration 92, loss = 0.27116444\n",
      "Iteration 99, loss = 0.27074822\n",
      "Iteration 93, loss = 0.27086750\n",
      "Iteration 100, loss = 0.27049084\n",
      "Iteration 94, loss = 0.27103313\n",
      "Iteration 101, loss = 0.27035498\n",
      "Iteration 95, loss = 0.27101776\n",
      "Iteration 102, loss = 0.27048001\n",
      "Iteration 96, loss = 0.27106309\n",
      "Iteration 103, loss = 0.27040058\n",
      "Iteration 104, loss = 0.27040399\n",
      "Iteration 97, loss = 0.27084507\n",
      "Iteration 105, loss = 0.27021630\n",
      "Iteration 98, loss = 0.27056944\n",
      "Iteration 106, loss = 0.27022919\n",
      "Iteration 99, loss = 0.27088991\n",
      "Iteration 100, loss = 0.27070716\n",
      "Iteration 107, loss = 0.27037497\n",
      "Iteration 108, loss = 0.27034544\n",
      "Iteration 101, loss = 0.27068617\n",
      "Iteration 109, loss = 0.27019795\n",
      "Iteration 102, loss = 0.27056307\n",
      "Iteration 110, loss = 0.26992512\n",
      "Iteration 103, loss = 0.27054553\n",
      "Iteration 111, loss = 0.27022375\n",
      "Iteration 104, loss = 0.27063196\n",
      "Iteration 112, loss = 0.27005308\n",
      "Iteration 105, loss = 0.27045699\n",
      "Iteration 113, loss = 0.27016194\n",
      "Iteration 106, loss = 0.27038351\n",
      "Iteration 114, loss = 0.27008006\n",
      "Iteration 107, loss = 0.27054140\n",
      "Iteration 115, loss = 0.27007958\n",
      "Iteration 108, loss = 0.27043404\n",
      "Iteration 116, loss = 0.27003183\n",
      "Iteration 109, loss = 0.27032428\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 117, loss = 0.27000959\n",
      "Iteration 118, loss = 0.26990401\n",
      "Iteration 1, loss = 0.32774431\n",
      "Iteration 119, loss = 0.27003628\n",
      "Iteration 2, loss = 0.28972592\n",
      "Iteration 120, loss = 0.26998499\n",
      "Iteration 3, loss = 0.28598457\n",
      "Iteration 121, loss = 0.26998568\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 4, loss = 0.28477971\n",
      "Iteration 5, loss = 0.28431533\n",
      "Iteration 1, loss = 0.32738795\n",
      "Iteration 6, loss = 0.28360906\n",
      "Iteration 2, loss = 0.28975455\n",
      "Iteration 7, loss = 0.28329716\n",
      "Iteration 3, loss = 0.28574341\n",
      "Iteration 8, loss = 0.28255822\n",
      "Iteration 4, loss = 0.28472986\n",
      "Iteration 9, loss = 0.28238270\n",
      "Iteration 5, loss = 0.28394608\n",
      "Iteration 6, loss = 0.28352784\n",
      "Iteration 10, loss = 0.28183491\n",
      "Iteration 7, loss = 0.28313119\n",
      "Iteration 8, loss = 0.28266017\n",
      "Iteration 11, loss = 0.28183092\n",
      "Iteration 9, loss = 0.28229546\n",
      "Iteration 12, loss = 0.28099523\n",
      "Iteration 10, loss = 0.28177806\n",
      "Iteration 13, loss = 0.28102077\n",
      "Iteration 11, loss = 0.28103251\n",
      "Iteration 12, loss = 0.28084576\n",
      "Iteration 14, loss = 0.28091339\n",
      "Iteration 13, loss = 0.28034609\n",
      "Iteration 15, loss = 0.28047428\n",
      "Iteration 14, loss = 0.28010057\n",
      "Iteration 16, loss = 0.27995249\n",
      "Iteration 15, loss = 0.27971574\n",
      "Iteration 17, loss = 0.28047401\n",
      "Iteration 16, loss = 0.27909381\n",
      "Iteration 18, loss = 0.27924926\n",
      "Iteration 17, loss = 0.27923699\n",
      "Iteration 19, loss = 0.27906055\n",
      "Iteration 18, loss = 0.27875939\n",
      "Iteration 20, loss = 0.27885103\n",
      "Iteration 19, loss = 0.27806824\n",
      "Iteration 21, loss = 0.27851428\n",
      "Iteration 20, loss = 0.27807514\n",
      "Iteration 22, loss = 0.27808957\n",
      "Iteration 21, loss = 0.27784126\n",
      "Iteration 23, loss = 0.27783866\n",
      "Iteration 22, loss = 0.27739811\n",
      "Iteration 24, loss = 0.27754344\n",
      "Iteration 23, loss = 0.27745585\n",
      "Iteration 25, loss = 0.27721299\n",
      "Iteration 24, loss = 0.27711824\n",
      "Iteration 26, loss = 0.27699287\n",
      "Iteration 25, loss = 0.27714629\n",
      "Iteration 27, loss = 0.27662715\n",
      "Iteration 28, loss = 0.27660907\n",
      "Iteration 26, loss = 0.27643351\n",
      "Iteration 29, loss = 0.27612432\n",
      "Iteration 27, loss = 0.27643872\n",
      "Iteration 30, loss = 0.27605502\n",
      "Iteration 28, loss = 0.27610747\n",
      "Iteration 29, loss = 0.27621357\n",
      "Iteration 31, loss = 0.27577140\n",
      "Iteration 30, loss = 0.27600945\n",
      "Iteration 32, loss = 0.27563820\n",
      "Iteration 31, loss = 0.27577539\n",
      "Iteration 33, loss = 0.27559286\n",
      "Iteration 32, loss = 0.27557871\n",
      "Iteration 34, loss = 0.27536570\n",
      "Iteration 33, loss = 0.27546708\n",
      "Iteration 35, loss = 0.27558879\n",
      "Iteration 34, loss = 0.27540658\n",
      "Iteration 36, loss = 0.27499662\n",
      "Iteration 35, loss = 0.27531372\n",
      "Iteration 36, loss = 0.27506860\n",
      "Iteration 37, loss = 0.27490313\n",
      "Iteration 38, loss = 0.27479602\n",
      "Iteration 37, loss = 0.27489711\n",
      "Iteration 38, loss = 0.27495432\n",
      "Iteration 39, loss = 0.27466658\n",
      "Iteration 40, loss = 0.27449669\n",
      "Iteration 39, loss = 0.27477643\n",
      "Iteration 41, loss = 0.27420875\n",
      "Iteration 40, loss = 0.27443563\n",
      "Iteration 42, loss = 0.27419163\n",
      "Iteration 41, loss = 0.27432048\n",
      "Iteration 43, loss = 0.27405141\n",
      "Iteration 42, loss = 0.27439491\n",
      "Iteration 44, loss = 0.27415346\n",
      "Iteration 43, loss = 0.27436721\n",
      "Iteration 45, loss = 0.27415910\n",
      "Iteration 44, loss = 0.27439604\n",
      "Iteration 46, loss = 0.27391719\n",
      "Iteration 45, loss = 0.27412852\n",
      "Iteration 47, loss = 0.27362551\n",
      "Iteration 46, loss = 0.27392475\n",
      "Iteration 48, loss = 0.27347091\n",
      "Iteration 47, loss = 0.27397731\n",
      "Iteration 49, loss = 0.27392685\n",
      "Iteration 48, loss = 0.27355103\n",
      "Iteration 50, loss = 0.27332698\n",
      "Iteration 49, loss = 0.27349264\n",
      "Iteration 51, loss = 0.27328591\n",
      "Iteration 50, loss = 0.27358493\n",
      "Iteration 52, loss = 0.27313973\n",
      "Iteration 51, loss = 0.27340671\n",
      "Iteration 53, loss = 0.27297746\n",
      "Iteration 52, loss = 0.27360308\n",
      "Iteration 54, loss = 0.27290129\n",
      "Iteration 53, loss = 0.27339511\n",
      "Iteration 55, loss = 0.27349531\n",
      "Iteration 54, loss = 0.27315876\n",
      "Iteration 56, loss = 0.27274903\n",
      "Iteration 55, loss = 0.27308759\n",
      "Iteration 57, loss = 0.27264804\n",
      "Iteration 56, loss = 0.27291187\n",
      "Iteration 58, loss = 0.27266630\n",
      "Iteration 57, loss = 0.27302157\n",
      "Iteration 59, loss = 0.27257528\n",
      "Iteration 58, loss = 0.27299008\n",
      "Iteration 59, loss = 0.27265675\n",
      "Iteration 60, loss = 0.27247178\n",
      "Iteration 60, loss = 0.27278196\n",
      "Iteration 61, loss = 0.27252555\n",
      "Iteration 61, loss = 0.27259747\n",
      "Iteration 62, loss = 0.27240079\n",
      "Iteration 62, loss = 0.27250875\n",
      "Iteration 63, loss = 0.27227481\n",
      "Iteration 63, loss = 0.27267854\n",
      "Iteration 64, loss = 0.27198717\n",
      "Iteration 64, loss = 0.27235726\n",
      "Iteration 65, loss = 0.27204094\n",
      "Iteration 65, loss = 0.27237802\n",
      "Iteration 66, loss = 0.27205669\n",
      "Iteration 66, loss = 0.27238063\n",
      "Iteration 67, loss = 0.27217078\n",
      "Iteration 67, loss = 0.27229035\n",
      "Iteration 68, loss = 0.27209654\n",
      "Iteration 68, loss = 0.27239173\n",
      "Iteration 69, loss = 0.27167227\n",
      "Iteration 69, loss = 0.27205851\n",
      "Iteration 70, loss = 0.27190312\n",
      "Iteration 70, loss = 0.27219987\n",
      "Iteration 71, loss = 0.27187899\n",
      "Iteration 71, loss = 0.27189254\n",
      "Iteration 72, loss = 0.27156910\n",
      "Iteration 72, loss = 0.27213093\n",
      "Iteration 73, loss = 0.27194393\n",
      "Iteration 73, loss = 0.27219431\n",
      "Iteration 74, loss = 0.27170569\n",
      "Iteration 74, loss = 0.27209430\n",
      "Iteration 75, loss = 0.27163708\n",
      "Iteration 75, loss = 0.27196266\n",
      "Iteration 76, loss = 0.27176817\n",
      "Iteration 76, loss = 0.27205215\n",
      "Iteration 77, loss = 0.27159530\n",
      "Iteration 77, loss = 0.27182205\n",
      "Iteration 78, loss = 0.27160354\n",
      "Iteration 78, loss = 0.27166797\n",
      "Iteration 79, loss = 0.27140133\n",
      "Iteration 79, loss = 0.27163997\n",
      "Iteration 80, loss = 0.27126178\n",
      "Iteration 80, loss = 0.27177537\n",
      "Iteration 81, loss = 0.27135123\n",
      "Iteration 81, loss = 0.27163229\n",
      "Iteration 82, loss = 0.27125057\n",
      "Iteration 82, loss = 0.27153537\n",
      "Iteration 83, loss = 0.27126548\n",
      "Iteration 83, loss = 0.27183938\n",
      "Iteration 84, loss = 0.27142033\n",
      "Iteration 84, loss = 0.27153331\n",
      "Iteration 85, loss = 0.27128120\n",
      "Iteration 85, loss = 0.27167941\n",
      "Iteration 86, loss = 0.27134681\n",
      "Iteration 86, loss = 0.27167637\n",
      "Iteration 87, loss = 0.27121620\n",
      "Iteration 87, loss = 0.27162327\n",
      "Iteration 88, loss = 0.27115870\n",
      "Iteration 88, loss = 0.27146093\n",
      "Iteration 89, loss = 0.27118462\n",
      "Iteration 89, loss = 0.27151679\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 90, loss = 0.27119885\n",
      "Iteration 91, loss = 0.27107497\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.32877859\n",
      "Iteration 2, loss = 0.29101993\n",
      "Iteration 1, loss = 0.32838468\n",
      "Iteration 3, loss = 0.28716665\n",
      "Iteration 2, loss = 0.29013381\n",
      "Iteration 4, loss = 0.28596799\n",
      "Iteration 3, loss = 0.28616336\n",
      "Iteration 5, loss = 0.28535405\n",
      "Iteration 4, loss = 0.28485060\n",
      "Iteration 6, loss = 0.28492176\n",
      "Iteration 5, loss = 0.28443951\n",
      "Iteration 7, loss = 0.28465374\n",
      "Iteration 8, loss = 0.28387005\n",
      "Iteration 6, loss = 0.28408499\n",
      "Iteration 9, loss = 0.28374097\n",
      "Iteration 7, loss = 0.28351488\n",
      "Iteration 10, loss = 0.28329042\n",
      "Iteration 8, loss = 0.28309346\n",
      "Iteration 11, loss = 0.28237172\n",
      "Iteration 9, loss = 0.28314560\n",
      "Iteration 12, loss = 0.28230645\n",
      "Iteration 10, loss = 0.28223452\n",
      "Iteration 13, loss = 0.28161423\n",
      "Iteration 11, loss = 0.28166564\n",
      "Iteration 14, loss = 0.28133207\n",
      "Iteration 12, loss = 0.28144853\n",
      "Iteration 15, loss = 0.28077966\n",
      "Iteration 13, loss = 0.28097544\n",
      "Iteration 16, loss = 0.28027357\n",
      "Iteration 14, loss = 0.28064879\n",
      "Iteration 15, loss = 0.28004553\n",
      "Iteration 17, loss = 0.28044746\n",
      "Iteration 16, loss = 0.27957324\n",
      "Iteration 18, loss = 0.27972039\n",
      "Iteration 17, loss = 0.27959589\n",
      "Iteration 19, loss = 0.27908865\n",
      "Iteration 18, loss = 0.27920465\n",
      "Iteration 20, loss = 0.27912448\n",
      "Iteration 19, loss = 0.27849189\n",
      "Iteration 21, loss = 0.27860438\n",
      "Iteration 20, loss = 0.27841106\n",
      "Iteration 22, loss = 0.27846546\n",
      "Iteration 21, loss = 0.27793465\n",
      "Iteration 23, loss = 0.27825335\n",
      "Iteration 22, loss = 0.27768795\n",
      "Iteration 24, loss = 0.27808815\n",
      "Iteration 23, loss = 0.27731039\n",
      "Iteration 25, loss = 0.27813067\n",
      "Iteration 24, loss = 0.27710670\n",
      "Iteration 26, loss = 0.27743446\n",
      "Iteration 25, loss = 0.27716885\n",
      "Iteration 27, loss = 0.27763109\n",
      "Iteration 26, loss = 0.27650091\n",
      "Iteration 28, loss = 0.27707007\n",
      "Iteration 27, loss = 0.27647239\n",
      "Iteration 28, loss = 0.27619857\n",
      "Iteration 29, loss = 0.27724689\n",
      "Iteration 30, loss = 0.27701322\n",
      "Iteration 29, loss = 0.27620838\n",
      "Iteration 31, loss = 0.27677114\n",
      "Iteration 30, loss = 0.27590422\n",
      "Iteration 31, loss = 0.27574886\n",
      "Iteration 32, loss = 0.27653409\n",
      "Iteration 32, loss = 0.27565375\n",
      "Iteration 33, loss = 0.27544744\n",
      "Iteration 33, loss = 0.27633402\n",
      "Iteration 34, loss = 0.27527410\n",
      "Iteration 34, loss = 0.27617907\n",
      "Iteration 35, loss = 0.27517852\n",
      "Iteration 36, loss = 0.27502830\n",
      "Iteration 35, loss = 0.27606471\n",
      "Iteration 36, loss = 0.27596276\n",
      "Iteration 37, loss = 0.27501616\n",
      "Iteration 37, loss = 0.27604673\n",
      "Iteration 38, loss = 0.27467343\n",
      "Iteration 38, loss = 0.27567978\n",
      "Iteration 39, loss = 0.27461491\n",
      "Iteration 39, loss = 0.27570169\n",
      "Iteration 40, loss = 0.27449398\n",
      "Iteration 40, loss = 0.27542401\n",
      "Iteration 41, loss = 0.27429883\n",
      "Iteration 41, loss = 0.27521617\n",
      "Iteration 42, loss = 0.27451379\n",
      "Iteration 43, loss = 0.27425363\n",
      "Iteration 42, loss = 0.27541282\n",
      "Iteration 44, loss = 0.27439610\n",
      "Iteration 43, loss = 0.27518374\n",
      "Iteration 45, loss = 0.27422731\n",
      "Iteration 44, loss = 0.27529496\n",
      "Iteration 46, loss = 0.27358866\n",
      "Iteration 45, loss = 0.27519669\n",
      "Iteration 47, loss = 0.27376806\n",
      "Iteration 46, loss = 0.27493882\n",
      "Iteration 48, loss = 0.27345336\n",
      "Iteration 47, loss = 0.27490959\n",
      "Iteration 49, loss = 0.27337599\n",
      "Iteration 48, loss = 0.27472868\n",
      "Iteration 50, loss = 0.27336101\n",
      "Iteration 49, loss = 0.27454767\n",
      "Iteration 51, loss = 0.27322771\n",
      "Iteration 50, loss = 0.27453291\n",
      "Iteration 52, loss = 0.27330035\n",
      "Iteration 51, loss = 0.27438303\n",
      "Iteration 53, loss = 0.27308473\n",
      "Iteration 52, loss = 0.27464979\n",
      "Iteration 54, loss = 0.27295362\n",
      "Iteration 53, loss = 0.27435187\n",
      "Iteration 55, loss = 0.27271263\n",
      "Iteration 54, loss = 0.27427361\n",
      "Iteration 56, loss = 0.27281847\n",
      "Iteration 55, loss = 0.27412036\n",
      "Iteration 57, loss = 0.27282637\n",
      "Iteration 56, loss = 0.27390306\n",
      "Iteration 58, loss = 0.27265486\n",
      "Iteration 59, loss = 0.27251244\n",
      "Iteration 57, loss = 0.27414485\n",
      "Iteration 58, loss = 0.27396416\n",
      "Iteration 60, loss = 0.27253377\n",
      "Iteration 59, loss = 0.27383616\n",
      "Iteration 61, loss = 0.27243282\n",
      "Iteration 60, loss = 0.27378555\n",
      "Iteration 62, loss = 0.27235188\n",
      "Iteration 61, loss = 0.27355066\n",
      "Iteration 63, loss = 0.27259524\n",
      "Iteration 62, loss = 0.27377659\n",
      "Iteration 64, loss = 0.27224171\n",
      "Iteration 63, loss = 0.27376775\n",
      "Iteration 65, loss = 0.27218150\n",
      "Iteration 66, loss = 0.27231993\n",
      "Iteration 64, loss = 0.27345035\n",
      "Iteration 65, loss = 0.27344710\n",
      "Iteration 67, loss = 0.27218278\n",
      "Iteration 68, loss = 0.27214737\n",
      "Iteration 66, loss = 0.27361456\n",
      "Iteration 69, loss = 0.27208619\n",
      "Iteration 67, loss = 0.27331367\n",
      "Iteration 70, loss = 0.27181406\n",
      "Iteration 68, loss = 0.27338325\n",
      "Iteration 71, loss = 0.27180531\n",
      "Iteration 69, loss = 0.27327740\n",
      "Iteration 72, loss = 0.27215167\n",
      "Iteration 70, loss = 0.27326281\n",
      "Iteration 73, loss = 0.27198931\n",
      "Iteration 71, loss = 0.27294350\n",
      "Iteration 74, loss = 0.27172362\n",
      "Iteration 72, loss = 0.27316595\n",
      "Iteration 75, loss = 0.27169933\n",
      "Iteration 73, loss = 0.27323104\n",
      "Iteration 76, loss = 0.27183984\n",
      "Iteration 74, loss = 0.27311074\n",
      "Iteration 77, loss = 0.27182152\n",
      "Iteration 75, loss = 0.27313017\n",
      "Iteration 78, loss = 0.27151214\n",
      "Iteration 76, loss = 0.27288188\n",
      "Iteration 79, loss = 0.27172359\n",
      "Iteration 77, loss = 0.27301218\n",
      "Iteration 80, loss = 0.27167104\n",
      "Iteration 78, loss = 0.27275721\n",
      "Iteration 81, loss = 0.27162352\n",
      "Iteration 79, loss = 0.27288311\n",
      "Iteration 82, loss = 0.27156654\n",
      "Iteration 80, loss = 0.27304443\n",
      "Iteration 83, loss = 0.27153081\n",
      "Iteration 81, loss = 0.27270016\n",
      "Iteration 84, loss = 0.27136136\n",
      "Iteration 82, loss = 0.27256388\n",
      "Iteration 85, loss = 0.27146927\n",
      "Iteration 83, loss = 0.27287664\n",
      "Iteration 86, loss = 0.27183618\n",
      "Iteration 84, loss = 0.27261737\n",
      "Iteration 87, loss = 0.27147067\n",
      "Iteration 85, loss = 0.27258644\n",
      "Iteration 88, loss = 0.27111658\n",
      "Iteration 86, loss = 0.27294175\n",
      "Iteration 89, loss = 0.27152335\n",
      "Iteration 87, loss = 0.27258163\n",
      "Iteration 90, loss = 0.27116275\n",
      "Iteration 91, loss = 0.27130624\n",
      "Iteration 88, loss = 0.27250631\n",
      "Iteration 92, loss = 0.27125958\n",
      "Iteration 89, loss = 0.27255652\n",
      "Iteration 90, loss = 0.27231272\n",
      "Iteration 93, loss = 0.27121547\n",
      "Iteration 94, loss = 0.27118393\n",
      "Iteration 91, loss = 0.27240578\n",
      "Iteration 95, loss = 0.27101539\n",
      "Iteration 92, loss = 0.27257653\n",
      "Iteration 96, loss = 0.27115471\n",
      "Iteration 93, loss = 0.27240426\n",
      "Iteration 97, loss = 0.27126484\n",
      "Iteration 94, loss = 0.27239885\n",
      "Iteration 98, loss = 0.27108864\n",
      "Iteration 95, loss = 0.27229277\n",
      "Iteration 99, loss = 0.27103700\n",
      "Iteration 96, loss = 0.27238598\n",
      "Iteration 100, loss = 0.27092602\n",
      "Iteration 97, loss = 0.27261791\n",
      "Iteration 101, loss = 0.27067816\n",
      "Iteration 98, loss = 0.27233151\n",
      "Iteration 102, loss = 0.27094372\n",
      "Iteration 99, loss = 0.27220012\n",
      "Iteration 103, loss = 0.27076351\n",
      "Iteration 100, loss = 0.27224182\n",
      "Iteration 104, loss = 0.27072733\n",
      "Iteration 101, loss = 0.27211393\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 105, loss = 0.27089950\n",
      "Iteration 106, loss = 0.27083259\n",
      "Iteration 107, loss = 0.27082768\n",
      "Iteration 1, loss = 0.32878355\n",
      "Iteration 108, loss = 0.27072702\n",
      "Iteration 2, loss = 0.29026042\n",
      "Iteration 109, loss = 0.27076065\n",
      "Iteration 3, loss = 0.28637059\n",
      "Iteration 110, loss = 0.27070905\n",
      "Iteration 4, loss = 0.28521781\n",
      "Iteration 111, loss = 0.27064472\n",
      "Iteration 5, loss = 0.28466888\n",
      "Iteration 112, loss = 0.27094801\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 6, loss = 0.28432109\n",
      "Iteration 7, loss = 0.28368619\n",
      "Iteration 1, loss = 0.32862073\n",
      "Iteration 8, loss = 0.28322703\n",
      "Iteration 2, loss = 0.28980013\n",
      "Iteration 9, loss = 0.28338891\n",
      "Iteration 3, loss = 0.28593462\n",
      "Iteration 10, loss = 0.28228886\n",
      "Iteration 4, loss = 0.28483775\n",
      "Iteration 11, loss = 0.28174158\n",
      "Iteration 5, loss = 0.28419276\n",
      "Iteration 12, loss = 0.28177573\n",
      "Iteration 6, loss = 0.28392045\n",
      "Iteration 13, loss = 0.28131847\n",
      "Iteration 7, loss = 0.28324904\n",
      "Iteration 14, loss = 0.28115193\n",
      "Iteration 8, loss = 0.28280284\n",
      "Iteration 15, loss = 0.28053205\n",
      "Iteration 9, loss = 0.28281466\n",
      "Iteration 16, loss = 0.28001556\n",
      "Iteration 10, loss = 0.28191931\n",
      "Iteration 17, loss = 0.27993334\n",
      "Iteration 11, loss = 0.28145265\n",
      "Iteration 18, loss = 0.27939236\n",
      "Iteration 12, loss = 0.28126256\n",
      "Iteration 19, loss = 0.27889176\n",
      "Iteration 13, loss = 0.28082029\n",
      "Iteration 20, loss = 0.27879133\n",
      "Iteration 14, loss = 0.28110805\n",
      "Iteration 21, loss = 0.27848640\n",
      "Iteration 15, loss = 0.28033499\n",
      "Iteration 22, loss = 0.27814794\n",
      "Iteration 16, loss = 0.28005016\n",
      "Iteration 23, loss = 0.27790734\n",
      "Iteration 17, loss = 0.27951556\n",
      "Iteration 24, loss = 0.27786515\n",
      "Iteration 18, loss = 0.27910715\n",
      "Iteration 25, loss = 0.27757021\n",
      "Iteration 19, loss = 0.27860116\n",
      "Iteration 26, loss = 0.27729886\n",
      "Iteration 20, loss = 0.27858674\n",
      "Iteration 27, loss = 0.27703327\n",
      "Iteration 21, loss = 0.27824296\n",
      "Iteration 28, loss = 0.27685321\n",
      "Iteration 22, loss = 0.27781035\n",
      "Iteration 29, loss = 0.27684871\n",
      "Iteration 23, loss = 0.27773817\n",
      "Iteration 30, loss = 0.27659032\n",
      "Iteration 24, loss = 0.27743317\n",
      "Iteration 31, loss = 0.27645487\n",
      "Iteration 25, loss = 0.27725872\n",
      "Iteration 32, loss = 0.27640433\n",
      "Iteration 26, loss = 0.27698774\n",
      "Iteration 33, loss = 0.27604357\n",
      "Iteration 27, loss = 0.27673225\n",
      "Iteration 34, loss = 0.27570244\n",
      "Iteration 28, loss = 0.27641666\n",
      "Iteration 35, loss = 0.27575943\n",
      "Iteration 29, loss = 0.27664781\n",
      "Iteration 36, loss = 0.27559566\n",
      "Iteration 30, loss = 0.27626072\n",
      "Iteration 37, loss = 0.27546439\n",
      "Iteration 31, loss = 0.27636545\n",
      "Iteration 38, loss = 0.27530039\n",
      "Iteration 32, loss = 0.27607387\n",
      "Iteration 33, loss = 0.27576453\n",
      "Iteration 39, loss = 0.27523419\n",
      "Iteration 34, loss = 0.27570289\n",
      "Iteration 40, loss = 0.27512938\n",
      "Iteration 35, loss = 0.27558600\n",
      "Iteration 41, loss = 0.27516698\n",
      "Iteration 36, loss = 0.27552919\n",
      "Iteration 42, loss = 0.27499314\n",
      "Iteration 37, loss = 0.27547929\n",
      "Iteration 38, loss = 0.27535038\n",
      "Iteration 43, loss = 0.27481708\n",
      "Iteration 39, loss = 0.27536835\n",
      "Iteration 44, loss = 0.27482779\n",
      "Iteration 40, loss = 0.27524099\n",
      "Iteration 45, loss = 0.27448189\n",
      "Iteration 46, loss = 0.27431394Iteration 41, loss = 0.27524708\n",
      "\n",
      "Iteration 47, loss = 0.27433324\n",
      "Iteration 42, loss = 0.27517342\n",
      "Iteration 48, loss = 0.27423625\n",
      "Iteration 43, loss = 0.27512227\n",
      "Iteration 49, loss = 0.27403680\n",
      "Iteration 44, loss = 0.27516647\n",
      "Iteration 50, loss = 0.27401439\n",
      "Iteration 45, loss = 0.27482493\n",
      "Iteration 51, loss = 0.27390998\n",
      "Iteration 46, loss = 0.27473299\n",
      "Iteration 52, loss = 0.27404827\n",
      "Iteration 47, loss = 0.27484483\n",
      "Iteration 53, loss = 0.27384063\n",
      "Iteration 48, loss = 0.27461714\n",
      "Iteration 54, loss = 0.27377815\n",
      "Iteration 49, loss = 0.27457175\n",
      "Iteration 55, loss = 0.27367416\n",
      "Iteration 50, loss = 0.27449756\n",
      "Iteration 56, loss = 0.27371069\n",
      "Iteration 51, loss = 0.27454176\n",
      "Iteration 57, loss = 0.27353776\n",
      "Iteration 52, loss = 0.27451752\n",
      "Iteration 58, loss = 0.27356655\n",
      "Iteration 53, loss = 0.27437062\n",
      "Iteration 59, loss = 0.27353419\n",
      "Iteration 54, loss = 0.27432515\n",
      "Iteration 60, loss = 0.27326943\n",
      "Iteration 55, loss = 0.27400825\n",
      "Iteration 61, loss = 0.27328981\n",
      "Iteration 56, loss = 0.27425690\n",
      "Iteration 57, loss = 0.27405602\n",
      "Iteration 62, loss = 0.27335035\n",
      "Iteration 58, loss = 0.27414722\n",
      "Iteration 63, loss = 0.27328247\n",
      "Iteration 59, loss = 0.27404236\n",
      "Iteration 64, loss = 0.27326584\n",
      "Iteration 60, loss = 0.27390802\n",
      "Iteration 61, loss = 0.27380190\n",
      "Iteration 65, loss = 0.27316212\n",
      "Iteration 66, loss = 0.27323561\n",
      "Iteration 62, loss = 0.27386891\n",
      "Iteration 67, loss = 0.27308780\n",
      "Iteration 63, loss = 0.27384686\n",
      "Iteration 68, loss = 0.27332197\n",
      "Iteration 69, loss = 0.27294316\n",
      "Iteration 64, loss = 0.27387163\n",
      "Iteration 70, loss = 0.27299655\n",
      "Iteration 65, loss = 0.27366931\n",
      "Iteration 71, loss = 0.27286519\n",
      "Iteration 66, loss = 0.27362766\n",
      "Iteration 72, loss = 0.27301192\n",
      "Iteration 67, loss = 0.27372757\n",
      "Iteration 73, loss = 0.27293771\n",
      "Iteration 68, loss = 0.27360693\n",
      "Iteration 74, loss = 0.27285511\n",
      "Iteration 69, loss = 0.27354721\n",
      "Iteration 75, loss = 0.27266455\n",
      "Iteration 70, loss = 0.27343054\n",
      "Iteration 76, loss = 0.27285224\n",
      "Iteration 71, loss = 0.27345280\n",
      "Iteration 77, loss = 0.27278738\n",
      "Iteration 72, loss = 0.27354699\n",
      "Iteration 78, loss = 0.27277894\n",
      "Iteration 73, loss = 0.27322290\n",
      "Iteration 79, loss = 0.27268251\n",
      "Iteration 74, loss = 0.27333116\n",
      "Iteration 80, loss = 0.27269376\n",
      "Iteration 75, loss = 0.27318303\n",
      "Iteration 81, loss = 0.27246544\n",
      "Iteration 76, loss = 0.27321398\n",
      "Iteration 82, loss = 0.27274465\n",
      "Iteration 77, loss = 0.27331499\n",
      "Iteration 83, loss = 0.27262158\n",
      "Iteration 78, loss = 0.27305783\n",
      "Iteration 84, loss = 0.27244403\n",
      "Iteration 79, loss = 0.27299785\n",
      "Iteration 85, loss = 0.27246979\n",
      "Iteration 80, loss = 0.27323911\n",
      "Iteration 86, loss = 0.27263356\n",
      "Iteration 81, loss = 0.27296415\n",
      "Iteration 87, loss = 0.27243144\n",
      "Iteration 82, loss = 0.27297013\n",
      "Iteration 88, loss = 0.27237669\n",
      "Iteration 83, loss = 0.27317048\n",
      "Iteration 84, loss = 0.27274619\n",
      "Iteration 89, loss = 0.27238645\n",
      "Iteration 85, loss = 0.27279880\n",
      "Iteration 90, loss = 0.27208362\n",
      "Iteration 86, loss = 0.27310859\n",
      "Iteration 91, loss = 0.27215823\n",
      "Iteration 87, loss = 0.27297401\n",
      "Iteration 92, loss = 0.27217981\n",
      "Iteration 88, loss = 0.27275742\n",
      "Iteration 93, loss = 0.27233927\n",
      "Iteration 89, loss = 0.27270568\n",
      "Iteration 94, loss = 0.27212613\n",
      "Iteration 95, loss = 0.27212909\n",
      "Iteration 90, loss = 0.27267532\n",
      "Iteration 96, loss = 0.27201956\n",
      "Iteration 91, loss = 0.27258188\n",
      "Iteration 97, loss = 0.27214865\n",
      "Iteration 92, loss = 0.27257410\n",
      "Iteration 98, loss = 0.27197984\n",
      "Iteration 93, loss = 0.27261535\n",
      "Iteration 99, loss = 0.27195669\n",
      "Iteration 94, loss = 0.27248001\n",
      "Iteration 100, loss = 0.27195868\n",
      "Iteration 95, loss = 0.27236975\n",
      "Iteration 101, loss = 0.27178018\n",
      "Iteration 96, loss = 0.27249262\n",
      "Iteration 102, loss = 0.27199700\n",
      "Iteration 97, loss = 0.27239116\n",
      "Iteration 103, loss = 0.27177892\n",
      "Iteration 98, loss = 0.27235163\n",
      "Iteration 104, loss = 0.27167653\n",
      "Iteration 99, loss = 0.27230569\n",
      "Iteration 100, loss = 0.27215543\n",
      "Iteration 105, loss = 0.27197684\n",
      "Iteration 101, loss = 0.27198943\n",
      "Iteration 106, loss = 0.27195159\n",
      "Iteration 107, loss = 0.27197650\n",
      "Iteration 102, loss = 0.27226095\n",
      "Iteration 108, loss = 0.27180006\n",
      "Iteration 103, loss = 0.27223310\n",
      "Iteration 109, loss = 0.27173438\n",
      "Iteration 104, loss = 0.27203941\n",
      "Iteration 110, loss = 0.27177603\n",
      "Iteration 105, loss = 0.27214350\n",
      "Iteration 111, loss = 0.27186379\n",
      "Iteration 106, loss = 0.27223519\n",
      "Iteration 107, loss = 0.27219020\n",
      "Iteration 112, loss = 0.27163403\n",
      "Iteration 108, loss = 0.27204369\n",
      "Iteration 113, loss = 0.27173932\n",
      "Iteration 109, loss = 0.27181373\n",
      "Iteration 114, loss = 0.27159350\n",
      "Iteration 110, loss = 0.27221082\n",
      "Iteration 115, loss = 0.27152332\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 111, loss = 0.27185452\n",
      "Iteration 112, loss = 0.27192052\n",
      "Iteration 113, loss = 0.27192147\n",
      "Iteration 114, loss = 0.27169199\n",
      "Iteration 115, loss = 0.27166454\n",
      "Iteration 116, loss = 0.27161256\n",
      "Iteration 117, loss = 0.27175479\n",
      "Iteration 118, loss = 0.27165833\n",
      "Iteration 119, loss = 0.27165213\n",
      "Iteration 120, loss = 0.27159075\n",
      "Iteration 121, loss = 0.27171640\n",
      "Iteration 122, loss = 0.27147061\n",
      "Iteration 123, loss = 0.27167939\n",
      "Iteration 124, loss = 0.27138137\n",
      "Iteration 125, loss = 0.27158175\n",
      "Iteration 126, loss = 0.27137446\n",
      "Iteration 127, loss = 0.27158148\n",
      "Iteration 128, loss = 0.27131304\n",
      "Iteration 129, loss = 0.27144459\n",
      "Iteration 130, loss = 0.27147207\n",
      "Iteration 131, loss = 0.27138437\n",
      "Iteration 132, loss = 0.27130500\n",
      "Iteration 133, loss = 0.27120711\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.31991570\n",
      "Iteration 2, loss = 0.28526811\n",
      "Iteration 3, loss = 0.28186092\n",
      "Iteration 4, loss = 0.28073675\n",
      "Iteration 5, loss = 0.28043023\n",
      "Iteration 6, loss = 0.28004377\n",
      "Iteration 7, loss = 0.27933184\n",
      "Iteration 8, loss = 0.27894097\n",
      "Iteration 9, loss = 0.27891733\n",
      "Iteration 10, loss = 0.27867056\n",
      "Iteration 11, loss = 0.27807439\n",
      "Iteration 12, loss = 0.27793093\n",
      "Iteration 13, loss = 0.27770747\n",
      "Iteration 14, loss = 0.27749266\n",
      "Iteration 15, loss = 0.27697423\n",
      "Iteration 16, loss = 0.27732803\n",
      "Iteration 17, loss = 0.27665059\n",
      "Iteration 18, loss = 0.27640831\n",
      "Iteration 19, loss = 0.27636019\n",
      "Iteration 20, loss = 0.27596347\n",
      "Iteration 21, loss = 0.27578165\n",
      "Iteration 22, loss = 0.27580444\n",
      "Iteration 23, loss = 0.27550445\n",
      "Iteration 24, loss = 0.27539862\n",
      "Iteration 25, loss = 0.27508426\n",
      "Iteration 26, loss = 0.27517060\n",
      "Iteration 27, loss = 0.27529637\n",
      "Iteration 28, loss = 0.27476204\n",
      "Iteration 29, loss = 0.27449657\n",
      "Iteration 30, loss = 0.27501687\n",
      "Iteration 31, loss = 0.27455217\n",
      "Iteration 32, loss = 0.27446058\n",
      "Iteration 33, loss = 0.27420993\n",
      "Iteration 34, loss = 0.27422804\n",
      "Iteration 35, loss = 0.27421293\n",
      "Iteration 36, loss = 0.27405100\n",
      "Iteration 37, loss = 0.27412879\n",
      "Iteration 38, loss = 0.27415052\n",
      "Iteration 39, loss = 0.27408618\n",
      "Iteration 40, loss = 0.27380981\n",
      "Iteration 41, loss = 0.27386036\n",
      "Iteration 42, loss = 0.27359368\n",
      "Iteration 43, loss = 0.27385539\n",
      "Iteration 44, loss = 0.27356481\n",
      "Iteration 45, loss = 0.27355434\n",
      "Iteration 46, loss = 0.27356782\n",
      "Iteration 47, loss = 0.27358304\n",
      "Iteration 48, loss = 0.27336916\n",
      "Iteration 49, loss = 0.27339005\n",
      "Iteration 50, loss = 0.27342291\n",
      "Iteration 51, loss = 0.27350830\n",
      "Iteration 52, loss = 0.27319056\n",
      "Iteration 53, loss = 0.27331170\n",
      "Iteration 54, loss = 0.27318575\n",
      "Iteration 55, loss = 0.27320318\n",
      "Iteration 56, loss = 0.27320160\n",
      "Iteration 57, loss = 0.27331520\n",
      "Iteration 58, loss = 0.27301532\n",
      "Iteration 59, loss = 0.27302145\n",
      "Iteration 60, loss = 0.27313433\n",
      "Iteration 61, loss = 0.27296703\n",
      "Iteration 62, loss = 0.27295218\n",
      "Iteration 63, loss = 0.27287039\n",
      "Iteration 64, loss = 0.27310693\n",
      "Iteration 65, loss = 0.27292060\n",
      "Iteration 66, loss = 0.27262060\n",
      "Iteration 67, loss = 0.27299764\n",
      "Iteration 68, loss = 0.27289650\n",
      "Iteration 69, loss = 0.27273972\n",
      "Iteration 70, loss = 0.27278895\n",
      "Iteration 71, loss = 0.27267970\n",
      "Iteration 72, loss = 0.27287767\n",
      "Iteration 73, loss = 0.27249161\n",
      "Iteration 74, loss = 0.27249847\n",
      "Iteration 75, loss = 0.27267780\n",
      "Iteration 76, loss = 0.27246276\n",
      "Iteration 77, loss = 0.27297969\n",
      "Iteration 78, loss = 0.27237886\n",
      "Iteration 79, loss = 0.27243480\n",
      "Iteration 80, loss = 0.27243846\n",
      "Iteration 81, loss = 0.27266561\n",
      "Iteration 82, loss = 0.27241290\n",
      "Iteration 83, loss = 0.27269893\n",
      "Iteration 84, loss = 0.27239378\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Model Evaluation Metrics Using Untouched Test Dataset\n",
      "*****************************************************\n",
      "Model Training Time (s):   4.04837\n",
      "Model Prediction Time (s): 0.00292\n",
      "\n",
      "F1 Score:  0.34\n",
      "Accuracy:  0.90     AUC:       0.61\n",
      "Precision: 0.63     Recall:    0.23\n",
      "*****************************************************\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bankX,bankY = import_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(bankX),np.array(bankY), test_size=0.20)\n",
    "hyperNN(X_train, y_train, X_test, y_test,title=\"Model Complexity Curve for NN (Banking Data)\\nHyperparameter : No. Hidden Units\")\n",
    "h_units, learn_rate = NNGridSearchCV(X_train, y_train)\n",
    "estimator_bank = MLPClassifier(hidden_layer_sizes=(h_units,), solver='adam', activation='logistic', \n",
    "                               learning_rate_init=learn_rate, random_state=100,verbose=True)\n",
    "train_samp_bank, NN_train_score_bank, NN_fit_time_bank, NN_pred_time_bank = plot_learning_curve(estimator_bank, X_train, y_train,title=\"Neural Net Banking Data\")\n",
    "final_classifier_evaluation(estimator_bank, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankX,bankY = import_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(bankX),np.array(bankY), test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
