{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 45307 rows and 21 columns.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df_bank = pd.read_csv('/workspaces/7641HW1/data/BankMarketingData.csv')\n",
    "print(\"Data has\",len(df_bank),\"rows and\", len(df_bank.columns),\"columns.\")\n",
    "if df_bank.isnull().values.any():\n",
    "    print(\"Warning: Missing Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>housemaid</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.4y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.6y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>32</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.6y</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>wed</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.856</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>41</td>\n",
       "      <td>services</td>\n",
       "      <td>single</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>wed</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.856</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>59</td>\n",
       "      <td>housemaid</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.6y</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>wed</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.856</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>57</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.9y</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>wed</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.856</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>30</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>wed</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.856</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age           job  marital    education  default housing loan    contact  \\\n",
       "0     56     housemaid  married     basic.4y       no      no   no  telephone   \n",
       "1     57      services  married  high.school  unknown      no   no  telephone   \n",
       "2     37      services  married  high.school       no     yes   no  telephone   \n",
       "3     40        admin.  married     basic.6y       no      no   no  telephone   \n",
       "4     56      services  married  high.school       no      no  yes  telephone   \n",
       "..   ...           ...      ...          ...      ...     ...  ...        ...   \n",
       "995   32  entrepreneur  married     basic.6y       no     yes   no  telephone   \n",
       "996   41      services   single  high.school       no     yes  yes  telephone   \n",
       "997   59     housemaid  married     basic.6y       no     yes   no  telephone   \n",
       "998   57    technician  married     basic.9y       no     yes   no  telephone   \n",
       "999   30      services  married      unknown       no      no   no  telephone   \n",
       "\n",
       "    month day_of_week  ...  campaign  pdays  previous     poutcome  \\\n",
       "0     may         mon  ...         1    999         0  nonexistent   \n",
       "1     may         mon  ...         1    999         0  nonexistent   \n",
       "2     may         mon  ...         1    999         0  nonexistent   \n",
       "3     may         mon  ...         1    999         0  nonexistent   \n",
       "4     may         mon  ...         1    999         0  nonexistent   \n",
       "..    ...         ...  ...       ...    ...       ...          ...   \n",
       "995   may         wed  ...         1    999         0  nonexistent   \n",
       "996   may         wed  ...         1    999         0  nonexistent   \n",
       "997   may         wed  ...         1    999         0  nonexistent   \n",
       "998   may         wed  ...         1    999         0  nonexistent   \n",
       "999   may         wed  ...         3    999         0  nonexistent   \n",
       "\n",
       "    emp.var.rate  cons.price.idx  cons.conf.idx  euribor3m  nr.employed   y  \n",
       "0            1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "1            1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "2            1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "3            1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "4            1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "..           ...             ...            ...        ...          ...  ..  \n",
       "995          1.1          93.994          -36.4      4.856       5191.0  no  \n",
       "996          1.1          93.994          -36.4      4.856       5191.0  no  \n",
       "997          1.1          93.994          -36.4      4.856       5191.0  no  \n",
       "998          1.1          93.994          -36.4      4.856       5191.0  no  \n",
       "999          1.1          93.994          -36.4      4.856       5191.0  no  \n",
       "\n",
       "[1000 rows x 21 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_df = df_bank [:1000]\n",
    "subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31726/1444988200.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_bank['y'].replace(\"no\",0,inplace=True)\n",
      "/tmp/ipykernel_31726/1444988200.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_bank['y'].replace(\"yes\",1,inplace=True)\n",
      "/tmp/ipykernel_31726/1444988200.py:10: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_bank['y'].replace(\"yes\",1,inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>job_admin.</th>\n",
       "      <th>job_blue-collar</th>\n",
       "      <th>job_entrepreneur</th>\n",
       "      <th>job_housemaid</th>\n",
       "      <th>job_management</th>\n",
       "      <th>job_retired</th>\n",
       "      <th>job_self-employed</th>\n",
       "      <th>job_services</th>\n",
       "      <th>job_student</th>\n",
       "      <th>...</th>\n",
       "      <th>age</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>45307.0</td>\n",
       "      <td>45307</td>\n",
       "      <td>45307</td>\n",
       "      <td>45307</td>\n",
       "      <td>45307</td>\n",
       "      <td>45307</td>\n",
       "      <td>45307</td>\n",
       "      <td>45307</td>\n",
       "      <td>45307</td>\n",
       "      <td>45307</td>\n",
       "      <td>...</td>\n",
       "      <td>45307.000000</td>\n",
       "      <td>45307.000000</td>\n",
       "      <td>45307.000000</td>\n",
       "      <td>45307.000000</td>\n",
       "      <td>45307.000000</td>\n",
       "      <td>45307.000000</td>\n",
       "      <td>45307.000000</td>\n",
       "      <td>45307.000000</td>\n",
       "      <td>45307.000000</td>\n",
       "      <td>45307.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>40216.0</td>\n",
       "      <td>33873</td>\n",
       "      <td>35169</td>\n",
       "      <td>43703</td>\n",
       "      <td>44137</td>\n",
       "      <td>42059</td>\n",
       "      <td>43421</td>\n",
       "      <td>43727</td>\n",
       "      <td>40945</td>\n",
       "      <td>44350</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.284348</td>\n",
       "      <td>0.052491</td>\n",
       "      <td>0.028452</td>\n",
       "      <td>0.963252</td>\n",
       "      <td>0.024935</td>\n",
       "      <td>0.725451</td>\n",
       "      <td>0.535866</td>\n",
       "      <td>0.430867</td>\n",
       "      <td>0.677238</td>\n",
       "      <td>0.768943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128536</td>\n",
       "      <td>0.052636</td>\n",
       "      <td>0.050041</td>\n",
       "      <td>0.187558</td>\n",
       "      <td>0.071338</td>\n",
       "      <td>0.327131</td>\n",
       "      <td>0.225597</td>\n",
       "      <td>0.193519</td>\n",
       "      <td>0.393188</td>\n",
       "      <td>0.273651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.020740</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.340608</td>\n",
       "      <td>0.338912</td>\n",
       "      <td>0.160961</td>\n",
       "      <td>0.512287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.036600</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.603274</td>\n",
       "      <td>0.376569</td>\n",
       "      <td>0.957379</td>\n",
       "      <td>0.859735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.064864</td>\n",
       "      <td>0.036364</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.698753</td>\n",
       "      <td>0.602510</td>\n",
       "      <td>0.980957</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              y job_admin. job_blue-collar job_entrepreneur job_housemaid  \\\n",
       "count   45307.0      45307           45307            45307         45307   \n",
       "unique      2.0          2               2                2             2   \n",
       "top         0.0      False           False            False         False   \n",
       "freq    40216.0      33873           35169            43703         44137   \n",
       "mean        NaN        NaN             NaN              NaN           NaN   \n",
       "std         NaN        NaN             NaN              NaN           NaN   \n",
       "min         NaN        NaN             NaN              NaN           NaN   \n",
       "25%         NaN        NaN             NaN              NaN           NaN   \n",
       "50%         NaN        NaN             NaN              NaN           NaN   \n",
       "75%         NaN        NaN             NaN              NaN           NaN   \n",
       "max         NaN        NaN             NaN              NaN           NaN   \n",
       "\n",
       "       job_management job_retired job_self-employed job_services job_student  \\\n",
       "count           45307       45307             45307        45307       45307   \n",
       "unique              2           2                 2            2           2   \n",
       "top             False       False             False        False       False   \n",
       "freq            42059       43421             43727        40945       44350   \n",
       "mean              NaN         NaN               NaN          NaN         NaN   \n",
       "std               NaN         NaN               NaN          NaN         NaN   \n",
       "min               NaN         NaN               NaN          NaN         NaN   \n",
       "25%               NaN         NaN               NaN          NaN         NaN   \n",
       "50%               NaN         NaN               NaN          NaN         NaN   \n",
       "75%               NaN         NaN               NaN          NaN         NaN   \n",
       "max               NaN         NaN               NaN          NaN         NaN   \n",
       "\n",
       "        ...           age      duration      campaign         pdays  \\\n",
       "count   ...  45307.000000  45307.000000  45307.000000  45307.000000   \n",
       "unique  ...           NaN           NaN           NaN           NaN   \n",
       "top     ...           NaN           NaN           NaN           NaN   \n",
       "freq    ...           NaN           NaN           NaN           NaN   \n",
       "mean    ...      0.284348      0.052491      0.028452      0.963252   \n",
       "std     ...      0.128536      0.052636      0.050041      0.187558   \n",
       "min     ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%     ...      0.185185      0.020740      0.000000      1.000000   \n",
       "50%     ...      0.259259      0.036600      0.018182      1.000000   \n",
       "75%     ...      0.370370      0.064864      0.036364      1.000000   \n",
       "max     ...      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "            previous  emp.var.rate cons.price.idx cons.conf.idx     euribor3m  \\\n",
       "count   45307.000000  45307.000000   45307.000000  45307.000000  45307.000000   \n",
       "unique           NaN           NaN            NaN           NaN           NaN   \n",
       "top              NaN           NaN            NaN           NaN           NaN   \n",
       "freq             NaN           NaN            NaN           NaN           NaN   \n",
       "mean        0.024935      0.725451       0.535866      0.430867      0.677238   \n",
       "std         0.071338      0.327131       0.225597      0.193519      0.393188   \n",
       "min         0.000000      0.000000       0.000000      0.000000      0.000000   \n",
       "25%         0.000000      0.333333       0.340608      0.338912      0.160961   \n",
       "50%         0.000000      0.937500       0.603274      0.376569      0.957379   \n",
       "75%         0.000000      1.000000       0.698753      0.602510      0.980957   \n",
       "max         1.000000      1.000000       1.000000      1.000000      1.000000   \n",
       "\n",
       "         nr.employed  \n",
       "count   45307.000000  \n",
       "unique           NaN  \n",
       "top              NaN  \n",
       "freq             NaN  \n",
       "mean        0.768943  \n",
       "std         0.273651  \n",
       "min         0.000000  \n",
       "25%         0.512287  \n",
       "50%         0.859735  \n",
       "75%         1.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[11 rows x 64 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "col_1hot = ['job','marital','education','default','housing','loan','contact','month','day_of_week','poutcome']\n",
    "df_1hot = df_bank[col_1hot]\n",
    "df_1hot = pd.get_dummies(df_1hot).astype('category')\n",
    "df_others = df_bank.drop(col_1hot,axis=1)\n",
    "df_bank = pd.concat([df_others,df_1hot],axis=1)\n",
    "column_order = list(df_bank)\n",
    "column_order.insert(0, column_order.pop(column_order.index('y')))\n",
    "df_bank = df_bank.loc[:, column_order]\n",
    "df_bank['y'].replace(\"no\",0,inplace=True)\n",
    "df_bank['y'].replace(\"yes\",1,inplace=True)\n",
    "df_bank['y'] = df_bank['y'].astype('category')\n",
    "\n",
    "numericcols = ['age','duration','campaign','pdays','previous','emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m','nr.employed']\n",
    "df_num = df_bank[numericcols]\n",
    "df_stand =(df_num-df_num.min())/(df_num.max()-df_num.min())\n",
    "df_bank_categorical = df_bank.drop(numericcols,axis=1)\n",
    "df_bank = pd.concat([df_bank_categorical,df_stand],axis=1)\n",
    "df_bank.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_validate, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import itertools\n",
    "import timeit\n",
    "\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "def import_data():\n",
    "\n",
    "    #X1 = np.array(df_phish.values[:,1:-1],dtype='int64')\n",
    "    #Y1 = np.array(df_phish.values[:,0],dtype='int64')\n",
    "    X2 = np.array(df_bank.values[:,1:-1],dtype='int64')\n",
    "    Y2 = np.array(df_bank.values[:,0],dtype='int64')\n",
    "    return X2, Y2\n",
    "\n",
    "\n",
    "def plot_learning_curve(clf, X, y, title=\"Insert Title\"):\n",
    "    \n",
    "    n = len(y)\n",
    "    train_mean = []; train_std = [] #model performance score (f1)\n",
    "    cv_mean = []; cv_std = [] #model performance score (f1)\n",
    "    fit_mean = []; fit_std = [] #model fit/training time\n",
    "    pred_mean = []; pred_std = [] #model test/prediction times\n",
    "    train_sizes=(np.linspace(.05, 1.0, 20)*n).astype('int')  \n",
    "    \n",
    "    for i in train_sizes:\n",
    "        idx = np.random.randint(X.shape[0], size=i)\n",
    "        X_subset = X[idx,:]\n",
    "        y_subset = y[idx]\n",
    "        scores = cross_validate(clf, X_subset, y_subset, cv=10, scoring='f1', n_jobs=-1, return_train_score=True)\n",
    "        \n",
    "        train_mean.append(np.mean(scores['train_score'])); train_std.append(np.std(scores['train_score']))\n",
    "        cv_mean.append(np.mean(scores['test_score'])); cv_std.append(np.std(scores['test_score']))\n",
    "        fit_mean.append(np.mean(scores['fit_time'])); fit_std.append(np.std(scores['fit_time']))\n",
    "        pred_mean.append(np.mean(scores['score_time'])); pred_std.append(np.std(scores['score_time']))\n",
    "    \n",
    "    train_mean = np.array(train_mean); train_std = np.array(train_std)\n",
    "    cv_mean = np.array(cv_mean); cv_std = np.array(cv_std)\n",
    "    fit_mean = np.array(fit_mean); fit_std = np.array(fit_std)\n",
    "    pred_mean = np.array(pred_mean); pred_std = np.array(pred_std)\n",
    "    \n",
    "    plot_LC(train_sizes, train_mean, train_std, cv_mean, cv_std, title)\n",
    "    plot_times(train_sizes, fit_mean, fit_std, pred_mean, pred_std, title)\n",
    "    \n",
    "    return train_sizes, train_mean, fit_mean, pred_mean\n",
    "    \n",
    "\n",
    "def plot_LC(train_sizes, train_mean, train_std, cv_mean, cv_std, title):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Learning Curve: \"+ title)\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(\"Model F1 Score\")\n",
    "    plt.fill_between(train_sizes, train_mean - 2*train_std, train_mean + 2*train_std, alpha=0.1, color=\"b\")\n",
    "    plt.fill_between(train_sizes, cv_mean - 2*cv_std, cv_mean + 2*cv_std, alpha=0.1, color=\"r\")\n",
    "    plt.plot(train_sizes, train_mean, 'o-', color=\"b\", label=\"Training Score\")\n",
    "    plt.plot(train_sizes, cv_mean, 'o-', color=\"r\", label=\"Cross-Validation Score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_times(train_sizes, fit_mean, fit_std, pred_mean, pred_std, title):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Modeling Time: \"+ title)\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(\"Training Time (s)\")\n",
    "    plt.fill_between(train_sizes, fit_mean - 2*fit_std, fit_mean + 2*fit_std, alpha=0.1, color=\"b\")\n",
    "    plt.fill_between(train_sizes, pred_mean - 2*pred_std, pred_mean + 2*pred_std, alpha=0.1, color=\"r\")\n",
    "    plt.plot(train_sizes, fit_mean, 'o-', color=\"b\", label=\"Training Time (s)\")\n",
    "    plt.plot(train_sizes, pred_std, 'o-', color=\"r\", label=\"Prediction Time (s)\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(2), range(2)):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    \n",
    "    \n",
    "def final_classifier_evaluation(clf,X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end_time = timeit.default_timer()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    start_time = timeit.default_timer()    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    end_time = timeit.default_timer()\n",
    "    pred_time = end_time - start_time\n",
    "    \n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    recall = recall_score(y_test,y_pred)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "    print(\"Model Evaluation Metrics Using Untouched Test Dataset\")\n",
    "    print(\"*****************************************************\")\n",
    "    print(\"Model Training Time (s):   \"+\"{:.5f}\".format(training_time))\n",
    "    print(\"Model Prediction Time (s): \"+\"{:.5f}\\n\".format(pred_time))\n",
    "    print(\"F1 Score:  \"+\"{:.2f}\".format(f1))\n",
    "    print(\"Accuracy:  \"+\"{:.2f}\".format(accuracy)+\"     AUC:       \"+\"{:.2f}\".format(auc))\n",
    "    print(\"Precision: \"+\"{:.2f}\".format(precision)+\"     Recall:    \"+\"{:.2f}\".format(recall))\n",
    "    print(\"*****************************************************\")\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cm, classes=[\"0\",\"1\"], title='Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def hyperNN(X_train, y_train, X_test, y_test, title):\n",
    "\n",
    "    f1_test = []\n",
    "    f1_train = []\n",
    "    hlist = np.linspace(1,90,30).astype('int')\n",
    "    for i in hlist:         \n",
    "            clf = MLPClassifier(hidden_layer_sizes=(i,), solver='adam', activation='logistic', \n",
    "                                learning_rate_init=0.05, random_state=100,verbose=True)\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred_test = clf.predict(X_test)\n",
    "            y_pred_train = clf.predict(X_train)\n",
    "            f1_test.append(f1_score(y_test, y_pred_test))\n",
    "            f1_train.append(f1_score(y_train, y_pred_train))\n",
    "      \n",
    "    plt.plot(hlist, f1_test, 'o-', color='r', label='Test F1 Score')\n",
    "    plt.plot(hlist, f1_train, 'o-', color = 'b', label='Train F1 Score')\n",
    "    plt.ylabel('Model F1 Score')\n",
    "    plt.xlabel('No. Hidden Units')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def NNGridSearchCV(X_train, y_train):\n",
    "    #parameters to search:\n",
    "    #number of hidden units\n",
    "    #learning_rate\n",
    "    h_units = [5, 10, 20, 30, 40, 50, 75, 100]\n",
    "    learning_rates = [0.01, 0.05, .1]\n",
    "    param_grid = {'hidden_layer_sizes': h_units, 'learning_rate_init': learning_rates}\n",
    "\n",
    "    net = GridSearchCV(estimator = MLPClassifier(solver='adam',activation='logistic',random_state=100),\n",
    "                       param_grid=param_grid, cv=3,verbose=True)\n",
    "    net.fit(X_train, y_train)\n",
    "    print(\"Per Hyperparameter tuning, best parameters are:\")\n",
    "    print(net.best_params_)\n",
    "    return net.best_params_['hidden_layer_sizes'], net.best_params_['learning_rate_init']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = np.array(df_bank.values[:,1:-1],dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_bank\u001b[38;5;241m.\u001b[39mvalues[:,\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "df_bank.values[:,1:-1],dtype='int64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'header'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheader\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'header'"
     ]
    }
   ],
   "source": [
    "X_train.header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankX,bankY = import_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.35992336\n",
      "Iteration 2, loss = 0.29097786\n",
      "Iteration 3, loss = 0.28783044\n",
      "Iteration 4, loss = 0.28705482\n",
      "Iteration 5, loss = 0.28720119\n",
      "Iteration 6, loss = 0.28683867\n",
      "Iteration 7, loss = 0.28728489\n",
      "Iteration 8, loss = 0.28594984\n",
      "Iteration 9, loss = 0.28641790\n",
      "Iteration 10, loss = 0.28680522\n",
      "Iteration 11, loss = 0.28656475\n",
      "Iteration 12, loss = 0.28658110\n",
      "Iteration 13, loss = 0.28760290\n",
      "Iteration 14, loss = 0.28624616\n",
      "Iteration 15, loss = 0.28633988\n",
      "Iteration 16, loss = 0.28628086\n",
      "Iteration 17, loss = 0.28623259\n",
      "Iteration 18, loss = 0.28641448\n",
      "Iteration 19, loss = 0.28694738\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.35520356\n",
      "Iteration 2, loss = 0.29084595\n",
      "Iteration 3, loss = 0.28797916\n",
      "Iteration 4, loss = 0.28702911\n",
      "Iteration 5, loss = 0.28681816\n",
      "Iteration 6, loss = 0.28755737\n",
      "Iteration 7, loss = 0.28683097\n",
      "Iteration 8, loss = 0.28691882\n",
      "Iteration 9, loss = 0.28731996\n",
      "Iteration 10, loss = 0.28523203\n",
      "Iteration 11, loss = 0.28417642\n",
      "Iteration 12, loss = 0.28402719\n",
      "Iteration 13, loss = 0.28381774\n",
      "Iteration 14, loss = 0.28375395\n",
      "Iteration 15, loss = 0.28356286\n",
      "Iteration 16, loss = 0.28381570\n",
      "Iteration 17, loss = 0.28354057\n",
      "Iteration 18, loss = 0.28241462\n",
      "Iteration 19, loss = 0.28258506\n",
      "Iteration 20, loss = 0.28196773\n",
      "Iteration 21, loss = 0.28170172\n",
      "Iteration 22, loss = 0.28185180\n",
      "Iteration 23, loss = 0.28090391\n",
      "Iteration 24, loss = 0.28095085\n",
      "Iteration 25, loss = 0.28088539\n",
      "Iteration 26, loss = 0.28061940\n",
      "Iteration 27, loss = 0.28024676\n",
      "Iteration 28, loss = 0.28051242\n",
      "Iteration 29, loss = 0.28058092\n",
      "Iteration 30, loss = 0.28060576\n",
      "Iteration 31, loss = 0.28025067\n",
      "Iteration 32, loss = 0.28018442\n",
      "Iteration 33, loss = 0.27969688\n",
      "Iteration 34, loss = 0.27979559\n",
      "Iteration 35, loss = 0.27962158\n",
      "Iteration 36, loss = 0.27935968\n",
      "Iteration 37, loss = 0.27977727\n",
      "Iteration 38, loss = 0.27918916\n",
      "Iteration 39, loss = 0.27960006\n",
      "Iteration 40, loss = 0.27942073\n",
      "Iteration 41, loss = 0.27945995\n",
      "Iteration 42, loss = 0.27981459\n",
      "Iteration 43, loss = 0.27969996\n",
      "Iteration 44, loss = 0.27979484\n",
      "Iteration 45, loss = 0.27948296\n",
      "Iteration 46, loss = 0.27987878\n",
      "Iteration 47, loss = 0.27934381\n",
      "Iteration 48, loss = 0.27922908\n",
      "Iteration 49, loss = 0.27946944\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.31157221\n",
      "Iteration 2, loss = 0.28584158\n",
      "Iteration 3, loss = 0.28410222\n",
      "Iteration 4, loss = 0.28238721\n",
      "Iteration 5, loss = 0.28176759\n",
      "Iteration 6, loss = 0.28120481\n",
      "Iteration 7, loss = 0.28105182\n",
      "Iteration 8, loss = 0.28053435\n",
      "Iteration 9, loss = 0.28006665\n",
      "Iteration 10, loss = 0.28022455\n",
      "Iteration 11, loss = 0.27989963\n",
      "Iteration 12, loss = 0.27938196\n",
      "Iteration 13, loss = 0.27952318\n",
      "Iteration 14, loss = 0.27959732\n",
      "Iteration 15, loss = 0.27980296\n",
      "Iteration 16, loss = 0.27981881\n",
      "Iteration 17, loss = 0.27978457\n",
      "Iteration 18, loss = 0.27918047\n",
      "Iteration 19, loss = 0.27963466\n",
      "Iteration 20, loss = 0.27900955\n",
      "Iteration 21, loss = 0.27889035\n",
      "Iteration 22, loss = 0.27875384\n",
      "Iteration 23, loss = 0.27887599\n",
      "Iteration 24, loss = 0.27838790\n",
      "Iteration 25, loss = 0.27878085\n",
      "Iteration 26, loss = 0.27857319\n",
      "Iteration 27, loss = 0.27818245\n",
      "Iteration 28, loss = 0.27778991\n",
      "Iteration 29, loss = 0.27810398\n",
      "Iteration 30, loss = 0.27812186\n",
      "Iteration 31, loss = 0.27759487\n",
      "Iteration 32, loss = 0.27797106\n",
      "Iteration 33, loss = 0.27796249\n",
      "Iteration 34, loss = 0.27798529\n",
      "Iteration 35, loss = 0.27754053\n",
      "Iteration 36, loss = 0.27721185\n",
      "Iteration 37, loss = 0.27805757\n",
      "Iteration 38, loss = 0.27762041\n",
      "Iteration 39, loss = 0.27714041\n",
      "Iteration 40, loss = 0.27702588\n",
      "Iteration 41, loss = 0.27718634\n",
      "Iteration 42, loss = 0.27799523\n",
      "Iteration 43, loss = 0.27742294\n",
      "Iteration 44, loss = 0.27756152\n",
      "Iteration 45, loss = 0.27723246\n",
      "Iteration 46, loss = 0.27657947\n",
      "Iteration 47, loss = 0.27725400\n",
      "Iteration 48, loss = 0.27770683\n",
      "Iteration 49, loss = 0.27711965\n",
      "Iteration 50, loss = 0.27706387\n",
      "Iteration 51, loss = 0.27726556\n",
      "Iteration 52, loss = 0.27732279\n",
      "Iteration 53, loss = 0.27652064\n",
      "Iteration 54, loss = 0.27659415\n",
      "Iteration 55, loss = 0.27737238\n",
      "Iteration 56, loss = 0.27669902\n",
      "Iteration 57, loss = 0.27685852\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.31625234\n",
      "Iteration 2, loss = 0.28524794\n",
      "Iteration 3, loss = 0.28290199\n",
      "Iteration 4, loss = 0.28201874\n",
      "Iteration 5, loss = 0.28097434\n",
      "Iteration 6, loss = 0.28081639\n",
      "Iteration 7, loss = 0.27994213\n",
      "Iteration 8, loss = 0.27964559\n",
      "Iteration 9, loss = 0.27916598\n",
      "Iteration 10, loss = 0.27937934\n",
      "Iteration 11, loss = 0.27934614\n",
      "Iteration 12, loss = 0.27839541\n",
      "Iteration 13, loss = 0.27825076\n",
      "Iteration 14, loss = 0.27744304\n",
      "Iteration 15, loss = 0.27762899\n",
      "Iteration 16, loss = 0.27764741\n",
      "Iteration 17, loss = 0.27816251\n",
      "Iteration 18, loss = 0.27665543\n",
      "Iteration 19, loss = 0.27715997\n",
      "Iteration 20, loss = 0.27648212\n",
      "Iteration 21, loss = 0.27680094\n",
      "Iteration 22, loss = 0.27740364\n",
      "Iteration 23, loss = 0.27624729\n",
      "Iteration 24, loss = 0.27641052\n",
      "Iteration 25, loss = 0.27678689\n",
      "Iteration 26, loss = 0.27638374\n",
      "Iteration 27, loss = 0.27676711\n",
      "Iteration 28, loss = 0.27583843\n",
      "Iteration 29, loss = 0.27584260\n",
      "Iteration 30, loss = 0.27574604\n",
      "Iteration 31, loss = 0.27672485\n",
      "Iteration 32, loss = 0.27584334\n",
      "Iteration 33, loss = 0.27546751\n",
      "Iteration 34, loss = 0.27523811\n",
      "Iteration 35, loss = 0.27630528\n",
      "Iteration 36, loss = 0.27597421\n",
      "Iteration 37, loss = 0.27509204\n",
      "Iteration 38, loss = 0.27552276\n",
      "Iteration 39, loss = 0.27606137\n",
      "Iteration 40, loss = 0.27527754\n",
      "Iteration 41, loss = 0.27462737\n",
      "Iteration 42, loss = 0.27494288\n",
      "Iteration 43, loss = 0.27432550\n",
      "Iteration 44, loss = 0.27546544\n",
      "Iteration 45, loss = 0.27487519\n",
      "Iteration 46, loss = 0.27478091\n",
      "Iteration 47, loss = 0.27436075\n",
      "Iteration 48, loss = 0.27499491\n",
      "Iteration 49, loss = 0.27429658\n",
      "Iteration 50, loss = 0.27455816\n",
      "Iteration 51, loss = 0.27436476\n",
      "Iteration 52, loss = 0.27412129\n",
      "Iteration 53, loss = 0.27437672\n",
      "Iteration 54, loss = 0.27466844\n",
      "Iteration 55, loss = 0.27413160\n",
      "Iteration 56, loss = 0.27410755\n",
      "Iteration 57, loss = 0.27407888\n",
      "Iteration 58, loss = 0.27407873\n",
      "Iteration 59, loss = 0.27420145\n",
      "Iteration 60, loss = 0.27367295\n",
      "Iteration 61, loss = 0.27315007\n",
      "Iteration 62, loss = 0.27418470\n",
      "Iteration 63, loss = 0.27392434\n",
      "Iteration 64, loss = 0.27380392\n",
      "Iteration 65, loss = 0.27363847\n",
      "Iteration 66, loss = 0.27431505\n",
      "Iteration 67, loss = 0.27357320\n",
      "Iteration 68, loss = 0.27420508\n",
      "Iteration 69, loss = 0.27414042\n",
      "Iteration 70, loss = 0.27413797\n",
      "Iteration 71, loss = 0.27348886\n",
      "Iteration 72, loss = 0.27337431\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30262944\n",
      "Iteration 2, loss = 0.28640611\n",
      "Iteration 3, loss = 0.28374063\n",
      "Iteration 4, loss = 0.28284604\n",
      "Iteration 5, loss = 0.28142959\n",
      "Iteration 6, loss = 0.28144134\n",
      "Iteration 7, loss = 0.28024284\n",
      "Iteration 8, loss = 0.27996930\n",
      "Iteration 9, loss = 0.28012485\n",
      "Iteration 10, loss = 0.28022868\n",
      "Iteration 11, loss = 0.27886738\n",
      "Iteration 12, loss = 0.27927521\n",
      "Iteration 13, loss = 0.27829445\n",
      "Iteration 14, loss = 0.27683085\n",
      "Iteration 15, loss = 0.27663709\n",
      "Iteration 16, loss = 0.27683410\n",
      "Iteration 17, loss = 0.27595485\n",
      "Iteration 18, loss = 0.27614682\n",
      "Iteration 19, loss = 0.27565440\n",
      "Iteration 20, loss = 0.27536003\n",
      "Iteration 21, loss = 0.27553965\n",
      "Iteration 22, loss = 0.27579865\n",
      "Iteration 23, loss = 0.27612106\n",
      "Iteration 24, loss = 0.27465154\n",
      "Iteration 25, loss = 0.27478499\n",
      "Iteration 26, loss = 0.27480615\n",
      "Iteration 27, loss = 0.27481079\n",
      "Iteration 28, loss = 0.27382432\n",
      "Iteration 29, loss = 0.27519723\n",
      "Iteration 30, loss = 0.27410047\n",
      "Iteration 31, loss = 0.27392558\n",
      "Iteration 32, loss = 0.27398553\n",
      "Iteration 33, loss = 0.27430763\n",
      "Iteration 34, loss = 0.27322046\n",
      "Iteration 35, loss = 0.27403682\n",
      "Iteration 36, loss = 0.27335683\n",
      "Iteration 37, loss = 0.27315041\n",
      "Iteration 38, loss = 0.27380122\n",
      "Iteration 39, loss = 0.27357298\n",
      "Iteration 40, loss = 0.27339168\n",
      "Iteration 41, loss = 0.27262595\n",
      "Iteration 42, loss = 0.27314993\n",
      "Iteration 43, loss = 0.27349111\n",
      "Iteration 44, loss = 0.27306469\n",
      "Iteration 45, loss = 0.27232908\n",
      "Iteration 46, loss = 0.27272453\n",
      "Iteration 47, loss = 0.27264444\n",
      "Iteration 48, loss = 0.27286118\n",
      "Iteration 49, loss = 0.27282917\n",
      "Iteration 50, loss = 0.27279245\n",
      "Iteration 51, loss = 0.27274460\n",
      "Iteration 52, loss = 0.27287701\n",
      "Iteration 53, loss = 0.27186772\n",
      "Iteration 54, loss = 0.27246534\n",
      "Iteration 55, loss = 0.27192514\n",
      "Iteration 56, loss = 0.27270633\n",
      "Iteration 57, loss = 0.27241348\n",
      "Iteration 58, loss = 0.27241124\n",
      "Iteration 59, loss = 0.27176177\n",
      "Iteration 60, loss = 0.27182576\n",
      "Iteration 61, loss = 0.27185665\n",
      "Iteration 62, loss = 0.27258983\n",
      "Iteration 63, loss = 0.27156821\n",
      "Iteration 64, loss = 0.27213934\n",
      "Iteration 65, loss = 0.27213848\n",
      "Iteration 66, loss = 0.27272774\n",
      "Iteration 67, loss = 0.27212533\n",
      "Iteration 68, loss = 0.27276965\n",
      "Iteration 69, loss = 0.27235258\n",
      "Iteration 70, loss = 0.27110661\n",
      "Iteration 71, loss = 0.27195453\n",
      "Iteration 72, loss = 0.27208183\n",
      "Iteration 73, loss = 0.27210052\n",
      "Iteration 74, loss = 0.27246293\n",
      "Iteration 75, loss = 0.27198769\n",
      "Iteration 76, loss = 0.27196034\n",
      "Iteration 77, loss = 0.27134135\n",
      "Iteration 78, loss = 0.27163160\n",
      "Iteration 79, loss = 0.27103558\n",
      "Iteration 80, loss = 0.27179564\n",
      "Iteration 81, loss = 0.27116535\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29712420\n",
      "Iteration 2, loss = 0.28667623\n",
      "Iteration 3, loss = 0.28424645\n",
      "Iteration 4, loss = 0.28306005\n",
      "Iteration 5, loss = 0.28220291\n",
      "Iteration 6, loss = 0.28097228\n",
      "Iteration 7, loss = 0.28158067\n",
      "Iteration 8, loss = 0.28006181\n",
      "Iteration 9, loss = 0.27873182\n",
      "Iteration 10, loss = 0.27865097\n",
      "Iteration 11, loss = 0.27866613\n",
      "Iteration 12, loss = 0.27804497\n",
      "Iteration 13, loss = 0.27759121\n",
      "Iteration 14, loss = 0.27645037\n",
      "Iteration 15, loss = 0.27598401\n",
      "Iteration 16, loss = 0.27787277\n",
      "Iteration 17, loss = 0.27603496\n",
      "Iteration 18, loss = 0.27634433\n",
      "Iteration 19, loss = 0.27527980\n",
      "Iteration 20, loss = 0.27461645\n",
      "Iteration 21, loss = 0.27352679\n",
      "Iteration 22, loss = 0.27406770\n",
      "Iteration 23, loss = 0.27308355\n",
      "Iteration 24, loss = 0.27310668\n",
      "Iteration 25, loss = 0.27265342\n",
      "Iteration 26, loss = 0.27283798\n",
      "Iteration 27, loss = 0.27197971\n",
      "Iteration 28, loss = 0.27217158\n",
      "Iteration 29, loss = 0.27271241\n",
      "Iteration 30, loss = 0.27218923\n",
      "Iteration 31, loss = 0.27258063\n",
      "Iteration 32, loss = 0.27228017\n",
      "Iteration 33, loss = 0.27179301\n",
      "Iteration 34, loss = 0.27165789\n",
      "Iteration 35, loss = 0.27216871\n",
      "Iteration 36, loss = 0.27156859\n",
      "Iteration 37, loss = 0.27048849\n",
      "Iteration 38, loss = 0.27115397\n",
      "Iteration 39, loss = 0.27087959\n",
      "Iteration 40, loss = 0.27035580\n",
      "Iteration 41, loss = 0.27064679\n",
      "Iteration 42, loss = 0.27094868\n",
      "Iteration 43, loss = 0.27117594\n",
      "Iteration 44, loss = 0.27012629\n",
      "Iteration 45, loss = 0.27056622\n",
      "Iteration 46, loss = 0.27025008\n",
      "Iteration 47, loss = 0.27065011\n",
      "Iteration 48, loss = 0.27055644\n",
      "Iteration 49, loss = 0.27126843\n",
      "Iteration 50, loss = 0.27061900\n",
      "Iteration 51, loss = 0.27047203\n",
      "Iteration 52, loss = 0.27105028\n",
      "Iteration 53, loss = 0.27086674\n",
      "Iteration 54, loss = 0.27063508\n",
      "Iteration 55, loss = 0.26919847\n",
      "Iteration 56, loss = 0.27001038\n",
      "Iteration 57, loss = 0.26999700\n",
      "Iteration 58, loss = 0.27041339\n",
      "Iteration 59, loss = 0.27032595\n",
      "Iteration 60, loss = 0.27026696\n",
      "Iteration 61, loss = 0.26987254\n",
      "Iteration 62, loss = 0.27021763\n",
      "Iteration 63, loss = 0.27013205\n",
      "Iteration 64, loss = 0.27034783\n",
      "Iteration 65, loss = 0.27080607\n",
      "Iteration 66, loss = 0.26957166\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29764325\n",
      "Iteration 2, loss = 0.28673558\n",
      "Iteration 3, loss = 0.28469189\n",
      "Iteration 4, loss = 0.28320722\n",
      "Iteration 5, loss = 0.28202819\n",
      "Iteration 6, loss = 0.28069856\n",
      "Iteration 7, loss = 0.28000023\n",
      "Iteration 8, loss = 0.28017480\n",
      "Iteration 9, loss = 0.27922131\n",
      "Iteration 10, loss = 0.27819290\n",
      "Iteration 11, loss = 0.27795943\n",
      "Iteration 12, loss = 0.27661458\n",
      "Iteration 13, loss = 0.27774497\n",
      "Iteration 14, loss = 0.27573859\n",
      "Iteration 15, loss = 0.27598274\n",
      "Iteration 16, loss = 0.27615450\n",
      "Iteration 17, loss = 0.27436515\n",
      "Iteration 18, loss = 0.27489723\n",
      "Iteration 19, loss = 0.27379588\n",
      "Iteration 20, loss = 0.27330904\n",
      "Iteration 21, loss = 0.27325906\n",
      "Iteration 22, loss = 0.27378868\n",
      "Iteration 23, loss = 0.27208857\n",
      "Iteration 24, loss = 0.27335144\n",
      "Iteration 25, loss = 0.27279252\n",
      "Iteration 26, loss = 0.27165004\n",
      "Iteration 27, loss = 0.27257534\n",
      "Iteration 28, loss = 0.27235045\n",
      "Iteration 29, loss = 0.27016993\n",
      "Iteration 30, loss = 0.27174712\n",
      "Iteration 31, loss = 0.27119488\n",
      "Iteration 32, loss = 0.27135243\n",
      "Iteration 33, loss = 0.27134054\n",
      "Iteration 34, loss = 0.27069260\n",
      "Iteration 35, loss = 0.27014436\n",
      "Iteration 36, loss = 0.27110905\n",
      "Iteration 37, loss = 0.27118155\n",
      "Iteration 38, loss = 0.27022297\n",
      "Iteration 39, loss = 0.27051852\n",
      "Iteration 40, loss = 0.26997859\n",
      "Iteration 41, loss = 0.27028582\n",
      "Iteration 42, loss = 0.27045779\n",
      "Iteration 43, loss = 0.26946164\n",
      "Iteration 44, loss = 0.27079720\n",
      "Iteration 45, loss = 0.26919562\n",
      "Iteration 46, loss = 0.26932677\n",
      "Iteration 47, loss = 0.26901749\n",
      "Iteration 48, loss = 0.26966843\n",
      "Iteration 49, loss = 0.26898364\n",
      "Iteration 50, loss = 0.26956068\n",
      "Iteration 51, loss = 0.26846794\n",
      "Iteration 52, loss = 0.26905471\n",
      "Iteration 53, loss = 0.26915250\n",
      "Iteration 54, loss = 0.26951831\n",
      "Iteration 55, loss = 0.26909634\n",
      "Iteration 56, loss = 0.26876471\n",
      "Iteration 57, loss = 0.26909151\n",
      "Iteration 58, loss = 0.26879504\n",
      "Iteration 59, loss = 0.26926582\n",
      "Iteration 60, loss = 0.26856708\n",
      "Iteration 61, loss = 0.26814076\n",
      "Iteration 62, loss = 0.26839402\n",
      "Iteration 63, loss = 0.26875663\n",
      "Iteration 64, loss = 0.26856175\n",
      "Iteration 65, loss = 0.26780485\n",
      "Iteration 66, loss = 0.26880000\n",
      "Iteration 67, loss = 0.26824188\n",
      "Iteration 68, loss = 0.26831274\n",
      "Iteration 69, loss = 0.26966532\n",
      "Iteration 70, loss = 0.26895774\n",
      "Iteration 71, loss = 0.26793670\n",
      "Iteration 72, loss = 0.26829683\n",
      "Iteration 73, loss = 0.26835854\n",
      "Iteration 74, loss = 0.26824507\n",
      "Iteration 75, loss = 0.26853668\n",
      "Iteration 76, loss = 0.26784168\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29956588\n",
      "Iteration 2, loss = 0.28612149\n",
      "Iteration 3, loss = 0.28482682\n",
      "Iteration 4, loss = 0.28207600\n",
      "Iteration 5, loss = 0.28159419\n",
      "Iteration 6, loss = 0.28109746\n",
      "Iteration 7, loss = 0.28045589\n",
      "Iteration 8, loss = 0.27938987\n",
      "Iteration 9, loss = 0.27830782\n",
      "Iteration 10, loss = 0.27742318\n",
      "Iteration 11, loss = 0.27721473\n",
      "Iteration 12, loss = 0.27632845\n",
      "Iteration 13, loss = 0.27556059\n",
      "Iteration 14, loss = 0.27521562\n",
      "Iteration 15, loss = 0.27486606\n",
      "Iteration 16, loss = 0.27426834\n",
      "Iteration 17, loss = 0.27517575\n",
      "Iteration 18, loss = 0.27383990\n",
      "Iteration 19, loss = 0.27369026\n",
      "Iteration 20, loss = 0.27275332\n",
      "Iteration 21, loss = 0.27212430\n",
      "Iteration 22, loss = 0.27199041\n",
      "Iteration 23, loss = 0.27099701\n",
      "Iteration 24, loss = 0.27086307\n",
      "Iteration 25, loss = 0.27141778\n",
      "Iteration 26, loss = 0.26998995\n",
      "Iteration 27, loss = 0.27073270\n",
      "Iteration 28, loss = 0.26935489\n",
      "Iteration 29, loss = 0.26915221\n",
      "Iteration 30, loss = 0.26927740\n",
      "Iteration 31, loss = 0.26889509\n",
      "Iteration 32, loss = 0.26822582\n",
      "Iteration 33, loss = 0.26869036\n",
      "Iteration 34, loss = 0.26847204\n",
      "Iteration 35, loss = 0.26849118\n",
      "Iteration 36, loss = 0.26794410\n",
      "Iteration 37, loss = 0.26874514\n",
      "Iteration 38, loss = 0.26894059\n",
      "Iteration 39, loss = 0.26861723\n",
      "Iteration 40, loss = 0.26800123\n",
      "Iteration 41, loss = 0.26766951\n",
      "Iteration 42, loss = 0.26727543\n",
      "Iteration 43, loss = 0.26792138\n",
      "Iteration 44, loss = 0.26743967\n",
      "Iteration 45, loss = 0.26736557\n",
      "Iteration 46, loss = 0.26784652\n",
      "Iteration 47, loss = 0.26753789\n",
      "Iteration 48, loss = 0.26764285\n",
      "Iteration 49, loss = 0.26734909\n",
      "Iteration 50, loss = 0.26731346\n",
      "Iteration 51, loss = 0.26737421\n",
      "Iteration 52, loss = 0.26707713\n",
      "Iteration 53, loss = 0.26659248\n",
      "Iteration 54, loss = 0.26645426\n",
      "Iteration 55, loss = 0.26808335\n",
      "Iteration 56, loss = 0.26644727\n",
      "Iteration 57, loss = 0.26616777\n",
      "Iteration 58, loss = 0.26676240\n",
      "Iteration 59, loss = 0.26655440\n",
      "Iteration 60, loss = 0.26686411\n",
      "Iteration 61, loss = 0.26583457\n",
      "Iteration 62, loss = 0.26616943\n",
      "Iteration 63, loss = 0.26666667\n",
      "Iteration 64, loss = 0.26592040\n",
      "Iteration 65, loss = 0.26580768\n",
      "Iteration 66, loss = 0.26640443\n",
      "Iteration 67, loss = 0.26605931\n",
      "Iteration 68, loss = 0.26597881\n",
      "Iteration 69, loss = 0.26640640\n",
      "Iteration 70, loss = 0.26544533\n",
      "Iteration 71, loss = 0.26498123\n",
      "Iteration 72, loss = 0.26681653\n",
      "Iteration 73, loss = 0.26567576\n",
      "Iteration 74, loss = 0.26558884\n",
      "Iteration 75, loss = 0.26585782\n",
      "Iteration 76, loss = 0.26457727\n",
      "Iteration 77, loss = 0.26584893\n",
      "Iteration 78, loss = 0.26589485\n",
      "Iteration 79, loss = 0.26552649\n",
      "Iteration 80, loss = 0.26577136\n",
      "Iteration 81, loss = 0.26494166\n",
      "Iteration 82, loss = 0.26508010\n",
      "Iteration 83, loss = 0.26567249\n",
      "Iteration 84, loss = 0.26649479\n",
      "Iteration 85, loss = 0.26559807\n",
      "Iteration 86, loss = 0.26682721\n",
      "Iteration 87, loss = 0.26561407\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.31075249\n",
      "Iteration 2, loss = 0.28591868\n",
      "Iteration 3, loss = 0.28536024\n",
      "Iteration 4, loss = 0.28408951\n",
      "Iteration 5, loss = 0.28322960\n",
      "Iteration 6, loss = 0.28045132\n",
      "Iteration 7, loss = 0.28021386\n",
      "Iteration 8, loss = 0.27955011\n",
      "Iteration 9, loss = 0.27934308\n",
      "Iteration 10, loss = 0.27809976\n",
      "Iteration 11, loss = 0.27866650\n",
      "Iteration 12, loss = 0.27754031\n",
      "Iteration 13, loss = 0.27706162\n",
      "Iteration 14, loss = 0.27690918\n",
      "Iteration 15, loss = 0.27608916\n",
      "Iteration 16, loss = 0.27698906\n",
      "Iteration 17, loss = 0.27636120\n",
      "Iteration 18, loss = 0.27549197\n",
      "Iteration 19, loss = 0.27424598\n",
      "Iteration 20, loss = 0.27474850\n",
      "Iteration 21, loss = 0.27436687\n",
      "Iteration 22, loss = 0.27404766\n",
      "Iteration 23, loss = 0.27384139\n",
      "Iteration 24, loss = 0.27288013\n",
      "Iteration 25, loss = 0.27234676\n",
      "Iteration 26, loss = 0.27229075\n",
      "Iteration 27, loss = 0.27187982\n",
      "Iteration 28, loss = 0.27111256\n",
      "Iteration 29, loss = 0.27088678\n",
      "Iteration 30, loss = 0.27152717\n",
      "Iteration 31, loss = 0.27106741\n",
      "Iteration 32, loss = 0.27139149\n",
      "Iteration 33, loss = 0.27015764\n",
      "Iteration 34, loss = 0.27040401\n",
      "Iteration 35, loss = 0.26970778\n",
      "Iteration 36, loss = 0.27047891\n",
      "Iteration 37, loss = 0.26880127\n",
      "Iteration 38, loss = 0.26940388\n",
      "Iteration 39, loss = 0.27042948\n",
      "Iteration 40, loss = 0.26865873\n",
      "Iteration 41, loss = 0.26818352\n",
      "Iteration 42, loss = 0.26896780\n",
      "Iteration 43, loss = 0.26947256\n",
      "Iteration 44, loss = 0.26826104\n",
      "Iteration 45, loss = 0.26782345\n",
      "Iteration 46, loss = 0.26765397\n",
      "Iteration 47, loss = 0.26812823\n",
      "Iteration 48, loss = 0.26791877\n",
      "Iteration 49, loss = 0.26708648\n",
      "Iteration 50, loss = 0.26877246\n",
      "Iteration 51, loss = 0.26783500\n",
      "Iteration 52, loss = 0.26815511\n",
      "Iteration 53, loss = 0.26881461\n",
      "Iteration 54, loss = 0.26817416\n",
      "Iteration 55, loss = 0.26783621\n",
      "Iteration 56, loss = 0.26777339\n",
      "Iteration 57, loss = 0.26677792\n",
      "Iteration 58, loss = 0.26787352\n",
      "Iteration 59, loss = 0.26705705\n",
      "Iteration 60, loss = 0.26756295\n",
      "Iteration 61, loss = 0.26682700\n",
      "Iteration 62, loss = 0.26705144\n",
      "Iteration 63, loss = 0.26707900\n",
      "Iteration 64, loss = 0.26718347\n",
      "Iteration 65, loss = 0.26678412\n",
      "Iteration 66, loss = 0.26623312\n",
      "Iteration 67, loss = 0.26746176\n",
      "Iteration 68, loss = 0.26694707\n",
      "Iteration 69, loss = 0.26632239\n",
      "Iteration 70, loss = 0.26675078\n",
      "Iteration 71, loss = 0.26653593\n",
      "Iteration 72, loss = 0.26610112\n",
      "Iteration 73, loss = 0.26630733\n",
      "Iteration 74, loss = 0.26594217\n",
      "Iteration 75, loss = 0.26616973\n",
      "Iteration 76, loss = 0.26565641\n",
      "Iteration 77, loss = 0.26710743\n",
      "Iteration 78, loss = 0.26584523\n",
      "Iteration 79, loss = 0.26605721\n",
      "Iteration 80, loss = 0.26502897\n",
      "Iteration 81, loss = 0.26609167\n",
      "Iteration 82, loss = 0.26556797\n",
      "Iteration 83, loss = 0.26636784\n",
      "Iteration 84, loss = 0.26562936\n",
      "Iteration 85, loss = 0.26632389\n",
      "Iteration 86, loss = 0.26608535\n",
      "Iteration 87, loss = 0.26610588\n",
      "Iteration 88, loss = 0.26586091\n",
      "Iteration 89, loss = 0.26612098\n",
      "Iteration 90, loss = 0.26656248\n",
      "Iteration 91, loss = 0.26600134\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29669285\n",
      "Iteration 2, loss = 0.28791636\n",
      "Iteration 3, loss = 0.28492861\n",
      "Iteration 4, loss = 0.28334265\n",
      "Iteration 5, loss = 0.28158256\n",
      "Iteration 6, loss = 0.28084084\n",
      "Iteration 7, loss = 0.27984992\n",
      "Iteration 8, loss = 0.28001077\n",
      "Iteration 9, loss = 0.27880177\n",
      "Iteration 10, loss = 0.27978847\n",
      "Iteration 11, loss = 0.27870153\n",
      "Iteration 12, loss = 0.27796879\n",
      "Iteration 13, loss = 0.27692111\n",
      "Iteration 14, loss = 0.27561936\n",
      "Iteration 15, loss = 0.27588467\n",
      "Iteration 16, loss = 0.27600964\n",
      "Iteration 17, loss = 0.27488756\n",
      "Iteration 18, loss = 0.27440536\n",
      "Iteration 19, loss = 0.27418022\n",
      "Iteration 20, loss = 0.27304451\n",
      "Iteration 21, loss = 0.27338803\n",
      "Iteration 22, loss = 0.27248992\n",
      "Iteration 23, loss = 0.27244915\n",
      "Iteration 24, loss = 0.27188644\n",
      "Iteration 25, loss = 0.27213918\n",
      "Iteration 26, loss = 0.27180469\n",
      "Iteration 27, loss = 0.27137450\n",
      "Iteration 28, loss = 0.27059504\n",
      "Iteration 29, loss = 0.27071507\n",
      "Iteration 30, loss = 0.27038425\n",
      "Iteration 31, loss = 0.27106511\n",
      "Iteration 32, loss = 0.27065951\n",
      "Iteration 33, loss = 0.26987873\n",
      "Iteration 34, loss = 0.26980731\n",
      "Iteration 35, loss = 0.26947544\n",
      "Iteration 36, loss = 0.26938170\n",
      "Iteration 37, loss = 0.26933485\n",
      "Iteration 38, loss = 0.26917383\n",
      "Iteration 39, loss = 0.26887482\n",
      "Iteration 40, loss = 0.26871130\n",
      "Iteration 41, loss = 0.26895335\n",
      "Iteration 42, loss = 0.26946341\n",
      "Iteration 43, loss = 0.26978985\n",
      "Iteration 44, loss = 0.26881743\n",
      "Iteration 45, loss = 0.26919949\n",
      "Iteration 46, loss = 0.26832863\n",
      "Iteration 47, loss = 0.26936632\n",
      "Iteration 48, loss = 0.26851740\n",
      "Iteration 49, loss = 0.26992187\n",
      "Iteration 50, loss = 0.26840584\n",
      "Iteration 51, loss = 0.26782394\n",
      "Iteration 52, loss = 0.26766656\n",
      "Iteration 53, loss = 0.26801652\n",
      "Iteration 54, loss = 0.26790100\n",
      "Iteration 55, loss = 0.26849964\n",
      "Iteration 56, loss = 0.26910497\n",
      "Iteration 57, loss = 0.26854714\n",
      "Iteration 58, loss = 0.26718622\n",
      "Iteration 59, loss = 0.26742539\n",
      "Iteration 60, loss = 0.26745984\n",
      "Iteration 61, loss = 0.26842717\n",
      "Iteration 62, loss = 0.26805092\n",
      "Iteration 63, loss = 0.26802708\n",
      "Iteration 64, loss = 0.26739637\n",
      "Iteration 65, loss = 0.26792187\n",
      "Iteration 66, loss = 0.26845676\n",
      "Iteration 67, loss = 0.26698920\n",
      "Iteration 68, loss = 0.26904992\n",
      "Iteration 69, loss = 0.26698773\n",
      "Iteration 70, loss = 0.26679439\n",
      "Iteration 71, loss = 0.26743519\n",
      "Iteration 72, loss = 0.26727468\n",
      "Iteration 73, loss = 0.26649174\n",
      "Iteration 74, loss = 0.26711520\n",
      "Iteration 75, loss = 0.26764600\n",
      "Iteration 76, loss = 0.26658211\n",
      "Iteration 77, loss = 0.26650654\n",
      "Iteration 78, loss = 0.26639818\n",
      "Iteration 79, loss = 0.26627362\n",
      "Iteration 80, loss = 0.26655790\n",
      "Iteration 81, loss = 0.26601467\n",
      "Iteration 82, loss = 0.26686661\n",
      "Iteration 83, loss = 0.26730585\n",
      "Iteration 84, loss = 0.26768682\n",
      "Iteration 85, loss = 0.26582937\n",
      "Iteration 86, loss = 0.26634402\n",
      "Iteration 87, loss = 0.26663230\n",
      "Iteration 88, loss = 0.26728971\n",
      "Iteration 89, loss = 0.26614150\n",
      "Iteration 90, loss = 0.26737899\n",
      "Iteration 91, loss = 0.26604494\n",
      "Iteration 92, loss = 0.26694172\n",
      "Iteration 93, loss = 0.26579383\n",
      "Iteration 94, loss = 0.26751158\n",
      "Iteration 95, loss = 0.26586770\n",
      "Iteration 96, loss = 0.26555813\n",
      "Iteration 97, loss = 0.26607309\n",
      "Iteration 98, loss = 0.26619662\n",
      "Iteration 99, loss = 0.26617795\n",
      "Iteration 100, loss = 0.26672308\n",
      "Iteration 101, loss = 0.26660845\n",
      "Iteration 102, loss = 0.26632043\n",
      "Iteration 103, loss = 0.26654251\n",
      "Iteration 104, loss = 0.26625911\n",
      "Iteration 105, loss = 0.26642094\n",
      "Iteration 106, loss = 0.26630571\n",
      "Iteration 107, loss = 0.26597210\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30278776\n",
      "Iteration 2, loss = 0.28590402\n",
      "Iteration 3, loss = 0.28494599\n",
      "Iteration 4, loss = 0.28289460\n",
      "Iteration 5, loss = 0.28178992\n",
      "Iteration 6, loss = 0.28115639\n",
      "Iteration 7, loss = 0.28057356\n",
      "Iteration 8, loss = 0.27919576\n",
      "Iteration 9, loss = 0.27955939\n",
      "Iteration 10, loss = 0.27816202\n",
      "Iteration 11, loss = 0.27845887\n",
      "Iteration 12, loss = 0.27666004\n",
      "Iteration 13, loss = 0.27856894\n",
      "Iteration 14, loss = 0.27531043\n",
      "Iteration 15, loss = 0.27552219\n",
      "Iteration 16, loss = 0.27446581\n",
      "Iteration 17, loss = 0.27467407\n",
      "Iteration 18, loss = 0.27427049\n",
      "Iteration 19, loss = 0.27478493\n",
      "Iteration 20, loss = 0.27170655\n",
      "Iteration 21, loss = 0.27149017\n",
      "Iteration 22, loss = 0.27269151\n",
      "Iteration 23, loss = 0.27151023\n",
      "Iteration 24, loss = 0.27126227\n",
      "Iteration 25, loss = 0.27089529\n",
      "Iteration 26, loss = 0.26977918\n",
      "Iteration 27, loss = 0.26964599\n",
      "Iteration 28, loss = 0.26942328\n",
      "Iteration 29, loss = 0.26845636\n",
      "Iteration 30, loss = 0.26965693\n",
      "Iteration 31, loss = 0.26832212\n",
      "Iteration 32, loss = 0.26962651\n",
      "Iteration 33, loss = 0.26832011\n",
      "Iteration 34, loss = 0.26752984\n",
      "Iteration 35, loss = 0.26719159\n",
      "Iteration 36, loss = 0.26686546\n",
      "Iteration 37, loss = 0.26696216\n",
      "Iteration 38, loss = 0.26735113\n",
      "Iteration 39, loss = 0.26603605\n",
      "Iteration 40, loss = 0.26553720\n",
      "Iteration 41, loss = 0.26618790\n",
      "Iteration 42, loss = 0.26606100\n",
      "Iteration 43, loss = 0.26604138\n",
      "Iteration 44, loss = 0.26619718\n",
      "Iteration 45, loss = 0.26634847\n",
      "Iteration 46, loss = 0.26549800\n",
      "Iteration 47, loss = 0.26587515\n",
      "Iteration 48, loss = 0.26576043\n",
      "Iteration 49, loss = 0.26617596\n",
      "Iteration 50, loss = 0.26522203\n",
      "Iteration 51, loss = 0.26513628\n",
      "Iteration 52, loss = 0.26474415\n",
      "Iteration 53, loss = 0.26547964\n",
      "Iteration 54, loss = 0.26521896\n",
      "Iteration 55, loss = 0.26516643\n",
      "Iteration 56, loss = 0.26477337\n",
      "Iteration 57, loss = 0.26484619\n",
      "Iteration 58, loss = 0.26511821\n",
      "Iteration 59, loss = 0.26557894\n",
      "Iteration 60, loss = 0.26399479\n",
      "Iteration 61, loss = 0.26477989\n",
      "Iteration 62, loss = 0.26503000\n",
      "Iteration 63, loss = 0.26402392\n",
      "Iteration 64, loss = 0.26411280\n",
      "Iteration 65, loss = 0.26375571\n",
      "Iteration 66, loss = 0.26428439\n",
      "Iteration 67, loss = 0.26348785\n",
      "Iteration 68, loss = 0.26459882\n",
      "Iteration 69, loss = 0.26424833\n",
      "Iteration 70, loss = 0.26346850\n",
      "Iteration 71, loss = 0.26435587\n",
      "Iteration 72, loss = 0.26337770\n",
      "Iteration 73, loss = 0.26344089\n",
      "Iteration 74, loss = 0.26356624\n",
      "Iteration 75, loss = 0.26367292\n",
      "Iteration 76, loss = 0.26319432\n",
      "Iteration 77, loss = 0.26418014\n",
      "Iteration 78, loss = 0.26498306\n",
      "Iteration 79, loss = 0.26339105\n",
      "Iteration 80, loss = 0.26334201\n",
      "Iteration 81, loss = 0.26331528\n",
      "Iteration 82, loss = 0.26301741\n",
      "Iteration 83, loss = 0.26399008\n",
      "Iteration 84, loss = 0.26371432\n",
      "Iteration 85, loss = 0.26386533\n",
      "Iteration 86, loss = 0.26269390\n",
      "Iteration 87, loss = 0.26334215\n",
      "Iteration 88, loss = 0.26461721\n",
      "Iteration 89, loss = 0.26521919\n",
      "Iteration 90, loss = 0.26436375\n",
      "Iteration 91, loss = 0.26306791\n",
      "Iteration 92, loss = 0.26393951\n",
      "Iteration 93, loss = 0.26261639\n",
      "Iteration 94, loss = 0.26412387\n",
      "Iteration 95, loss = 0.26382985\n",
      "Iteration 96, loss = 0.26363501\n",
      "Iteration 97, loss = 0.26351625\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29748082\n",
      "Iteration 2, loss = 0.28614447\n",
      "Iteration 3, loss = 0.28704027\n",
      "Iteration 4, loss = 0.28408576\n",
      "Iteration 5, loss = 0.28214257\n",
      "Iteration 6, loss = 0.28102300\n",
      "Iteration 7, loss = 0.28033771\n",
      "Iteration 8, loss = 0.27987456\n",
      "Iteration 9, loss = 0.27882077\n",
      "Iteration 10, loss = 0.27910200\n",
      "Iteration 11, loss = 0.27760938\n",
      "Iteration 12, loss = 0.27615945\n",
      "Iteration 13, loss = 0.27601646\n",
      "Iteration 14, loss = 0.27563520\n",
      "Iteration 15, loss = 0.27453733\n",
      "Iteration 16, loss = 0.27414596\n",
      "Iteration 17, loss = 0.27421450\n",
      "Iteration 18, loss = 0.27338573\n",
      "Iteration 19, loss = 0.27309885\n",
      "Iteration 20, loss = 0.27270725\n",
      "Iteration 21, loss = 0.27177670\n",
      "Iteration 22, loss = 0.27234425\n",
      "Iteration 23, loss = 0.27070624\n",
      "Iteration 24, loss = 0.27072594\n",
      "Iteration 25, loss = 0.26995190\n",
      "Iteration 26, loss = 0.26982915\n",
      "Iteration 27, loss = 0.26977354\n",
      "Iteration 28, loss = 0.26945734\n",
      "Iteration 29, loss = 0.26895482\n",
      "Iteration 30, loss = 0.26887333\n",
      "Iteration 31, loss = 0.26730833\n",
      "Iteration 32, loss = 0.26843601\n",
      "Iteration 33, loss = 0.26800473\n",
      "Iteration 34, loss = 0.26777609\n",
      "Iteration 35, loss = 0.26652953\n",
      "Iteration 36, loss = 0.26569989\n",
      "Iteration 37, loss = 0.26617695\n",
      "Iteration 38, loss = 0.26462470\n",
      "Iteration 39, loss = 0.26482748\n",
      "Iteration 40, loss = 0.26406187\n",
      "Iteration 41, loss = 0.26392650\n",
      "Iteration 42, loss = 0.26383287\n",
      "Iteration 43, loss = 0.26432960\n",
      "Iteration 44, loss = 0.26282561\n",
      "Iteration 45, loss = 0.26269676\n",
      "Iteration 46, loss = 0.26299352\n",
      "Iteration 47, loss = 0.26253726\n",
      "Iteration 48, loss = 0.26223478\n",
      "Iteration 49, loss = 0.26307905\n",
      "Iteration 50, loss = 0.26261941\n",
      "Iteration 51, loss = 0.26304818\n",
      "Iteration 52, loss = 0.26139941\n",
      "Iteration 53, loss = 0.26200675\n",
      "Iteration 54, loss = 0.26258732\n",
      "Iteration 55, loss = 0.26254381\n",
      "Iteration 56, loss = 0.26161147\n",
      "Iteration 57, loss = 0.26134724\n",
      "Iteration 58, loss = 0.26148865\n",
      "Iteration 59, loss = 0.26223276\n",
      "Iteration 60, loss = 0.26130089\n",
      "Iteration 61, loss = 0.26146499\n",
      "Iteration 62, loss = 0.26156563\n",
      "Iteration 63, loss = 0.26114756\n",
      "Iteration 64, loss = 0.26098463\n",
      "Iteration 65, loss = 0.26079351\n",
      "Iteration 66, loss = 0.25978214\n",
      "Iteration 67, loss = 0.26083755\n",
      "Iteration 68, loss = 0.26049954\n",
      "Iteration 69, loss = 0.26135178\n",
      "Iteration 70, loss = 0.26144793\n",
      "Iteration 71, loss = 0.26112719\n",
      "Iteration 72, loss = 0.26008275\n",
      "Iteration 73, loss = 0.25994173\n",
      "Iteration 74, loss = 0.25999867\n",
      "Iteration 75, loss = 0.26069107\n",
      "Iteration 76, loss = 0.26001387\n",
      "Iteration 77, loss = 0.26012098\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30448248\n",
      "Iteration 2, loss = 0.28618079\n",
      "Iteration 3, loss = 0.28396941\n",
      "Iteration 4, loss = 0.28247854\n",
      "Iteration 5, loss = 0.28172565\n",
      "Iteration 6, loss = 0.28113322\n",
      "Iteration 7, loss = 0.27960004\n",
      "Iteration 8, loss = 0.27956857\n",
      "Iteration 9, loss = 0.27976573\n",
      "Iteration 10, loss = 0.27762316\n",
      "Iteration 11, loss = 0.27763059\n",
      "Iteration 12, loss = 0.27715219\n",
      "Iteration 13, loss = 0.27567267\n",
      "Iteration 14, loss = 0.27519401\n",
      "Iteration 15, loss = 0.27485038\n",
      "Iteration 16, loss = 0.27475784\n",
      "Iteration 17, loss = 0.27376647\n",
      "Iteration 18, loss = 0.27219018\n",
      "Iteration 19, loss = 0.27258766\n",
      "Iteration 20, loss = 0.27180451\n",
      "Iteration 21, loss = 0.27101410\n",
      "Iteration 22, loss = 0.27052658\n",
      "Iteration 23, loss = 0.26946620\n",
      "Iteration 24, loss = 0.26936012\n",
      "Iteration 25, loss = 0.26860060\n",
      "Iteration 26, loss = 0.26797706\n",
      "Iteration 27, loss = 0.26796628\n",
      "Iteration 28, loss = 0.26841125\n",
      "Iteration 29, loss = 0.26844430\n",
      "Iteration 30, loss = 0.26688779\n",
      "Iteration 31, loss = 0.26671444\n",
      "Iteration 32, loss = 0.26761954\n",
      "Iteration 33, loss = 0.26596003\n",
      "Iteration 34, loss = 0.26567332\n",
      "Iteration 35, loss = 0.26608970\n",
      "Iteration 36, loss = 0.26516665\n",
      "Iteration 37, loss = 0.26435263\n",
      "Iteration 38, loss = 0.26483304\n",
      "Iteration 39, loss = 0.26468161\n",
      "Iteration 40, loss = 0.26468345\n",
      "Iteration 41, loss = 0.26469033\n",
      "Iteration 42, loss = 0.26402340\n",
      "Iteration 43, loss = 0.26453497\n",
      "Iteration 44, loss = 0.26380016\n",
      "Iteration 45, loss = 0.26324370\n",
      "Iteration 46, loss = 0.26393266\n",
      "Iteration 47, loss = 0.26286904\n",
      "Iteration 48, loss = 0.26268912\n",
      "Iteration 49, loss = 0.26222022\n",
      "Iteration 50, loss = 0.26365413\n",
      "Iteration 51, loss = 0.26331391\n",
      "Iteration 52, loss = 0.26280011\n",
      "Iteration 53, loss = 0.26252967\n",
      "Iteration 54, loss = 0.26343420\n",
      "Iteration 55, loss = 0.26213981\n",
      "Iteration 56, loss = 0.26221370\n",
      "Iteration 57, loss = 0.26153864\n",
      "Iteration 58, loss = 0.26163861\n",
      "Iteration 59, loss = 0.26399658\n",
      "Iteration 60, loss = 0.26168037\n",
      "Iteration 61, loss = 0.26232993\n",
      "Iteration 62, loss = 0.26257509\n",
      "Iteration 63, loss = 0.26250105\n",
      "Iteration 64, loss = 0.26251741\n",
      "Iteration 65, loss = 0.26192261\n",
      "Iteration 66, loss = 0.26276469\n",
      "Iteration 67, loss = 0.26182350\n",
      "Iteration 68, loss = 0.26187399\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29797996\n",
      "Iteration 2, loss = 0.28668770\n",
      "Iteration 3, loss = 0.28466915\n",
      "Iteration 4, loss = 0.28340820\n",
      "Iteration 5, loss = 0.28250242\n",
      "Iteration 6, loss = 0.28149533\n",
      "Iteration 7, loss = 0.28046714\n",
      "Iteration 8, loss = 0.28093077\n",
      "Iteration 9, loss = 0.27861779\n",
      "Iteration 10, loss = 0.27796643\n",
      "Iteration 11, loss = 0.27666344\n",
      "Iteration 12, loss = 0.27534528\n",
      "Iteration 13, loss = 0.27567557\n",
      "Iteration 14, loss = 0.27459124\n",
      "Iteration 15, loss = 0.27420473\n",
      "Iteration 16, loss = 0.27382918\n",
      "Iteration 17, loss = 0.27300939\n",
      "Iteration 18, loss = 0.27234334\n",
      "Iteration 19, loss = 0.27361133\n",
      "Iteration 20, loss = 0.27045650\n",
      "Iteration 21, loss = 0.27081097\n",
      "Iteration 22, loss = 0.26970187\n",
      "Iteration 23, loss = 0.26847006\n",
      "Iteration 24, loss = 0.26870035\n",
      "Iteration 25, loss = 0.26722326\n",
      "Iteration 26, loss = 0.26711177\n",
      "Iteration 27, loss = 0.26613636\n",
      "Iteration 28, loss = 0.26537036\n",
      "Iteration 29, loss = 0.26568334\n",
      "Iteration 30, loss = 0.26503826\n",
      "Iteration 31, loss = 0.26404459\n",
      "Iteration 32, loss = 0.26458805\n",
      "Iteration 33, loss = 0.26343926\n",
      "Iteration 34, loss = 0.26388231\n",
      "Iteration 35, loss = 0.26312823\n",
      "Iteration 36, loss = 0.26329046\n",
      "Iteration 37, loss = 0.26212521\n",
      "Iteration 38, loss = 0.26239921\n",
      "Iteration 39, loss = 0.26234536\n",
      "Iteration 40, loss = 0.26160851\n",
      "Iteration 41, loss = 0.26140468\n",
      "Iteration 42, loss = 0.26106003\n",
      "Iteration 43, loss = 0.26140062\n",
      "Iteration 44, loss = 0.26102701\n",
      "Iteration 45, loss = 0.26156987\n",
      "Iteration 46, loss = 0.26055303\n",
      "Iteration 47, loss = 0.26136928\n",
      "Iteration 48, loss = 0.25975337\n",
      "Iteration 49, loss = 0.26041628\n",
      "Iteration 50, loss = 0.26105802\n",
      "Iteration 51, loss = 0.26078262\n",
      "Iteration 52, loss = 0.26038337\n",
      "Iteration 53, loss = 0.25956020\n",
      "Iteration 54, loss = 0.25950127\n",
      "Iteration 55, loss = 0.26029413\n",
      "Iteration 56, loss = 0.26010252\n",
      "Iteration 57, loss = 0.25998728\n",
      "Iteration 58, loss = 0.25997512\n",
      "Iteration 59, loss = 0.26013228\n",
      "Iteration 60, loss = 0.25947225\n",
      "Iteration 61, loss = 0.26010332\n",
      "Iteration 62, loss = 0.25935555\n",
      "Iteration 63, loss = 0.25928665\n",
      "Iteration 64, loss = 0.25951961\n",
      "Iteration 65, loss = 0.25920237\n",
      "Iteration 66, loss = 0.25906516\n",
      "Iteration 67, loss = 0.25946576\n",
      "Iteration 68, loss = 0.26001629\n",
      "Iteration 69, loss = 0.25867331\n",
      "Iteration 70, loss = 0.25930747\n",
      "Iteration 71, loss = 0.25949269\n",
      "Iteration 72, loss = 0.25824624\n",
      "Iteration 73, loss = 0.26088785\n",
      "Iteration 74, loss = 0.25923013\n",
      "Iteration 75, loss = 0.25909939\n",
      "Iteration 76, loss = 0.25828386\n",
      "Iteration 77, loss = 0.25954185\n",
      "Iteration 78, loss = 0.25780433\n",
      "Iteration 79, loss = 0.25855150\n",
      "Iteration 80, loss = 0.25908376\n",
      "Iteration 81, loss = 0.25992920\n",
      "Iteration 82, loss = 0.25840522\n",
      "Iteration 83, loss = 0.25838597\n",
      "Iteration 84, loss = 0.25995317\n",
      "Iteration 85, loss = 0.25914790\n",
      "Iteration 86, loss = 0.25859136\n",
      "Iteration 87, loss = 0.25800428\n",
      "Iteration 88, loss = 0.25809242\n",
      "Iteration 89, loss = 0.25866127\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29927290\n",
      "Iteration 2, loss = 0.28636727\n",
      "Iteration 3, loss = 0.28534289\n",
      "Iteration 4, loss = 0.28388217\n",
      "Iteration 5, loss = 0.28325163\n",
      "Iteration 6, loss = 0.28165045\n",
      "Iteration 7, loss = 0.28098491\n",
      "Iteration 8, loss = 0.27951840\n",
      "Iteration 9, loss = 0.27948926\n",
      "Iteration 10, loss = 0.27788994\n",
      "Iteration 11, loss = 0.27778927\n",
      "Iteration 12, loss = 0.27728214\n",
      "Iteration 13, loss = 0.27662157\n",
      "Iteration 14, loss = 0.27576921\n",
      "Iteration 15, loss = 0.27432705\n",
      "Iteration 16, loss = 0.27381316\n",
      "Iteration 17, loss = 0.27235352\n",
      "Iteration 18, loss = 0.27119378\n",
      "Iteration 19, loss = 0.27189263\n",
      "Iteration 20, loss = 0.27041144\n",
      "Iteration 21, loss = 0.26919083\n",
      "Iteration 22, loss = 0.26986569\n",
      "Iteration 23, loss = 0.26946157\n",
      "Iteration 24, loss = 0.26804814\n",
      "Iteration 25, loss = 0.26874366\n",
      "Iteration 26, loss = 0.26712398\n",
      "Iteration 27, loss = 0.26772766\n",
      "Iteration 28, loss = 0.26610872\n",
      "Iteration 29, loss = 0.26553031\n",
      "Iteration 30, loss = 0.26507677\n",
      "Iteration 31, loss = 0.26398564\n",
      "Iteration 32, loss = 0.26440978\n",
      "Iteration 33, loss = 0.26453937\n",
      "Iteration 34, loss = 0.26335040\n",
      "Iteration 35, loss = 0.26205765\n",
      "Iteration 36, loss = 0.26186496\n",
      "Iteration 37, loss = 0.26214199\n",
      "Iteration 38, loss = 0.26173835\n",
      "Iteration 39, loss = 0.26106920\n",
      "Iteration 40, loss = 0.26153597\n",
      "Iteration 41, loss = 0.26170085\n",
      "Iteration 42, loss = 0.26124515\n",
      "Iteration 43, loss = 0.26214823\n",
      "Iteration 44, loss = 0.26114758\n",
      "Iteration 45, loss = 0.26080639\n",
      "Iteration 46, loss = 0.26103383\n",
      "Iteration 47, loss = 0.26009468\n",
      "Iteration 48, loss = 0.26080272\n",
      "Iteration 49, loss = 0.26049969\n",
      "Iteration 50, loss = 0.26126820\n",
      "Iteration 51, loss = 0.25993677\n",
      "Iteration 52, loss = 0.26017959\n",
      "Iteration 53, loss = 0.25964074\n",
      "Iteration 54, loss = 0.25884454\n",
      "Iteration 55, loss = 0.25894562\n",
      "Iteration 56, loss = 0.26089517\n",
      "Iteration 57, loss = 0.25986573\n",
      "Iteration 58, loss = 0.25914630\n",
      "Iteration 59, loss = 0.26022494\n",
      "Iteration 60, loss = 0.25855070\n",
      "Iteration 61, loss = 0.25949339\n",
      "Iteration 62, loss = 0.25911799\n",
      "Iteration 63, loss = 0.25917131\n",
      "Iteration 64, loss = 0.25846991\n",
      "Iteration 65, loss = 0.25859601\n",
      "Iteration 66, loss = 0.25953885\n",
      "Iteration 67, loss = 0.25906480\n",
      "Iteration 68, loss = 0.25868278\n",
      "Iteration 69, loss = 0.25839629\n",
      "Iteration 70, loss = 0.25810682\n",
      "Iteration 71, loss = 0.25790063\n",
      "Iteration 72, loss = 0.25886001\n",
      "Iteration 73, loss = 0.25875092\n",
      "Iteration 74, loss = 0.26003298\n",
      "Iteration 75, loss = 0.25754886\n",
      "Iteration 76, loss = 0.25799974\n",
      "Iteration 77, loss = 0.25796166\n",
      "Iteration 78, loss = 0.25837895\n",
      "Iteration 79, loss = 0.25821056\n",
      "Iteration 80, loss = 0.25767327\n",
      "Iteration 81, loss = 0.25737868\n",
      "Iteration 82, loss = 0.25765552\n",
      "Iteration 83, loss = 0.25762913\n",
      "Iteration 84, loss = 0.25823575\n",
      "Iteration 85, loss = 0.25751795\n",
      "Iteration 86, loss = 0.25794443\n",
      "Iteration 87, loss = 0.25685034\n",
      "Iteration 88, loss = 0.25680372\n",
      "Iteration 89, loss = 0.25822829\n",
      "Iteration 90, loss = 0.25742583\n",
      "Iteration 91, loss = 0.25818733\n",
      "Iteration 92, loss = 0.25751424\n",
      "Iteration 93, loss = 0.25894226\n",
      "Iteration 94, loss = 0.25763744\n",
      "Iteration 95, loss = 0.25868011\n",
      "Iteration 96, loss = 0.25818338\n",
      "Iteration 97, loss = 0.25666754\n",
      "Iteration 98, loss = 0.25738975\n",
      "Iteration 99, loss = 0.25714081\n",
      "Iteration 100, loss = 0.25632304\n",
      "Iteration 101, loss = 0.25731721\n",
      "Iteration 102, loss = 0.25897120\n",
      "Iteration 103, loss = 0.25752299\n",
      "Iteration 104, loss = 0.25710364\n",
      "Iteration 105, loss = 0.25763921\n",
      "Iteration 106, loss = 0.25764609\n",
      "Iteration 107, loss = 0.25715166\n",
      "Iteration 108, loss = 0.25681158\n",
      "Iteration 109, loss = 0.25646616\n",
      "Iteration 110, loss = 0.25747755\n",
      "Iteration 111, loss = 0.25824357\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29981871\n",
      "Iteration 2, loss = 0.28780843\n",
      "Iteration 3, loss = 0.28456232\n",
      "Iteration 4, loss = 0.28483907\n",
      "Iteration 5, loss = 0.28222341\n",
      "Iteration 6, loss = 0.28241995\n",
      "Iteration 7, loss = 0.28085955\n",
      "Iteration 8, loss = 0.27921455\n",
      "Iteration 9, loss = 0.27981764\n",
      "Iteration 10, loss = 0.27875105\n",
      "Iteration 11, loss = 0.27669071\n",
      "Iteration 12, loss = 0.27541781\n",
      "Iteration 13, loss = 0.27464742\n",
      "Iteration 14, loss = 0.27294905\n",
      "Iteration 15, loss = 0.27169208\n",
      "Iteration 16, loss = 0.27082467\n",
      "Iteration 17, loss = 0.27107385\n",
      "Iteration 18, loss = 0.26969875\n",
      "Iteration 19, loss = 0.26930331\n",
      "Iteration 20, loss = 0.26784096\n",
      "Iteration 21, loss = 0.26811142\n",
      "Iteration 22, loss = 0.26748951\n",
      "Iteration 23, loss = 0.26825235\n",
      "Iteration 24, loss = 0.26708179\n",
      "Iteration 25, loss = 0.26665591\n",
      "Iteration 26, loss = 0.26538027\n",
      "Iteration 27, loss = 0.26567759\n",
      "Iteration 28, loss = 0.26536142\n",
      "Iteration 29, loss = 0.26360960\n",
      "Iteration 30, loss = 0.26529885\n",
      "Iteration 31, loss = 0.26454407\n",
      "Iteration 32, loss = 0.26252116\n",
      "Iteration 33, loss = 0.26279614\n",
      "Iteration 34, loss = 0.26244762\n",
      "Iteration 35, loss = 0.26088750\n",
      "Iteration 36, loss = 0.26220762\n",
      "Iteration 37, loss = 0.26163768\n",
      "Iteration 38, loss = 0.26183516\n",
      "Iteration 39, loss = 0.26230580\n",
      "Iteration 40, loss = 0.26048948\n",
      "Iteration 41, loss = 0.26058623\n",
      "Iteration 42, loss = 0.25991154\n",
      "Iteration 43, loss = 0.25979595\n",
      "Iteration 44, loss = 0.25936114\n",
      "Iteration 45, loss = 0.25934468\n",
      "Iteration 46, loss = 0.25986284\n",
      "Iteration 47, loss = 0.25816314\n",
      "Iteration 48, loss = 0.25871709\n",
      "Iteration 49, loss = 0.25855858\n",
      "Iteration 50, loss = 0.25730309\n",
      "Iteration 51, loss = 0.25724407\n",
      "Iteration 52, loss = 0.25803664\n",
      "Iteration 53, loss = 0.25736881\n",
      "Iteration 54, loss = 0.25682038\n",
      "Iteration 55, loss = 0.25690443\n",
      "Iteration 56, loss = 0.25652436\n",
      "Iteration 57, loss = 0.25600410\n",
      "Iteration 58, loss = 0.25603645\n",
      "Iteration 59, loss = 0.25682816\n",
      "Iteration 60, loss = 0.25647170\n",
      "Iteration 61, loss = 0.25614658\n",
      "Iteration 62, loss = 0.25542985\n",
      "Iteration 63, loss = 0.25544756\n",
      "Iteration 64, loss = 0.25599807\n",
      "Iteration 65, loss = 0.25454525\n",
      "Iteration 66, loss = 0.25464842\n",
      "Iteration 67, loss = 0.25679766\n",
      "Iteration 68, loss = 0.25521729\n",
      "Iteration 69, loss = 0.25516871\n",
      "Iteration 70, loss = 0.25466326\n",
      "Iteration 71, loss = 0.25364316\n",
      "Iteration 72, loss = 0.25411611\n",
      "Iteration 73, loss = 0.25363725\n",
      "Iteration 74, loss = 0.25466781\n",
      "Iteration 75, loss = 0.25495763\n",
      "Iteration 76, loss = 0.25425236\n",
      "Iteration 77, loss = 0.25442859\n",
      "Iteration 78, loss = 0.25482949\n",
      "Iteration 79, loss = 0.25333647\n",
      "Iteration 80, loss = 0.25307317\n",
      "Iteration 81, loss = 0.25299132\n",
      "Iteration 82, loss = 0.25325636\n",
      "Iteration 83, loss = 0.25413080\n",
      "Iteration 84, loss = 0.25182740\n",
      "Iteration 85, loss = 0.25299156\n",
      "Iteration 86, loss = 0.25221705\n",
      "Iteration 87, loss = 0.25320901\n",
      "Iteration 88, loss = 0.25236404\n",
      "Iteration 89, loss = 0.25370643\n",
      "Iteration 90, loss = 0.25334694\n",
      "Iteration 91, loss = 0.25349497\n",
      "Iteration 92, loss = 0.25330922\n",
      "Iteration 93, loss = 0.25354541\n",
      "Iteration 94, loss = 0.25209837\n",
      "Iteration 95, loss = 0.25227659\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30403001\n",
      "Iteration 2, loss = 0.28673142\n",
      "Iteration 3, loss = 0.28407476\n",
      "Iteration 4, loss = 0.28325693\n",
      "Iteration 5, loss = 0.28305446\n",
      "Iteration 6, loss = 0.28072070\n",
      "Iteration 7, loss = 0.28047305\n",
      "Iteration 8, loss = 0.27987214\n",
      "Iteration 9, loss = 0.27933378\n",
      "Iteration 10, loss = 0.27807534\n",
      "Iteration 11, loss = 0.27885260\n",
      "Iteration 12, loss = 0.27756252\n",
      "Iteration 13, loss = 0.27621716\n",
      "Iteration 14, loss = 0.27486222\n",
      "Iteration 15, loss = 0.27427215\n",
      "Iteration 16, loss = 0.27327896\n",
      "Iteration 17, loss = 0.27233004\n",
      "Iteration 18, loss = 0.27182969\n",
      "Iteration 19, loss = 0.27221531\n",
      "Iteration 20, loss = 0.26978391\n",
      "Iteration 21, loss = 0.27134670\n",
      "Iteration 22, loss = 0.26916344\n",
      "Iteration 23, loss = 0.26874280\n",
      "Iteration 24, loss = 0.26832074\n",
      "Iteration 25, loss = 0.26812615\n",
      "Iteration 26, loss = 0.26719813\n",
      "Iteration 27, loss = 0.26737084\n",
      "Iteration 28, loss = 0.26588879\n",
      "Iteration 29, loss = 0.26606079\n",
      "Iteration 30, loss = 0.26540569\n",
      "Iteration 31, loss = 0.26408265\n",
      "Iteration 32, loss = 0.26276324\n",
      "Iteration 33, loss = 0.26247651\n",
      "Iteration 34, loss = 0.26237393\n",
      "Iteration 35, loss = 0.26157769\n",
      "Iteration 36, loss = 0.26213855\n",
      "Iteration 37, loss = 0.26143064\n",
      "Iteration 38, loss = 0.26196757\n",
      "Iteration 39, loss = 0.26110572\n",
      "Iteration 40, loss = 0.25906171\n",
      "Iteration 41, loss = 0.26054438\n",
      "Iteration 42, loss = 0.25945131\n",
      "Iteration 43, loss = 0.25893028\n",
      "Iteration 44, loss = 0.25884727\n",
      "Iteration 45, loss = 0.25872357\n",
      "Iteration 46, loss = 0.25911371\n",
      "Iteration 47, loss = 0.25825352\n",
      "Iteration 48, loss = 0.25825454\n",
      "Iteration 49, loss = 0.26012200\n",
      "Iteration 50, loss = 0.25787074\n",
      "Iteration 51, loss = 0.25844199\n",
      "Iteration 52, loss = 0.25675445\n",
      "Iteration 53, loss = 0.25723295\n",
      "Iteration 54, loss = 0.25761861\n",
      "Iteration 55, loss = 0.25706482\n",
      "Iteration 56, loss = 0.25665345\n",
      "Iteration 57, loss = 0.25652147\n",
      "Iteration 58, loss = 0.25696253\n",
      "Iteration 59, loss = 0.25643837\n",
      "Iteration 60, loss = 0.25644627\n",
      "Iteration 61, loss = 0.25661358\n",
      "Iteration 62, loss = 0.25664523\n",
      "Iteration 63, loss = 0.25715747\n",
      "Iteration 64, loss = 0.25680376\n",
      "Iteration 65, loss = 0.25582381\n",
      "Iteration 66, loss = 0.25588407\n",
      "Iteration 67, loss = 0.25587197\n",
      "Iteration 68, loss = 0.25648390\n",
      "Iteration 69, loss = 0.25585863\n",
      "Iteration 70, loss = 0.25596174\n",
      "Iteration 71, loss = 0.25529646\n",
      "Iteration 72, loss = 0.25640074\n",
      "Iteration 73, loss = 0.25556567\n",
      "Iteration 74, loss = 0.25541316\n",
      "Iteration 75, loss = 0.25511731\n",
      "Iteration 76, loss = 0.25483994\n",
      "Iteration 77, loss = 0.25537925\n",
      "Iteration 78, loss = 0.25553777\n",
      "Iteration 79, loss = 0.25513830\n",
      "Iteration 80, loss = 0.25583625\n",
      "Iteration 81, loss = 0.25587503\n",
      "Iteration 82, loss = 0.25525490\n",
      "Iteration 83, loss = 0.25461502\n",
      "Iteration 84, loss = 0.25459212\n",
      "Iteration 85, loss = 0.25507765\n",
      "Iteration 86, loss = 0.25606955\n",
      "Iteration 87, loss = 0.25570397\n",
      "Iteration 88, loss = 0.25434087\n",
      "Iteration 89, loss = 0.25611388\n",
      "Iteration 90, loss = 0.25454252\n",
      "Iteration 91, loss = 0.25532438\n",
      "Iteration 92, loss = 0.25491661\n",
      "Iteration 93, loss = 0.25463722\n",
      "Iteration 94, loss = 0.25399053\n",
      "Iteration 95, loss = 0.25445401\n",
      "Iteration 96, loss = 0.25492445\n",
      "Iteration 97, loss = 0.25397653\n",
      "Iteration 98, loss = 0.25439364\n",
      "Iteration 99, loss = 0.25428859\n",
      "Iteration 100, loss = 0.25368397\n",
      "Iteration 101, loss = 0.25548483\n",
      "Iteration 102, loss = 0.25512043\n",
      "Iteration 103, loss = 0.25431850\n",
      "Iteration 104, loss = 0.25370747\n",
      "Iteration 105, loss = 0.25491243\n",
      "Iteration 106, loss = 0.25428027\n",
      "Iteration 107, loss = 0.25415042\n",
      "Iteration 108, loss = 0.25401422\n",
      "Iteration 109, loss = 0.25457544\n",
      "Iteration 110, loss = 0.25398668\n",
      "Iteration 111, loss = 0.25407321\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30385530\n",
      "Iteration 2, loss = 0.28702543\n",
      "Iteration 3, loss = 0.28625875\n",
      "Iteration 4, loss = 0.28434725\n",
      "Iteration 5, loss = 0.28231424\n",
      "Iteration 6, loss = 0.28216487\n",
      "Iteration 7, loss = 0.28160947\n",
      "Iteration 8, loss = 0.27993933\n",
      "Iteration 9, loss = 0.28027722\n",
      "Iteration 10, loss = 0.27790479\n",
      "Iteration 11, loss = 0.27642152\n",
      "Iteration 12, loss = 0.27592334\n",
      "Iteration 13, loss = 0.27564648\n",
      "Iteration 14, loss = 0.27446923\n",
      "Iteration 15, loss = 0.27300326\n",
      "Iteration 16, loss = 0.27302200\n",
      "Iteration 17, loss = 0.27145908\n",
      "Iteration 18, loss = 0.27207182\n",
      "Iteration 19, loss = 0.27186472\n",
      "Iteration 20, loss = 0.27028347\n",
      "Iteration 21, loss = 0.26866442\n",
      "Iteration 22, loss = 0.26967974\n",
      "Iteration 23, loss = 0.26769446\n",
      "Iteration 24, loss = 0.26800098\n",
      "Iteration 25, loss = 0.26616930\n",
      "Iteration 26, loss = 0.26738158\n",
      "Iteration 27, loss = 0.26454220\n",
      "Iteration 28, loss = 0.26486256\n",
      "Iteration 29, loss = 0.26423951\n",
      "Iteration 30, loss = 0.26349407\n",
      "Iteration 31, loss = 0.26351077\n",
      "Iteration 32, loss = 0.26331399\n",
      "Iteration 33, loss = 0.26367302\n",
      "Iteration 34, loss = 0.26376696\n",
      "Iteration 35, loss = 0.26246725\n",
      "Iteration 36, loss = 0.26184508\n",
      "Iteration 37, loss = 0.26249820\n",
      "Iteration 38, loss = 0.26072457\n",
      "Iteration 39, loss = 0.26097945\n",
      "Iteration 40, loss = 0.26046595\n",
      "Iteration 41, loss = 0.26123204\n",
      "Iteration 42, loss = 0.25938498\n",
      "Iteration 43, loss = 0.25986547\n",
      "Iteration 44, loss = 0.26005114\n",
      "Iteration 45, loss = 0.26076132\n",
      "Iteration 46, loss = 0.26121540\n",
      "Iteration 47, loss = 0.25943236\n",
      "Iteration 48, loss = 0.25940106\n",
      "Iteration 49, loss = 0.25903331\n",
      "Iteration 50, loss = 0.25925117\n",
      "Iteration 51, loss = 0.25884599\n",
      "Iteration 52, loss = 0.25895924\n",
      "Iteration 53, loss = 0.25834428\n",
      "Iteration 54, loss = 0.25822224\n",
      "Iteration 55, loss = 0.25808166\n",
      "Iteration 56, loss = 0.25818550\n",
      "Iteration 57, loss = 0.25941222\n",
      "Iteration 58, loss = 0.25736622\n",
      "Iteration 59, loss = 0.25748046\n",
      "Iteration 60, loss = 0.25779798\n",
      "Iteration 61, loss = 0.25780327\n",
      "Iteration 62, loss = 0.25821756\n",
      "Iteration 63, loss = 0.25739814\n",
      "Iteration 64, loss = 0.25812631\n",
      "Iteration 65, loss = 0.25718527\n",
      "Iteration 66, loss = 0.25660964\n",
      "Iteration 67, loss = 0.25713595\n",
      "Iteration 68, loss = 0.25794557\n",
      "Iteration 69, loss = 0.25815336\n",
      "Iteration 70, loss = 0.25856051\n",
      "Iteration 71, loss = 0.25721874\n",
      "Iteration 72, loss = 0.25740539\n",
      "Iteration 73, loss = 0.25726241\n",
      "Iteration 74, loss = 0.25868690\n",
      "Iteration 75, loss = 0.25733796\n",
      "Iteration 76, loss = 0.25682194\n",
      "Iteration 77, loss = 0.25798057\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30275973\n",
      "Iteration 2, loss = 0.28613093\n",
      "Iteration 3, loss = 0.28507805\n",
      "Iteration 4, loss = 0.28337737\n",
      "Iteration 5, loss = 0.28169711\n",
      "Iteration 6, loss = 0.28085023\n",
      "Iteration 7, loss = 0.28061597\n",
      "Iteration 8, loss = 0.27931702\n",
      "Iteration 9, loss = 0.27835209\n",
      "Iteration 10, loss = 0.27824928\n",
      "Iteration 11, loss = 0.27714496\n",
      "Iteration 12, loss = 0.27673191\n",
      "Iteration 13, loss = 0.27727105\n",
      "Iteration 14, loss = 0.27538824\n",
      "Iteration 15, loss = 0.27445035\n",
      "Iteration 16, loss = 0.27617959\n",
      "Iteration 17, loss = 0.27340188\n",
      "Iteration 18, loss = 0.27244255\n",
      "Iteration 19, loss = 0.27194656\n",
      "Iteration 20, loss = 0.27175463\n",
      "Iteration 21, loss = 0.27076982\n",
      "Iteration 22, loss = 0.27128029\n",
      "Iteration 23, loss = 0.27077329\n",
      "Iteration 24, loss = 0.26955740\n",
      "Iteration 25, loss = 0.26933699\n",
      "Iteration 26, loss = 0.26900139\n",
      "Iteration 27, loss = 0.26816504\n",
      "Iteration 28, loss = 0.26784570\n",
      "Iteration 29, loss = 0.26793461\n",
      "Iteration 30, loss = 0.26788989\n",
      "Iteration 31, loss = 0.26713547\n",
      "Iteration 32, loss = 0.26689605\n",
      "Iteration 33, loss = 0.26711555\n",
      "Iteration 34, loss = 0.26692419\n",
      "Iteration 35, loss = 0.26529585\n",
      "Iteration 36, loss = 0.26460846\n",
      "Iteration 37, loss = 0.26509049\n",
      "Iteration 38, loss = 0.26539366\n",
      "Iteration 39, loss = 0.26420090\n",
      "Iteration 40, loss = 0.26458069\n",
      "Iteration 41, loss = 0.26459968\n",
      "Iteration 42, loss = 0.26407862\n",
      "Iteration 43, loss = 0.26436313\n",
      "Iteration 44, loss = 0.26402471\n",
      "Iteration 45, loss = 0.26431687\n",
      "Iteration 46, loss = 0.26358891\n",
      "Iteration 47, loss = 0.26437891\n",
      "Iteration 48, loss = 0.26271266\n",
      "Iteration 49, loss = 0.26252192\n",
      "Iteration 50, loss = 0.26277201\n",
      "Iteration 51, loss = 0.26226566\n",
      "Iteration 52, loss = 0.26280310\n",
      "Iteration 53, loss = 0.26344652\n",
      "Iteration 54, loss = 0.26323457\n",
      "Iteration 55, loss = 0.26185149\n",
      "Iteration 56, loss = 0.26254698\n",
      "Iteration 57, loss = 0.26199819\n",
      "Iteration 58, loss = 0.26209990\n",
      "Iteration 59, loss = 0.26187772\n",
      "Iteration 60, loss = 0.26110435\n",
      "Iteration 61, loss = 0.26216569\n",
      "Iteration 62, loss = 0.26194325\n",
      "Iteration 63, loss = 0.26271244\n",
      "Iteration 64, loss = 0.26170483\n",
      "Iteration 65, loss = 0.26073669\n",
      "Iteration 66, loss = 0.26171976\n",
      "Iteration 67, loss = 0.26127394\n",
      "Iteration 68, loss = 0.26124069\n",
      "Iteration 69, loss = 0.26130010\n",
      "Iteration 70, loss = 0.26084208\n",
      "Iteration 71, loss = 0.26123744\n",
      "Iteration 72, loss = 0.26095209\n",
      "Iteration 73, loss = 0.26113409\n",
      "Iteration 74, loss = 0.26171112\n",
      "Iteration 75, loss = 0.26005141\n",
      "Iteration 76, loss = 0.26096338\n",
      "Iteration 77, loss = 0.26064389\n",
      "Iteration 78, loss = 0.26056006\n",
      "Iteration 79, loss = 0.26086184\n",
      "Iteration 80, loss = 0.26078392\n",
      "Iteration 81, loss = 0.26038822\n",
      "Iteration 82, loss = 0.26157907\n",
      "Iteration 83, loss = 0.26102488\n",
      "Iteration 84, loss = 0.26108288\n",
      "Iteration 85, loss = 0.26129375\n",
      "Iteration 86, loss = 0.26115279\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30173082\n",
      "Iteration 2, loss = 0.28790543\n",
      "Iteration 3, loss = 0.28509772\n",
      "Iteration 4, loss = 0.28250250\n",
      "Iteration 5, loss = 0.28306350\n",
      "Iteration 6, loss = 0.28087842\n",
      "Iteration 7, loss = 0.27960381\n",
      "Iteration 8, loss = 0.27913565\n",
      "Iteration 9, loss = 0.27866247\n",
      "Iteration 10, loss = 0.27690123\n",
      "Iteration 11, loss = 0.27626037\n",
      "Iteration 12, loss = 0.27642300\n",
      "Iteration 13, loss = 0.27443498\n",
      "Iteration 14, loss = 0.27419310\n",
      "Iteration 15, loss = 0.27287854\n",
      "Iteration 16, loss = 0.27257063\n",
      "Iteration 17, loss = 0.27163219\n",
      "Iteration 18, loss = 0.27125506\n",
      "Iteration 19, loss = 0.27089994\n",
      "Iteration 20, loss = 0.27144518\n",
      "Iteration 21, loss = 0.26881874\n",
      "Iteration 22, loss = 0.26878559\n",
      "Iteration 23, loss = 0.26723500\n",
      "Iteration 24, loss = 0.26858432\n",
      "Iteration 25, loss = 0.26621230\n",
      "Iteration 26, loss = 0.26629409\n",
      "Iteration 27, loss = 0.26632195\n",
      "Iteration 28, loss = 0.26476009\n",
      "Iteration 29, loss = 0.26365189\n",
      "Iteration 30, loss = 0.26519669\n",
      "Iteration 31, loss = 0.26334590\n",
      "Iteration 32, loss = 0.26385600\n",
      "Iteration 33, loss = 0.26333305\n",
      "Iteration 34, loss = 0.26212510\n",
      "Iteration 35, loss = 0.26252025\n",
      "Iteration 36, loss = 0.26118404\n",
      "Iteration 37, loss = 0.26125868\n",
      "Iteration 38, loss = 0.26094822\n",
      "Iteration 39, loss = 0.26026655\n",
      "Iteration 40, loss = 0.26023630\n",
      "Iteration 41, loss = 0.26032710\n",
      "Iteration 42, loss = 0.25944631\n",
      "Iteration 43, loss = 0.25946691\n",
      "Iteration 44, loss = 0.25992137\n",
      "Iteration 45, loss = 0.25955732\n",
      "Iteration 46, loss = 0.26034717\n",
      "Iteration 47, loss = 0.25983550\n",
      "Iteration 48, loss = 0.25882817\n",
      "Iteration 49, loss = 0.26145015\n",
      "Iteration 50, loss = 0.25841246\n",
      "Iteration 51, loss = 0.25921809\n",
      "Iteration 52, loss = 0.25828885\n",
      "Iteration 53, loss = 0.25766602\n",
      "Iteration 54, loss = 0.25813983\n",
      "Iteration 55, loss = 0.25799490\n",
      "Iteration 56, loss = 0.25807705\n",
      "Iteration 57, loss = 0.25725363\n",
      "Iteration 58, loss = 0.25768137\n",
      "Iteration 59, loss = 0.25825224\n",
      "Iteration 60, loss = 0.25826213\n",
      "Iteration 61, loss = 0.25739675\n",
      "Iteration 62, loss = 0.25786663\n",
      "Iteration 63, loss = 0.25886710\n",
      "Iteration 64, loss = 0.25783569\n",
      "Iteration 65, loss = 0.25634440\n",
      "Iteration 66, loss = 0.25666813\n",
      "Iteration 67, loss = 0.25657046\n",
      "Iteration 68, loss = 0.25791154\n",
      "Iteration 69, loss = 0.25752232\n",
      "Iteration 70, loss = 0.25678440\n",
      "Iteration 71, loss = 0.25639105\n",
      "Iteration 72, loss = 0.25602335\n",
      "Iteration 73, loss = 0.25648993\n",
      "Iteration 74, loss = 0.25649083\n",
      "Iteration 75, loss = 0.25658602\n",
      "Iteration 76, loss = 0.25593413\n",
      "Iteration 77, loss = 0.25626221\n",
      "Iteration 78, loss = 0.25567618\n",
      "Iteration 79, loss = 0.25671431\n",
      "Iteration 80, loss = 0.25669199\n",
      "Iteration 81, loss = 0.25652789\n",
      "Iteration 82, loss = 0.25741151\n",
      "Iteration 83, loss = 0.25626057\n",
      "Iteration 84, loss = 0.25682054\n",
      "Iteration 85, loss = 0.25626711\n",
      "Iteration 86, loss = 0.25535601\n",
      "Iteration 87, loss = 0.25584097\n",
      "Iteration 88, loss = 0.25562666\n",
      "Iteration 89, loss = 0.25529391\n",
      "Iteration 90, loss = 0.25590521\n",
      "Iteration 91, loss = 0.25640991\n",
      "Iteration 92, loss = 0.25685048\n",
      "Iteration 93, loss = 0.25487541\n",
      "Iteration 94, loss = 0.25457518\n",
      "Iteration 95, loss = 0.25472241\n",
      "Iteration 96, loss = 0.25726828\n",
      "Iteration 97, loss = 0.25650328\n",
      "Iteration 98, loss = 0.25509215\n",
      "Iteration 99, loss = 0.25475766\n",
      "Iteration 100, loss = 0.25480479\n",
      "Iteration 101, loss = 0.25570784\n",
      "Iteration 102, loss = 0.25478610\n",
      "Iteration 103, loss = 0.25605284\n",
      "Iteration 104, loss = 0.25532424\n",
      "Iteration 105, loss = 0.25531636\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30394705\n",
      "Iteration 2, loss = 0.28628472\n",
      "Iteration 3, loss = 0.28478243\n",
      "Iteration 4, loss = 0.28342513\n",
      "Iteration 5, loss = 0.28294299\n",
      "Iteration 6, loss = 0.28123474\n",
      "Iteration 7, loss = 0.27954160\n",
      "Iteration 8, loss = 0.27893335\n",
      "Iteration 9, loss = 0.27825836\n",
      "Iteration 10, loss = 0.27699460\n",
      "Iteration 11, loss = 0.27671140\n",
      "Iteration 12, loss = 0.27681579\n",
      "Iteration 13, loss = 0.27391991\n",
      "Iteration 14, loss = 0.27354888\n",
      "Iteration 15, loss = 0.27396891\n",
      "Iteration 16, loss = 0.27222441\n",
      "Iteration 17, loss = 0.27144066\n",
      "Iteration 18, loss = 0.27067584\n",
      "Iteration 19, loss = 0.27002052\n",
      "Iteration 20, loss = 0.26837486\n",
      "Iteration 21, loss = 0.26875416\n",
      "Iteration 22, loss = 0.26700233\n",
      "Iteration 23, loss = 0.26656841\n",
      "Iteration 24, loss = 0.26590507\n",
      "Iteration 25, loss = 0.26474831\n",
      "Iteration 26, loss = 0.26503373\n",
      "Iteration 27, loss = 0.26251157\n",
      "Iteration 28, loss = 0.26337195\n",
      "Iteration 29, loss = 0.26249809\n",
      "Iteration 30, loss = 0.26190414\n",
      "Iteration 31, loss = 0.26178579\n",
      "Iteration 32, loss = 0.26026439\n",
      "Iteration 33, loss = 0.25926436\n",
      "Iteration 34, loss = 0.25942620\n",
      "Iteration 35, loss = 0.25903476\n",
      "Iteration 36, loss = 0.25906158\n",
      "Iteration 37, loss = 0.25844944\n",
      "Iteration 38, loss = 0.25789883\n",
      "Iteration 39, loss = 0.25777336\n",
      "Iteration 40, loss = 0.25723707\n",
      "Iteration 41, loss = 0.25753060\n",
      "Iteration 42, loss = 0.25566469\n",
      "Iteration 43, loss = 0.25583951\n",
      "Iteration 44, loss = 0.25606628\n",
      "Iteration 45, loss = 0.25609810\n",
      "Iteration 46, loss = 0.25555378\n",
      "Iteration 47, loss = 0.25591449\n",
      "Iteration 48, loss = 0.25596320\n",
      "Iteration 49, loss = 0.25423962\n",
      "Iteration 50, loss = 0.25475159\n",
      "Iteration 51, loss = 0.25409678\n",
      "Iteration 52, loss = 0.25447419\n",
      "Iteration 53, loss = 0.25359729\n",
      "Iteration 54, loss = 0.25340788\n",
      "Iteration 55, loss = 0.25286920\n",
      "Iteration 56, loss = 0.25412338\n",
      "Iteration 57, loss = 0.25276293\n",
      "Iteration 58, loss = 0.25266363\n",
      "Iteration 59, loss = 0.25348238\n",
      "Iteration 60, loss = 0.25258743\n",
      "Iteration 61, loss = 0.25338703\n",
      "Iteration 62, loss = 0.25288125\n",
      "Iteration 63, loss = 0.25235274\n",
      "Iteration 64, loss = 0.25176019\n",
      "Iteration 65, loss = 0.25204332\n",
      "Iteration 66, loss = 0.25270460\n",
      "Iteration 67, loss = 0.25272982\n",
      "Iteration 68, loss = 0.25235078\n",
      "Iteration 69, loss = 0.25308819\n",
      "Iteration 70, loss = 0.25198724\n",
      "Iteration 71, loss = 0.25224207\n",
      "Iteration 72, loss = 0.25248957\n",
      "Iteration 73, loss = 0.25299287\n",
      "Iteration 74, loss = 0.25317866\n",
      "Iteration 75, loss = 0.25415473\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30219543\n",
      "Iteration 2, loss = 0.28736183\n",
      "Iteration 3, loss = 0.28716570\n",
      "Iteration 4, loss = 0.28401580\n",
      "Iteration 5, loss = 0.28430547\n",
      "Iteration 6, loss = 0.28192458\n",
      "Iteration 7, loss = 0.28061666\n",
      "Iteration 8, loss = 0.27984563\n",
      "Iteration 9, loss = 0.27890916\n",
      "Iteration 10, loss = 0.27769644\n",
      "Iteration 11, loss = 0.27612130\n",
      "Iteration 12, loss = 0.27626085\n",
      "Iteration 13, loss = 0.27579918\n",
      "Iteration 14, loss = 0.27379164\n",
      "Iteration 15, loss = 0.27323386\n",
      "Iteration 16, loss = 0.27326378\n",
      "Iteration 17, loss = 0.27269188\n",
      "Iteration 18, loss = 0.27123574\n",
      "Iteration 19, loss = 0.27102194\n",
      "Iteration 20, loss = 0.27111439\n",
      "Iteration 21, loss = 0.26918983\n",
      "Iteration 22, loss = 0.26934036\n",
      "Iteration 23, loss = 0.26779131\n",
      "Iteration 24, loss = 0.26796820\n",
      "Iteration 25, loss = 0.26698482\n",
      "Iteration 26, loss = 0.26757679\n",
      "Iteration 27, loss = 0.26646601\n",
      "Iteration 28, loss = 0.26714078\n",
      "Iteration 29, loss = 0.26564913\n",
      "Iteration 30, loss = 0.26547793\n",
      "Iteration 31, loss = 0.26453216\n",
      "Iteration 32, loss = 0.26514484\n",
      "Iteration 33, loss = 0.26410163\n",
      "Iteration 34, loss = 0.26394708\n",
      "Iteration 35, loss = 0.26431023\n",
      "Iteration 36, loss = 0.26341799\n",
      "Iteration 37, loss = 0.26355398\n",
      "Iteration 38, loss = 0.26307684\n",
      "Iteration 39, loss = 0.26335642\n",
      "Iteration 40, loss = 0.26308070\n",
      "Iteration 41, loss = 0.26261329\n",
      "Iteration 42, loss = 0.26259920\n",
      "Iteration 43, loss = 0.26262767\n",
      "Iteration 44, loss = 0.26153412\n",
      "Iteration 45, loss = 0.26196578\n",
      "Iteration 46, loss = 0.26144482\n",
      "Iteration 47, loss = 0.26362844\n",
      "Iteration 48, loss = 0.26210441\n",
      "Iteration 49, loss = 0.26239085\n",
      "Iteration 50, loss = 0.26174906\n",
      "Iteration 51, loss = 0.26064459\n",
      "Iteration 52, loss = 0.26073097\n",
      "Iteration 53, loss = 0.26063229\n",
      "Iteration 54, loss = 0.26005130\n",
      "Iteration 55, loss = 0.26071550\n",
      "Iteration 56, loss = 0.26100156\n",
      "Iteration 57, loss = 0.26076237\n",
      "Iteration 58, loss = 0.25969987\n",
      "Iteration 59, loss = 0.26060238\n",
      "Iteration 60, loss = 0.25974041\n",
      "Iteration 61, loss = 0.25961996\n",
      "Iteration 62, loss = 0.25972889\n",
      "Iteration 63, loss = 0.25923046\n",
      "Iteration 64, loss = 0.25989428\n",
      "Iteration 65, loss = 0.25915452\n",
      "Iteration 66, loss = 0.26094093\n",
      "Iteration 67, loss = 0.25832693\n",
      "Iteration 68, loss = 0.26002742\n",
      "Iteration 69, loss = 0.25924086\n",
      "Iteration 70, loss = 0.25995766\n",
      "Iteration 71, loss = 0.25883764\n",
      "Iteration 72, loss = 0.25844765\n",
      "Iteration 73, loss = 0.25947953\n",
      "Iteration 74, loss = 0.25864530\n",
      "Iteration 75, loss = 0.25882880\n",
      "Iteration 76, loss = 0.25879728\n",
      "Iteration 77, loss = 0.25986680\n",
      "Iteration 78, loss = 0.25757954\n",
      "Iteration 79, loss = 0.25854312\n",
      "Iteration 80, loss = 0.25859952\n",
      "Iteration 81, loss = 0.25883046\n",
      "Iteration 82, loss = 0.25867180\n",
      "Iteration 83, loss = 0.25848043\n",
      "Iteration 84, loss = 0.25845482\n",
      "Iteration 85, loss = 0.25907629\n",
      "Iteration 86, loss = 0.25806418\n",
      "Iteration 87, loss = 0.25903746\n",
      "Iteration 88, loss = 0.25856226\n",
      "Iteration 89, loss = 0.25804798\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30245462\n",
      "Iteration 2, loss = 0.28777155\n",
      "Iteration 3, loss = 0.28530923\n",
      "Iteration 4, loss = 0.28403336\n",
      "Iteration 5, loss = 0.28205200\n",
      "Iteration 6, loss = 0.28156420\n",
      "Iteration 7, loss = 0.27898979\n",
      "Iteration 8, loss = 0.27911155\n",
      "Iteration 9, loss = 0.27866667\n",
      "Iteration 10, loss = 0.27609901\n",
      "Iteration 11, loss = 0.27636363\n",
      "Iteration 12, loss = 0.27663146\n",
      "Iteration 13, loss = 0.27492875\n",
      "Iteration 14, loss = 0.27355335\n",
      "Iteration 15, loss = 0.27264160\n",
      "Iteration 16, loss = 0.27191649\n",
      "Iteration 17, loss = 0.27119275\n",
      "Iteration 18, loss = 0.27071806\n",
      "Iteration 19, loss = 0.26983112\n",
      "Iteration 20, loss = 0.26889868\n",
      "Iteration 21, loss = 0.26790740\n",
      "Iteration 22, loss = 0.26745956\n",
      "Iteration 23, loss = 0.26678555\n",
      "Iteration 24, loss = 0.26623402\n",
      "Iteration 25, loss = 0.26481538\n",
      "Iteration 26, loss = 0.26489561\n",
      "Iteration 27, loss = 0.26308572\n",
      "Iteration 28, loss = 0.26263048\n",
      "Iteration 29, loss = 0.26281302\n",
      "Iteration 30, loss = 0.26328293\n",
      "Iteration 31, loss = 0.26189296\n",
      "Iteration 32, loss = 0.26180501\n",
      "Iteration 33, loss = 0.26192799\n",
      "Iteration 34, loss = 0.26041230\n",
      "Iteration 35, loss = 0.26049109\n",
      "Iteration 36, loss = 0.25984846\n",
      "Iteration 37, loss = 0.26102939\n",
      "Iteration 38, loss = 0.25956906\n",
      "Iteration 39, loss = 0.25945676\n",
      "Iteration 40, loss = 0.25912946\n",
      "Iteration 41, loss = 0.25961838\n",
      "Iteration 42, loss = 0.25830251\n",
      "Iteration 43, loss = 0.25744486\n",
      "Iteration 44, loss = 0.25811060\n",
      "Iteration 45, loss = 0.25854587\n",
      "Iteration 46, loss = 0.25698918\n",
      "Iteration 47, loss = 0.25626150\n",
      "Iteration 48, loss = 0.25765255\n",
      "Iteration 49, loss = 0.25729687\n",
      "Iteration 50, loss = 0.25540815\n",
      "Iteration 51, loss = 0.25592105\n",
      "Iteration 52, loss = 0.25592546\n",
      "Iteration 53, loss = 0.25583477\n",
      "Iteration 54, loss = 0.25593418\n",
      "Iteration 55, loss = 0.25477796\n",
      "Iteration 56, loss = 0.25495287\n",
      "Iteration 57, loss = 0.25511297\n",
      "Iteration 58, loss = 0.25402174\n",
      "Iteration 59, loss = 0.25517422\n",
      "Iteration 60, loss = 0.25304576\n",
      "Iteration 61, loss = 0.25344802\n",
      "Iteration 62, loss = 0.25399204\n",
      "Iteration 63, loss = 0.25398317\n",
      "Iteration 64, loss = 0.25386764\n",
      "Iteration 65, loss = 0.25366297\n",
      "Iteration 66, loss = 0.25329405\n",
      "Iteration 67, loss = 0.25327477\n",
      "Iteration 68, loss = 0.25360814\n",
      "Iteration 69, loss = 0.25349909\n",
      "Iteration 70, loss = 0.25300767\n",
      "Iteration 71, loss = 0.25174401\n",
      "Iteration 72, loss = 0.25214174\n",
      "Iteration 73, loss = 0.25132902\n",
      "Iteration 74, loss = 0.25192614\n",
      "Iteration 75, loss = 0.25336269\n",
      "Iteration 76, loss = 0.25298413\n",
      "Iteration 77, loss = 0.25306940\n",
      "Iteration 78, loss = 0.25234638\n",
      "Iteration 79, loss = 0.25087770\n",
      "Iteration 80, loss = 0.25249239\n",
      "Iteration 81, loss = 0.25199618\n",
      "Iteration 82, loss = 0.25192140\n",
      "Iteration 83, loss = 0.25311598\n",
      "Iteration 84, loss = 0.25148039\n",
      "Iteration 85, loss = 0.25201085\n",
      "Iteration 86, loss = 0.25206652\n",
      "Iteration 87, loss = 0.25227159\n",
      "Iteration 88, loss = 0.25225610\n",
      "Iteration 89, loss = 0.25223895\n",
      "Iteration 90, loss = 0.25176046\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30413538\n",
      "Iteration 2, loss = 0.28658794\n",
      "Iteration 3, loss = 0.28562065\n",
      "Iteration 4, loss = 0.28321958\n",
      "Iteration 5, loss = 0.28459946\n",
      "Iteration 6, loss = 0.28141572\n",
      "Iteration 7, loss = 0.28141736\n",
      "Iteration 8, loss = 0.27990604\n",
      "Iteration 9, loss = 0.27934001\n",
      "Iteration 10, loss = 0.27812547\n",
      "Iteration 11, loss = 0.27746162\n",
      "Iteration 12, loss = 0.27492497\n",
      "Iteration 13, loss = 0.27613912\n",
      "Iteration 14, loss = 0.27538700\n",
      "Iteration 15, loss = 0.27424197\n",
      "Iteration 16, loss = 0.27325280\n",
      "Iteration 17, loss = 0.27141614\n",
      "Iteration 18, loss = 0.27100119\n",
      "Iteration 19, loss = 0.26984585\n",
      "Iteration 20, loss = 0.26956691\n",
      "Iteration 21, loss = 0.26924105\n",
      "Iteration 22, loss = 0.26787803\n",
      "Iteration 23, loss = 0.26774074\n",
      "Iteration 24, loss = 0.26610046\n",
      "Iteration 25, loss = 0.26550607\n",
      "Iteration 26, loss = 0.26566450\n",
      "Iteration 27, loss = 0.26468534\n",
      "Iteration 28, loss = 0.26420542\n",
      "Iteration 29, loss = 0.26312587\n",
      "Iteration 30, loss = 0.26339566\n",
      "Iteration 31, loss = 0.26343960\n",
      "Iteration 32, loss = 0.26101273\n",
      "Iteration 33, loss = 0.26252409\n",
      "Iteration 34, loss = 0.26054860\n",
      "Iteration 35, loss = 0.25979326\n",
      "Iteration 36, loss = 0.25934453\n",
      "Iteration 37, loss = 0.26017818\n",
      "Iteration 38, loss = 0.25872670\n",
      "Iteration 39, loss = 0.25786373\n",
      "Iteration 40, loss = 0.25784609\n",
      "Iteration 41, loss = 0.25743730\n",
      "Iteration 42, loss = 0.25746137\n",
      "Iteration 43, loss = 0.25482345\n",
      "Iteration 44, loss = 0.25728813\n",
      "Iteration 45, loss = 0.25628494\n",
      "Iteration 46, loss = 0.25540539\n",
      "Iteration 47, loss = 0.25515574\n",
      "Iteration 48, loss = 0.25595564\n",
      "Iteration 49, loss = 0.25604835\n",
      "Iteration 50, loss = 0.25670614\n",
      "Iteration 51, loss = 0.25512407\n",
      "Iteration 52, loss = 0.25503448\n",
      "Iteration 53, loss = 0.25560809\n",
      "Iteration 54, loss = 0.25389671\n",
      "Iteration 55, loss = 0.25449159\n",
      "Iteration 56, loss = 0.25515674\n",
      "Iteration 57, loss = 0.25493093\n",
      "Iteration 58, loss = 0.25300496\n",
      "Iteration 59, loss = 0.25344118\n",
      "Iteration 60, loss = 0.25363442\n",
      "Iteration 61, loss = 0.25317114\n",
      "Iteration 62, loss = 0.25315263\n",
      "Iteration 63, loss = 0.25368524\n",
      "Iteration 64, loss = 0.25397898\n",
      "Iteration 65, loss = 0.25369944\n",
      "Iteration 66, loss = 0.25364755\n",
      "Iteration 67, loss = 0.25330867\n",
      "Iteration 68, loss = 0.25192404\n",
      "Iteration 69, loss = 0.25234180\n",
      "Iteration 70, loss = 0.25343741\n",
      "Iteration 71, loss = 0.25178608\n",
      "Iteration 72, loss = 0.25305114\n",
      "Iteration 73, loss = 0.25261622\n",
      "Iteration 74, loss = 0.25267496\n",
      "Iteration 75, loss = 0.25245197\n",
      "Iteration 76, loss = 0.25287035\n",
      "Iteration 77, loss = 0.25256995\n",
      "Iteration 78, loss = 0.25214722\n",
      "Iteration 79, loss = 0.25247370\n",
      "Iteration 80, loss = 0.25257869\n",
      "Iteration 81, loss = 0.25249237\n",
      "Iteration 82, loss = 0.25179145\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30399815\n",
      "Iteration 2, loss = 0.28740327\n",
      "Iteration 3, loss = 0.28681337\n",
      "Iteration 4, loss = 0.28432369\n",
      "Iteration 5, loss = 0.28239692\n",
      "Iteration 6, loss = 0.28093925\n",
      "Iteration 7, loss = 0.28093556\n",
      "Iteration 8, loss = 0.27891270\n",
      "Iteration 9, loss = 0.27893601\n",
      "Iteration 10, loss = 0.27795938\n",
      "Iteration 11, loss = 0.27739131\n",
      "Iteration 12, loss = 0.27590282\n",
      "Iteration 13, loss = 0.27441371\n",
      "Iteration 14, loss = 0.27377243\n",
      "Iteration 15, loss = 0.27340849\n",
      "Iteration 16, loss = 0.27223551\n",
      "Iteration 17, loss = 0.27162570\n",
      "Iteration 18, loss = 0.27066084\n",
      "Iteration 19, loss = 0.26987172\n",
      "Iteration 20, loss = 0.26896104\n",
      "Iteration 21, loss = 0.26662086\n",
      "Iteration 22, loss = 0.26752567\n",
      "Iteration 23, loss = 0.26579263\n",
      "Iteration 24, loss = 0.26510541\n",
      "Iteration 25, loss = 0.26348291\n",
      "Iteration 26, loss = 0.26490162\n",
      "Iteration 27, loss = 0.26363134\n",
      "Iteration 28, loss = 0.26273218\n",
      "Iteration 29, loss = 0.26194060\n",
      "Iteration 30, loss = 0.26007837\n",
      "Iteration 31, loss = 0.26047861\n",
      "Iteration 32, loss = 0.26045606\n",
      "Iteration 33, loss = 0.25910413\n",
      "Iteration 34, loss = 0.25913839\n",
      "Iteration 35, loss = 0.25864481\n",
      "Iteration 36, loss = 0.25975142\n",
      "Iteration 37, loss = 0.25729526\n",
      "Iteration 38, loss = 0.25662327\n",
      "Iteration 39, loss = 0.25854141\n",
      "Iteration 40, loss = 0.25605546\n",
      "Iteration 41, loss = 0.25674510\n",
      "Iteration 42, loss = 0.25618612\n",
      "Iteration 43, loss = 0.25626667\n",
      "Iteration 44, loss = 0.25562402\n",
      "Iteration 45, loss = 0.25510468\n",
      "Iteration 46, loss = 0.25492888\n",
      "Iteration 47, loss = 0.25442486\n",
      "Iteration 48, loss = 0.25390603\n",
      "Iteration 49, loss = 0.25586271\n",
      "Iteration 50, loss = 0.25550634\n",
      "Iteration 51, loss = 0.25397929\n",
      "Iteration 52, loss = 0.25340332\n",
      "Iteration 53, loss = 0.25441836\n",
      "Iteration 54, loss = 0.25465833\n",
      "Iteration 55, loss = 0.25355241\n",
      "Iteration 56, loss = 0.25342697\n",
      "Iteration 57, loss = 0.25338785\n",
      "Iteration 58, loss = 0.25324167\n",
      "Iteration 59, loss = 0.25224810\n",
      "Iteration 60, loss = 0.25336255\n",
      "Iteration 61, loss = 0.25346647\n",
      "Iteration 62, loss = 0.25210817\n",
      "Iteration 63, loss = 0.25277828\n",
      "Iteration 64, loss = 0.25344245\n",
      "Iteration 65, loss = 0.25340476\n",
      "Iteration 66, loss = 0.25366251\n",
      "Iteration 67, loss = 0.25219903\n",
      "Iteration 68, loss = 0.25245461\n",
      "Iteration 69, loss = 0.25259356\n",
      "Iteration 70, loss = 0.25333090\n",
      "Iteration 71, loss = 0.25300607\n",
      "Iteration 72, loss = 0.25234541\n",
      "Iteration 73, loss = 0.25140024\n",
      "Iteration 74, loss = 0.25201407\n",
      "Iteration 75, loss = 0.25184623\n",
      "Iteration 76, loss = 0.25169632\n",
      "Iteration 77, loss = 0.25227521\n",
      "Iteration 78, loss = 0.25181316\n",
      "Iteration 79, loss = 0.25215762\n",
      "Iteration 80, loss = 0.25259339\n",
      "Iteration 81, loss = 0.25234761\n",
      "Iteration 82, loss = 0.25261141\n",
      "Iteration 83, loss = 0.25132101\n",
      "Iteration 84, loss = 0.25147297\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30125888\n",
      "Iteration 2, loss = 0.28645812\n",
      "Iteration 3, loss = 0.28509239\n",
      "Iteration 4, loss = 0.28299863\n",
      "Iteration 5, loss = 0.28211287\n",
      "Iteration 6, loss = 0.28139486\n",
      "Iteration 7, loss = 0.28102575\n",
      "Iteration 8, loss = 0.28007216\n",
      "Iteration 9, loss = 0.27805843\n",
      "Iteration 10, loss = 0.27777307\n",
      "Iteration 11, loss = 0.27676689\n",
      "Iteration 12, loss = 0.27540175\n",
      "Iteration 13, loss = 0.27522508\n",
      "Iteration 14, loss = 0.27455941\n",
      "Iteration 15, loss = 0.27181830\n",
      "Iteration 16, loss = 0.27283092\n",
      "Iteration 17, loss = 0.27081346\n",
      "Iteration 18, loss = 0.26982457\n",
      "Iteration 19, loss = 0.26948360\n",
      "Iteration 20, loss = 0.26830902\n",
      "Iteration 21, loss = 0.26754808\n",
      "Iteration 22, loss = 0.26828313\n",
      "Iteration 23, loss = 0.26722373\n",
      "Iteration 24, loss = 0.26581138\n",
      "Iteration 25, loss = 0.26521953\n",
      "Iteration 26, loss = 0.26508589\n",
      "Iteration 27, loss = 0.26372063\n",
      "Iteration 28, loss = 0.26263895\n",
      "Iteration 29, loss = 0.26270869\n",
      "Iteration 30, loss = 0.26194213\n",
      "Iteration 31, loss = 0.26198720\n",
      "Iteration 32, loss = 0.26155517\n",
      "Iteration 33, loss = 0.26071624\n",
      "Iteration 34, loss = 0.26114003\n",
      "Iteration 35, loss = 0.26143974\n",
      "Iteration 36, loss = 0.26066182\n",
      "Iteration 37, loss = 0.26010094\n",
      "Iteration 38, loss = 0.26214010\n",
      "Iteration 39, loss = 0.25996292\n",
      "Iteration 40, loss = 0.25951212\n",
      "Iteration 41, loss = 0.25972685\n",
      "Iteration 42, loss = 0.25930413\n",
      "Iteration 43, loss = 0.25949430\n",
      "Iteration 44, loss = 0.25894340\n",
      "Iteration 45, loss = 0.25913639\n",
      "Iteration 46, loss = 0.25860670\n",
      "Iteration 47, loss = 0.25859163\n",
      "Iteration 48, loss = 0.25819248\n",
      "Iteration 49, loss = 0.25798985\n",
      "Iteration 50, loss = 0.25753680\n",
      "Iteration 51, loss = 0.25718836\n",
      "Iteration 52, loss = 0.25910149\n",
      "Iteration 53, loss = 0.25798787\n",
      "Iteration 54, loss = 0.25803180\n",
      "Iteration 55, loss = 0.25904328\n",
      "Iteration 56, loss = 0.25782483\n",
      "Iteration 57, loss = 0.25770658\n",
      "Iteration 58, loss = 0.25821592\n",
      "Iteration 59, loss = 0.25776500\n",
      "Iteration 60, loss = 0.25727850\n",
      "Iteration 61, loss = 0.25694820\n",
      "Iteration 62, loss = 0.25678460\n",
      "Iteration 63, loss = 0.25536399\n",
      "Iteration 64, loss = 0.25686647\n",
      "Iteration 65, loss = 0.25660295\n",
      "Iteration 66, loss = 0.25685475\n",
      "Iteration 67, loss = 0.25629962\n",
      "Iteration 68, loss = 0.25663132\n",
      "Iteration 69, loss = 0.25593030\n",
      "Iteration 70, loss = 0.25723826\n",
      "Iteration 71, loss = 0.25560340\n",
      "Iteration 72, loss = 0.25611558\n",
      "Iteration 73, loss = 0.25564998\n",
      "Iteration 74, loss = 0.25681286\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30287741\n",
      "Iteration 2, loss = 0.28666233\n",
      "Iteration 3, loss = 0.28596206\n",
      "Iteration 4, loss = 0.28397161\n",
      "Iteration 5, loss = 0.28240942\n",
      "Iteration 6, loss = 0.28115313\n",
      "Iteration 7, loss = 0.28043011\n",
      "Iteration 8, loss = 0.27999303\n",
      "Iteration 9, loss = 0.27859921\n",
      "Iteration 10, loss = 0.27777675\n",
      "Iteration 11, loss = 0.27627224\n",
      "Iteration 12, loss = 0.27625547\n",
      "Iteration 13, loss = 0.27548784\n",
      "Iteration 14, loss = 0.27555847\n",
      "Iteration 15, loss = 0.27437910\n",
      "Iteration 16, loss = 0.27264499\n",
      "Iteration 17, loss = 0.27133954\n",
      "Iteration 18, loss = 0.27038780\n",
      "Iteration 19, loss = 0.26918428\n",
      "Iteration 20, loss = 0.26877867\n",
      "Iteration 21, loss = 0.26807448\n",
      "Iteration 22, loss = 0.26763685\n",
      "Iteration 23, loss = 0.26718922\n",
      "Iteration 24, loss = 0.26537567\n",
      "Iteration 25, loss = 0.26535434\n",
      "Iteration 26, loss = 0.26475808\n",
      "Iteration 27, loss = 0.26291892\n",
      "Iteration 28, loss = 0.26284325\n",
      "Iteration 29, loss = 0.26406817\n",
      "Iteration 30, loss = 0.26207621\n",
      "Iteration 31, loss = 0.26269481\n",
      "Iteration 32, loss = 0.26123047\n",
      "Iteration 33, loss = 0.26121919\n",
      "Iteration 34, loss = 0.26063501\n",
      "Iteration 35, loss = 0.26026709\n",
      "Iteration 36, loss = 0.25990109\n",
      "Iteration 37, loss = 0.26062460\n",
      "Iteration 38, loss = 0.25938459\n",
      "Iteration 39, loss = 0.26006273\n",
      "Iteration 40, loss = 0.25809863\n",
      "Iteration 41, loss = 0.25832924\n",
      "Iteration 42, loss = 0.25782423\n",
      "Iteration 43, loss = 0.25762490\n",
      "Iteration 44, loss = 0.25748055\n",
      "Iteration 45, loss = 0.25732204\n",
      "Iteration 46, loss = 0.25558830\n",
      "Iteration 47, loss = 0.25598222\n",
      "Iteration 48, loss = 0.25773381\n",
      "Iteration 49, loss = 0.25616529\n",
      "Iteration 50, loss = 0.25468578\n",
      "Iteration 51, loss = 0.25397332\n",
      "Iteration 52, loss = 0.25508272\n",
      "Iteration 53, loss = 0.25484302\n",
      "Iteration 54, loss = 0.25505118\n",
      "Iteration 55, loss = 0.25480387\n",
      "Iteration 56, loss = 0.25412155\n",
      "Iteration 57, loss = 0.25485123\n",
      "Iteration 58, loss = 0.25397032\n",
      "Iteration 59, loss = 0.25446844\n",
      "Iteration 60, loss = 0.25426979\n",
      "Iteration 61, loss = 0.25314553\n",
      "Iteration 62, loss = 0.25355348\n",
      "Iteration 63, loss = 0.25249658\n",
      "Iteration 64, loss = 0.25263901\n",
      "Iteration 65, loss = 0.25438117\n",
      "Iteration 66, loss = 0.25367256\n",
      "Iteration 67, loss = 0.25264742\n",
      "Iteration 68, loss = 0.25282582\n",
      "Iteration 69, loss = 0.25346568\n",
      "Iteration 70, loss = 0.25259544\n",
      "Iteration 71, loss = 0.25222809\n",
      "Iteration 72, loss = 0.25305521\n",
      "Iteration 73, loss = 0.25252956\n",
      "Iteration 74, loss = 0.25150302\n",
      "Iteration 75, loss = 0.25135797\n",
      "Iteration 76, loss = 0.25195766\n",
      "Iteration 77, loss = 0.25220583\n",
      "Iteration 78, loss = 0.25188304\n",
      "Iteration 79, loss = 0.25217414\n",
      "Iteration 80, loss = 0.25169192\n",
      "Iteration 81, loss = 0.25258885\n",
      "Iteration 82, loss = 0.25152248\n",
      "Iteration 83, loss = 0.25100256\n",
      "Iteration 84, loss = 0.25221425\n",
      "Iteration 85, loss = 0.25189873\n",
      "Iteration 86, loss = 0.25165692\n",
      "Iteration 87, loss = 0.25158985\n",
      "Iteration 88, loss = 0.25174097\n",
      "Iteration 89, loss = 0.25153913\n",
      "Iteration 90, loss = 0.25053919\n",
      "Iteration 91, loss = 0.25225017\n",
      "Iteration 92, loss = 0.25079962\n",
      "Iteration 93, loss = 0.25065921\n",
      "Iteration 94, loss = 0.25071701\n",
      "Iteration 95, loss = 0.24990181\n",
      "Iteration 96, loss = 0.24999715\n",
      "Iteration 97, loss = 0.25103866\n",
      "Iteration 98, loss = 0.24956710\n",
      "Iteration 99, loss = 0.25074743\n",
      "Iteration 100, loss = 0.25053232\n",
      "Iteration 101, loss = 0.25212785\n",
      "Iteration 102, loss = 0.24953854\n",
      "Iteration 103, loss = 0.25032892\n",
      "Iteration 104, loss = 0.25048222\n",
      "Iteration 105, loss = 0.25162346\n",
      "Iteration 106, loss = 0.25113752\n",
      "Iteration 107, loss = 0.24981987\n",
      "Iteration 108, loss = 0.25071724\n",
      "Iteration 109, loss = 0.25036769\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29952512\n",
      "Iteration 2, loss = 0.28763129\n",
      "Iteration 3, loss = 0.28503605\n",
      "Iteration 4, loss = 0.28452235\n",
      "Iteration 5, loss = 0.28253664\n",
      "Iteration 6, loss = 0.28213884\n",
      "Iteration 7, loss = 0.28052274\n",
      "Iteration 8, loss = 0.28011506\n",
      "Iteration 9, loss = 0.28027859\n",
      "Iteration 10, loss = 0.27856402\n",
      "Iteration 11, loss = 0.27845468\n",
      "Iteration 12, loss = 0.27652282\n",
      "Iteration 13, loss = 0.27551688\n",
      "Iteration 14, loss = 0.27454369\n",
      "Iteration 15, loss = 0.27442996\n",
      "Iteration 16, loss = 0.27356017\n",
      "Iteration 17, loss = 0.27263694\n",
      "Iteration 18, loss = 0.27189063\n",
      "Iteration 19, loss = 0.27033391\n",
      "Iteration 20, loss = 0.26935163\n",
      "Iteration 21, loss = 0.26942419\n",
      "Iteration 22, loss = 0.27006868\n",
      "Iteration 23, loss = 0.26829998\n",
      "Iteration 24, loss = 0.26691683\n",
      "Iteration 25, loss = 0.26556407\n",
      "Iteration 26, loss = 0.26567198\n",
      "Iteration 27, loss = 0.26479275\n",
      "Iteration 28, loss = 0.26443384\n",
      "Iteration 29, loss = 0.26369632\n",
      "Iteration 30, loss = 0.26260920\n",
      "Iteration 31, loss = 0.26235053\n",
      "Iteration 32, loss = 0.26339701\n",
      "Iteration 33, loss = 0.26212030\n",
      "Iteration 34, loss = 0.26088023\n",
      "Iteration 35, loss = 0.26095779\n",
      "Iteration 36, loss = 0.26060358\n",
      "Iteration 37, loss = 0.25941951\n",
      "Iteration 38, loss = 0.26010818\n",
      "Iteration 39, loss = 0.25900946\n",
      "Iteration 40, loss = 0.25857510\n",
      "Iteration 41, loss = 0.25946336\n",
      "Iteration 42, loss = 0.25733159\n",
      "Iteration 43, loss = 0.25671126\n",
      "Iteration 44, loss = 0.25843070\n",
      "Iteration 45, loss = 0.25602861\n",
      "Iteration 46, loss = 0.25710938\n",
      "Iteration 47, loss = 0.25655309\n",
      "Iteration 48, loss = 0.25718432\n",
      "Iteration 49, loss = 0.25491538\n",
      "Iteration 50, loss = 0.25609975\n",
      "Iteration 51, loss = 0.25521076\n",
      "Iteration 52, loss = 0.25450765\n",
      "Iteration 53, loss = 0.25600973\n",
      "Iteration 54, loss = 0.25473756\n",
      "Iteration 55, loss = 0.25496073\n",
      "Iteration 56, loss = 0.25399957\n",
      "Iteration 57, loss = 0.25503994\n",
      "Iteration 58, loss = 0.25500952\n",
      "Iteration 59, loss = 0.25544506\n",
      "Iteration 60, loss = 0.25472448\n",
      "Iteration 61, loss = 0.25373020\n",
      "Iteration 62, loss = 0.25393649\n",
      "Iteration 63, loss = 0.25345297\n",
      "Iteration 64, loss = 0.25405731\n",
      "Iteration 65, loss = 0.25319420\n",
      "Iteration 66, loss = 0.25372999\n",
      "Iteration 67, loss = 0.25337601\n",
      "Iteration 68, loss = 0.25337388\n",
      "Iteration 69, loss = 0.25280439\n",
      "Iteration 70, loss = 0.25345921\n",
      "Iteration 71, loss = 0.25430733\n",
      "Iteration 72, loss = 0.25226898\n",
      "Iteration 73, loss = 0.25412238\n",
      "Iteration 74, loss = 0.25413913\n",
      "Iteration 75, loss = 0.25290628\n",
      "Iteration 76, loss = 0.25192252\n",
      "Iteration 77, loss = 0.25278352\n",
      "Iteration 78, loss = 0.25285036\n",
      "Iteration 79, loss = 0.25286266\n",
      "Iteration 80, loss = 0.25291936\n",
      "Iteration 81, loss = 0.25222745\n",
      "Iteration 82, loss = 0.25215961\n",
      "Iteration 83, loss = 0.25200951\n",
      "Iteration 84, loss = 0.25293878\n",
      "Iteration 85, loss = 0.25251486\n",
      "Iteration 86, loss = 0.25207474\n",
      "Iteration 87, loss = 0.25193060\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30138636\n",
      "Iteration 2, loss = 0.28590734\n",
      "Iteration 3, loss = 0.28518088\n",
      "Iteration 4, loss = 0.28451139\n",
      "Iteration 5, loss = 0.28358740\n",
      "Iteration 6, loss = 0.28194750\n",
      "Iteration 7, loss = 0.28099379\n",
      "Iteration 8, loss = 0.28007069\n",
      "Iteration 9, loss = 0.27849678\n",
      "Iteration 10, loss = 0.27719844\n",
      "Iteration 11, loss = 0.27774164\n",
      "Iteration 12, loss = 0.27512446\n",
      "Iteration 13, loss = 0.27514161\n",
      "Iteration 14, loss = 0.27427881\n",
      "Iteration 15, loss = 0.27231228\n",
      "Iteration 16, loss = 0.27201961\n",
      "Iteration 17, loss = 0.27059395\n",
      "Iteration 18, loss = 0.27011376\n",
      "Iteration 19, loss = 0.26943839\n",
      "Iteration 20, loss = 0.26941230\n",
      "Iteration 21, loss = 0.26717409\n",
      "Iteration 22, loss = 0.26710283\n",
      "Iteration 23, loss = 0.26645650\n",
      "Iteration 24, loss = 0.26616928\n",
      "Iteration 25, loss = 0.26384040\n",
      "Iteration 26, loss = 0.26417837\n",
      "Iteration 27, loss = 0.26351824\n",
      "Iteration 28, loss = 0.26230694\n",
      "Iteration 29, loss = 0.26282930\n",
      "Iteration 30, loss = 0.26119629\n",
      "Iteration 31, loss = 0.26103670\n",
      "Iteration 32, loss = 0.25998937\n",
      "Iteration 33, loss = 0.25950356\n",
      "Iteration 34, loss = 0.25909999\n",
      "Iteration 35, loss = 0.25835620\n",
      "Iteration 36, loss = 0.25789897\n",
      "Iteration 37, loss = 0.25733282\n",
      "Iteration 38, loss = 0.25662347\n",
      "Iteration 39, loss = 0.25730018\n",
      "Iteration 40, loss = 0.25602897\n",
      "Iteration 41, loss = 0.25628143\n",
      "Iteration 42, loss = 0.25553650\n",
      "Iteration 43, loss = 0.25568252\n",
      "Iteration 44, loss = 0.25520601\n",
      "Iteration 45, loss = 0.25448823\n",
      "Iteration 46, loss = 0.25542709\n",
      "Iteration 47, loss = 0.25467085\n",
      "Iteration 48, loss = 0.25412930\n",
      "Iteration 49, loss = 0.25339090\n",
      "Iteration 50, loss = 0.25318239\n",
      "Iteration 51, loss = 0.25313576\n",
      "Iteration 52, loss = 0.25361484\n",
      "Iteration 53, loss = 0.25351545\n",
      "Iteration 54, loss = 0.25293771\n",
      "Iteration 55, loss = 0.25160857\n",
      "Iteration 56, loss = 0.25269415\n",
      "Iteration 57, loss = 0.25176004\n",
      "Iteration 58, loss = 0.25169953\n",
      "Iteration 59, loss = 0.25288268\n",
      "Iteration 60, loss = 0.25116724\n",
      "Iteration 61, loss = 0.25153739\n",
      "Iteration 62, loss = 0.25077240\n",
      "Iteration 63, loss = 0.25131164\n",
      "Iteration 64, loss = 0.25123820\n",
      "Iteration 65, loss = 0.25133102\n",
      "Iteration 66, loss = 0.25079006\n",
      "Iteration 67, loss = 0.25090865\n",
      "Iteration 68, loss = 0.25021034\n",
      "Iteration 69, loss = 0.25090616\n",
      "Iteration 70, loss = 0.25132758\n",
      "Iteration 71, loss = 0.25099608\n",
      "Iteration 72, loss = 0.25043384\n",
      "Iteration 73, loss = 0.25112297\n",
      "Iteration 74, loss = 0.25071359\n",
      "Iteration 75, loss = 0.24994767\n",
      "Iteration 76, loss = 0.25097403\n",
      "Iteration 77, loss = 0.25142350\n",
      "Iteration 78, loss = 0.25146710\n",
      "Iteration 79, loss = 0.25040678\n",
      "Iteration 80, loss = 0.25120072\n",
      "Iteration 81, loss = 0.24888012\n",
      "Iteration 82, loss = 0.25054425\n",
      "Iteration 83, loss = 0.25076175\n",
      "Iteration 84, loss = 0.25083948\n",
      "Iteration 85, loss = 0.25160913\n",
      "Iteration 86, loss = 0.24944485\n",
      "Iteration 87, loss = 0.25010618\n",
      "Iteration 88, loss = 0.24998613\n",
      "Iteration 89, loss = 0.24919816\n",
      "Iteration 90, loss = 0.24917637\n",
      "Iteration 91, loss = 0.24885800\n",
      "Iteration 92, loss = 0.24907652\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.31257308\n",
      "Iteration 2, loss = 0.28703493\n",
      "Iteration 3, loss = 0.28470494\n",
      "Iteration 4, loss = 0.28450174\n",
      "Iteration 5, loss = 0.28323848\n",
      "Iteration 6, loss = 0.28198523\n",
      "Iteration 7, loss = 0.28015452\n",
      "Iteration 8, loss = 0.27962731\n",
      "Iteration 9, loss = 0.27884996\n",
      "Iteration 10, loss = 0.27850835\n",
      "Iteration 11, loss = 0.27704949\n",
      "Iteration 12, loss = 0.27542240\n",
      "Iteration 13, loss = 0.27536950\n",
      "Iteration 14, loss = 0.27492388\n",
      "Iteration 15, loss = 0.27359933\n",
      "Iteration 16, loss = 0.27376525\n",
      "Iteration 17, loss = 0.27337446\n",
      "Iteration 18, loss = 0.27266357\n",
      "Iteration 19, loss = 0.27127977\n",
      "Iteration 20, loss = 0.27199192\n",
      "Iteration 21, loss = 0.27034839\n",
      "Iteration 22, loss = 0.26999838\n",
      "Iteration 23, loss = 0.26905386\n",
      "Iteration 24, loss = 0.26863490\n",
      "Iteration 25, loss = 0.26836687\n",
      "Iteration 26, loss = 0.26803393\n",
      "Iteration 27, loss = 0.26771133\n",
      "Iteration 28, loss = 0.26799305\n",
      "Iteration 29, loss = 0.26687587\n",
      "Iteration 30, loss = 0.26671309\n",
      "Iteration 31, loss = 0.26640548\n",
      "Iteration 32, loss = 0.26584992\n",
      "Iteration 33, loss = 0.26553130\n",
      "Iteration 34, loss = 0.26551863\n",
      "Iteration 35, loss = 0.26430405\n",
      "Iteration 36, loss = 0.26344245\n",
      "Iteration 37, loss = 0.26574608\n",
      "Iteration 38, loss = 0.26337096\n",
      "Iteration 39, loss = 0.26348170\n",
      "Iteration 40, loss = 0.26212009\n",
      "Iteration 41, loss = 0.26181745\n",
      "Iteration 42, loss = 0.26204139\n",
      "Iteration 43, loss = 0.26122319\n",
      "Iteration 44, loss = 0.26224909\n",
      "Iteration 45, loss = 0.26149796\n",
      "Iteration 46, loss = 0.26167625\n",
      "Iteration 47, loss = 0.26168460\n",
      "Iteration 48, loss = 0.26111226\n",
      "Iteration 49, loss = 0.26007485\n",
      "Iteration 50, loss = 0.26064274\n",
      "Iteration 51, loss = 0.26053223\n",
      "Iteration 52, loss = 0.26083877\n",
      "Iteration 53, loss = 0.25986329\n",
      "Iteration 54, loss = 0.26067017\n",
      "Iteration 55, loss = 0.26000178\n",
      "Iteration 56, loss = 0.26028345\n",
      "Iteration 57, loss = 0.25900165\n",
      "Iteration 58, loss = 0.26024536\n",
      "Iteration 59, loss = 0.25991228\n",
      "Iteration 60, loss = 0.25984297\n",
      "Iteration 61, loss = 0.25943122\n",
      "Iteration 62, loss = 0.26001993\n",
      "Iteration 63, loss = 0.26057125\n",
      "Iteration 64, loss = 0.25972863\n",
      "Iteration 65, loss = 0.25930756\n",
      "Iteration 66, loss = 0.25912458\n",
      "Iteration 67, loss = 0.25801885\n",
      "Iteration 68, loss = 0.25868260\n",
      "Iteration 69, loss = 0.25835538\n",
      "Iteration 70, loss = 0.25843566\n",
      "Iteration 71, loss = 0.25951396\n",
      "Iteration 72, loss = 0.25815768\n",
      "Iteration 73, loss = 0.25859467\n",
      "Iteration 74, loss = 0.25950599\n",
      "Iteration 75, loss = 0.25805530\n",
      "Iteration 76, loss = 0.25801380\n",
      "Iteration 77, loss = 0.25896244\n",
      "Iteration 78, loss = 0.25806672\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per Hyperparameter tuning, best parameters are:\n",
      "{'hidden_layer_sizes': 5, 'learning_rate_init': 0.1}\n",
      "Iteration 1, loss = 0.39122629\n",
      "Iteration 2, loss = 0.35362936\n",
      "Iteration 3, loss = 0.31528661\n",
      "Iteration 1, loss = 0.39320930\n",
      "Iteration 4, loss = 0.30136539\n",
      "Iteration 2, loss = 0.35607765\n",
      "Iteration 5, loss = 0.29181263\n",
      "Iteration 3, loss = 0.31716130\n",
      "Iteration 6, loss = 0.28596297\n",
      "Iteration 4, loss = 0.30697719\n",
      "Iteration 7, loss = 0.28090298\n",
      "Iteration 5, loss = 0.29573665\n",
      "Iteration 8, loss = 0.27859130\n",
      "Iteration 6, loss = 0.28526214\n",
      "Iteration 9, loss = 0.27585206\n",
      "Iteration 7, loss = 0.28040570\n",
      "Iteration 10, loss = 0.27204198\n",
      "Iteration 8, loss = 0.27787837\n",
      "Iteration 11, loss = 0.26932377\n",
      "Iteration 9, loss = 0.27523059\n",
      "Iteration 12, loss = 0.26590553\n",
      "Iteration 10, loss = 0.27363598\n",
      "Iteration 11, loss = 0.26883356\n",
      "Iteration 13, loss = 0.26337207Iteration 12, loss = 0.26733525\n",
      "\n",
      "Iteration 13, loss = 0.26599417\n",
      "Iteration 14, loss = 0.26023779\n",
      "Iteration 15, loss = 0.25897879\n",
      "Iteration 14, loss = 0.26211278\n",
      "Iteration 16, loss = 0.25583550\n",
      "Iteration 17, loss = 0.25429423\n",
      "Iteration 15, loss = 0.26003770\n",
      "Iteration 18, loss = 0.25728703\n",
      "Iteration 19, loss = 0.25310517\n",
      "Iteration 20, loss = 0.25280186Iteration 16, loss = 0.26084023\n",
      "\n",
      "Iteration 21, loss = 0.24674637\n",
      "Iteration 17, loss = 0.25755615Iteration 22, loss = 0.24585151\n",
      "\n",
      "Iteration 23, loss = 0.24832389\n",
      "Iteration 18, loss = 0.25769263\n",
      "Iteration 19, loss = 0.25017049\n",
      "Iteration 24, loss = 0.24230478\n",
      "Iteration 20, loss = 0.25092248\n",
      "Iteration 25, loss = 0.24306424\n",
      "Iteration 21, loss = 0.24667806\n",
      "Iteration 22, loss = 0.24460044Iteration 26, loss = 0.24335345\n",
      "Iteration 27, loss = 0.24086190\n",
      "\n",
      "Iteration 28, loss = 0.23946575\n",
      "Iteration 23, loss = 0.24463591\n",
      "Iteration 29, loss = 0.23431865\n",
      "Iteration 30, loss = 0.23428344\n",
      "Iteration 31, loss = 0.23225232\n",
      "Iteration 32, loss = 0.23247174\n",
      "Iteration 24, loss = 0.24282192\n",
      "Iteration 33, loss = 0.22980339\n",
      "Iteration 25, loss = 0.24205548\n",
      "Iteration 34, loss = 0.22771962\n",
      "Iteration 26, loss = 0.23869853\n",
      "Iteration 35, loss = 0.22296292\n",
      "Iteration 36, loss = 0.22139355\n",
      "Iteration 37, loss = 0.22190518\n",
      "Iteration 38, loss = 0.22207352\n",
      "Iteration 27, loss = 0.23880167\n",
      "Iteration 39, loss = 0.22301786\n",
      "Iteration 28, loss = 0.23828987Iteration 40, loss = 0.21857286\n",
      "Iteration 41, loss = 0.21923043\n",
      "Iteration 29, loss = 0.23532743\n",
      "\n",
      "Iteration 42, loss = 0.21665735Iteration 30, loss = 0.23464550\n",
      "Iteration 43, loss = 0.22016072\n",
      "Iteration 31, loss = 0.23260100\n",
      "\n",
      "Iteration 32, loss = 0.23543988Iteration 44, loss = 0.22251832\n",
      "\n",
      "Iteration 45, loss = 0.21665150Iteration 33, loss = 0.23346196\n",
      "Iteration 46, loss = 0.21185765\n",
      "\n",
      "Iteration 34, loss = 0.23952812\n",
      "Iteration 47, loss = 0.21824996\n",
      "Iteration 35, loss = 0.23447909\n",
      "Iteration 48, loss = 0.21196235\n",
      "Iteration 36, loss = 0.22882588\n",
      "Iteration 49, loss = 0.21414031\n",
      "Iteration 37, loss = 0.23083797\n",
      "Iteration 50, loss = 0.21042518\n",
      "Iteration 38, loss = 0.23199728\n",
      "Iteration 51, loss = 0.20786133\n",
      "Iteration 39, loss = 0.22648223\n",
      "Iteration 52, loss = 0.20822529\n",
      "Iteration 40, loss = 0.22694188\n",
      "Iteration 53, loss = 0.20642414\n",
      "Iteration 41, loss = 0.22633036\n",
      "Iteration 54, loss = 0.20376101\n",
      "Iteration 42, loss = 0.23233937Iteration 55, loss = 0.20294904\n",
      "\n",
      "Iteration 56, loss = 0.20144817\n",
      "Iteration 43, loss = 0.23332318\n",
      "Iteration 57, loss = 0.20257823\n",
      "Iteration 44, loss = 0.23068162\n",
      "Iteration 58, loss = 0.19985094\n",
      "Iteration 45, loss = 0.22960000\n",
      "Iteration 59, loss = 0.19897859\n",
      "Iteration 46, loss = 0.22523876\n",
      "Iteration 60, loss = 0.20130304\n",
      "Iteration 47, loss = 0.22714122\n",
      "Iteration 61, loss = 0.20110393\n",
      "Iteration 48, loss = 0.22797463\n",
      "Iteration 62, loss = 0.19816417\n",
      "Iteration 49, loss = 0.22570109\n",
      "Iteration 50, loss = 0.22355610\n",
      "Iteration 63, loss = 0.20042382\n",
      "Iteration 51, loss = 0.22133148\n",
      "Iteration 64, loss = 0.20036088\n",
      "Iteration 52, loss = 0.22059298\n",
      "Iteration 65, loss = 0.19362933\n",
      "Iteration 53, loss = 0.22007669Iteration 66, loss = 0.19539373\n",
      "\n",
      "Iteration 67, loss = 0.19190646\n",
      "Iteration 54, loss = 0.22221694\n",
      "Iteration 68, loss = 0.19092703\n",
      "Iteration 55, loss = 0.22088897\n",
      "Iteration 69, loss = 0.19542244\n",
      "Iteration 56, loss = 0.22021548\n",
      "Iteration 70, loss = 0.19673706\n",
      "Iteration 57, loss = 0.21745036\n",
      "Iteration 71, loss = 0.19596234\n",
      "Iteration 58, loss = 0.21862885\n",
      "Iteration 72, loss = 0.19447816\n",
      "Iteration 59, loss = 0.21670936\n",
      "Iteration 73, loss = 0.19012815\n",
      "Iteration 60, loss = 0.21641613\n",
      "Iteration 74, loss = 0.19404673\n",
      "Iteration 61, loss = 0.21616547\n",
      "Iteration 75, loss = 0.18842521\n",
      "Iteration 62, loss = 0.21691114\n",
      "Iteration 76, loss = 0.18975593\n",
      "Iteration 63, loss = 0.21994390\n",
      "Iteration 77, loss = 0.19081649\n",
      "Iteration 64, loss = 0.21687004\n",
      "Iteration 78, loss = 0.19018801\n",
      "Iteration 65, loss = 0.21728145\n",
      "Iteration 79, loss = 0.19141398\n",
      "Iteration 66, loss = 0.21587521\n",
      "Iteration 80, loss = 0.19070089\n",
      "Iteration 67, loss = 0.21819701\n",
      "Iteration 81, loss = 0.19107817\n",
      "Iteration 68, loss = 0.21813913\n",
      "Iteration 82, loss = 0.18787962\n",
      "Iteration 69, loss = 0.21631368\n",
      "Iteration 83, loss = 0.18639694\n",
      "Iteration 70, loss = 0.21572889Iteration 84, loss = 0.19151037\n",
      "\n",
      "Iteration 85, loss = 0.19237197\n",
      "Iteration 71, loss = 0.21443579\n",
      "Iteration 86, loss = 0.19026045\n",
      "Iteration 72, loss = 0.21455317\n",
      "Iteration 87, loss = 0.18501343\n",
      "Iteration 73, loss = 0.21206499\n",
      "Iteration 88, loss = 0.18581591Iteration 74, loss = 0.21209328\n",
      "\n",
      "Iteration 75, loss = 0.21144787\n",
      "Iteration 89, loss = 0.19586475\n",
      "Iteration 76, loss = 0.21106934\n",
      "Iteration 90, loss = 0.19024783\n",
      "Iteration 77, loss = 0.21392209\n",
      "Iteration 91, loss = 0.18857363\n",
      "Iteration 78, loss = 0.21431420\n",
      "Iteration 92, loss = 0.18857209\n",
      "Iteration 79, loss = 0.21376248\n",
      "Iteration 93, loss = 0.18441324\n",
      "Iteration 80, loss = 0.21244495\n",
      "Iteration 94, loss = 0.18206321\n",
      "Iteration 81, loss = 0.21130554\n",
      "Iteration 95, loss = 0.18267465\n",
      "Iteration 82, loss = 0.21008648\n",
      "Iteration 96, loss = 0.18393175\n",
      "Iteration 83, loss = 0.20861827\n",
      "Iteration 97, loss = 0.18203200\n",
      "Iteration 84, loss = 0.20983380\n",
      "Iteration 98, loss = 0.18030145\n",
      "Iteration 85, loss = 0.20921759\n",
      "Iteration 99, loss = 0.18068150\n",
      "Iteration 86, loss = 0.20911861Iteration 100, loss = 0.17970661\n",
      "\n",
      "Iteration 101, loss = 0.18301608Iteration 87, loss = 0.20801511\n",
      "\n",
      "Iteration 102, loss = 0.17926328Iteration 88, loss = 0.20742373\n",
      "\n",
      "Iteration 103, loss = 0.17976617Iteration 89, loss = 0.21106192\n",
      "\n",
      "Iteration 104, loss = 0.18318713\n",
      "Iteration 90, loss = 0.20779708\n",
      "Iteration 105, loss = 0.18949525\n",
      "Iteration 91, loss = 0.20743751\n",
      "Iteration 106, loss = 0.18271604\n",
      "Iteration 92, loss = 0.20607015\n",
      "Iteration 107, loss = 0.18452699\n",
      "Iteration 93, loss = 0.20473593\n",
      "Iteration 108, loss = 0.18116067\n",
      "Iteration 94, loss = 0.20481431\n",
      "Iteration 109, loss = 0.18130515\n",
      "Iteration 95, loss = 0.20555514\n",
      "Iteration 110, loss = 0.17721890\n",
      "Iteration 96, loss = 0.20650614\n",
      "Iteration 97, loss = 0.20600943\n",
      "Iteration 111, loss = 0.17933672\n",
      "Iteration 98, loss = 0.20587619\n",
      "Iteration 112, loss = 0.17781339\n",
      "Iteration 99, loss = 0.20609041\n",
      "Iteration 113, loss = 0.17739436\n",
      "Iteration 100, loss = 0.20417364\n",
      "Iteration 114, loss = 0.17735099\n",
      "Iteration 101, loss = 0.20409295\n",
      "Iteration 115, loss = 0.17957442\n",
      "Iteration 102, loss = 0.20443643\n",
      "Iteration 116, loss = 0.18503235\n",
      "Iteration 103, loss = 0.20598381\n",
      "Iteration 117, loss = 0.17949421\n",
      "Iteration 104, loss = 0.20792885\n",
      "Iteration 118, loss = 0.17917509\n",
      "Iteration 105, loss = 0.20772983\n",
      "Iteration 119, loss = 0.19527588\n",
      "Iteration 106, loss = 0.20489913Iteration 120, loss = 0.18910243\n",
      "\n",
      "Iteration 121, loss = 0.18318509\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 107, loss = 0.20437898\n",
      "Iteration 108, loss = 0.20308950\n",
      "Iteration 109, loss = 0.20390351\n",
      "Iteration 110, loss = 0.20313917\n",
      "Iteration 111, loss = 0.20184963\n",
      "Iteration 112, loss = 0.20254890\n",
      "Iteration 113, loss = 0.20217191\n",
      "Iteration 114, loss = 0.20080486\n",
      "Iteration 115, loss = 0.20176782\n",
      "Iteration 116, loss = 0.20842149\n",
      "Iteration 1, loss = 0.38366870\n",
      "Iteration 117, loss = 0.20836584\n",
      "Iteration 2, loss = 0.34115513\n",
      "Iteration 118, loss = 0.20419035\n",
      "Iteration 3, loss = 0.31379369\n",
      "Iteration 119, loss = 0.20289422\n",
      "Iteration 4, loss = 0.29855823\n",
      "Iteration 120, loss = 0.20239397\n",
      "Iteration 121, loss = 0.20168995Iteration 5, loss = 0.29024356\n",
      "\n",
      "Iteration 6, loss = 0.28463077\n",
      "Iteration 122, loss = 0.20034563\n",
      "Iteration 7, loss = 0.28038569\n",
      "Iteration 123, loss = 0.20202934\n",
      "Iteration 8, loss = 0.27667194\n",
      "Iteration 124, loss = 0.19977906\n",
      "Iteration 9, loss = 0.27136236\n",
      "Iteration 125, loss = 0.20374558\n",
      "Iteration 10, loss = 0.27356431\n",
      "Iteration 126, loss = 0.20049995\n",
      "Iteration 11, loss = 0.26797953\n",
      "Iteration 127, loss = 0.19993819\n",
      "Iteration 12, loss = 0.26445179\n",
      "Iteration 128, loss = 0.20039946\n",
      "Iteration 13, loss = 0.26242482\n",
      "Iteration 129, loss = 0.20090995\n",
      "Iteration 14, loss = 0.26234889\n",
      "Iteration 15, loss = 0.25657582\n",
      "Iteration 16, loss = 0.25561873\n",
      "Iteration 130, loss = 0.20181211\n",
      "Iteration 17, loss = 0.25319149\n",
      "Iteration 131, loss = 0.20155789\n",
      "Iteration 18, loss = 0.25225673\n",
      "Iteration 132, loss = 0.20176040\n",
      "Iteration 19, loss = 0.24815160\n",
      "Iteration 133, loss = 0.20240442\n",
      "Iteration 20, loss = 0.24669576\n",
      "Iteration 134, loss = 0.20107402\n",
      "Iteration 21, loss = 0.24596717\n",
      "Iteration 135, loss = 0.20005924\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 22, loss = 0.24399643\n",
      "Iteration 23, loss = 0.24300066\n",
      "Iteration 24, loss = 0.23958652\n",
      "Iteration 25, loss = 0.23988135\n",
      "Iteration 26, loss = 0.23863604\n",
      "Iteration 27, loss = 0.24195309\n",
      "Iteration 28, loss = 0.24721655\n",
      "Iteration 29, loss = 0.25150854\n",
      "Iteration 30, loss = 0.23966441\n",
      "Iteration 31, loss = 0.23246395\n",
      "Iteration 1, loss = 0.38622159\n",
      "Iteration 32, loss = 0.22856965\n",
      "Iteration 2, loss = 0.35624380\n",
      "Iteration 33, loss = 0.23165729\n",
      "Iteration 3, loss = 0.31856758\n",
      "Iteration 4, loss = 0.31201540\n",
      "Iteration 5, loss = 0.29655347\n",
      "Iteration 34, loss = 0.22908900Iteration 6, loss = 0.29057702\n",
      "Iteration 7, loss = 0.28560537\n",
      "\n",
      "Iteration 8, loss = 0.28099206\n",
      "Iteration 35, loss = 0.23473949\n",
      "Iteration 9, loss = 0.27879594\n",
      "Iteration 36, loss = 0.23574480\n",
      "Iteration 37, loss = 0.22923595\n",
      "Iteration 10, loss = 0.27822234\n",
      "Iteration 38, loss = 0.22680538\n",
      "Iteration 11, loss = 0.27166906Iteration 39, loss = 0.22859354\n",
      "Iteration 40, loss = 0.22532214\n",
      "\n",
      "Iteration 41, loss = 0.22615410\n",
      "Iteration 42, loss = 0.22270603\n",
      "Iteration 12, loss = 0.26923410\n",
      "Iteration 13, loss = 0.27076817\n",
      "Iteration 43, loss = 0.22150891\n",
      "Iteration 14, loss = 0.26473888\n",
      "Iteration 44, loss = 0.22233976\n",
      "Iteration 45, loss = 0.21995326\n",
      "Iteration 46, loss = 0.21874051\n",
      "Iteration 15, loss = 0.26119921\n",
      "Iteration 47, loss = 0.21865606\n",
      "Iteration 16, loss = 0.26026452\n",
      "Iteration 48, loss = 0.21871900\n",
      "Iteration 49, loss = 0.21624052\n",
      "Iteration 17, loss = 0.25476473\n",
      "Iteration 18, loss = 0.25507939\n",
      "Iteration 50, loss = 0.21485843Iteration 19, loss = 0.25269011\n",
      "\n",
      "Iteration 20, loss = 0.25135380\n",
      "Iteration 21, loss = 0.25018747\n",
      "Iteration 51, loss = 0.21549847\n",
      "Iteration 22, loss = 0.25038196\n",
      "Iteration 52, loss = 0.21635891Iteration 23, loss = 0.24690074\n",
      "\n",
      "Iteration 24, loss = 0.24287457\n",
      "Iteration 25, loss = 0.24446602\n",
      "Iteration 53, loss = 0.21478796Iteration 26, loss = 0.24348576\n",
      "\n",
      "Iteration 27, loss = 0.24543759\n",
      "Iteration 54, loss = 0.21877897Iteration 28, loss = 0.25555523\n",
      "Iteration 29, loss = 0.26056639\n",
      "Iteration 30, loss = 0.24281263\n",
      "\n",
      "Iteration 31, loss = 0.23591616\n",
      "Iteration 55, loss = 0.21395695\n",
      "Iteration 32, loss = 0.23245393\n",
      "Iteration 56, loss = 0.21633452Iteration 33, loss = 0.23022964\n",
      "\n",
      "Iteration 34, loss = 0.23056680\n",
      "Iteration 57, loss = 0.21372784\n",
      "Iteration 35, loss = 0.23363422\n",
      "Iteration 58, loss = 0.21741057\n",
      "Iteration 36, loss = 0.23364156Iteration 59, loss = 0.21628013\n",
      "Iteration 60, loss = 0.21251515\n",
      "\n",
      "Iteration 61, loss = 0.21006894\n",
      "Iteration 37, loss = 0.22955918\n",
      "Iteration 62, loss = 0.20959598\n",
      "Iteration 38, loss = 0.22426096Iteration 63, loss = 0.21085170\n",
      "\n",
      "Iteration 64, loss = 0.20720927\n",
      "Iteration 65, loss = 0.20590374\n",
      "Iteration 39, loss = 0.22349221\n",
      "Iteration 66, loss = 0.20841763\n",
      "Iteration 40, loss = 0.22128969\n",
      "Iteration 67, loss = 0.20845957\n",
      "Iteration 68, loss = 0.20974939\n",
      "Iteration 69, loss = 0.20572262\n",
      "Iteration 41, loss = 0.22534740\n",
      "Iteration 70, loss = 0.20671519Iteration 42, loss = 0.22527830\n",
      "\n",
      "Iteration 43, loss = 0.22286665\n",
      "Iteration 71, loss = 0.20466836\n",
      "Iteration 44, loss = 0.22115281\n",
      "Iteration 72, loss = 0.20395024\n",
      "Iteration 73, loss = 0.20307240\n",
      "Iteration 45, loss = 0.21983057\n",
      "Iteration 74, loss = 0.20282570\n",
      "Iteration 46, loss = 0.21860235\n",
      "Iteration 75, loss = 0.20315534\n",
      "Iteration 47, loss = 0.21816030\n",
      "Iteration 76, loss = 0.20198145\n",
      "Iteration 48, loss = 0.22239432\n",
      "Iteration 77, loss = 0.20158835\n",
      "Iteration 49, loss = 0.21655339\n",
      "Iteration 78, loss = 0.20227184\n",
      "Iteration 50, loss = 0.21614786\n",
      "Iteration 79, loss = 0.20576864\n",
      "Iteration 51, loss = 0.21572301\n",
      "Iteration 80, loss = 0.20448850\n",
      "Iteration 52, loss = 0.21691041\n",
      "Iteration 81, loss = 0.20520921\n",
      "Iteration 53, loss = 0.21596113\n",
      "Iteration 82, loss = 0.20204841\n",
      "Iteration 54, loss = 0.21738222\n",
      "Iteration 83, loss = 0.20146429\n",
      "Iteration 55, loss = 0.21663269Iteration 84, loss = 0.19984798\n",
      "\n",
      "Iteration 85, loss = 0.19933492\n",
      "Iteration 56, loss = 0.21884182\n",
      "Iteration 86, loss = 0.20342139\n",
      "Iteration 57, loss = 0.21562113\n",
      "Iteration 87, loss = 0.19817501\n",
      "Iteration 58, loss = 0.21529305\n",
      "Iteration 88, loss = 0.20135363\n",
      "Iteration 59, loss = 0.21323950\n",
      "Iteration 89, loss = 0.20060266\n",
      "Iteration 60, loss = 0.21292465\n",
      "Iteration 90, loss = 0.19826430\n",
      "Iteration 91, loss = 0.19857639\n",
      "Iteration 61, loss = 0.21042958\n",
      "Iteration 92, loss = 0.19864433\n",
      "Iteration 62, loss = 0.21332157\n",
      "Iteration 93, loss = 0.19998205\n",
      "Iteration 63, loss = 0.21509963\n",
      "Iteration 94, loss = 0.20100529\n",
      "Iteration 64, loss = 0.21073707\n",
      "Iteration 95, loss = 0.19994444\n",
      "Iteration 65, loss = 0.20719774\n",
      "Iteration 96, loss = 0.20182336\n",
      "Iteration 66, loss = 0.20715749\n",
      "Iteration 97, loss = 0.20135121\n",
      "Iteration 67, loss = 0.20512010\n",
      "Iteration 98, loss = 0.19867047\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 68, loss = 0.20960146\n",
      "Iteration 69, loss = 0.20726161\n",
      "Iteration 70, loss = 0.20812385\n",
      "Iteration 71, loss = 0.20743874\n",
      "Iteration 1, loss = 0.39004624\n",
      "Iteration 72, loss = 0.20483765\n",
      "Iteration 2, loss = 0.35327877\n",
      "Iteration 3, loss = 0.31512912\n",
      "Iteration 4, loss = 0.30635791\n",
      "Iteration 5, loss = 0.28933963\n",
      "Iteration 6, loss = 0.28231831\n",
      "Iteration 73, loss = 0.20396294\n",
      "Iteration 7, loss = 0.27851498\n",
      "Iteration 8, loss = 0.27346136\n",
      "Iteration 74, loss = 0.20482010\n",
      "Iteration 9, loss = 0.27161959\n",
      "Iteration 75, loss = 0.20179800\n",
      "Iteration 10, loss = 0.27349812\n",
      "Iteration 76, loss = 0.20300168\n",
      "Iteration 11, loss = 0.26420198\n",
      "Iteration 77, loss = 0.20119673Iteration 12, loss = 0.26288092\n",
      "\n",
      "Iteration 13, loss = 0.26029791\n",
      "Iteration 78, loss = 0.20197000\n",
      "Iteration 14, loss = 0.25887361\n",
      "Iteration 79, loss = 0.20165935\n",
      "Iteration 15, loss = 0.25798825\n",
      "Iteration 80, loss = 0.20040373\n",
      "Iteration 16, loss = 0.25599606\n",
      "Iteration 81, loss = 0.20204973\n",
      "Iteration 17, loss = 0.24949313\n",
      "Iteration 82, loss = 0.20133568\n",
      "Iteration 18, loss = 0.24798502\n",
      "Iteration 83, loss = 0.20307653\n",
      "Iteration 19, loss = 0.24402659\n",
      "Iteration 84, loss = 0.20194481\n",
      "Iteration 20, loss = 0.24113826\n",
      "Iteration 21, loss = 0.23947949Iteration 85, loss = 0.20153000\n",
      "\n",
      "Iteration 22, loss = 0.23785779\n",
      "Iteration 86, loss = 0.20127229\n",
      "Iteration 23, loss = 0.23458186\n",
      "Iteration 87, loss = 0.19847821\n",
      "Iteration 24, loss = 0.23315861\n",
      "Iteration 88, loss = 0.20184014\n",
      "Iteration 25, loss = 0.23092842\n",
      "Iteration 89, loss = 0.20360279\n",
      "Iteration 26, loss = 0.23027095\n",
      "Iteration 90, loss = 0.20241107\n",
      "Iteration 27, loss = 0.23600635\n",
      "Iteration 91, loss = 0.20033524\n",
      "Iteration 28, loss = 0.23363209\n",
      "Iteration 29, loss = 0.23431621Iteration 92, loss = 0.20292165\n",
      "\n",
      "Iteration 30, loss = 0.22508852\n",
      "Iteration 93, loss = 0.20124981\n",
      "Iteration 31, loss = 0.22293438\n",
      "Iteration 94, loss = 0.20039342\n",
      "Iteration 32, loss = 0.22105544\n",
      "Iteration 95, loss = 0.19825125\n",
      "Iteration 33, loss = 0.22043535\n",
      "Iteration 96, loss = 0.19781104\n",
      "Iteration 34, loss = 0.22377889\n",
      "Iteration 97, loss = 0.19822939\n",
      "Iteration 35, loss = 0.23020490\n",
      "Iteration 98, loss = 0.19748642\n",
      "Iteration 36, loss = 0.22347007\n",
      "Iteration 99, loss = 0.19803755\n",
      "Iteration 37, loss = 0.22142217\n",
      "Iteration 38, loss = 0.21620472\n",
      "Iteration 100, loss = 0.19876749\n",
      "Iteration 39, loss = 0.21604290\n",
      "Iteration 101, loss = 0.19708921\n",
      "Iteration 102, loss = 0.19787003\n",
      "Iteration 40, loss = 0.21584823\n",
      "Iteration 103, loss = 0.19794852\n",
      "Iteration 41, loss = 0.21715096\n",
      "Iteration 104, loss = 0.19892857\n",
      "Iteration 42, loss = 0.21304374\n",
      "Iteration 105, loss = 0.19914806\n",
      "Iteration 43, loss = 0.21190149\n",
      "Iteration 106, loss = 0.19876065\n",
      "Iteration 44, loss = 0.20776623\n",
      "Iteration 107, loss = 0.19626069\n",
      "Iteration 45, loss = 0.20812570\n",
      "Iteration 108, loss = 0.19530983\n",
      "Iteration 46, loss = 0.20612822\n",
      "Iteration 109, loss = 0.19581381\n",
      "Iteration 47, loss = 0.21281769\n",
      "Iteration 110, loss = 0.19877378\n",
      "Iteration 48, loss = 0.20948280\n",
      "Iteration 111, loss = 0.19727690\n",
      "Iteration 49, loss = 0.20914136\n",
      "Iteration 112, loss = 0.20184950\n",
      "Iteration 50, loss = 0.20752853\n",
      "Iteration 113, loss = 0.19611378\n",
      "Iteration 51, loss = 0.20607214\n",
      "Iteration 114, loss = 0.19829973\n",
      "Iteration 52, loss = 0.20400838\n",
      "Iteration 115, loss = 0.19804166\n",
      "Iteration 53, loss = 0.20184702\n",
      "Iteration 116, loss = 0.19637186\n",
      "Iteration 54, loss = 0.20372185\n",
      "Iteration 117, loss = 0.19529655\n",
      "Iteration 55, loss = 0.20433131\n",
      "Iteration 118, loss = 0.19699725\n",
      "Iteration 56, loss = 0.20627131\n",
      "Iteration 119, loss = 0.19769884\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 57, loss = 0.20496187\n",
      "Iteration 58, loss = 0.20575130\n",
      "Iteration 59, loss = 0.20028142\n",
      "Iteration 60, loss = 0.19660433\n",
      "Iteration 1, loss = 0.39323267\n",
      "Iteration 61, loss = 0.19675910\n",
      "Iteration 2, loss = 0.35776767\n",
      "Iteration 62, loss = 0.19815743\n",
      "Iteration 3, loss = 0.31516875\n",
      "Iteration 63, loss = 0.20178885\n",
      "Iteration 4, loss = 0.30628042\n",
      "Iteration 64, loss = 0.19989515\n",
      "Iteration 5, loss = 0.29092185\n",
      "Iteration 65, loss = 0.19313795\n",
      "Iteration 6, loss = 0.28505791\n",
      "Iteration 66, loss = 0.19685539\n",
      "Iteration 7, loss = 0.28217569\n",
      "Iteration 67, loss = 0.19405674\n",
      "Iteration 8, loss = 0.27763310\n",
      "Iteration 68, loss = 0.19477597\n",
      "Iteration 9, loss = 0.27414878\n",
      "Iteration 69, loss = 0.19313975\n",
      "Iteration 10, loss = 0.27661549\n",
      "Iteration 70, loss = 0.19353602\n",
      "Iteration 11, loss = 0.26800032\n",
      "Iteration 71, loss = 0.19159974\n",
      "Iteration 12, loss = 0.26526539\n",
      "Iteration 72, loss = 0.18927828\n",
      "Iteration 13, loss = 0.26447964\n",
      "Iteration 73, loss = 0.18910848\n",
      "Iteration 14, loss = 0.26430813\n",
      "Iteration 74, loss = 0.19089229\n",
      "Iteration 15, loss = 0.25719571\n",
      "Iteration 75, loss = 0.19081108\n",
      "Iteration 16, loss = 0.25720524\n",
      "Iteration 76, loss = 0.19085329\n",
      "Iteration 17, loss = 0.25331024\n",
      "Iteration 77, loss = 0.18891514\n",
      "Iteration 18, loss = 0.25257300\n",
      "Iteration 78, loss = 0.18942490\n",
      "Iteration 19, loss = 0.24834883\n",
      "Iteration 20, loss = 0.24818142\n",
      "Iteration 79, loss = 0.18932193\n",
      "Iteration 21, loss = 0.24525576\n",
      "Iteration 80, loss = 0.18804906Iteration 22, loss = 0.24386980\n",
      "\n",
      "Iteration 23, loss = 0.23901268\n",
      "Iteration 81, loss = 0.18815718Iteration 24, loss = 0.23586629\n",
      "\n",
      "Iteration 25, loss = 0.23490391\n",
      "Iteration 26, loss = 0.23246035\n",
      "Iteration 82, loss = 0.18767443\n",
      "Iteration 27, loss = 0.23271674\n",
      "Iteration 83, loss = 0.18741743Iteration 28, loss = 0.24266757\n",
      "\n",
      "Iteration 29, loss = 0.23525298\n",
      "Iteration 30, loss = 0.23093389\n",
      "Iteration 84, loss = 0.18948032\n",
      "Iteration 85, loss = 0.18602681\n",
      "Iteration 86, loss = 0.18632496\n",
      "Iteration 31, loss = 0.22843644\n",
      "Iteration 87, loss = 0.18554592\n",
      "Iteration 32, loss = 0.22604466\n",
      "Iteration 88, loss = 0.18485242\n",
      "Iteration 33, loss = 0.22370034\n",
      "Iteration 89, loss = 0.18532728\n",
      "Iteration 34, loss = 0.22615753\n",
      "Iteration 90, loss = 0.18350606\n",
      "Iteration 35, loss = 0.22847942\n",
      "Iteration 91, loss = 0.18437185\n",
      "Iteration 36, loss = 0.22355634\n",
      "Iteration 92, loss = 0.18396463\n",
      "Iteration 93, loss = 0.18257033\n",
      "Iteration 94, loss = 0.18866514\n",
      "Iteration 37, loss = 0.22246448\n",
      "Iteration 95, loss = 0.18517395\n",
      "Iteration 38, loss = 0.21976807\n",
      "Iteration 39, loss = 0.21758554\n",
      "Iteration 96, loss = 0.18726632\n",
      "Iteration 40, loss = 0.21719624\n",
      "Iteration 97, loss = 0.18995204Iteration 41, loss = 0.21632513\n",
      "\n",
      "Iteration 98, loss = 0.18590274\n",
      "Iteration 42, loss = 0.21530517\n",
      "Iteration 43, loss = 0.21399713\n",
      "Iteration 44, loss = 0.21378228\n",
      "Iteration 99, loss = 0.18669657\n",
      "Iteration 45, loss = 0.21285388\n",
      "Iteration 100, loss = 0.18929099\n",
      "Iteration 101, loss = 0.18803535\n",
      "Iteration 102, loss = 0.18582807\n",
      "Iteration 46, loss = 0.21547824\n",
      "Iteration 103, loss = 0.18443804\n",
      "Iteration 104, loss = 0.18220014\n",
      "Iteration 105, loss = 0.18347669\n",
      "Iteration 106, loss = 0.18362457\n",
      "Iteration 47, loss = 0.21368532\n",
      "Iteration 107, loss = 0.18543019\n",
      "Iteration 48, loss = 0.21349994\n",
      "Iteration 108, loss = 0.18263582\n",
      "Iteration 49, loss = 0.21145610\n",
      "Iteration 109, loss = 0.18443566\n",
      "Iteration 50, loss = 0.21024990\n",
      "Iteration 110, loss = 0.19506769\n",
      "Iteration 51, loss = 0.21006178\n",
      "Iteration 111, loss = 0.19064495\n",
      "Iteration 52, loss = 0.20964174\n",
      "Iteration 112, loss = 0.18642922\n",
      "Iteration 113, loss = 0.18772402\n",
      "Iteration 114, loss = 0.18083684\n",
      "Iteration 53, loss = 0.20818754\n",
      "Iteration 115, loss = 0.18213821\n",
      "Iteration 116, loss = 0.18089459\n",
      "Iteration 54, loss = 0.21024523\n",
      "Iteration 117, loss = 0.17892891\n",
      "Iteration 118, loss = 0.17844730\n",
      "Iteration 55, loss = 0.20792476\n",
      "Iteration 56, loss = 0.21278648\n",
      "Iteration 119, loss = 0.17773320Iteration 57, loss = 0.21518926\n",
      "\n",
      "Iteration 58, loss = 0.20844417\n",
      "Iteration 120, loss = 0.17787560\n",
      "Iteration 59, loss = 0.20731924\n",
      "Iteration 60, loss = 0.20590497\n",
      "Iteration 61, loss = 0.20476813\n",
      "Iteration 121, loss = 0.18103497\n",
      "Iteration 62, loss = 0.20537157\n",
      "Iteration 122, loss = 0.17985604\n",
      "Iteration 63, loss = 0.20564267\n",
      "Iteration 64, loss = 0.20675854\n",
      "Iteration 123, loss = 0.18021677\n",
      "Iteration 124, loss = 0.17807231\n",
      "Iteration 125, loss = 0.18128372\n",
      "Iteration 126, loss = 0.18015243\n",
      "Iteration 65, loss = 0.20505892\n",
      "Iteration 127, loss = 0.18544919\n",
      "Iteration 66, loss = 0.20475409\n",
      "Iteration 128, loss = 0.18048575\n",
      "Iteration 67, loss = 0.20588973\n",
      "Iteration 129, loss = 0.17984898\n",
      "Iteration 68, loss = 0.20589738\n",
      "Iteration 130, loss = 0.18068329\n",
      "Iteration 69, loss = 0.20531021\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 70, loss = 0.20474511\n",
      "Iteration 71, loss = 0.20368442\n",
      "Iteration 72, loss = 0.20270709\n",
      "Iteration 73, loss = 0.20226607\n",
      "Iteration 74, loss = 0.20413233\n",
      "Iteration 75, loss = 0.20022201Iteration 1, loss = 0.39215997\n",
      "\n",
      "Iteration 76, loss = 0.20388203\n",
      "Iteration 77, loss = 0.20060198\n",
      "Iteration 2, loss = 0.35638366\n",
      "Iteration 78, loss = 0.19903494\n",
      "Iteration 3, loss = 0.31754593\n",
      "Iteration 79, loss = 0.19797129\n",
      "Iteration 4, loss = 0.31061211\n",
      "Iteration 80, loss = 0.19774080\n",
      "Iteration 5, loss = 0.29349902\n",
      "Iteration 81, loss = 0.19895510\n",
      "Iteration 6, loss = 0.28732247Iteration 82, loss = 0.20161379\n",
      "Iteration 83, loss = 0.19891018\n",
      "\n",
      "Iteration 84, loss = 0.20041055\n",
      "Iteration 7, loss = 0.28355439\n",
      "Iteration 85, loss = 0.19986455\n",
      "Iteration 8, loss = 0.27747971\n",
      "Iteration 86, loss = 0.19753445\n",
      "Iteration 9, loss = 0.27363262\n",
      "Iteration 87, loss = 0.20011882\n",
      "Iteration 10, loss = 0.27606828\n",
      "Iteration 88, loss = 0.19794585\n",
      "Iteration 11, loss = 0.26958425Iteration 89, loss = 0.19912598\n",
      "\n",
      "Iteration 90, loss = 0.19819037\n",
      "Iteration 12, loss = 0.26456811\n",
      "Iteration 91, loss = 0.19570848\n",
      "Iteration 13, loss = 0.26346021\n",
      "Iteration 92, loss = 0.19483071\n",
      "Iteration 14, loss = 0.27008496\n",
      "Iteration 93, loss = 0.19420021\n",
      "Iteration 15, loss = 0.25914823\n",
      "Iteration 94, loss = 0.19458131\n",
      "Iteration 95, loss = 0.19540496\n",
      "Iteration 16, loss = 0.26021029\n",
      "Iteration 96, loss = 0.19625779\n",
      "Iteration 17, loss = 0.25269527\n",
      "Iteration 97, loss = 0.20098090\n",
      "Iteration 18, loss = 0.25186092\n",
      "Iteration 98, loss = 0.19920631\n",
      "Iteration 19, loss = 0.25038092\n",
      "Iteration 99, loss = 0.20089492\n",
      "Iteration 20, loss = 0.24609607\n",
      "Iteration 100, loss = 0.19931812\n",
      "Iteration 101, loss = 0.19769746\n",
      "Iteration 21, loss = 0.24312759\n",
      "Iteration 102, loss = 0.19672312\n",
      "Iteration 22, loss = 0.24100504\n",
      "Iteration 103, loss = 0.19406844\n",
      "Iteration 23, loss = 0.24166086\n",
      "Iteration 104, loss = 0.19455730\n",
      "Iteration 24, loss = 0.23727830\n",
      "Iteration 105, loss = 0.19341052\n",
      "Iteration 25, loss = 0.23683827\n",
      "Iteration 106, loss = 0.19798085\n",
      "Iteration 26, loss = 0.23188659\n",
      "Iteration 107, loss = 0.19350582\n",
      "Iteration 108, loss = 0.19221723Iteration 27, loss = 0.23791203\n",
      "\n",
      "Iteration 28, loss = 0.23339725Iteration 109, loss = 0.19510520\n",
      "\n",
      "Iteration 110, loss = 0.19753553\n",
      "Iteration 29, loss = 0.23096848\n",
      "Iteration 111, loss = 0.20168885\n",
      "Iteration 30, loss = 0.23112118\n",
      "Iteration 112, loss = 0.19712887\n",
      "Iteration 31, loss = 0.22958100\n",
      "Iteration 113, loss = 0.19460469\n",
      "Iteration 32, loss = 0.22953164\n",
      "Iteration 114, loss = 0.19208688\n",
      "Iteration 33, loss = 0.22613411\n",
      "Iteration 115, loss = 0.19067952\n",
      "Iteration 34, loss = 0.22249896\n",
      "Iteration 116, loss = 0.19180391\n",
      "Iteration 35, loss = 0.22338874\n",
      "Iteration 117, loss = 0.19064460\n",
      "Iteration 36, loss = 0.22431517\n",
      "Iteration 118, loss = 0.18971225\n",
      "Iteration 37, loss = 0.22114488\n",
      "Iteration 119, loss = 0.19010402\n",
      "Iteration 38, loss = 0.21803457\n",
      "Iteration 120, loss = 0.18976563\n",
      "Iteration 39, loss = 0.21682843\n",
      "Iteration 121, loss = 0.18906476\n",
      "Iteration 40, loss = 0.21599434\n",
      "Iteration 122, loss = 0.18849890\n",
      "Iteration 123, loss = 0.18957692Iteration 41, loss = 0.21767859\n",
      "\n",
      "Iteration 124, loss = 0.18957538Iteration 42, loss = 0.21545337\n",
      "\n",
      "Iteration 43, loss = 0.21865004Iteration 125, loss = 0.19114559\n",
      "\n",
      "Iteration 126, loss = 0.19126474\n",
      "Iteration 44, loss = 0.21521874\n",
      "Iteration 127, loss = 0.19423318\n",
      "Iteration 45, loss = 0.21398731\n",
      "Iteration 46, loss = 0.21989639\n",
      "Iteration 128, loss = 0.19245574\n",
      "Iteration 47, loss = 0.22738104\n",
      "Iteration 129, loss = 0.19086139\n",
      "Iteration 48, loss = 0.23634930\n",
      "Iteration 130, loss = 0.19310022\n",
      "Iteration 49, loss = 0.22587159\n",
      "Iteration 131, loss = 0.19089618\n",
      "Iteration 50, loss = 0.22203633\n",
      "Iteration 132, loss = 0.18954725\n",
      "Iteration 51, loss = 0.21125703\n",
      "Iteration 133, loss = 0.18754852\n",
      "Iteration 52, loss = 0.20750004\n",
      "Iteration 134, loss = 0.18997752\n",
      "Iteration 53, loss = 0.20504501\n",
      "Iteration 54, loss = 0.20409332Iteration 135, loss = 0.19013552\n",
      "\n",
      "Iteration 55, loss = 0.20483580\n",
      "Iteration 136, loss = 0.18947442\n",
      "Iteration 56, loss = 0.20290467\n",
      "Iteration 137, loss = 0.18701029\n",
      "Iteration 57, loss = 0.20473790\n",
      "Iteration 138, loss = 0.18981696\n",
      "Iteration 58, loss = 0.20210387\n",
      "Iteration 139, loss = 0.18781400\n",
      "Iteration 59, loss = 0.20542792\n",
      "Iteration 60, loss = 0.20157042Iteration 140, loss = 0.18741974\n",
      "\n",
      "Iteration 61, loss = 0.20157949\n",
      "Iteration 141, loss = 0.18745186\n",
      "Iteration 62, loss = 0.20548659\n",
      "Iteration 142, loss = 0.18576880\n",
      "Iteration 63, loss = 0.21039858\n",
      "Iteration 143, loss = 0.18504560\n",
      "Iteration 64, loss = 0.20238230\n",
      "Iteration 144, loss = 0.18750209\n",
      "Iteration 145, loss = 0.18785574Iteration 65, loss = 0.20272106\n",
      "\n",
      "Iteration 66, loss = 0.19896707\n",
      "Iteration 146, loss = 0.18716550\n",
      "Iteration 67, loss = 0.19705334\n",
      "Iteration 147, loss = 0.18541420\n",
      "Iteration 68, loss = 0.19849388\n",
      "Iteration 148, loss = 0.18445043\n",
      "Iteration 69, loss = 0.19550435\n",
      "Iteration 149, loss = 0.18484282\n",
      "Iteration 70, loss = 0.19649057\n",
      "Iteration 150, loss = 0.18167092\n",
      "Iteration 71, loss = 0.19967310\n",
      "Iteration 72, loss = 0.19715064\n",
      "Iteration 151, loss = 0.18301414\n",
      "Iteration 73, loss = 0.19448215\n",
      "Iteration 152, loss = 0.18081105\n",
      "Iteration 74, loss = 0.19704318\n",
      "Iteration 153, loss = 0.17896877\n",
      "Iteration 75, loss = 0.19543653\n",
      "Iteration 154, loss = 0.17802573\n",
      "Iteration 76, loss = 0.19844148\n",
      "Iteration 155, loss = 0.17796952\n",
      "Iteration 77, loss = 0.19530501\n",
      "Iteration 156, loss = 0.17786842\n",
      "Iteration 78, loss = 0.19336755\n",
      "Iteration 157, loss = 0.18064672\n",
      "Iteration 79, loss = 0.19422756\n",
      "Iteration 158, loss = 0.17835808Iteration 80, loss = 0.19133400\n",
      "\n",
      "Iteration 81, loss = 0.19177470\n",
      "Iteration 159, loss = 0.18075082\n",
      "Iteration 82, loss = 0.19232953\n",
      "Iteration 160, loss = 0.18134672\n",
      "Iteration 83, loss = 0.19113473\n",
      "Iteration 161, loss = 0.18100689\n",
      "Iteration 84, loss = 0.19190127\n",
      "Iteration 162, loss = 0.18189489\n",
      "Iteration 85, loss = 0.19089292\n",
      "Iteration 86, loss = 0.19198399\n",
      "Iteration 87, loss = 0.19326617\n",
      "Iteration 163, loss = 0.18012704\n",
      "Iteration 88, loss = 0.19628527\n",
      "Iteration 89, loss = 0.19300329\n",
      "Iteration 90, loss = 0.19249572\n",
      "Iteration 164, loss = 0.17857938\n",
      "Iteration 91, loss = 0.19010789\n",
      "Iteration 165, loss = 0.17635222\n",
      "Iteration 92, loss = 0.19402416\n",
      "Iteration 166, loss = 0.17613614\n",
      "Iteration 93, loss = 0.19167588\n",
      "Iteration 167, loss = 0.17714971\n",
      "Iteration 94, loss = 0.19729743\n",
      "Iteration 168, loss = 0.17796383\n",
      "Iteration 95, loss = 0.19237708\n",
      "Iteration 169, loss = 0.17946144\n",
      "Iteration 96, loss = 0.19162225\n",
      "Iteration 170, loss = 0.17848706\n",
      "Iteration 97, loss = 0.19792856\n",
      "Iteration 98, loss = 0.19309740\n",
      "Iteration 171, loss = 0.17812093\n",
      "Iteration 99, loss = 0.20054788\n",
      "Iteration 172, loss = 0.17495658\n",
      "Iteration 100, loss = 0.19922189\n",
      "Iteration 173, loss = 0.17610838\n",
      "Iteration 101, loss = 0.19509595\n",
      "Iteration 174, loss = 0.17602944\n",
      "Iteration 102, loss = 0.19399119\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 175, loss = 0.17564177\n",
      "Iteration 176, loss = 0.17721534\n",
      "Iteration 177, loss = 0.17633497\n",
      "Iteration 178, loss = 0.17701167\n",
      "Iteration 1, loss = 0.39850339Iteration 179, loss = 0.17600225\n",
      "\n",
      "Iteration 2, loss = 0.36195556\n",
      "Iteration 180, loss = 0.17566291\n",
      "Iteration 3, loss = 0.32049018\n",
      "Iteration 181, loss = 0.17878097\n",
      "Iteration 4, loss = 0.30917658\n",
      "Iteration 182, loss = 0.17768398\n",
      "Iteration 5, loss = 0.29503209\n",
      "Iteration 183, loss = 0.17432669\n",
      "Iteration 6, loss = 0.28643938\n",
      "Iteration 184, loss = 0.17731415\n",
      "Iteration 7, loss = 0.28057953\n",
      "Iteration 185, loss = 0.17436385\n",
      "Iteration 8, loss = 0.27598395\n",
      "Iteration 186, loss = 0.17365382\n",
      "Iteration 9, loss = 0.27271171\n",
      "Iteration 187, loss = 0.17383519\n",
      "Iteration 10, loss = 0.27348555\n",
      "Iteration 188, loss = 0.17458860\n",
      "Iteration 11, loss = 0.27078097\n",
      "Iteration 189, loss = 0.17502772\n",
      "Iteration 12, loss = 0.26437608\n",
      "Iteration 190, loss = 0.17466608\n",
      "Iteration 13, loss = 0.26494674\n",
      "Iteration 191, loss = 0.17597155\n",
      "Iteration 14, loss = 0.26635874\n",
      "Iteration 192, loss = 0.17623728\n",
      "Iteration 15, loss = 0.26264412\n",
      "Iteration 16, loss = 0.26114500\n",
      "Iteration 17, loss = 0.25556401\n",
      "Iteration 18, loss = 0.25537902\n",
      "Iteration 193, loss = 0.17666506Iteration 19, loss = 0.25306139\n",
      "\n",
      "Iteration 20, loss = 0.24957010\n",
      "Iteration 194, loss = 0.17480527\n",
      "Iteration 21, loss = 0.24755761\n",
      "Iteration 195, loss = 0.17707792\n",
      "Iteration 196, loss = 0.17847988\n",
      "Iteration 22, loss = 0.24645689\n",
      "Iteration 197, loss = 0.17411199\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 23, loss = 0.24409308\n",
      "Iteration 24, loss = 0.24333684\n",
      "Iteration 25, loss = 0.24075231\n",
      "Iteration 26, loss = 0.23953163\n",
      "Iteration 1, loss = 0.39709584\n",
      "Iteration 27, loss = 0.23520549\n",
      "Iteration 28, loss = 0.23545067Iteration 2, loss = 0.35960498\n",
      "\n",
      "Iteration 3, loss = 0.31896684Iteration 29, loss = 0.23642193\n",
      "\n",
      "Iteration 4, loss = 0.30706740\n",
      "Iteration 30, loss = 0.23428923\n",
      "Iteration 5, loss = 0.29616224\n",
      "Iteration 31, loss = 0.23244904\n",
      "Iteration 6, loss = 0.28724610\n",
      "Iteration 32, loss = 0.23143006\n",
      "Iteration 7, loss = 0.28121699\n",
      "Iteration 8, loss = 0.27837382\n",
      "Iteration 33, loss = 0.22907099\n",
      "Iteration 34, loss = 0.22674350\n",
      "Iteration 9, loss = 0.27569246\n",
      "Iteration 35, loss = 0.22689694\n",
      "Iteration 10, loss = 0.27258172\n",
      "Iteration 11, loss = 0.27042009\n",
      "Iteration 36, loss = 0.22718270\n",
      "Iteration 37, loss = 0.22617649\n",
      "Iteration 12, loss = 0.26496577\n",
      "Iteration 38, loss = 0.22523040\n",
      "Iteration 13, loss = 0.26177045\n",
      "Iteration 14, loss = 0.26193420\n",
      "Iteration 39, loss = 0.22273337\n",
      "Iteration 15, loss = 0.26137997\n",
      "Iteration 40, loss = 0.22347964\n",
      "Iteration 16, loss = 0.25889468\n",
      "Iteration 41, loss = 0.22280208\n",
      "Iteration 17, loss = 0.25218750\n",
      "Iteration 42, loss = 0.21939122\n",
      "Iteration 18, loss = 0.24917370\n",
      "Iteration 43, loss = 0.21739076\n",
      "Iteration 19, loss = 0.24775817\n",
      "Iteration 44, loss = 0.21625206\n",
      "Iteration 20, loss = 0.24688155\n",
      "Iteration 45, loss = 0.21518459\n",
      "Iteration 21, loss = 0.24378770\n",
      "Iteration 46, loss = 0.21481996\n",
      "Iteration 22, loss = 0.23963903\n",
      "Iteration 47, loss = 0.21933304\n",
      "Iteration 48, loss = 0.21416645\n",
      "Iteration 23, loss = 0.23839126\n",
      "Iteration 49, loss = 0.21806336\n",
      "Iteration 24, loss = 0.23475256\n",
      "Iteration 50, loss = 0.21206434\n",
      "Iteration 25, loss = 0.23550051\n",
      "Iteration 51, loss = 0.21052597\n",
      "Iteration 26, loss = 0.23237477\n",
      "Iteration 52, loss = 0.21449783\n",
      "Iteration 27, loss = 0.23268178\n",
      "Iteration 53, loss = 0.20791964\n",
      "Iteration 28, loss = 0.23167825\n",
      "Iteration 54, loss = 0.20960633\n",
      "Iteration 29, loss = 0.22961760\n",
      "Iteration 55, loss = 0.20653694\n",
      "Iteration 56, loss = 0.20881881\n",
      "Iteration 30, loss = 0.23007591\n",
      "Iteration 57, loss = 0.20617451\n",
      "Iteration 31, loss = 0.22751159\n",
      "Iteration 58, loss = 0.20530324\n",
      "Iteration 32, loss = 0.22487374\n",
      "Iteration 59, loss = 0.20420526\n",
      "Iteration 33, loss = 0.22335716\n",
      "Iteration 60, loss = 0.20487212\n",
      "Iteration 34, loss = 0.21997786\n",
      "Iteration 61, loss = 0.20255168\n",
      "Iteration 35, loss = 0.21895484\n",
      "Iteration 62, loss = 0.20451849\n",
      "Iteration 36, loss = 0.21895429\n",
      "Iteration 63, loss = 0.20567435\n",
      "Iteration 37, loss = 0.21933148\n",
      "Iteration 64, loss = 0.20244417\n",
      "Iteration 38, loss = 0.21726985\n",
      "Iteration 65, loss = 0.20198161\n",
      "Iteration 39, loss = 0.21615272\n",
      "Iteration 66, loss = 0.20276362\n",
      "Iteration 40, loss = 0.21448253\n",
      "Iteration 67, loss = 0.20101105\n",
      "Iteration 41, loss = 0.21491174\n",
      "Iteration 68, loss = 0.20186659\n",
      "Iteration 42, loss = 0.21200630\n",
      "Iteration 69, loss = 0.20138483\n",
      "Iteration 43, loss = 0.21093034\n",
      "Iteration 70, loss = 0.20001298\n",
      "Iteration 44, loss = 0.20993002\n",
      "Iteration 71, loss = 0.20076112\n",
      "Iteration 45, loss = 0.21533258\n",
      "Iteration 72, loss = 0.19859649\n",
      "Iteration 73, loss = 0.19906419\n",
      "Iteration 46, loss = 0.21283401\n",
      "Iteration 74, loss = 0.19924942\n",
      "Iteration 47, loss = 0.21184098\n",
      "Iteration 75, loss = 0.19852646\n",
      "Iteration 48, loss = 0.21094194\n",
      "Iteration 76, loss = 0.19789086\n",
      "Iteration 49, loss = 0.20992235\n",
      "Iteration 77, loss = 0.19752899\n",
      "Iteration 50, loss = 0.20697334\n",
      "Iteration 78, loss = 0.19942607\n",
      "Iteration 51, loss = 0.20646074\n",
      "Iteration 79, loss = 0.19731090\n",
      "Iteration 52, loss = 0.20453821\n",
      "Iteration 80, loss = 0.19707886\n",
      "Iteration 53, loss = 0.20349379\n",
      "Iteration 81, loss = 0.19835075\n",
      "Iteration 54, loss = 0.20032943\n",
      "Iteration 82, loss = 0.19752090\n",
      "Iteration 55, loss = 0.20030551\n",
      "Iteration 83, loss = 0.19664963\n",
      "Iteration 56, loss = 0.20250185\n",
      "Iteration 84, loss = 0.19900388\n",
      "Iteration 57, loss = 0.20456108\n",
      "Iteration 85, loss = 0.19509224\n",
      "Iteration 58, loss = 0.20115278\n",
      "Iteration 86, loss = 0.19713228\n",
      "Iteration 59, loss = 0.19852652\n",
      "Iteration 87, loss = 0.19793490\n",
      "Iteration 60, loss = 0.20003008\n",
      "Iteration 88, loss = 0.19411208\n",
      "Iteration 61, loss = 0.19880696\n",
      "Iteration 89, loss = 0.19591939\n",
      "Iteration 62, loss = 0.20582735\n",
      "Iteration 90, loss = 0.19335010\n",
      "Iteration 63, loss = 0.20645348\n",
      "Iteration 91, loss = 0.19452175\n",
      "Iteration 64, loss = 0.19951453\n",
      "Iteration 92, loss = 0.19197984\n",
      "Iteration 65, loss = 0.20147261\n",
      "Iteration 93, loss = 0.19236453\n",
      "Iteration 66, loss = 0.19978469\n",
      "Iteration 94, loss = 0.20084641\n",
      "Iteration 67, loss = 0.19623997\n",
      "Iteration 95, loss = 0.20018928\n",
      "Iteration 96, loss = 0.19415457\n",
      "Iteration 68, loss = 0.19785487\n",
      "Iteration 97, loss = 0.19243819\n",
      "Iteration 69, loss = 0.19678955\n",
      "Iteration 98, loss = 0.19664384\n",
      "Iteration 70, loss = 0.19424713\n",
      "Iteration 99, loss = 0.19565624\n",
      "Iteration 100, loss = 0.19109765\n",
      "Iteration 71, loss = 0.19556814\n",
      "Iteration 101, loss = 0.19747499\n",
      "Iteration 72, loss = 0.19231105\n",
      "Iteration 102, loss = 0.19430846\n",
      "Iteration 73, loss = 0.19162040\n",
      "Iteration 103, loss = 0.19283663\n",
      "Iteration 74, loss = 0.19211253\n",
      "Iteration 104, loss = 0.19745400\n",
      "Iteration 75, loss = 0.19430273\n",
      "Iteration 105, loss = 0.19646522\n",
      "Iteration 76, loss = 0.19385434\n",
      "Iteration 106, loss = 0.19111928\n",
      "Iteration 77, loss = 0.19137662\n",
      "Iteration 107, loss = 0.19076909\n",
      "Iteration 108, loss = 0.18979667\n",
      "Iteration 78, loss = 0.19077557\n",
      "Iteration 109, loss = 0.19645852\n",
      "Iteration 79, loss = 0.19219280\n",
      "Iteration 110, loss = 0.19670299\n",
      "Iteration 80, loss = 0.18847908\n",
      "Iteration 111, loss = 0.19294220\n",
      "Iteration 81, loss = 0.18807593\n",
      "Iteration 112, loss = 0.19177497\n",
      "Iteration 82, loss = 0.19209803\n",
      "Iteration 113, loss = 0.19462050\n",
      "Iteration 83, loss = 0.19070733\n",
      "Iteration 114, loss = 0.19238797\n",
      "Iteration 84, loss = 0.19535320\n",
      "Iteration 115, loss = 0.19357735\n",
      "Iteration 85, loss = 0.19090648\n",
      "Iteration 116, loss = 0.18994261\n",
      "Iteration 86, loss = 0.19226826\n",
      "Iteration 117, loss = 0.18801618\n",
      "Iteration 87, loss = 0.19220981\n",
      "Iteration 118, loss = 0.18800475\n",
      "Iteration 88, loss = 0.19216720\n",
      "Iteration 119, loss = 0.18913997\n",
      "Iteration 89, loss = 0.19008367\n",
      "Iteration 120, loss = 0.18993667\n",
      "Iteration 90, loss = 0.18584805\n",
      "Iteration 121, loss = 0.19172862\n",
      "Iteration 122, loss = 0.18821603\n",
      "Iteration 123, loss = 0.18672930\n",
      "Iteration 91, loss = 0.18487630\n",
      "Iteration 124, loss = 0.18615592\n",
      "Iteration 125, loss = 0.18802218\n",
      "Iteration 92, loss = 0.18646867\n",
      "Iteration 126, loss = 0.18767322\n",
      "Iteration 93, loss = 0.18751140\n",
      "Iteration 127, loss = 0.19143049\n",
      "Iteration 128, loss = 0.18958100\n",
      "Iteration 94, loss = 0.19552067\n",
      "Iteration 129, loss = 0.18889630\n",
      "Iteration 130, loss = 0.18604064\n",
      "Iteration 131, loss = 0.18906411\n",
      "Iteration 95, loss = 0.19132831\n",
      "Iteration 132, loss = 0.18597151\n",
      "Iteration 133, loss = 0.18741200\n",
      "Iteration 134, loss = 0.18606340\n",
      "Iteration 96, loss = 0.19412557\n",
      "Iteration 135, loss = 0.18722554\n",
      "Iteration 97, loss = 0.18977618\n",
      "Iteration 136, loss = 0.18592848\n",
      "Iteration 137, loss = 0.18567007\n",
      "Iteration 138, loss = 0.18724669\n",
      "Iteration 98, loss = 0.18855205\n",
      "Iteration 139, loss = 0.18998369\n",
      "Iteration 99, loss = 0.18729421\n",
      "Iteration 140, loss = 0.19700683\n",
      "Iteration 100, loss = 0.18694880\n",
      "Iteration 141, loss = 0.18834663Iteration 101, loss = 0.18732340\n",
      "\n",
      "Iteration 102, loss = 0.18622064\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 142, loss = 0.18511335\n",
      "Iteration 143, loss = 0.18656471\n",
      "Iteration 144, loss = 0.18736642\n",
      "Iteration 1, loss = 0.39707624\n",
      "Iteration 145, loss = 0.18449537Iteration 2, loss = 0.36131580\n",
      "\n",
      "Iteration 146, loss = 0.18625894\n",
      "Iteration 3, loss = 0.31983140\n",
      "Iteration 147, loss = 0.18736944\n",
      "Iteration 4, loss = 0.31024630\n",
      "Iteration 148, loss = 0.18378029\n",
      "Iteration 5, loss = 0.29793850\n",
      "Iteration 149, loss = 0.18340596\n",
      "Iteration 150, loss = 0.18344247Iteration 6, loss = 0.29093922\n",
      "\n",
      "Iteration 151, loss = 0.18400391Iteration 7, loss = 0.28558385\n",
      "\n",
      "Iteration 152, loss = 0.18485392\n",
      "Iteration 8, loss = 0.28388713\n",
      "Iteration 153, loss = 0.18397735\n",
      "Iteration 9, loss = 0.28080457\n",
      "Iteration 154, loss = 0.18341720\n",
      "Iteration 10, loss = 0.27771705\n",
      "Iteration 155, loss = 0.18399724\n",
      "Iteration 11, loss = 0.28122900\n",
      "Iteration 156, loss = 0.18158189\n",
      "Iteration 12, loss = 0.26944146\n",
      "Iteration 157, loss = 0.18203522\n",
      "Iteration 13, loss = 0.26511378\n",
      "Iteration 158, loss = 0.18256973\n",
      "Iteration 14, loss = 0.26776800\n",
      "Iteration 159, loss = 0.18525942\n",
      "Iteration 15, loss = 0.27414475\n",
      "Iteration 160, loss = 0.20055035\n",
      "Iteration 16, loss = 0.26106563\n",
      "Iteration 161, loss = 0.19040528\n",
      "Iteration 17, loss = 0.25392923\n",
      "Iteration 162, loss = 0.18875937\n",
      "Iteration 18, loss = 0.25162167\n",
      "Iteration 163, loss = 0.18504400\n",
      "Iteration 19, loss = 0.25010953\n",
      "Iteration 164, loss = 0.18393745\n",
      "Iteration 165, loss = 0.18775594\n",
      "Iteration 20, loss = 0.24795919\n",
      "Iteration 166, loss = 0.18980587\n",
      "Iteration 21, loss = 0.24637603\n",
      "Iteration 167, loss = 0.18633885\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 22, loss = 0.24338675\n",
      "Iteration 23, loss = 0.24334139\n",
      "Iteration 24, loss = 0.24064452\n",
      "Iteration 25, loss = 0.24133581\n",
      "Iteration 26, loss = 0.23986506\n",
      "Iteration 27, loss = 0.23503702\n",
      "Iteration 28, loss = 0.23780225\n",
      "Iteration 29, loss = 0.23436006\n",
      "Iteration 30, loss = 0.23791664\n",
      "Iteration 31, loss = 0.23388518\n",
      "Iteration 32, loss = 0.23334615\n",
      "Iteration 33, loss = 0.22761413\n",
      "Iteration 34, loss = 0.22329952\n",
      "Iteration 35, loss = 0.21935403\n",
      "Iteration 36, loss = 0.21946837\n",
      "Iteration 37, loss = 0.21793728\n",
      "Iteration 38, loss = 0.21514048\n",
      "Iteration 39, loss = 0.21413747\n",
      "Iteration 40, loss = 0.21465515\n",
      "Iteration 41, loss = 0.22973240\n",
      "Iteration 42, loss = 0.22384698\n",
      "Iteration 43, loss = 0.22130114\n",
      "Iteration 44, loss = 0.21445271\n",
      "Iteration 45, loss = 0.21232713\n",
      "Iteration 46, loss = 0.20969137\n",
      "Iteration 47, loss = 0.20866056\n",
      "Iteration 48, loss = 0.20503051\n",
      "Iteration 49, loss = 0.20468107\n",
      "Iteration 50, loss = 0.20150366\n",
      "Iteration 51, loss = 0.20018502\n",
      "Iteration 52, loss = 0.19834244\n",
      "Iteration 53, loss = 0.20042275\n",
      "Iteration 54, loss = 0.20153335\n",
      "Iteration 55, loss = 0.19977571\n",
      "Iteration 56, loss = 0.20322929\n",
      "Iteration 57, loss = 0.19815129\n",
      "Iteration 58, loss = 0.19862320\n",
      "Iteration 59, loss = 0.19182006\n",
      "Iteration 60, loss = 0.19714524\n",
      "Iteration 61, loss = 0.19187686\n",
      "Iteration 62, loss = 0.19255515\n",
      "Iteration 63, loss = 0.19358820\n",
      "Iteration 64, loss = 0.19082275\n",
      "Iteration 65, loss = 0.18785257\n",
      "Iteration 66, loss = 0.18800994\n",
      "Iteration 67, loss = 0.18620260\n",
      "Iteration 68, loss = 0.18790706\n",
      "Iteration 69, loss = 0.19206472\n",
      "Iteration 70, loss = 0.19057920\n",
      "Iteration 71, loss = 0.19085352\n",
      "Iteration 72, loss = 0.18584477\n",
      "Iteration 73, loss = 0.18712218\n",
      "Iteration 74, loss = 0.18981889\n",
      "Iteration 75, loss = 0.18734542\n",
      "Iteration 76, loss = 0.18895203\n",
      "Iteration 77, loss = 0.18517933\n",
      "Iteration 78, loss = 0.18535311\n",
      "Iteration 79, loss = 0.18458827\n",
      "Iteration 80, loss = 0.18326416\n",
      "Iteration 81, loss = 0.18532176\n",
      "Iteration 82, loss = 0.19824049\n",
      "Iteration 83, loss = 0.19537212\n",
      "Iteration 84, loss = 0.19457770\n",
      "Iteration 85, loss = 0.18837238\n",
      "Iteration 86, loss = 0.18564176\n",
      "Iteration 87, loss = 0.18728204\n",
      "Iteration 88, loss = 0.18638553\n",
      "Iteration 89, loss = 0.18671706\n",
      "Iteration 90, loss = 0.18408176\n",
      "Iteration 91, loss = 0.18358104\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.37048238\n",
      "Iteration 1, loss = 0.36942342\n",
      "Iteration 2, loss = 0.31979516\n",
      "Iteration 3, loss = 0.30762081\n",
      "Iteration 4, loss = 0.29596817\n",
      "Iteration 2, loss = 0.32021510\n",
      "Iteration 5, loss = 0.29220245\n",
      "Iteration 6, loss = 0.28928876\n",
      "Iteration 3, loss = 0.30576548\n",
      "Iteration 7, loss = 0.29179884\n",
      "Iteration 4, loss = 0.29191171\n",
      "Iteration 8, loss = 0.28102255\n",
      "Iteration 9, loss = 0.27631575\n",
      "Iteration 10, loss = 0.27554549\n",
      "Iteration 5, loss = 0.28538359\n",
      "Iteration 11, loss = 0.27203824\n",
      "Iteration 12, loss = 0.26945111\n",
      "Iteration 6, loss = 0.27839819\n",
      "Iteration 13, loss = 0.26665317\n",
      "Iteration 14, loss = 0.26701476\n",
      "Iteration 7, loss = 0.28256636\n",
      "Iteration 8, loss = 0.27171595\n",
      "Iteration 15, loss = 0.26646513\n",
      "Iteration 16, loss = 0.26399898\n",
      "Iteration 17, loss = 0.26333955\n",
      "Iteration 9, loss = 0.26718477\n",
      "Iteration 10, loss = 0.26695191\n",
      "Iteration 11, loss = 0.26258860\n",
      "Iteration 18, loss = 0.25963373\n",
      "Iteration 19, loss = 0.25898606\n",
      "Iteration 12, loss = 0.26000963\n",
      "Iteration 20, loss = 0.25615432\n",
      "Iteration 13, loss = 0.25951446\n",
      "Iteration 21, loss = 0.25437997\n",
      "Iteration 14, loss = 0.25795211\n",
      "Iteration 22, loss = 0.25542442\n",
      "Iteration 15, loss = 0.25535836\n",
      "Iteration 23, loss = 0.25346667\n",
      "Iteration 24, loss = 0.25355321\n",
      "Iteration 16, loss = 0.25326798\n",
      "Iteration 25, loss = 0.25033392\n",
      "Iteration 17, loss = 0.25324964\n",
      "Iteration 26, loss = 0.24758965\n",
      "Iteration 27, loss = 0.24950707\n",
      "Iteration 28, loss = 0.24734133\n",
      "Iteration 18, loss = 0.25347220\n",
      "Iteration 29, loss = 0.24605594\n",
      "Iteration 30, loss = 0.24681418\n",
      "Iteration 19, loss = 0.24880392\n",
      "Iteration 31, loss = 0.24413593\n",
      "Iteration 32, loss = 0.24428015\n",
      "Iteration 20, loss = 0.25041198\n",
      "Iteration 33, loss = 0.24128963\n",
      "Iteration 34, loss = 0.24264745\n",
      "Iteration 21, loss = 0.24496703\n",
      "Iteration 35, loss = 0.24310226\n",
      "Iteration 22, loss = 0.24891291\n",
      "Iteration 36, loss = 0.24229393\n",
      "Iteration 37, loss = 0.23994574\n",
      "Iteration 23, loss = 0.24616390\n",
      "Iteration 38, loss = 0.24212380\n",
      "Iteration 24, loss = 0.24736948\n",
      "Iteration 25, loss = 0.24402945\n",
      "Iteration 39, loss = 0.24284953\n",
      "Iteration 26, loss = 0.24431413\n",
      "Iteration 40, loss = 0.24178608\n",
      "Iteration 27, loss = 0.24031865\n",
      "Iteration 41, loss = 0.23880779\n",
      "Iteration 28, loss = 0.24166383\n",
      "Iteration 42, loss = 0.23758422\n",
      "Iteration 29, loss = 0.24311981\n",
      "Iteration 43, loss = 0.23970420\n",
      "Iteration 30, loss = 0.23901167\n",
      "Iteration 44, loss = 0.23991586\n",
      "Iteration 31, loss = 0.23976876\n",
      "Iteration 45, loss = 0.24099965\n",
      "Iteration 32, loss = 0.23838364\n",
      "Iteration 46, loss = 0.23491013\n",
      "Iteration 33, loss = 0.23799191\n",
      "Iteration 47, loss = 0.23718616\n",
      "Iteration 34, loss = 0.23803065\n",
      "Iteration 48, loss = 0.23734886\n",
      "Iteration 35, loss = 0.23647401\n",
      "Iteration 49, loss = 0.23458249\n",
      "Iteration 36, loss = 0.23689694\n",
      "Iteration 50, loss = 0.23486184\n",
      "Iteration 37, loss = 0.23437342\n",
      "Iteration 51, loss = 0.23520353\n",
      "Iteration 38, loss = 0.23485972\n",
      "Iteration 52, loss = 0.23410170\n",
      "Iteration 39, loss = 0.23369430\n",
      "Iteration 53, loss = 0.23553471\n",
      "Iteration 40, loss = 0.23495665\n",
      "Iteration 54, loss = 0.23717623\n",
      "Iteration 41, loss = 0.23573177\n",
      "Iteration 55, loss = 0.23508989\n",
      "Iteration 42, loss = 0.23428730\n",
      "Iteration 56, loss = 0.23947728\n",
      "Iteration 43, loss = 0.23529010\n",
      "Iteration 57, loss = 0.23560334\n",
      "Iteration 44, loss = 0.23204721\n",
      "Iteration 58, loss = 0.23608330\n",
      "Iteration 45, loss = 0.23405209\n",
      "Iteration 59, loss = 0.23323551\n",
      "Iteration 46, loss = 0.23377788\n",
      "Iteration 60, loss = 0.23339867\n",
      "Iteration 47, loss = 0.23229962\n",
      "Iteration 61, loss = 0.23248534\n",
      "Iteration 48, loss = 0.23319261\n",
      "Iteration 62, loss = 0.23265837\n",
      "Iteration 49, loss = 0.23102529\n",
      "Iteration 63, loss = 0.23210263\n",
      "Iteration 50, loss = 0.22985371\n",
      "Iteration 64, loss = 0.23122621\n",
      "Iteration 51, loss = 0.22955248\n",
      "Iteration 65, loss = 0.23206064\n",
      "Iteration 52, loss = 0.22906042\n",
      "Iteration 66, loss = 0.23309256\n",
      "Iteration 53, loss = 0.22951612\n",
      "Iteration 67, loss = 0.23335261\n",
      "Iteration 54, loss = 0.22742823\n",
      "Iteration 68, loss = 0.23174201\n",
      "Iteration 55, loss = 0.22815898\n",
      "Iteration 69, loss = 0.23080120\n",
      "Iteration 56, loss = 0.22732689\n",
      "Iteration 70, loss = 0.23173777\n",
      "Iteration 57, loss = 0.22715307\n",
      "Iteration 71, loss = 0.23059306\n",
      "Iteration 58, loss = 0.22701945\n",
      "Iteration 72, loss = 0.23102279\n",
      "Iteration 59, loss = 0.22860144\n",
      "Iteration 73, loss = 0.22988642\n",
      "Iteration 60, loss = 0.22898964\n",
      "Iteration 74, loss = 0.23135866\n",
      "Iteration 61, loss = 0.22801807\n",
      "Iteration 62, loss = 0.22869001\n",
      "Iteration 75, loss = 0.22983141\n",
      "Iteration 63, loss = 0.22628396\n",
      "Iteration 76, loss = 0.23104111\n",
      "Iteration 64, loss = 0.22373455\n",
      "Iteration 77, loss = 0.22984382\n",
      "Iteration 65, loss = 0.22502570\n",
      "Iteration 78, loss = 0.22970758\n",
      "Iteration 66, loss = 0.22821210\n",
      "Iteration 79, loss = 0.23426138\n",
      "Iteration 67, loss = 0.22993179\n",
      "Iteration 80, loss = 0.23365891\n",
      "Iteration 68, loss = 0.22601689\n",
      "Iteration 81, loss = 0.23359461\n",
      "Iteration 69, loss = 0.22400890\n",
      "Iteration 82, loss = 0.22852367\n",
      "Iteration 70, loss = 0.22611233\n",
      "Iteration 83, loss = 0.22993330\n",
      "Iteration 71, loss = 0.22452485\n",
      "Iteration 84, loss = 0.22807334\n",
      "Iteration 72, loss = 0.22382966\n",
      "Iteration 85, loss = 0.22908337\n",
      "Iteration 73, loss = 0.22384604\n",
      "Iteration 86, loss = 0.22875474\n",
      "Iteration 74, loss = 0.22255879\n",
      "Iteration 87, loss = 0.22752035\n",
      "Iteration 75, loss = 0.22449687\n",
      "Iteration 88, loss = 0.23176774\n",
      "Iteration 76, loss = 0.22712021\n",
      "Iteration 89, loss = 0.22862864\n",
      "Iteration 77, loss = 0.22319777\n",
      "Iteration 90, loss = 0.23001490\n",
      "Iteration 78, loss = 0.22333272\n",
      "Iteration 91, loss = 0.23164376\n",
      "Iteration 79, loss = 0.22357945\n",
      "Iteration 92, loss = 0.22732834\n",
      "Iteration 80, loss = 0.22471538\n",
      "Iteration 93, loss = 0.22850253\n",
      "Iteration 81, loss = 0.22073858\n",
      "Iteration 94, loss = 0.22727123\n",
      "Iteration 82, loss = 0.22021904\n",
      "Iteration 95, loss = 0.22764156\n",
      "Iteration 83, loss = 0.22159270\n",
      "Iteration 96, loss = 0.22760588\n",
      "Iteration 84, loss = 0.22126332\n",
      "Iteration 97, loss = 0.22991378\n",
      "Iteration 85, loss = 0.22022564\n",
      "Iteration 98, loss = 0.22887129\n",
      "Iteration 86, loss = 0.21964199\n",
      "Iteration 99, loss = 0.22988870\n",
      "Iteration 87, loss = 0.21932589\n",
      "Iteration 100, loss = 0.22866659\n",
      "Iteration 88, loss = 0.22103966\n",
      "Iteration 89, loss = 0.22087633\n",
      "Iteration 101, loss = 0.22807813\n",
      "Iteration 90, loss = 0.22239633\n",
      "Iteration 102, loss = 0.22758438\n",
      "Iteration 91, loss = 0.22304843\n",
      "Iteration 103, loss = 0.22606893\n",
      "Iteration 92, loss = 0.22087251\n",
      "Iteration 104, loss = 0.22620291\n",
      "Iteration 93, loss = 0.22173850\n",
      "Iteration 105, loss = 0.22565015\n",
      "Iteration 94, loss = 0.21980929\n",
      "Iteration 106, loss = 0.22626190\n",
      "Iteration 95, loss = 0.21861652\n",
      "Iteration 107, loss = 0.22769317\n",
      "Iteration 96, loss = 0.21912616\n",
      "Iteration 108, loss = 0.22607502\n",
      "Iteration 97, loss = 0.21892113\n",
      "Iteration 109, loss = 0.22673935\n",
      "Iteration 98, loss = 0.21898566\n",
      "Iteration 110, loss = 0.22821973\n",
      "Iteration 99, loss = 0.21864565\n",
      "Iteration 100, loss = 0.22118137Iteration 111, loss = 0.22681073\n",
      "\n",
      "Iteration 101, loss = 0.21845847\n",
      "Iteration 112, loss = 0.22652019\n",
      "Iteration 102, loss = 0.22089884\n",
      "Iteration 113, loss = 0.22600933\n",
      "Iteration 103, loss = 0.21799100\n",
      "Iteration 114, loss = 0.22639726\n",
      "Iteration 104, loss = 0.21700158\n",
      "Iteration 115, loss = 0.22478791\n",
      "Iteration 105, loss = 0.21824702\n",
      "Iteration 116, loss = 0.22682170\n",
      "Iteration 106, loss = 0.21834778\n",
      "Iteration 117, loss = 0.22867446\n",
      "Iteration 107, loss = 0.21932938\n",
      "Iteration 118, loss = 0.22677781\n",
      "Iteration 108, loss = 0.21985518\n",
      "Iteration 119, loss = 0.23163623\n",
      "Iteration 109, loss = 0.21922573\n",
      "Iteration 110, loss = 0.21984252\n",
      "Iteration 120, loss = 0.23123582\n",
      "Iteration 111, loss = 0.22175300\n",
      "Iteration 121, loss = 0.22710478\n",
      "Iteration 112, loss = 0.21694150\n",
      "Iteration 122, loss = 0.22679736\n",
      "Iteration 113, loss = 0.21980369\n",
      "Iteration 123, loss = 0.22569949\n",
      "Iteration 114, loss = 0.21859283\n",
      "Iteration 124, loss = 0.22452331\n",
      "Iteration 115, loss = 0.21802702\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 125, loss = 0.22480293\n",
      "Iteration 126, loss = 0.22655906\n",
      "Iteration 1, loss = 0.37228401\n",
      "Iteration 127, loss = 0.22540035\n",
      "Iteration 2, loss = 0.32111258\n",
      "Iteration 128, loss = 0.22483337\n",
      "Iteration 3, loss = 0.30531261\n",
      "Iteration 129, loss = 0.22634669\n",
      "Iteration 130, loss = 0.22426925\n",
      "Iteration 131, loss = 0.22570131\n",
      "Iteration 4, loss = 0.29214433\n",
      "Iteration 132, loss = 0.22644151\n",
      "Iteration 5, loss = 0.28755767\n",
      "Iteration 133, loss = 0.22488850\n",
      "Iteration 6, loss = 0.27956749\n",
      "Iteration 134, loss = 0.22792256\n",
      "Iteration 7, loss = 0.28318130\n",
      "Iteration 135, loss = 0.22468417Iteration 8, loss = 0.27351165\n",
      "\n",
      "Iteration 9, loss = 0.27052330\n",
      "Iteration 136, loss = 0.22734824Iteration 10, loss = 0.27031961\n",
      "\n",
      "Iteration 137, loss = 0.22794316\n",
      "Iteration 11, loss = 0.26690591\n",
      "Iteration 138, loss = 0.22388340\n",
      "Iteration 139, loss = 0.22340377\n",
      "Iteration 12, loss = 0.26244791\n",
      "Iteration 140, loss = 0.22349066\n",
      "Iteration 13, loss = 0.26150850\n",
      "Iteration 141, loss = 0.22529760\n",
      "Iteration 14, loss = 0.26108318\n",
      "Iteration 142, loss = 0.22646757\n",
      "Iteration 15, loss = 0.25860359\n",
      "Iteration 143, loss = 0.22586190\n",
      "Iteration 16, loss = 0.25893407\n",
      "Iteration 144, loss = 0.22662858\n",
      "Iteration 17, loss = 0.25933211\n",
      "Iteration 145, loss = 0.22420119\n",
      "Iteration 18, loss = 0.25379344\n",
      "Iteration 146, loss = 0.22298084\n",
      "Iteration 19, loss = 0.25333593\n",
      "Iteration 147, loss = 0.22706793\n",
      "Iteration 20, loss = 0.25429303\n",
      "Iteration 148, loss = 0.22566181\n",
      "Iteration 21, loss = 0.25110526\n",
      "Iteration 22, loss = 0.25394967\n",
      "Iteration 149, loss = 0.22417372\n",
      "Iteration 23, loss = 0.25107329\n",
      "Iteration 150, loss = 0.22496501\n",
      "Iteration 24, loss = 0.24828506\n",
      "Iteration 151, loss = 0.22425338\n",
      "Iteration 25, loss = 0.24732572\n",
      "Iteration 152, loss = 0.22355521\n",
      "Iteration 26, loss = 0.24707011\n",
      "Iteration 153, loss = 0.22366694\n",
      "Iteration 27, loss = 0.24585839\n",
      "Iteration 154, loss = 0.22586176\n",
      "Iteration 28, loss = 0.24550434\n",
      "Iteration 155, loss = 0.22355869\n",
      "Iteration 29, loss = 0.24640240\n",
      "Iteration 156, loss = 0.22342763\n",
      "Iteration 30, loss = 0.24558461\n",
      "Iteration 157, loss = 0.22396187\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 31, loss = 0.24347208\n",
      "Iteration 32, loss = 0.24626273\n",
      "Iteration 33, loss = 0.24493365\n",
      "Iteration 34, loss = 0.24169366\n",
      "Iteration 1, loss = 0.36911763\n",
      "Iteration 35, loss = 0.23970873\n",
      "Iteration 2, loss = 0.31821667\n",
      "Iteration 36, loss = 0.24026157\n",
      "Iteration 3, loss = 0.30260711\n",
      "Iteration 37, loss = 0.23930557\n",
      "Iteration 4, loss = 0.29038432\n",
      "Iteration 38, loss = 0.24154004\n",
      "Iteration 5, loss = 0.28546515\n",
      "Iteration 39, loss = 0.23857065\n",
      "Iteration 6, loss = 0.28019591Iteration 40, loss = 0.23881473\n",
      "\n",
      "Iteration 41, loss = 0.23736952\n",
      "Iteration 7, loss = 0.28152605\n",
      "Iteration 42, loss = 0.24052629\n",
      "Iteration 8, loss = 0.27284181\n",
      "Iteration 43, loss = 0.23685766\n",
      "Iteration 9, loss = 0.26840995\n",
      "Iteration 44, loss = 0.23399860\n",
      "Iteration 10, loss = 0.26877708\n",
      "Iteration 45, loss = 0.23397050\n",
      "Iteration 11, loss = 0.26474727\n",
      "Iteration 46, loss = 0.23285982\n",
      "Iteration 12, loss = 0.26116786\n",
      "Iteration 47, loss = 0.23257382\n",
      "Iteration 13, loss = 0.26057630\n",
      "Iteration 48, loss = 0.23344674\n",
      "Iteration 14, loss = 0.25918457\n",
      "Iteration 49, loss = 0.23257134\n",
      "Iteration 50, loss = 0.23189376\n",
      "Iteration 15, loss = 0.25826471\n",
      "Iteration 51, loss = 0.23181631\n",
      "Iteration 16, loss = 0.25802152\n",
      "Iteration 52, loss = 0.23171039\n",
      "Iteration 17, loss = 0.25680017\n",
      "Iteration 53, loss = 0.23231252\n",
      "Iteration 18, loss = 0.25352094\n",
      "Iteration 54, loss = 0.23039003\n",
      "Iteration 19, loss = 0.25560120\n",
      "Iteration 55, loss = 0.23116405\n",
      "Iteration 20, loss = 0.25345883\n",
      "Iteration 56, loss = 0.23516885\n",
      "Iteration 21, loss = 0.25171765\n",
      "Iteration 57, loss = 0.22965680\n",
      "Iteration 22, loss = 0.25175384\n",
      "Iteration 58, loss = 0.22969364\n",
      "Iteration 23, loss = 0.25056370\n",
      "Iteration 59, loss = 0.23226927\n",
      "Iteration 24, loss = 0.24799786\n",
      "Iteration 60, loss = 0.23207575\n",
      "Iteration 25, loss = 0.24660382Iteration 61, loss = 0.23076778\n",
      "\n",
      "Iteration 62, loss = 0.22876749\n",
      "Iteration 26, loss = 0.24655122\n",
      "Iteration 63, loss = 0.22877680\n",
      "Iteration 27, loss = 0.24556089\n",
      "Iteration 64, loss = 0.22696351\n",
      "Iteration 28, loss = 0.24563914\n",
      "Iteration 65, loss = 0.22870518\n",
      "Iteration 29, loss = 0.24407518\n",
      "Iteration 66, loss = 0.22899286\n",
      "Iteration 30, loss = 0.24800703\n",
      "Iteration 67, loss = 0.22932393\n",
      "Iteration 31, loss = 0.24526301\n",
      "Iteration 68, loss = 0.22806627\n",
      "Iteration 32, loss = 0.24263748Iteration 69, loss = 0.22737515\n",
      "\n",
      "Iteration 70, loss = 0.22612055\n",
      "Iteration 33, loss = 0.24365409\n",
      "Iteration 71, loss = 0.22511734\n",
      "Iteration 34, loss = 0.24326590\n",
      "Iteration 72, loss = 0.22580864\n",
      "Iteration 35, loss = 0.24324062\n",
      "Iteration 73, loss = 0.22583486\n",
      "Iteration 36, loss = 0.24162254\n",
      "Iteration 74, loss = 0.22512902\n",
      "Iteration 37, loss = 0.23978534\n",
      "Iteration 38, loss = 0.23888373\n",
      "Iteration 75, loss = 0.22707840\n",
      "Iteration 39, loss = 0.23994251\n",
      "Iteration 76, loss = 0.23118539\n",
      "Iteration 77, loss = 0.22619798\n",
      "Iteration 40, loss = 0.23849377\n",
      "Iteration 78, loss = 0.22737003\n",
      "Iteration 41, loss = 0.23728286\n",
      "Iteration 79, loss = 0.22625618\n",
      "Iteration 42, loss = 0.23939901\n",
      "Iteration 80, loss = 0.22735283\n",
      "Iteration 43, loss = 0.23376654\n",
      "Iteration 81, loss = 0.22655212\n",
      "Iteration 44, loss = 0.23779807\n",
      "Iteration 82, loss = 0.22703602\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 45, loss = 0.23465431\n",
      "Iteration 46, loss = 0.23565408\n",
      "Iteration 47, loss = 0.23449161\n",
      "Iteration 1, loss = 0.36965316\n",
      "Iteration 48, loss = 0.23291177\n",
      "Iteration 2, loss = 0.31611752\n",
      "Iteration 49, loss = 0.23180049\n",
      "Iteration 3, loss = 0.29917053\n",
      "Iteration 50, loss = 0.23199914\n",
      "Iteration 4, loss = 0.29322355\n",
      "Iteration 51, loss = 0.23158946\n",
      "Iteration 5, loss = 0.29295121\n",
      "Iteration 52, loss = 0.23268762\n",
      "Iteration 6, loss = 0.29053674\n",
      "Iteration 53, loss = 0.23228971\n",
      "Iteration 7, loss = 0.28637986\n",
      "Iteration 54, loss = 0.23083313\n",
      "Iteration 8, loss = 0.28052046\n",
      "Iteration 55, loss = 0.23344151\n",
      "Iteration 9, loss = 0.27764032\n",
      "Iteration 56, loss = 0.23419422\n",
      "Iteration 10, loss = 0.27499190\n",
      "Iteration 57, loss = 0.22915487\n",
      "Iteration 11, loss = 0.27264006\n",
      "Iteration 58, loss = 0.22921542\n",
      "Iteration 12, loss = 0.27198835\n",
      "Iteration 59, loss = 0.22924364\n",
      "Iteration 13, loss = 0.26816140\n",
      "Iteration 60, loss = 0.23046458\n",
      "Iteration 61, loss = 0.22981799\n",
      "Iteration 14, loss = 0.26681004\n",
      "Iteration 62, loss = 0.22965794\n",
      "Iteration 63, loss = 0.22936612\n",
      "Iteration 15, loss = 0.26343164\n",
      "Iteration 64, loss = 0.22703327\n",
      "Iteration 16, loss = 0.26188337\n",
      "Iteration 65, loss = 0.22716449\n",
      "Iteration 17, loss = 0.25842253\n",
      "Iteration 18, loss = 0.25854831\n",
      "Iteration 66, loss = 0.22792094\n",
      "Iteration 19, loss = 0.25778873\n",
      "Iteration 67, loss = 0.22639356\n",
      "Iteration 20, loss = 0.25487944\n",
      "Iteration 68, loss = 0.22553663\n",
      "Iteration 21, loss = 0.25441032\n",
      "Iteration 69, loss = 0.22710155\n",
      "Iteration 22, loss = 0.25235755\n",
      "Iteration 70, loss = 0.22591630\n",
      "Iteration 23, loss = 0.25081648\n",
      "Iteration 71, loss = 0.22445620\n",
      "Iteration 24, loss = 0.24784801\n",
      "Iteration 72, loss = 0.22450293\n",
      "Iteration 25, loss = 0.24846905\n",
      "Iteration 73, loss = 0.22674719Iteration 26, loss = 0.24666024\n",
      "\n",
      "Iteration 27, loss = 0.24552937\n",
      "Iteration 74, loss = 0.22551586\n",
      "Iteration 28, loss = 0.24605307\n",
      "Iteration 75, loss = 0.22545795\n",
      "Iteration 29, loss = 0.24303763\n",
      "Iteration 76, loss = 0.22642900\n",
      "Iteration 30, loss = 0.24509777\n",
      "Iteration 77, loss = 0.22408107\n",
      "Iteration 31, loss = 0.24313917\n",
      "Iteration 78, loss = 0.22435624\n",
      "Iteration 32, loss = 0.24228306\n",
      "Iteration 79, loss = 0.22899054\n",
      "Iteration 33, loss = 0.24277160\n",
      "Iteration 80, loss = 0.22854721\n",
      "Iteration 34, loss = 0.24368164\n",
      "Iteration 81, loss = 0.22752226\n",
      "Iteration 35, loss = 0.23863759\n",
      "Iteration 82, loss = 0.22267877Iteration 36, loss = 0.23855032\n",
      "\n",
      "Iteration 37, loss = 0.23543671\n",
      "Iteration 83, loss = 0.22411156\n",
      "Iteration 38, loss = 0.23639154\n",
      "Iteration 84, loss = 0.22371900\n",
      "Iteration 39, loss = 0.23575737\n",
      "Iteration 85, loss = 0.22364029\n",
      "Iteration 40, loss = 0.23427339\n",
      "Iteration 86, loss = 0.22278704\n",
      "Iteration 41, loss = 0.23371166\n",
      "Iteration 87, loss = 0.22226011\n",
      "Iteration 42, loss = 0.23338154\n",
      "Iteration 88, loss = 0.22300927\n",
      "Iteration 43, loss = 0.23259524\n",
      "Iteration 89, loss = 0.22287861\n",
      "Iteration 44, loss = 0.23383191\n",
      "Iteration 90, loss = 0.22276475\n",
      "Iteration 45, loss = 0.23510376\n",
      "Iteration 46, loss = 0.23143050\n",
      "Iteration 91, loss = 0.22333770\n",
      "Iteration 47, loss = 0.23251876\n",
      "Iteration 92, loss = 0.22254458\n",
      "Iteration 48, loss = 0.23257976\n",
      "Iteration 93, loss = 0.22434003\n",
      "Iteration 49, loss = 0.23284984\n",
      "Iteration 94, loss = 0.22316393\n",
      "Iteration 50, loss = 0.23214234\n",
      "Iteration 95, loss = 0.22282921\n",
      "Iteration 51, loss = 0.23671532\n",
      "Iteration 52, loss = 0.23578771\n",
      "Iteration 96, loss = 0.22459962\n",
      "Iteration 53, loss = 0.22989643\n",
      "Iteration 54, loss = 0.23196963\n",
      "Iteration 97, loss = 0.22649011\n",
      "Iteration 55, loss = 0.23111766\n",
      "Iteration 98, loss = 0.22694800\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 56, loss = 0.22976608\n",
      "Iteration 57, loss = 0.23426814\n",
      "Iteration 58, loss = 0.22881701\n",
      "Iteration 1, loss = 0.37096883\n",
      "Iteration 59, loss = 0.22951740\n",
      "Iteration 2, loss = 0.31335165\n",
      "Iteration 60, loss = 0.22835888\n",
      "Iteration 61, loss = 0.23106297\n",
      "Iteration 3, loss = 0.29614546\n",
      "Iteration 62, loss = 0.23047925\n",
      "Iteration 4, loss = 0.28869142\n",
      "Iteration 63, loss = 0.22718061\n",
      "Iteration 5, loss = 0.29124210\n",
      "Iteration 6, loss = 0.28508247\n",
      "Iteration 64, loss = 0.23279082\n",
      "Iteration 7, loss = 0.27770096\n",
      "Iteration 8, loss = 0.27534147\n",
      "Iteration 65, loss = 0.23060765\n",
      "Iteration 9, loss = 0.27436269\n",
      "Iteration 10, loss = 0.27328257\n",
      "Iteration 66, loss = 0.22858451\n",
      "Iteration 67, loss = 0.23053222\n",
      "Iteration 11, loss = 0.27107247\n",
      "Iteration 68, loss = 0.23065821\n",
      "Iteration 12, loss = 0.26985465\n",
      "Iteration 69, loss = 0.22846716\n",
      "Iteration 13, loss = 0.26813518\n",
      "Iteration 14, loss = 0.26649127\n",
      "Iteration 15, loss = 0.26518347\n",
      "Iteration 70, loss = 0.22887637\n",
      "Iteration 16, loss = 0.26219602\n",
      "Iteration 17, loss = 0.25999729\n",
      "Iteration 18, loss = 0.26195872\n",
      "Iteration 71, loss = 0.22636714\n",
      "Iteration 19, loss = 0.25994317\n",
      "Iteration 72, loss = 0.22710842\n",
      "Iteration 20, loss = 0.26051440\n",
      "Iteration 21, loss = 0.25553836\n",
      "Iteration 73, loss = 0.22698011\n",
      "Iteration 22, loss = 0.25441204\n",
      "Iteration 23, loss = 0.25346879\n",
      "Iteration 74, loss = 0.22920295\n",
      "Iteration 24, loss = 0.25135743\n",
      "Iteration 75, loss = 0.22867699\n",
      "Iteration 76, loss = 0.22669614\n",
      "Iteration 25, loss = 0.25123157\n",
      "Iteration 77, loss = 0.22661778\n",
      "Iteration 26, loss = 0.24878549\n",
      "Iteration 78, loss = 0.22730064\n",
      "Iteration 27, loss = 0.24873678\n",
      "Iteration 79, loss = 0.22593674\n",
      "Iteration 28, loss = 0.24657406\n",
      "Iteration 80, loss = 0.22674623\n",
      "Iteration 29, loss = 0.24481694\n",
      "Iteration 81, loss = 0.22616130\n",
      "Iteration 30, loss = 0.25245224\n",
      "Iteration 82, loss = 0.22638612\n",
      "Iteration 31, loss = 0.24560961\n",
      "Iteration 83, loss = 0.22441922\n",
      "Iteration 32, loss = 0.24325404\n",
      "Iteration 84, loss = 0.22552833\n",
      "Iteration 33, loss = 0.24378661\n",
      "Iteration 85, loss = 0.22832402\n",
      "Iteration 34, loss = 0.24655116\n",
      "Iteration 86, loss = 0.22443982\n",
      "Iteration 87, loss = 0.22585540\n",
      "Iteration 35, loss = 0.24411347\n",
      "Iteration 88, loss = 0.22503584\n",
      "Iteration 36, loss = 0.24178012\n",
      "Iteration 89, loss = 0.22473218\n",
      "Iteration 37, loss = 0.24217195\n",
      "Iteration 90, loss = 0.22590713\n",
      "Iteration 38, loss = 0.24302638\n",
      "Iteration 91, loss = 0.22569407\n",
      "Iteration 92, loss = 0.22463417\n",
      "Iteration 39, loss = 0.24162969\n",
      "Iteration 93, loss = 0.22567455\n",
      "Iteration 40, loss = 0.24011912\n",
      "Iteration 94, loss = 0.22561874\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 41, loss = 0.23863785\n",
      "Iteration 42, loss = 0.23873740\n",
      "Iteration 43, loss = 0.23628098\n",
      "Iteration 1, loss = 0.36863398\n",
      "Iteration 44, loss = 0.24013692\n",
      "Iteration 2, loss = 0.31283238\n",
      "Iteration 45, loss = 0.23789433\n",
      "Iteration 3, loss = 0.29827658\n",
      "Iteration 46, loss = 0.23572886\n",
      "Iteration 4, loss = 0.29368905\n",
      "Iteration 47, loss = 0.23545151\n",
      "Iteration 5, loss = 0.29625709\n",
      "Iteration 48, loss = 0.23255777\n",
      "Iteration 6, loss = 0.28656929\n",
      "Iteration 49, loss = 0.23312470\n",
      "Iteration 7, loss = 0.27991206\n",
      "Iteration 50, loss = 0.23530232\n",
      "Iteration 8, loss = 0.27624680\n",
      "Iteration 51, loss = 0.23767322\n",
      "Iteration 9, loss = 0.27246777\n",
      "Iteration 10, loss = 0.26900402\n",
      "Iteration 52, loss = 0.23519264\n",
      "Iteration 53, loss = 0.23355349\n",
      "Iteration 54, loss = 0.23102028\n",
      "Iteration 11, loss = 0.26862710\n",
      "Iteration 55, loss = 0.22832644\n",
      "Iteration 12, loss = 0.26527426\n",
      "Iteration 56, loss = 0.23062831\n",
      "Iteration 13, loss = 0.26563972\n",
      "Iteration 14, loss = 0.26008825\n",
      "Iteration 57, loss = 0.23619554Iteration 15, loss = 0.25676174\n",
      "\n",
      "Iteration 16, loss = 0.25469534\n",
      "Iteration 58, loss = 0.23094679\n",
      "Iteration 17, loss = 0.25279251\n",
      "Iteration 18, loss = 0.25906716\n",
      "Iteration 59, loss = 0.22806264\n",
      "Iteration 60, loss = 0.22791484\n",
      "Iteration 61, loss = 0.22869409\n",
      "Iteration 19, loss = 0.25072056\n",
      "Iteration 62, loss = 0.22686205\n",
      "Iteration 63, loss = 0.22729596\n",
      "Iteration 20, loss = 0.25185159\n",
      "Iteration 64, loss = 0.22924682\n",
      "Iteration 65, loss = 0.22973616\n",
      "Iteration 66, loss = 0.22886581\n",
      "Iteration 67, loss = 0.22787836\n",
      "Iteration 68, loss = 0.22610623\n",
      "Iteration 21, loss = 0.25020525\n",
      "Iteration 69, loss = 0.22428947\n",
      "Iteration 22, loss = 0.24689335\n",
      "Iteration 70, loss = 0.22315203\n",
      "Iteration 23, loss = 0.24381746\n",
      "Iteration 71, loss = 0.22578427\n",
      "Iteration 24, loss = 0.24157331\n",
      "Iteration 72, loss = 0.22534771Iteration 25, loss = 0.23935892\n",
      "\n",
      "Iteration 73, loss = 0.22576618\n",
      "Iteration 26, loss = 0.23656183\n",
      "Iteration 74, loss = 0.22432042\n",
      "Iteration 75, loss = 0.22708760\n",
      "Iteration 27, loss = 0.23841781\n",
      "Iteration 28, loss = 0.23493233\n",
      "Iteration 76, loss = 0.22378664\n",
      "Iteration 77, loss = 0.22418002Iteration 29, loss = 0.23450644\n",
      "Iteration 30, loss = 0.23580184\n",
      "\n",
      "Iteration 78, loss = 0.22771721\n",
      "Iteration 79, loss = 0.22688298\n",
      "Iteration 31, loss = 0.23358004\n",
      "Iteration 80, loss = 0.22243896\n",
      "Iteration 32, loss = 0.23277469\n",
      "Iteration 81, loss = 0.22342935Iteration 33, loss = 0.23510807\n",
      "\n",
      "Iteration 34, loss = 0.23187314\n",
      "Iteration 35, loss = 0.23073837\n",
      "Iteration 82, loss = 0.22338310\n",
      "Iteration 83, loss = 0.22368155\n",
      "Iteration 36, loss = 0.22810519\n",
      "Iteration 84, loss = 0.22133635\n",
      "Iteration 37, loss = 0.22811691\n",
      "Iteration 85, loss = 0.22348847\n",
      "Iteration 38, loss = 0.22849343\n",
      "Iteration 86, loss = 0.22240698\n",
      "Iteration 39, loss = 0.22714411\n",
      "Iteration 87, loss = 0.22190915\n",
      "Iteration 40, loss = 0.22802430\n",
      "Iteration 41, loss = 0.22944202\n",
      "Iteration 88, loss = 0.22218601\n",
      "Iteration 42, loss = 0.22664698\n",
      "Iteration 43, loss = 0.22709853\n",
      "Iteration 89, loss = 0.22173917\n",
      "Iteration 44, loss = 0.22771947\n",
      "Iteration 90, loss = 0.22331527\n",
      "Iteration 45, loss = 0.22840826\n",
      "Iteration 91, loss = 0.22183437\n",
      "Iteration 46, loss = 0.22685499\n",
      "Iteration 92, loss = 0.22073669\n",
      "Iteration 47, loss = 0.22801290\n",
      "Iteration 93, loss = 0.22196018\n",
      "Iteration 48, loss = 0.22520436\n",
      "Iteration 94, loss = 0.22167475\n",
      "Iteration 49, loss = 0.22765234\n",
      "Iteration 95, loss = 0.22243921\n",
      "Iteration 50, loss = 0.23280081\n",
      "Iteration 96, loss = 0.22414443\n",
      "Iteration 51, loss = 0.23106711\n",
      "Iteration 97, loss = 0.22586693\n",
      "Iteration 52, loss = 0.22794010\n",
      "Iteration 98, loss = 0.22184949\n",
      "Iteration 53, loss = 0.22265952\n",
      "Iteration 99, loss = 0.22101561\n",
      "Iteration 54, loss = 0.22037822\n",
      "Iteration 100, loss = 0.22261361\n",
      "Iteration 55, loss = 0.21946601\n",
      "Iteration 101, loss = 0.22131765\n",
      "Iteration 56, loss = 0.22045759\n",
      "Iteration 102, loss = 0.22184355\n",
      "Iteration 57, loss = 0.22200453\n",
      "Iteration 103, loss = 0.21994682\n",
      "Iteration 104, loss = 0.22151182\n",
      "Iteration 58, loss = 0.22210842\n",
      "Iteration 105, loss = 0.22004368\n",
      "Iteration 59, loss = 0.21931726\n",
      "Iteration 106, loss = 0.21922996\n",
      "Iteration 60, loss = 0.21996471\n",
      "Iteration 107, loss = 0.21895701\n",
      "Iteration 61, loss = 0.21824892\n",
      "Iteration 108, loss = 0.22000897\n",
      "Iteration 62, loss = 0.21929609\n",
      "Iteration 109, loss = 0.22192774\n",
      "Iteration 63, loss = 0.21985308\n",
      "Iteration 110, loss = 0.21974065\n",
      "Iteration 64, loss = 0.22305684\n",
      "Iteration 111, loss = 0.22101960\n",
      "Iteration 65, loss = 0.22299968\n",
      "Iteration 112, loss = 0.21800025\n",
      "Iteration 66, loss = 0.22037172\n",
      "Iteration 113, loss = 0.21905266\n",
      "Iteration 67, loss = 0.22107917\n",
      "Iteration 114, loss = 0.22278431\n",
      "Iteration 68, loss = 0.21861739\n",
      "Iteration 115, loss = 0.22026767\n",
      "Iteration 69, loss = 0.21851995\n",
      "Iteration 116, loss = 0.21887415\n",
      "Iteration 70, loss = 0.21709818\n",
      "Iteration 117, loss = 0.21974279\n",
      "Iteration 118, loss = 0.22154889\n",
      "Iteration 71, loss = 0.21984592\n",
      "Iteration 119, loss = 0.21931266\n",
      "Iteration 72, loss = 0.21955407\n",
      "Iteration 120, loss = 0.22139698\n",
      "Iteration 73, loss = 0.21867405\n",
      "Iteration 121, loss = 0.21885540\n",
      "Iteration 74, loss = 0.21666951\n",
      "Iteration 122, loss = 0.22136670\n",
      "Iteration 75, loss = 0.21808083\n",
      "Iteration 76, loss = 0.21768654\n",
      "Iteration 123, loss = 0.22085489\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 77, loss = 0.21705355\n",
      "Iteration 78, loss = 0.21570164\n",
      "Iteration 79, loss = 0.21576383\n",
      "Iteration 80, loss = 0.21728937\n",
      "Iteration 1, loss = 0.36994764\n",
      "Iteration 81, loss = 0.21635791\n",
      "Iteration 2, loss = 0.31331985\n",
      "Iteration 82, loss = 0.21933550Iteration 3, loss = 0.29764355\n",
      "\n",
      "Iteration 4, loss = 0.29053207\n",
      "Iteration 83, loss = 0.21406248\n",
      "Iteration 84, loss = 0.21277519\n",
      "Iteration 85, loss = 0.21490208\n",
      "Iteration 5, loss = 0.29110472\n",
      "Iteration 86, loss = 0.21205797\n",
      "Iteration 87, loss = 0.21204482\n",
      "Iteration 6, loss = 0.28461798\n",
      "Iteration 7, loss = 0.28104511Iteration 88, loss = 0.21259090\n",
      "\n",
      "Iteration 8, loss = 0.27741845\n",
      "Iteration 89, loss = 0.21264243\n",
      "Iteration 9, loss = 0.27295838\n",
      "Iteration 90, loss = 0.21140858\n",
      "Iteration 10, loss = 0.26955250\n",
      "Iteration 91, loss = 0.21208600\n",
      "Iteration 11, loss = 0.26755972\n",
      "Iteration 92, loss = 0.21318987\n",
      "Iteration 12, loss = 0.26384957\n",
      "Iteration 93, loss = 0.21488979\n",
      "Iteration 13, loss = 0.26172186\n",
      "Iteration 94, loss = 0.21375166\n",
      "Iteration 14, loss = 0.26133912\n",
      "Iteration 95, loss = 0.21285458\n",
      "Iteration 15, loss = 0.26120458\n",
      "Iteration 96, loss = 0.21651370\n",
      "Iteration 16, loss = 0.25528502\n",
      "Iteration 97, loss = 0.21958718\n",
      "Iteration 17, loss = 0.25332410\n",
      "Iteration 98, loss = 0.21507879\n",
      "Iteration 18, loss = 0.25201249\n",
      "Iteration 99, loss = 0.21354846\n",
      "Iteration 19, loss = 0.25079235\n",
      "Iteration 100, loss = 0.21769685\n",
      "Iteration 20, loss = 0.25343901\n",
      "Iteration 101, loss = 0.21440615\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 21, loss = 0.25144701\n",
      "Iteration 22, loss = 0.24866799\n",
      "Iteration 23, loss = 0.24976461\n",
      "Iteration 24, loss = 0.24862203\n",
      "Iteration 1, loss = 0.36547919\n",
      "Iteration 25, loss = 0.24487680\n",
      "Iteration 2, loss = 0.31323888\n",
      "Iteration 26, loss = 0.24450803\n",
      "Iteration 3, loss = 0.29910270\n",
      "Iteration 27, loss = 0.24421000\n",
      "Iteration 4, loss = 0.29294604\n",
      "Iteration 28, loss = 0.24405116\n",
      "Iteration 5, loss = 0.28967780\n",
      "Iteration 29, loss = 0.24636380\n",
      "Iteration 6, loss = 0.28392088\n",
      "Iteration 30, loss = 0.25146904\n",
      "Iteration 7, loss = 0.28037335\n",
      "Iteration 31, loss = 0.24718103\n",
      "Iteration 8, loss = 0.27796386\n",
      "Iteration 32, loss = 0.24400234\n",
      "Iteration 9, loss = 0.27298502\n",
      "Iteration 33, loss = 0.24456441\n",
      "Iteration 10, loss = 0.27019561\n",
      "Iteration 34, loss = 0.23910141\n",
      "Iteration 35, loss = 0.23768228Iteration 11, loss = 0.26900951\n",
      "\n",
      "Iteration 36, loss = 0.23646281\n",
      "Iteration 12, loss = 0.26689275\n",
      "Iteration 37, loss = 0.23752308\n",
      "Iteration 13, loss = 0.26421199\n",
      "Iteration 38, loss = 0.23484777\n",
      "Iteration 14, loss = 0.26570031\n",
      "Iteration 39, loss = 0.23420213\n",
      "Iteration 15, loss = 0.26258560\n",
      "Iteration 40, loss = 0.23398326\n",
      "Iteration 16, loss = 0.25790128\n",
      "Iteration 41, loss = 0.23716060\n",
      "Iteration 17, loss = 0.25662674\n",
      "Iteration 42, loss = 0.23600266\n",
      "Iteration 18, loss = 0.25520216\n",
      "Iteration 43, loss = 0.23420870\n",
      "Iteration 19, loss = 0.25629443\n",
      "Iteration 44, loss = 0.23593362\n",
      "Iteration 20, loss = 0.25578608\n",
      "Iteration 45, loss = 0.23408838\n",
      "Iteration 21, loss = 0.25550818\n",
      "Iteration 46, loss = 0.23308368\n",
      "Iteration 22, loss = 0.25074964\n",
      "Iteration 47, loss = 0.23342869\n",
      "Iteration 48, loss = 0.23098084\n",
      "Iteration 23, loss = 0.24825566\n",
      "Iteration 49, loss = 0.23026042\n",
      "Iteration 24, loss = 0.24707017\n",
      "Iteration 50, loss = 0.23274940\n",
      "Iteration 25, loss = 0.24587381\n",
      "Iteration 51, loss = 0.23771320\n",
      "Iteration 26, loss = 0.24816405\n",
      "Iteration 52, loss = 0.23324555\n",
      "Iteration 27, loss = 0.24684540\n",
      "Iteration 53, loss = 0.23209986\n",
      "Iteration 28, loss = 0.24715729\n",
      "Iteration 54, loss = 0.22937703\n",
      "Iteration 29, loss = 0.24468352\n",
      "Iteration 55, loss = 0.22840204\n",
      "Iteration 30, loss = 0.24708876\n",
      "Iteration 56, loss = 0.22841618\n",
      "Iteration 31, loss = 0.24666304Iteration 57, loss = 0.23387886\n",
      "\n",
      "Iteration 58, loss = 0.22808023\n",
      "Iteration 32, loss = 0.24664513\n",
      "Iteration 59, loss = 0.22768275\n",
      "Iteration 33, loss = 0.24470579\n",
      "Iteration 60, loss = 0.22824534\n",
      "Iteration 34, loss = 0.24344725\n",
      "Iteration 61, loss = 0.22804229\n",
      "Iteration 35, loss = 0.24071960\n",
      "Iteration 62, loss = 0.23079817\n",
      "Iteration 36, loss = 0.24125797\n",
      "Iteration 63, loss = 0.22821090\n",
      "Iteration 37, loss = 0.24593096\n",
      "Iteration 64, loss = 0.23152249\n",
      "Iteration 38, loss = 0.24328232\n",
      "Iteration 65, loss = 0.23197631\n",
      "Iteration 39, loss = 0.24286572\n",
      "Iteration 66, loss = 0.22963206\n",
      "Iteration 40, loss = 0.24049773\n",
      "Iteration 67, loss = 0.22677588\n",
      "Iteration 41, loss = 0.23959614Iteration 68, loss = 0.22781579\n",
      "\n",
      "Iteration 42, loss = 0.23910000Iteration 69, loss = 0.22827665\n",
      "\n",
      "Iteration 70, loss = 0.22484634\n",
      "Iteration 43, loss = 0.23560933\n",
      "Iteration 71, loss = 0.22622867\n",
      "Iteration 44, loss = 0.23797183\n",
      "Iteration 72, loss = 0.22849109\n",
      "Iteration 45, loss = 0.24428214\n",
      "Iteration 73, loss = 0.22572106\n",
      "Iteration 46, loss = 0.23905721\n",
      "Iteration 74, loss = 0.22651240\n",
      "Iteration 47, loss = 0.24052888\n",
      "Iteration 75, loss = 0.22789691\n",
      "Iteration 48, loss = 0.23497610\n",
      "Iteration 76, loss = 0.23064385\n",
      "Iteration 77, loss = 0.22690608\n",
      "Iteration 49, loss = 0.23335461\n",
      "Iteration 78, loss = 0.22686532\n",
      "Iteration 50, loss = 0.23631168\n",
      "Iteration 79, loss = 0.22436555\n",
      "Iteration 51, loss = 0.23453152\n",
      "Iteration 80, loss = 0.22550426\n",
      "Iteration 52, loss = 0.23442933\n",
      "Iteration 81, loss = 0.22377168\n",
      "Iteration 53, loss = 0.23353552\n",
      "Iteration 82, loss = 0.22830641\n",
      "Iteration 54, loss = 0.23107973\n",
      "Iteration 83, loss = 0.22680065\n",
      "Iteration 55, loss = 0.23198804\n",
      "Iteration 84, loss = 0.22413248\n",
      "Iteration 56, loss = 0.23085814\n",
      "Iteration 85, loss = 0.22424880\n",
      "Iteration 57, loss = 0.23275924Iteration 86, loss = 0.22370796\n",
      "\n",
      "Iteration 87, loss = 0.22282564\n",
      "Iteration 58, loss = 0.23185176\n",
      "Iteration 88, loss = 0.22518151\n",
      "Iteration 59, loss = 0.23082774\n",
      "Iteration 89, loss = 0.22288747\n",
      "Iteration 60, loss = 0.23250688\n",
      "Iteration 61, loss = 0.23392322\n",
      "Iteration 90, loss = 0.22359876\n",
      "Iteration 91, loss = 0.22328094\n",
      "Iteration 62, loss = 0.23164271\n",
      "Iteration 92, loss = 0.22232675\n",
      "Iteration 63, loss = 0.22954199\n",
      "Iteration 93, loss = 0.22275620\n",
      "Iteration 64, loss = 0.23130144\n",
      "Iteration 94, loss = 0.22286350\n",
      "Iteration 65, loss = 0.22985112\n",
      "Iteration 95, loss = 0.22287417\n",
      "Iteration 66, loss = 0.22772187\n",
      "Iteration 96, loss = 0.22285582\n",
      "Iteration 67, loss = 0.22825236\n",
      "Iteration 97, loss = 0.22634239\n",
      "Iteration 68, loss = 0.23194836\n",
      "Iteration 98, loss = 0.22409706\n",
      "Iteration 69, loss = 0.23214019\n",
      "Iteration 99, loss = 0.22280460\n",
      "Iteration 70, loss = 0.22794922\n",
      "Iteration 100, loss = 0.22119401\n",
      "Iteration 101, loss = 0.22044815Iteration 71, loss = 0.22823672\n",
      "\n",
      "Iteration 72, loss = 0.23085842Iteration 102, loss = 0.22394562\n",
      "\n",
      "Iteration 73, loss = 0.22844223Iteration 103, loss = 0.22223079\n",
      "\n",
      "Iteration 104, loss = 0.22337913\n",
      "Iteration 74, loss = 0.22686593\n",
      "Iteration 105, loss = 0.22313600\n",
      "Iteration 75, loss = 0.23091192\n",
      "Iteration 106, loss = 0.22175221\n",
      "Iteration 76, loss = 0.23403616\n",
      "Iteration 107, loss = 0.22258318\n",
      "Iteration 77, loss = 0.22966393\n",
      "Iteration 108, loss = 0.22466430\n",
      "Iteration 78, loss = 0.22770002\n",
      "Iteration 109, loss = 0.22305206\n",
      "Iteration 79, loss = 0.22737035\n",
      "Iteration 110, loss = 0.22135383\n",
      "Iteration 80, loss = 0.22711816\n",
      "Iteration 111, loss = 0.21992111\n",
      "Iteration 81, loss = 0.22500295\n",
      "Iteration 112, loss = 0.21971431\n",
      "Iteration 82, loss = 0.22653539\n",
      "Iteration 113, loss = 0.22221613\n",
      "Iteration 83, loss = 0.22520902\n",
      "Iteration 114, loss = 0.22340441\n",
      "Iteration 84, loss = 0.22577432\n",
      "Iteration 115, loss = 0.22153442\n",
      "Iteration 85, loss = 0.22673215\n",
      "Iteration 116, loss = 0.21890135\n",
      "Iteration 86, loss = 0.22683625\n",
      "Iteration 117, loss = 0.22225347\n",
      "Iteration 87, loss = 0.22573873\n",
      "Iteration 118, loss = 0.22493660\n",
      "Iteration 119, loss = 0.22242444\n",
      "Iteration 88, loss = 0.22618587\n",
      "Iteration 120, loss = 0.22000356\n",
      "Iteration 89, loss = 0.22503358\n",
      "Iteration 121, loss = 0.22073222\n",
      "Iteration 90, loss = 0.22653918\n",
      "Iteration 122, loss = 0.22127216\n",
      "Iteration 91, loss = 0.22656059\n",
      "Iteration 123, loss = 0.21956767\n",
      "Iteration 92, loss = 0.22488461\n",
      "Iteration 124, loss = 0.22162179\n",
      "Iteration 93, loss = 0.22462748\n",
      "Iteration 125, loss = 0.22203931\n",
      "Iteration 94, loss = 0.22448180\n",
      "Iteration 126, loss = 0.22405094\n",
      "Iteration 95, loss = 0.22411021\n",
      "Iteration 127, loss = 0.22208565\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 96, loss = 0.23039769\n",
      "Iteration 97, loss = 0.22866201\n",
      "Iteration 98, loss = 0.22598178\n",
      "Iteration 1, loss = 0.36237914\n",
      "Iteration 99, loss = 0.22544807\n",
      "Iteration 2, loss = 0.31197521\n",
      "Iteration 100, loss = 0.22401708\n",
      "Iteration 3, loss = 0.29922523\n",
      "Iteration 101, loss = 0.22526171\n",
      "Iteration 4, loss = 0.29316754\n",
      "Iteration 102, loss = 0.22515717\n",
      "Iteration 5, loss = 0.28768354\n",
      "Iteration 103, loss = 0.22300102\n",
      "Iteration 6, loss = 0.28331475\n",
      "Iteration 104, loss = 0.22819196\n",
      "Iteration 7, loss = 0.28020132\n",
      "Iteration 105, loss = 0.22461164\n",
      "Iteration 8, loss = 0.27574949\n",
      "Iteration 106, loss = 0.22425940\n",
      "Iteration 9, loss = 0.27419581\n",
      "Iteration 107, loss = 0.22375089\n",
      "Iteration 10, loss = 0.27109955\n",
      "Iteration 108, loss = 0.22525462\n",
      "Iteration 11, loss = 0.27146820\n",
      "Iteration 109, loss = 0.22269009\n",
      "Iteration 12, loss = 0.26809232\n",
      "Iteration 110, loss = 0.22725745\n",
      "Iteration 13, loss = 0.26699227\n",
      "Iteration 111, loss = 0.22475216\n",
      "Iteration 14, loss = 0.26772588\n",
      "Iteration 112, loss = 0.22478367\n",
      "Iteration 15, loss = 0.26592729\n",
      "Iteration 113, loss = 0.22390882\n",
      "Iteration 16, loss = 0.26089010\n",
      "Iteration 114, loss = 0.22567162\n",
      "Iteration 17, loss = 0.26018440\n",
      "Iteration 115, loss = 0.22616305\n",
      "Iteration 18, loss = 0.25906775\n",
      "Iteration 116, loss = 0.22379792\n",
      "Iteration 19, loss = 0.25911538\n",
      "Iteration 117, loss = 0.22515106\n",
      "Iteration 20, loss = 0.25734120\n",
      "Iteration 118, loss = 0.22382771\n",
      "Iteration 21, loss = 0.25771475\n",
      "Iteration 119, loss = 0.22314258\n",
      "Iteration 22, loss = 0.25464502\n",
      "Iteration 120, loss = 0.22199251\n",
      "Iteration 23, loss = 0.25384929\n",
      "Iteration 121, loss = 0.22283092\n",
      "Iteration 24, loss = 0.25406588\n",
      "Iteration 122, loss = 0.22301067\n",
      "Iteration 25, loss = 0.25371163\n",
      "Iteration 123, loss = 0.22294925\n",
      "Iteration 26, loss = 0.25830023\n",
      "Iteration 124, loss = 0.22245970\n",
      "Iteration 27, loss = 0.25301943\n",
      "Iteration 125, loss = 0.22299529\n",
      "Iteration 28, loss = 0.25116009\n",
      "Iteration 126, loss = 0.22306952\n",
      "Iteration 29, loss = 0.25005222\n",
      "Iteration 127, loss = 0.22490604\n",
      "Iteration 128, loss = 0.22205946\n",
      "Iteration 30, loss = 0.25200526Iteration 129, loss = 0.22390003\n",
      "\n",
      "Iteration 130, loss = 0.22594350\n",
      "Iteration 131, loss = 0.22337997\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 31, loss = 0.24982263\n",
      "Iteration 32, loss = 0.24752023\n",
      "Iteration 33, loss = 0.25080489\n",
      "Iteration 34, loss = 0.25216586\n",
      "Iteration 35, loss = 0.24779895\n",
      "Iteration 36, loss = 0.24749121\n",
      "Iteration 37, loss = 0.24908125\n",
      "Iteration 38, loss = 0.24665457\n",
      "Iteration 39, loss = 0.24664199\n",
      "Iteration 40, loss = 0.24373077\n",
      "Iteration 41, loss = 0.24395339\n",
      "Iteration 42, loss = 0.24380622\n",
      "Iteration 43, loss = 0.24323838\n",
      "Iteration 44, loss = 0.24293501\n",
      "Iteration 45, loss = 0.24478421\n",
      "Iteration 46, loss = 0.24289315\n",
      "Iteration 47, loss = 0.24394299\n",
      "Iteration 48, loss = 0.24233203\n",
      "Iteration 49, loss = 0.24482963\n",
      "Iteration 50, loss = 0.24751507\n",
      "Iteration 51, loss = 0.24430100\n",
      "Iteration 52, loss = 0.24170769\n",
      "Iteration 53, loss = 0.24065193\n",
      "Iteration 54, loss = 0.24023249\n",
      "Iteration 55, loss = 0.24134164\n",
      "Iteration 56, loss = 0.24119364\n",
      "Iteration 57, loss = 0.24034151\n",
      "Iteration 58, loss = 0.24002574\n",
      "Iteration 59, loss = 0.23949207\n",
      "Iteration 60, loss = 0.23985122\n",
      "Iteration 61, loss = 0.24326783\n",
      "Iteration 62, loss = 0.23925951\n",
      "Iteration 63, loss = 0.23722251\n",
      "Iteration 64, loss = 0.24018208\n",
      "Iteration 65, loss = 0.23897880\n",
      "Iteration 66, loss = 0.23947755\n",
      "Iteration 67, loss = 0.23856759\n",
      "Iteration 68, loss = 0.23936139\n",
      "Iteration 69, loss = 0.23700213\n",
      "Iteration 70, loss = 0.23810588\n",
      "Iteration 71, loss = 0.23804088\n",
      "Iteration 72, loss = 0.23853383\n",
      "Iteration 73, loss = 0.23537118\n",
      "Iteration 74, loss = 0.23601885\n",
      "Iteration 75, loss = 0.23711116\n",
      "Iteration 76, loss = 0.24090600\n",
      "Iteration 77, loss = 0.23560334\n",
      "Iteration 78, loss = 0.23466956\n",
      "Iteration 79, loss = 0.23697362\n",
      "Iteration 80, loss = 0.23573754\n",
      "Iteration 81, loss = 0.23381833\n",
      "Iteration 82, loss = 0.23335795\n",
      "Iteration 83, loss = 0.23377953\n",
      "Iteration 84, loss = 0.23274369\n",
      "Iteration 85, loss = 0.23274000\n",
      "Iteration 86, loss = 0.23268364\n",
      "Iteration 87, loss = 0.23275979\n",
      "Iteration 88, loss = 0.23225730\n",
      "Iteration 89, loss = 0.23231328\n",
      "Iteration 90, loss = 0.23251974\n",
      "Iteration 91, loss = 0.23118384\n",
      "Iteration 92, loss = 0.23097033\n",
      "Iteration 93, loss = 0.23034579\n",
      "Iteration 94, loss = 0.23145009\n",
      "Iteration 95, loss = 0.23222594\n",
      "Iteration 96, loss = 0.23295100\n",
      "Iteration 97, loss = 0.23334087\n",
      "Iteration 98, loss = 0.23153685\n",
      "Iteration 99, loss = 0.23128730\n",
      "Iteration 100, loss = 0.23393622\n",
      "Iteration 101, loss = 0.23030928\n",
      "Iteration 102, loss = 0.23288495\n",
      "Iteration 103, loss = 0.23029418\n",
      "Iteration 104, loss = 0.23180422\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34840763\n",
      "Iteration 1, loss = 0.34794476\n",
      "Iteration 2, loss = 0.29733223\n",
      "Iteration 2, loss = 0.29803187\n",
      "Iteration 3, loss = 0.28448967\n",
      "Iteration 3, loss = 0.28644380\n",
      "Iteration 4, loss = 0.28102542\n",
      "Iteration 5, loss = 0.27701353Iteration 4, loss = 0.28205281\n",
      "\n",
      "Iteration 6, loss = 0.27575678\n",
      "Iteration 5, loss = 0.27908970\n",
      "Iteration 7, loss = 0.27420447\n",
      "Iteration 6, loss = 0.27753597\n",
      "Iteration 8, loss = 0.27254804\n",
      "Iteration 7, loss = 0.27530138\n",
      "Iteration 9, loss = 0.27356498\n",
      "Iteration 8, loss = 0.27311200\n",
      "Iteration 10, loss = 0.27089548\n",
      "Iteration 9, loss = 0.27244563Iteration 11, loss = 0.26998977\n",
      "\n",
      "Iteration 12, loss = 0.26598852\n",
      "Iteration 10, loss = 0.27142504\n",
      "Iteration 13, loss = 0.26713333\n",
      "Iteration 11, loss = 0.26996206\n",
      "Iteration 12, loss = 0.26916598Iteration 14, loss = 0.26780651\n",
      "\n",
      "Iteration 15, loss = 0.27051776\n",
      "Iteration 13, loss = 0.26831783\n",
      "Iteration 16, loss = 0.26234103\n",
      "Iteration 17, loss = 0.25920305\n",
      "Iteration 14, loss = 0.26757295\n",
      "Iteration 18, loss = 0.25773595\n",
      "Iteration 19, loss = 0.25945487\n",
      "Iteration 15, loss = 0.26750853\n",
      "Iteration 20, loss = 0.25776767\n",
      "Iteration 21, loss = 0.25502373\n",
      "Iteration 16, loss = 0.26530299Iteration 22, loss = 0.25619053\n",
      "\n",
      "Iteration 23, loss = 0.25590438\n",
      "Iteration 24, loss = 0.25406819\n",
      "Iteration 17, loss = 0.26445352\n",
      "Iteration 18, loss = 0.26395737\n",
      "Iteration 25, loss = 0.25216786\n",
      "Iteration 19, loss = 0.26270798\n",
      "Iteration 26, loss = 0.25172168\n",
      "Iteration 20, loss = 0.26356341\n",
      "Iteration 21, loss = 0.26288903\n",
      "Iteration 27, loss = 0.24971269\n",
      "Iteration 22, loss = 0.26454214\n",
      "Iteration 28, loss = 0.24962908\n",
      "Iteration 23, loss = 0.26221044\n",
      "Iteration 29, loss = 0.24921431\n",
      "Iteration 30, loss = 0.24915241\n",
      "Iteration 24, loss = 0.25916088\n",
      "Iteration 31, loss = 0.24643008\n",
      "Iteration 32, loss = 0.24881167\n",
      "Iteration 33, loss = 0.25356078\n",
      "Iteration 25, loss = 0.25820865\n",
      "Iteration 34, loss = 0.24853525\n",
      "Iteration 35, loss = 0.24730129\n",
      "Iteration 36, loss = 0.24336854\n",
      "Iteration 26, loss = 0.25679977\n",
      "Iteration 37, loss = 0.24690603\n",
      "Iteration 27, loss = 0.25609253Iteration 38, loss = 0.24529840\n",
      "\n",
      "Iteration 39, loss = 0.24452015\n",
      "Iteration 40, loss = 0.24533455\n",
      "Iteration 28, loss = 0.25537381Iteration 41, loss = 0.24698269\n",
      "\n",
      "Iteration 42, loss = 0.24448395\n",
      "Iteration 29, loss = 0.25617663\n",
      "Iteration 43, loss = 0.24127636\n",
      "Iteration 30, loss = 0.25583225\n",
      "Iteration 44, loss = 0.24446351\n",
      "Iteration 31, loss = 0.25374356\n",
      "Iteration 45, loss = 0.24405440\n",
      "Iteration 32, loss = 0.25701032\n",
      "Iteration 46, loss = 0.24349560\n",
      "Iteration 47, loss = 0.24379063\n",
      "Iteration 33, loss = 0.25723332\n",
      "Iteration 48, loss = 0.24259250\n",
      "Iteration 34, loss = 0.25375473\n",
      "Iteration 49, loss = 0.24233362\n",
      "Iteration 50, loss = 0.24168719\n",
      "Iteration 35, loss = 0.25823853\n",
      "Iteration 51, loss = 0.24206947\n",
      "Iteration 36, loss = 0.25302757\n",
      "Iteration 52, loss = 0.24039161\n",
      "Iteration 37, loss = 0.25265206\n",
      "Iteration 53, loss = 0.24126733\n",
      "Iteration 38, loss = 0.25282600\n",
      "Iteration 54, loss = 0.24132839\n",
      "Iteration 39, loss = 0.25234732\n",
      "Iteration 55, loss = 0.23936250\n",
      "Iteration 40, loss = 0.25354665\n",
      "Iteration 56, loss = 0.24109987\n",
      "Iteration 41, loss = 0.25664987\n",
      "Iteration 57, loss = 0.24191270\n",
      "Iteration 42, loss = 0.25526468\n",
      "Iteration 58, loss = 0.24210024\n",
      "Iteration 43, loss = 0.25026477\n",
      "Iteration 59, loss = 0.24136199\n",
      "Iteration 44, loss = 0.25054322\n",
      "Iteration 60, loss = 0.24055978\n",
      "Iteration 45, loss = 0.24995354\n",
      "Iteration 61, loss = 0.24049826\n",
      "Iteration 46, loss = 0.25052827Iteration 62, loss = 0.24215157\n",
      "\n",
      "Iteration 63, loss = 0.23948798\n",
      "Iteration 47, loss = 0.24955901\n",
      "Iteration 64, loss = 0.23939878\n",
      "Iteration 48, loss = 0.25013542\n",
      "Iteration 65, loss = 0.23889299\n",
      "Iteration 49, loss = 0.25007290\n",
      "Iteration 66, loss = 0.23881647\n",
      "Iteration 50, loss = 0.24970146\n",
      "Iteration 67, loss = 0.23871565\n",
      "Iteration 51, loss = 0.25093769\n",
      "Iteration 68, loss = 0.23956199\n",
      "Iteration 52, loss = 0.25144307\n",
      "Iteration 69, loss = 0.23941822\n",
      "Iteration 53, loss = 0.25190877\n",
      "Iteration 70, loss = 0.23821132\n",
      "Iteration 54, loss = 0.24772524\n",
      "Iteration 71, loss = 0.23861903\n",
      "Iteration 55, loss = 0.24670292\n",
      "Iteration 72, loss = 0.24222166\n",
      "Iteration 56, loss = 0.24885892Iteration 73, loss = 0.23908300\n",
      "\n",
      "Iteration 74, loss = 0.23777344\n",
      "Iteration 75, loss = 0.23936035\n",
      "Iteration 57, loss = 0.24992930\n",
      "Iteration 76, loss = 0.23943262\n",
      "Iteration 58, loss = 0.24774370\n",
      "Iteration 77, loss = 0.23906492\n",
      "Iteration 59, loss = 0.24674335\n",
      "Iteration 78, loss = 0.23981788\n",
      "Iteration 60, loss = 0.25011244\n",
      "Iteration 79, loss = 0.23916519\n",
      "Iteration 61, loss = 0.24719938\n",
      "Iteration 80, loss = 0.23850110\n",
      "Iteration 62, loss = 0.24814054\n",
      "Iteration 81, loss = 0.23929154\n",
      "Iteration 63, loss = 0.24741069\n",
      "Iteration 82, loss = 0.23712529\n",
      "Iteration 64, loss = 0.24740980\n",
      "Iteration 83, loss = 0.23861715\n",
      "Iteration 65, loss = 0.24733965\n",
      "Iteration 84, loss = 0.23924162\n",
      "Iteration 85, loss = 0.23735109\n",
      "Iteration 66, loss = 0.24806093\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 86, loss = 0.24170560\n",
      "Iteration 87, loss = 0.23647511\n",
      "Iteration 88, loss = 0.23766945\n",
      "Iteration 89, loss = 0.23996419\n",
      "Iteration 1, loss = 0.35163388\n",
      "Iteration 90, loss = 0.23762906\n",
      "Iteration 2, loss = 0.29917892\n",
      "Iteration 91, loss = 0.23838471\n",
      "Iteration 3, loss = 0.28615115\n",
      "Iteration 92, loss = 0.23706732\n",
      "Iteration 4, loss = 0.28099933\n",
      "Iteration 93, loss = 0.23690195\n",
      "Iteration 94, loss = 0.23824822\n",
      "Iteration 5, loss = 0.27825972\n",
      "Iteration 95, loss = 0.23610377\n",
      "Iteration 6, loss = 0.28061522\n",
      "Iteration 7, loss = 0.27672062\n",
      "Iteration 96, loss = 0.23769700\n",
      "Iteration 8, loss = 0.27541495\n",
      "Iteration 97, loss = 0.23839897\n",
      "Iteration 98, loss = 0.23633101\n",
      "Iteration 9, loss = 0.27625641\n",
      "Iteration 99, loss = 0.23564850\n",
      "Iteration 100, loss = 0.23589056\n",
      "Iteration 10, loss = 0.27601820\n",
      "Iteration 11, loss = 0.27468294\n",
      "Iteration 101, loss = 0.23664071\n",
      "Iteration 12, loss = 0.27247672\n",
      "Iteration 13, loss = 0.27083719\n",
      "Iteration 102, loss = 0.23959957\n",
      "Iteration 14, loss = 0.27526012\n",
      "Iteration 15, loss = 0.27478040\n",
      "Iteration 103, loss = 0.23589762\n",
      "Iteration 104, loss = 0.23874535\n",
      "Iteration 16, loss = 0.26841225\n",
      "Iteration 17, loss = 0.26599860\n",
      "Iteration 105, loss = 0.23879436\n",
      "Iteration 106, loss = 0.23651990\n",
      "Iteration 107, loss = 0.23727193\n",
      "Iteration 18, loss = 0.26594266\n",
      "Iteration 19, loss = 0.26550966\n",
      "Iteration 108, loss = 0.23488941\n",
      "Iteration 109, loss = 0.23426702\n",
      "Iteration 20, loss = 0.26536984\n",
      "Iteration 110, loss = 0.23577511\n",
      "Iteration 111, loss = 0.23507617\n",
      "Iteration 21, loss = 0.26304560\n",
      "Iteration 22, loss = 0.26358332\n",
      "Iteration 112, loss = 0.23683651\n",
      "Iteration 23, loss = 0.26501180\n",
      "Iteration 113, loss = 0.23623196\n",
      "Iteration 24, loss = 0.26103098\n",
      "Iteration 114, loss = 0.23472063\n",
      "Iteration 25, loss = 0.26050352\n",
      "Iteration 115, loss = 0.23728902\n",
      "Iteration 26, loss = 0.26004653\n",
      "Iteration 116, loss = 0.23794031Iteration 27, loss = 0.26121160\n",
      "\n",
      "Iteration 28, loss = 0.25906212\n",
      "Iteration 117, loss = 0.23668563\n",
      "Iteration 29, loss = 0.26229092\n",
      "Iteration 118, loss = 0.23571856\n",
      "Iteration 30, loss = 0.26024956\n",
      "Iteration 119, loss = 0.23733328Iteration 31, loss = 0.25749723\n",
      "\n",
      "Iteration 120, loss = 0.23616543\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 32, loss = 0.26123175\n",
      "Iteration 1, loss = 0.35365475\n",
      "Iteration 33, loss = 0.26332756\n",
      "Iteration 2, loss = 0.30293890\n",
      "Iteration 34, loss = 0.25669942\n",
      "Iteration 35, loss = 0.25669252\n",
      "Iteration 3, loss = 0.29130045\n",
      "Iteration 36, loss = 0.25567619\n",
      "Iteration 37, loss = 0.25478389\n",
      "Iteration 4, loss = 0.28515806\n",
      "Iteration 38, loss = 0.25363385\n",
      "Iteration 5, loss = 0.28293061\n",
      "Iteration 39, loss = 0.25511554\n",
      "Iteration 6, loss = 0.28285300\n",
      "Iteration 40, loss = 0.25272871\n",
      "Iteration 7, loss = 0.27913356\n",
      "Iteration 41, loss = 0.25358981\n",
      "Iteration 8, loss = 0.27716936\n",
      "Iteration 42, loss = 0.25081034\n",
      "Iteration 9, loss = 0.27591866\n",
      "Iteration 43, loss = 0.25029359\n",
      "Iteration 10, loss = 0.27370075\n",
      "Iteration 44, loss = 0.25066829\n",
      "Iteration 11, loss = 0.27335693\n",
      "Iteration 45, loss = 0.25020464\n",
      "Iteration 12, loss = 0.26983969\n",
      "Iteration 46, loss = 0.24976218\n",
      "Iteration 13, loss = 0.26873099\n",
      "Iteration 47, loss = 0.24775183\n",
      "Iteration 14, loss = 0.26963139\n",
      "Iteration 48, loss = 0.24927725\n",
      "Iteration 15, loss = 0.26886237\n",
      "Iteration 49, loss = 0.24903962\n",
      "Iteration 16, loss = 0.26585636\n",
      "Iteration 17, loss = 0.26296740Iteration 50, loss = 0.25032048\n",
      "\n",
      "Iteration 51, loss = 0.24906416\n",
      "Iteration 18, loss = 0.26391359\n",
      "Iteration 52, loss = 0.24690159\n",
      "Iteration 19, loss = 0.26184909\n",
      "Iteration 53, loss = 0.24778494\n",
      "Iteration 20, loss = 0.26030337\n",
      "Iteration 54, loss = 0.24701226\n",
      "Iteration 21, loss = 0.25985061\n",
      "Iteration 22, loss = 0.26376888\n",
      "Iteration 55, loss = 0.24665698\n",
      "Iteration 23, loss = 0.26274347\n",
      "Iteration 56, loss = 0.24695396\n",
      "Iteration 24, loss = 0.26021034\n",
      "Iteration 25, loss = 0.25850972\n",
      "Iteration 57, loss = 0.24756270\n",
      "Iteration 58, loss = 0.24616031\n",
      "Iteration 26, loss = 0.25530184\n",
      "Iteration 59, loss = 0.24637484\n",
      "Iteration 27, loss = 0.25586711\n",
      "Iteration 60, loss = 0.24617717\n",
      "Iteration 28, loss = 0.25544738\n",
      "Iteration 61, loss = 0.24642719\n",
      "Iteration 29, loss = 0.25462670\n",
      "Iteration 62, loss = 0.24749197\n",
      "Iteration 30, loss = 0.25428915\n",
      "Iteration 63, loss = 0.24553120\n",
      "Iteration 31, loss = 0.25227582\n",
      "Iteration 64, loss = 0.24519365\n",
      "Iteration 32, loss = 0.25788182\n",
      "Iteration 33, loss = 0.26154775\n",
      "Iteration 65, loss = 0.24547300\n",
      "Iteration 34, loss = 0.25549976\n",
      "Iteration 66, loss = 0.24589177\n",
      "Iteration 35, loss = 0.25209021\n",
      "Iteration 36, loss = 0.24902008\n",
      "Iteration 67, loss = 0.24458692\n",
      "Iteration 37, loss = 0.25022687\n",
      "Iteration 38, loss = 0.25198399\n",
      "Iteration 68, loss = 0.24460265\n",
      "Iteration 39, loss = 0.25118287\n",
      "Iteration 40, loss = 0.25137738\n",
      "Iteration 69, loss = 0.24365698\n",
      "Iteration 41, loss = 0.24827608Iteration 70, loss = 0.24469203\n",
      "\n",
      "Iteration 71, loss = 0.24582563\n",
      "Iteration 42, loss = 0.24702937\n",
      "Iteration 72, loss = 0.24939383\n",
      "Iteration 73, loss = 0.24370037\n",
      "Iteration 43, loss = 0.24792382\n",
      "Iteration 74, loss = 0.24470451\n",
      "Iteration 44, loss = 0.24694822\n",
      "Iteration 75, loss = 0.24459727\n",
      "Iteration 45, loss = 0.24763973\n",
      "Iteration 76, loss = 0.24372685\n",
      "Iteration 46, loss = 0.24672030\n",
      "Iteration 77, loss = 0.24462070\n",
      "Iteration 47, loss = 0.24604059\n",
      "Iteration 78, loss = 0.24372652\n",
      "Iteration 48, loss = 0.24778451\n",
      "Iteration 79, loss = 0.24503911\n",
      "Iteration 49, loss = 0.24613923\n",
      "Iteration 80, loss = 0.24378428\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 50, loss = 0.24462300\n",
      "Iteration 51, loss = 0.24600002\n",
      "Iteration 52, loss = 0.24489535\n",
      "Iteration 1, loss = 0.35149016\n",
      "Iteration 53, loss = 0.24717684\n",
      "Iteration 2, loss = 0.29849959\n",
      "Iteration 54, loss = 0.24654256\n",
      "Iteration 55, loss = 0.24551451\n",
      "Iteration 3, loss = 0.28932853\n",
      "Iteration 56, loss = 0.24460302\n",
      "Iteration 4, loss = 0.28250214\n",
      "Iteration 57, loss = 0.24496035\n",
      "Iteration 5, loss = 0.28075522\n",
      "Iteration 58, loss = 0.24649147\n",
      "Iteration 6, loss = 0.28054019\n",
      "Iteration 59, loss = 0.24791028\n",
      "Iteration 60, loss = 0.24767192\n",
      "Iteration 7, loss = 0.27867200\n",
      "Iteration 61, loss = 0.24631936\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 8, loss = 0.27715308\n",
      "Iteration 9, loss = 0.27471668\n",
      "Iteration 10, loss = 0.27228572\n",
      "Iteration 11, loss = 0.27288017\n",
      "Iteration 12, loss = 0.26927562\n",
      "Iteration 1, loss = 0.34989909\n",
      "Iteration 13, loss = 0.26842034\n",
      "Iteration 2, loss = 0.29941304\n",
      "Iteration 3, loss = 0.28874826\n",
      "Iteration 14, loss = 0.26804741\n",
      "Iteration 4, loss = 0.28487084\n",
      "Iteration 5, loss = 0.28233089\n",
      "Iteration 6, loss = 0.28036617\n",
      "Iteration 15, loss = 0.27009650\n",
      "Iteration 7, loss = 0.28219546\n",
      "Iteration 16, loss = 0.26965888\n",
      "Iteration 8, loss = 0.27941466\n",
      "Iteration 17, loss = 0.26508578\n",
      "Iteration 18, loss = 0.26423390\n",
      "Iteration 9, loss = 0.27604784\n",
      "Iteration 19, loss = 0.26636058\n",
      "Iteration 10, loss = 0.27408286\n",
      "Iteration 20, loss = 0.26418453\n",
      "Iteration 21, loss = 0.26233270\n",
      "Iteration 11, loss = 0.27425661\n",
      "Iteration 22, loss = 0.26416733\n",
      "Iteration 12, loss = 0.27299099\n",
      "Iteration 23, loss = 0.26399206\n",
      "Iteration 13, loss = 0.26959729\n",
      "Iteration 24, loss = 0.26065482\n",
      "Iteration 14, loss = 0.27155980\n",
      "Iteration 15, loss = 0.27487518\n",
      "Iteration 25, loss = 0.26182882\n",
      "Iteration 16, loss = 0.26905215\n",
      "Iteration 26, loss = 0.26166194\n",
      "Iteration 17, loss = 0.26647721\n",
      "Iteration 27, loss = 0.26163150\n",
      "Iteration 28, loss = 0.26039335\n",
      "Iteration 18, loss = 0.26666781\n",
      "Iteration 19, loss = 0.26487960Iteration 29, loss = 0.25990767\n",
      "\n",
      "Iteration 30, loss = 0.25852404\n",
      "Iteration 20, loss = 0.26447301\n",
      "Iteration 31, loss = 0.25711447\n",
      "Iteration 21, loss = 0.26441152\n",
      "Iteration 32, loss = 0.25907062\n",
      "Iteration 22, loss = 0.26414677\n",
      "Iteration 33, loss = 0.26177399\n",
      "Iteration 23, loss = 0.26369022\n",
      "Iteration 34, loss = 0.25808058\n",
      "Iteration 24, loss = 0.26154154\n",
      "Iteration 35, loss = 0.25470457\n",
      "Iteration 25, loss = 0.26163766\n",
      "Iteration 36, loss = 0.25354621\n",
      "Iteration 26, loss = 0.26054733\n",
      "Iteration 37, loss = 0.25230806\n",
      "Iteration 27, loss = 0.25929638\n",
      "Iteration 38, loss = 0.25178006\n",
      "Iteration 28, loss = 0.25888741\n",
      "Iteration 39, loss = 0.25740071\n",
      "Iteration 29, loss = 0.25522282\n",
      "Iteration 40, loss = 0.25049637\n",
      "Iteration 30, loss = 0.25409905\n",
      "Iteration 41, loss = 0.24909013\n",
      "Iteration 31, loss = 0.25193956\n",
      "Iteration 42, loss = 0.25004190\n",
      "Iteration 32, loss = 0.25385833\n",
      "Iteration 43, loss = 0.24914765\n",
      "Iteration 33, loss = 0.25552532\n",
      "Iteration 44, loss = 0.25011639\n",
      "Iteration 45, loss = 0.24859862\n",
      "Iteration 34, loss = 0.25436909\n",
      "Iteration 46, loss = 0.24752090\n",
      "Iteration 35, loss = 0.25232384\n",
      "Iteration 47, loss = 0.24712220\n",
      "Iteration 36, loss = 0.24968874\n",
      "Iteration 48, loss = 0.25030120\n",
      "Iteration 37, loss = 0.25093553\n",
      "Iteration 49, loss = 0.24873604\n",
      "Iteration 50, loss = 0.24625233\n",
      "Iteration 38, loss = 0.24901416Iteration 51, loss = 0.24606093\n",
      "\n",
      "Iteration 52, loss = 0.24627495\n",
      "Iteration 39, loss = 0.25122527\n",
      "Iteration 53, loss = 0.24612050\n",
      "Iteration 40, loss = 0.24925573\n",
      "Iteration 54, loss = 0.24755072\n",
      "Iteration 41, loss = 0.24879086\n",
      "Iteration 55, loss = 0.24495336\n",
      "Iteration 42, loss = 0.24772676\n",
      "Iteration 56, loss = 0.24529020\n",
      "Iteration 43, loss = 0.24867769\n",
      "Iteration 57, loss = 0.24541811\n",
      "Iteration 44, loss = 0.24987001\n",
      "Iteration 58, loss = 0.24591745\n",
      "Iteration 45, loss = 0.24916834\n",
      "Iteration 59, loss = 0.24664959\n",
      "Iteration 46, loss = 0.24828032\n",
      "Iteration 60, loss = 0.24432175\n",
      "Iteration 47, loss = 0.24746744\n",
      "Iteration 61, loss = 0.24842694\n",
      "Iteration 48, loss = 0.24757747\n",
      "Iteration 62, loss = 0.24543389\n",
      "Iteration 49, loss = 0.24859411\n",
      "Iteration 63, loss = 0.24374612\n",
      "Iteration 50, loss = 0.24664143\n",
      "Iteration 64, loss = 0.24420254\n",
      "Iteration 51, loss = 0.24736844\n",
      "Iteration 65, loss = 0.24439575\n",
      "Iteration 52, loss = 0.24750663\n",
      "Iteration 66, loss = 0.24466965\n",
      "Iteration 67, loss = 0.24482035Iteration 53, loss = 0.24770675\n",
      "\n",
      "Iteration 54, loss = 0.24439930\n",
      "Iteration 68, loss = 0.24537942\n",
      "Iteration 55, loss = 0.24707120\n",
      "Iteration 69, loss = 0.24280962\n",
      "Iteration 70, loss = 0.24379241\n",
      "Iteration 56, loss = 0.24580494\n",
      "Iteration 71, loss = 0.24480949\n",
      "Iteration 57, loss = 0.24614280\n",
      "Iteration 72, loss = 0.24869591\n",
      "Iteration 58, loss = 0.24536008\n",
      "Iteration 73, loss = 0.24390291\n",
      "Iteration 59, loss = 0.24473813\n",
      "Iteration 74, loss = 0.24383168\n",
      "Iteration 60, loss = 0.24327918\n",
      "Iteration 75, loss = 0.24459381\n",
      "Iteration 61, loss = 0.24681901\n",
      "Iteration 76, loss = 0.24381666\n",
      "Iteration 62, loss = 0.24430922\n",
      "Iteration 77, loss = 0.24409113\n",
      "Iteration 63, loss = 0.24271455\n",
      "Iteration 78, loss = 0.24394979\n",
      "Iteration 64, loss = 0.24254418\n",
      "Iteration 79, loss = 0.24460714\n",
      "Iteration 65, loss = 0.24210762\n",
      "Iteration 80, loss = 0.24556862\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 66, loss = 0.24187376\n",
      "Iteration 67, loss = 0.24090530\n",
      "Iteration 68, loss = 0.24195264\n",
      "Iteration 1, loss = 0.35757623\n",
      "Iteration 2, loss = 0.30860064\n",
      "Iteration 69, loss = 0.24120704\n",
      "Iteration 3, loss = 0.28917396\n",
      "Iteration 70, loss = 0.24292835\n",
      "Iteration 4, loss = 0.28271149\n",
      "Iteration 71, loss = 0.24283770\n",
      "Iteration 5, loss = 0.28126912\n",
      "Iteration 72, loss = 0.24346130\n",
      "Iteration 6, loss = 0.27931338\n",
      "Iteration 73, loss = 0.24092127\n",
      "Iteration 7, loss = 0.27709618\n",
      "Iteration 74, loss = 0.24233912\n",
      "Iteration 8, loss = 0.27529749\n",
      "Iteration 75, loss = 0.24207138\n",
      "Iteration 9, loss = 0.27575204\n",
      "Iteration 10, loss = 0.27267897\n",
      "Iteration 11, loss = 0.27159573\n",
      "Iteration 12, loss = 0.26900693\n",
      "Iteration 76, loss = 0.24120727\n",
      "Iteration 13, loss = 0.26839188\n",
      "Iteration 77, loss = 0.24049160\n",
      "Iteration 14, loss = 0.26951846\n",
      "Iteration 15, loss = 0.26633109\n",
      "Iteration 78, loss = 0.24174003Iteration 16, loss = 0.26829341\n",
      "Iteration 17, loss = 0.26457484\n",
      "\n",
      "Iteration 79, loss = 0.24145667\n",
      "Iteration 18, loss = 0.26583056\n",
      "Iteration 80, loss = 0.24348947\n",
      "Iteration 19, loss = 0.26581837\n",
      "Iteration 81, loss = 0.24309393\n",
      "Iteration 20, loss = 0.26627967\n",
      "Iteration 82, loss = 0.24069389\n",
      "Iteration 21, loss = 0.26646175\n",
      "Iteration 22, loss = 0.26263622\n",
      "Iteration 83, loss = 0.24097047\n",
      "Iteration 23, loss = 0.26181775\n",
      "Iteration 84, loss = 0.24012220\n",
      "Iteration 24, loss = 0.25970780\n",
      "Iteration 85, loss = 0.23970994\n",
      "Iteration 25, loss = 0.25754542\n",
      "Iteration 86, loss = 0.24140386\n",
      "Iteration 26, loss = 0.25824602\n",
      "Iteration 87, loss = 0.24127719\n",
      "Iteration 88, loss = 0.24065168\n",
      "Iteration 27, loss = 0.25674153\n",
      "Iteration 89, loss = 0.24234822\n",
      "Iteration 28, loss = 0.25657875\n",
      "Iteration 90, loss = 0.24066449\n",
      "Iteration 29, loss = 0.25503176\n",
      "Iteration 91, loss = 0.24205200\n",
      "Iteration 30, loss = 0.25513044\n",
      "Iteration 92, loss = 0.24253370\n",
      "Iteration 31, loss = 0.25502860\n",
      "Iteration 93, loss = 0.24141984\n",
      "Iteration 32, loss = 0.25456302\n",
      "Iteration 94, loss = 0.24131655\n",
      "Iteration 33, loss = 0.25501770\n",
      "Iteration 95, loss = 0.23952954\n",
      "Iteration 34, loss = 0.25268703\n",
      "Iteration 96, loss = 0.23931177\n",
      "Iteration 35, loss = 0.25159378\n",
      "Iteration 97, loss = 0.23866238\n",
      "Iteration 98, loss = 0.23980252\n",
      "Iteration 36, loss = 0.25374370\n",
      "Iteration 99, loss = 0.23906963\n",
      "Iteration 37, loss = 0.25541556\n",
      "Iteration 100, loss = 0.23896216\n",
      "Iteration 38, loss = 0.25391652\n",
      "Iteration 101, loss = 0.24101187\n",
      "Iteration 39, loss = 0.25180644\n",
      "Iteration 102, loss = 0.23920510\n",
      "Iteration 40, loss = 0.25079560\n",
      "Iteration 103, loss = 0.24030071\n",
      "Iteration 41, loss = 0.25155067\n",
      "Iteration 104, loss = 0.24149835\n",
      "Iteration 42, loss = 0.25087712\n",
      "Iteration 105, loss = 0.24227860\n",
      "Iteration 43, loss = 0.25069617\n",
      "Iteration 106, loss = 0.24097708\n",
      "Iteration 44, loss = 0.25004741\n",
      "Iteration 45, loss = 0.24831535\n",
      "Iteration 107, loss = 0.24080240\n",
      "Iteration 46, loss = 0.24907585\n",
      "Iteration 108, loss = 0.23985867\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 47, loss = 0.24999313\n",
      "Iteration 48, loss = 0.25045575\n",
      "Iteration 49, loss = 0.24890657\n",
      "Iteration 1, loss = 0.35630788\n",
      "Iteration 50, loss = 0.24973381\n",
      "Iteration 2, loss = 0.30897638\n",
      "Iteration 51, loss = 0.24940528\n",
      "Iteration 3, loss = 0.29009817\n",
      "Iteration 4, loss = 0.28699415Iteration 52, loss = 0.24947324\n",
      "\n",
      "Iteration 53, loss = 0.24836179Iteration 5, loss = 0.28360945\n",
      "\n",
      "Iteration 6, loss = 0.28290562\n",
      "Iteration 54, loss = 0.24945367\n",
      "Iteration 7, loss = 0.27848657\n",
      "Iteration 55, loss = 0.24621224\n",
      "Iteration 8, loss = 0.27713197\n",
      "Iteration 56, loss = 0.24844830\n",
      "Iteration 9, loss = 0.27677130\n",
      "Iteration 57, loss = 0.25013611\n",
      "Iteration 10, loss = 0.27469820\n",
      "Iteration 58, loss = 0.24914441\n",
      "Iteration 11, loss = 0.27364961\n",
      "Iteration 59, loss = 0.24684339\n",
      "Iteration 60, loss = 0.24618782\n",
      "Iteration 12, loss = 0.27157321\n",
      "Iteration 61, loss = 0.24510095\n",
      "Iteration 13, loss = 0.27145338\n",
      "Iteration 62, loss = 0.24673943\n",
      "Iteration 14, loss = 0.27102779\n",
      "Iteration 63, loss = 0.24719888\n",
      "Iteration 15, loss = 0.26981219\n",
      "Iteration 64, loss = 0.24852785\n",
      "Iteration 16, loss = 0.27151670\n",
      "Iteration 65, loss = 0.24502126\n",
      "Iteration 17, loss = 0.27100658\n",
      "Iteration 66, loss = 0.24602136\n",
      "Iteration 18, loss = 0.26826929\n",
      "Iteration 67, loss = 0.24507010\n",
      "Iteration 19, loss = 0.26999978\n",
      "Iteration 68, loss = 0.24369372\n",
      "Iteration 20, loss = 0.26650537\n",
      "Iteration 69, loss = 0.24524339\n",
      "Iteration 21, loss = 0.26479005\n",
      "Iteration 70, loss = 0.24591938\n",
      "Iteration 22, loss = 0.26261623\n",
      "Iteration 71, loss = 0.24532983\n",
      "Iteration 23, loss = 0.26128192\n",
      "Iteration 72, loss = 0.24525618\n",
      "Iteration 24, loss = 0.26065056\n",
      "Iteration 73, loss = 0.24407557\n",
      "Iteration 25, loss = 0.25819231\n",
      "Iteration 74, loss = 0.24577458\n",
      "Iteration 26, loss = 0.25721202\n",
      "Iteration 75, loss = 0.24273444\n",
      "Iteration 27, loss = 0.25691898\n",
      "Iteration 76, loss = 0.24432326\n",
      "Iteration 28, loss = 0.25562060\n",
      "Iteration 77, loss = 0.24341454\n",
      "Iteration 29, loss = 0.25712640\n",
      "Iteration 78, loss = 0.24303959\n",
      "Iteration 30, loss = 0.25681085\n",
      "Iteration 79, loss = 0.24487532\n",
      "Iteration 31, loss = 0.25558355\n",
      "Iteration 80, loss = 0.24293997\n",
      "Iteration 32, loss = 0.25548190\n",
      "Iteration 81, loss = 0.24267948\n",
      "Iteration 33, loss = 0.25456533\n",
      "Iteration 82, loss = 0.24659182\n",
      "Iteration 34, loss = 0.25380961\n",
      "Iteration 83, loss = 0.24413049\n",
      "Iteration 35, loss = 0.25267801\n",
      "Iteration 84, loss = 0.24499174\n",
      "Iteration 36, loss = 0.25251322\n",
      "Iteration 85, loss = 0.24265986\n",
      "Iteration 37, loss = 0.25685630\n",
      "Iteration 86, loss = 0.24257556\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 38, loss = 0.25393083\n",
      "Iteration 39, loss = 0.25257753\n",
      "Iteration 40, loss = 0.25213526\n",
      "Iteration 1, loss = 0.35102825\n",
      "Iteration 41, loss = 0.25341109\n",
      "Iteration 2, loss = 0.30285685\n",
      "Iteration 42, loss = 0.25143340\n",
      "Iteration 3, loss = 0.28886930\n",
      "Iteration 4, loss = 0.28663916\n",
      "Iteration 43, loss = 0.25168400\n",
      "Iteration 5, loss = 0.28454861\n",
      "Iteration 44, loss = 0.25202437\n",
      "Iteration 6, loss = 0.28396865\n",
      "Iteration 45, loss = 0.25052320\n",
      "Iteration 7, loss = 0.27815223\n",
      "Iteration 46, loss = 0.24992910\n",
      "Iteration 8, loss = 0.27657729\n",
      "Iteration 47, loss = 0.25266275\n",
      "Iteration 48, loss = 0.25082612\n",
      "Iteration 9, loss = 0.27676764\n",
      "Iteration 10, loss = 0.27445592\n",
      "Iteration 49, loss = 0.24997566\n",
      "Iteration 11, loss = 0.27229637\n",
      "Iteration 50, loss = 0.25166297\n",
      "Iteration 12, loss = 0.27090417\n",
      "Iteration 51, loss = 0.25147054\n",
      "Iteration 13, loss = 0.27026461\n",
      "Iteration 52, loss = 0.24925573\n",
      "Iteration 14, loss = 0.27196814\n",
      "Iteration 15, loss = 0.26805112\n",
      "Iteration 53, loss = 0.24815624\n",
      "Iteration 54, loss = 0.24963480\n",
      "Iteration 16, loss = 0.26787416\n",
      "Iteration 55, loss = 0.24769515\n",
      "Iteration 17, loss = 0.26762387\n",
      "Iteration 56, loss = 0.24880851\n",
      "Iteration 18, loss = 0.26545052\n",
      "Iteration 57, loss = 0.24948625\n",
      "Iteration 19, loss = 0.26362127\n",
      "Iteration 20, loss = 0.26216996\n",
      "Iteration 58, loss = 0.25026298\n",
      "Iteration 21, loss = 0.26533742\n",
      "Iteration 22, loss = 0.26191579\n",
      "Iteration 23, loss = 0.26128256\n",
      "Iteration 59, loss = 0.24829536\n",
      "Iteration 24, loss = 0.25871633\n",
      "Iteration 25, loss = 0.25758577\n",
      "Iteration 60, loss = 0.24853358\n",
      "Iteration 26, loss = 0.25782379\n",
      "Iteration 27, loss = 0.25964568\n",
      "Iteration 61, loss = 0.24738736\n",
      "Iteration 62, loss = 0.24785275\n",
      "Iteration 28, loss = 0.25521969\n",
      "Iteration 63, loss = 0.24870734\n",
      "Iteration 29, loss = 0.25413067\n",
      "Iteration 64, loss = 0.24927650\n",
      "Iteration 65, loss = 0.24876179\n",
      "Iteration 30, loss = 0.25465698\n",
      "Iteration 66, loss = 0.24866270\n",
      "Iteration 31, loss = 0.25665933\n",
      "Iteration 67, loss = 0.24713471\n",
      "Iteration 68, loss = 0.24843333\n",
      "Iteration 32, loss = 0.25281633\n",
      "Iteration 69, loss = 0.24737009\n",
      "Iteration 33, loss = 0.25283011\n",
      "Iteration 34, loss = 0.25029645\n",
      "Iteration 70, loss = 0.24769585\n",
      "Iteration 35, loss = 0.24941364\n",
      "Iteration 71, loss = 0.24838242\n",
      "Iteration 72, loss = 0.24725628\n",
      "Iteration 36, loss = 0.24951546\n",
      "Iteration 73, loss = 0.24687068\n",
      "Iteration 74, loss = 0.24780202\n",
      "Iteration 37, loss = 0.25009644Iteration 75, loss = 0.24646850\n",
      "\n",
      "Iteration 76, loss = 0.24855484\n",
      "Iteration 38, loss = 0.25039765Iteration 77, loss = 0.24778986\n",
      "\n",
      "Iteration 78, loss = 0.24753759\n",
      "Iteration 39, loss = 0.24909258\n",
      "Iteration 79, loss = 0.24848095\n",
      "Iteration 40, loss = 0.24885892\n",
      "Iteration 80, loss = 0.24725026\n",
      "Iteration 41, loss = 0.25003807\n",
      "Iteration 42, loss = 0.25004368\n",
      "Iteration 81, loss = 0.24647590\n",
      "Iteration 43, loss = 0.24787830\n",
      "Iteration 82, loss = 0.24991104\n",
      "Iteration 44, loss = 0.24880677\n",
      "Iteration 83, loss = 0.24750361\n",
      "Iteration 45, loss = 0.25059196\n",
      "Iteration 84, loss = 0.24774299\n",
      "Iteration 46, loss = 0.24679149\n",
      "Iteration 85, loss = 0.24799504\n",
      "Iteration 47, loss = 0.24697279\n",
      "Iteration 86, loss = 0.24725068\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 48, loss = 0.24956959\n",
      "Iteration 49, loss = 0.24710824\n",
      "Iteration 50, loss = 0.24645279\n",
      "Iteration 1, loss = 0.35089478\n",
      "Iteration 51, loss = 0.24686875\n",
      "Iteration 2, loss = 0.30527093\n",
      "Iteration 52, loss = 0.24617465\n",
      "Iteration 3, loss = 0.29318620\n",
      "Iteration 53, loss = 0.24447816\n",
      "Iteration 4, loss = 0.29173499\n",
      "Iteration 54, loss = 0.24779154\n",
      "Iteration 5, loss = 0.28826138\n",
      "Iteration 55, loss = 0.24490049\n",
      "Iteration 6, loss = 0.28750548\n",
      "Iteration 56, loss = 0.24570009\n",
      "Iteration 7, loss = 0.28221549\n",
      "Iteration 8, loss = 0.27953636\n",
      "Iteration 57, loss = 0.24594907\n",
      "Iteration 9, loss = 0.27942835\n",
      "Iteration 58, loss = 0.24548988\n",
      "Iteration 10, loss = 0.27520680\n",
      "Iteration 59, loss = 0.24490985\n",
      "Iteration 11, loss = 0.27524480\n",
      "Iteration 60, loss = 0.24449848\n",
      "Iteration 12, loss = 0.27434832\n",
      "Iteration 61, loss = 0.24531576\n",
      "Iteration 13, loss = 0.27243888\n",
      "Iteration 62, loss = 0.24324993\n",
      "Iteration 14, loss = 0.27316244\n",
      "Iteration 63, loss = 0.24490489\n",
      "Iteration 15, loss = 0.27140219\n",
      "Iteration 64, loss = 0.24727714\n",
      "Iteration 16, loss = 0.26887688\n",
      "Iteration 65, loss = 0.24717531\n",
      "Iteration 17, loss = 0.26982295\n",
      "Iteration 66, loss = 0.24552766\n",
      "Iteration 18, loss = 0.26736645\n",
      "Iteration 67, loss = 0.24264792\n",
      "Iteration 19, loss = 0.26699046\n",
      "Iteration 68, loss = 0.24496401\n",
      "Iteration 20, loss = 0.26712679\n",
      "Iteration 69, loss = 0.24357548\n",
      "Iteration 21, loss = 0.26898674\n",
      "Iteration 70, loss = 0.24524734\n",
      "Iteration 22, loss = 0.26505070\n",
      "Iteration 23, loss = 0.26080925\n",
      "Iteration 71, loss = 0.24662795\n",
      "Iteration 24, loss = 0.26013834\n",
      "Iteration 72, loss = 0.24507527\n",
      "Iteration 25, loss = 0.25980975\n",
      "Iteration 73, loss = 0.24254472\n",
      "Iteration 26, loss = 0.25786222\n",
      "Iteration 74, loss = 0.24263110\n",
      "Iteration 27, loss = 0.25711575\n",
      "Iteration 75, loss = 0.24252906\n",
      "Iteration 28, loss = 0.25616194\n",
      "Iteration 76, loss = 0.24449715\n",
      "Iteration 29, loss = 0.25473904\n",
      "Iteration 77, loss = 0.24416848\n",
      "Iteration 30, loss = 0.25439174\n",
      "Iteration 78, loss = 0.24250558\n",
      "Iteration 31, loss = 0.26060855\n",
      "Iteration 79, loss = 0.24268322\n",
      "Iteration 32, loss = 0.25368504\n",
      "Iteration 80, loss = 0.24314525\n",
      "Iteration 33, loss = 0.25425502\n",
      "Iteration 81, loss = 0.24262626\n",
      "Iteration 34, loss = 0.25301978\n",
      "Iteration 82, loss = 0.24297581\n",
      "Iteration 35, loss = 0.25108838\n",
      "Iteration 83, loss = 0.24168861\n",
      "Iteration 36, loss = 0.25186818\n",
      "Iteration 84, loss = 0.24230974\n",
      "Iteration 85, loss = 0.24267022\n",
      "Iteration 37, loss = 0.25081843\n",
      "Iteration 86, loss = 0.24284460\n",
      "Iteration 38, loss = 0.25116941\n",
      "Iteration 87, loss = 0.24515344\n",
      "Iteration 88, loss = 0.24448473Iteration 39, loss = 0.24996781\n",
      "\n",
      "Iteration 40, loss = 0.25091050\n",
      "Iteration 89, loss = 0.24046239\n",
      "Iteration 41, loss = 0.25511850\n",
      "Iteration 42, loss = 0.25455430\n",
      "Iteration 90, loss = 0.24346975\n",
      "Iteration 43, loss = 0.24967689\n",
      "Iteration 44, loss = 0.25267880\n",
      "Iteration 91, loss = 0.24343306\n",
      "Iteration 45, loss = 0.25403395\n",
      "Iteration 92, loss = 0.24234171\n",
      "Iteration 46, loss = 0.25030612\n",
      "Iteration 93, loss = 0.24687349\n",
      "Iteration 47, loss = 0.25043380\n",
      "Iteration 94, loss = 0.24222339\n",
      "Iteration 48, loss = 0.24855537\n",
      "Iteration 95, loss = 0.24233821\n",
      "Iteration 49, loss = 0.25090978\n",
      "Iteration 96, loss = 0.24136572\n",
      "Iteration 50, loss = 0.24951588\n",
      "Iteration 97, loss = 0.24164726\n",
      "Iteration 51, loss = 0.25068431\n",
      "Iteration 98, loss = 0.24256301\n",
      "Iteration 52, loss = 0.24839261\n",
      "Iteration 99, loss = 0.23992893\n",
      "Iteration 53, loss = 0.24726923\n",
      "Iteration 100, loss = 0.24133671\n",
      "Iteration 54, loss = 0.24947280\n",
      "Iteration 101, loss = 0.24069563\n",
      "Iteration 55, loss = 0.24804059\n",
      "Iteration 102, loss = 0.24250920\n",
      "Iteration 56, loss = 0.24797268\n",
      "Iteration 57, loss = 0.24700429\n",
      "Iteration 103, loss = 0.24098238\n",
      "Iteration 104, loss = 0.24265292\n",
      "Iteration 58, loss = 0.24754078\n",
      "Iteration 105, loss = 0.24098594\n",
      "Iteration 106, loss = 0.24299177\n",
      "Iteration 59, loss = 0.24605207\n",
      "Iteration 60, loss = 0.24646991\n",
      "Iteration 107, loss = 0.24313345\n",
      "Iteration 61, loss = 0.24729603\n",
      "Iteration 108, loss = 0.24330416\n",
      "Iteration 62, loss = 0.24782483\n",
      "Iteration 109, loss = 0.24221497\n",
      "Iteration 63, loss = 0.24640417\n",
      "Iteration 110, loss = 0.24337197\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 64, loss = 0.25070393\n",
      "Iteration 65, loss = 0.24875563\n",
      "Iteration 66, loss = 0.24905430\n",
      "Iteration 67, loss = 0.24827121\n",
      "Iteration 68, loss = 0.24760575\n",
      "Iteration 69, loss = 0.24664551\n",
      "Iteration 70, loss = 0.25196137\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33551061Iteration 1, loss = 0.33615696\n",
      "\n",
      "Iteration 2, loss = 0.28713972\n",
      "Iteration 2, loss = 0.28798390\n",
      "Iteration 3, loss = 0.27826322\n",
      "Iteration 3, loss = 0.28419607\n",
      "Iteration 4, loss = 0.27865224\n",
      "Iteration 4, loss = 0.27506093\n",
      "Iteration 5, loss = 0.27552472\n",
      "Iteration 5, loss = 0.27311230\n",
      "Iteration 6, loss = 0.27289080\n",
      "Iteration 6, loss = 0.26853451\n",
      "Iteration 7, loss = 0.27078582\n",
      "Iteration 7, loss = 0.26781499\n",
      "Iteration 8, loss = 0.26890585\n",
      "Iteration 8, loss = 0.26534360\n",
      "Iteration 9, loss = 0.26617388\n",
      "Iteration 9, loss = 0.26217548\n",
      "Iteration 10, loss = 0.26518238\n",
      "Iteration 10, loss = 0.26036035\n",
      "Iteration 11, loss = 0.26354057\n",
      "Iteration 11, loss = 0.25883158\n",
      "Iteration 12, loss = 0.26316279\n",
      "Iteration 12, loss = 0.25710292\n",
      "Iteration 13, loss = 0.26371722\n",
      "Iteration 13, loss = 0.25702057\n",
      "Iteration 14, loss = 0.26327339\n",
      "Iteration 14, loss = 0.25639872\n",
      "Iteration 15, loss = 0.26067536\n",
      "Iteration 15, loss = 0.25440287\n",
      "Iteration 16, loss = 0.25960664\n",
      "Iteration 16, loss = 0.25484268\n",
      "Iteration 17, loss = 0.25984323\n",
      "Iteration 17, loss = 0.25372948\n",
      "Iteration 18, loss = 0.26036217\n",
      "Iteration 18, loss = 0.25246259\n",
      "Iteration 19, loss = 0.25865309\n",
      "Iteration 19, loss = 0.25186678\n",
      "Iteration 20, loss = 0.26142629\n",
      "Iteration 20, loss = 0.25434102Iteration 21, loss = 0.25854557\n",
      "\n",
      "Iteration 21, loss = 0.25131762\n",
      "Iteration 22, loss = 0.25952526\n",
      "Iteration 22, loss = 0.25397835\n",
      "Iteration 23, loss = 0.25650967\n",
      "Iteration 23, loss = 0.24975627\n",
      "Iteration 24, loss = 0.25897246\n",
      "Iteration 24, loss = 0.25094170\n",
      "Iteration 25, loss = 0.25719243\n",
      "Iteration 26, loss = 0.25844075\n",
      "Iteration 25, loss = 0.24961714\n",
      "Iteration 27, loss = 0.25629516\n",
      "Iteration 26, loss = 0.24988843\n",
      "Iteration 28, loss = 0.25540696\n",
      "Iteration 27, loss = 0.25068893\n",
      "Iteration 29, loss = 0.25585125\n",
      "Iteration 28, loss = 0.24873213\n",
      "Iteration 30, loss = 0.25569194\n",
      "Iteration 29, loss = 0.24973260\n",
      "Iteration 31, loss = 0.25456545\n",
      "Iteration 30, loss = 0.24909542\n",
      "Iteration 32, loss = 0.25511051\n",
      "Iteration 31, loss = 0.24594639\n",
      "Iteration 33, loss = 0.25427092\n",
      "Iteration 32, loss = 0.24603763\n",
      "Iteration 34, loss = 0.25247247\n",
      "Iteration 33, loss = 0.24431659\n",
      "Iteration 35, loss = 0.25435409\n",
      "Iteration 34, loss = 0.24563371Iteration 36, loss = 0.25248559\n",
      "\n",
      "Iteration 37, loss = 0.25180132\n",
      "Iteration 35, loss = 0.24485846\n",
      "Iteration 38, loss = 0.25279887\n",
      "Iteration 36, loss = 0.24405555\n",
      "Iteration 39, loss = 0.25066754\n",
      "Iteration 37, loss = 0.24442263\n",
      "Iteration 40, loss = 0.25128793\n",
      "Iteration 38, loss = 0.24426423\n",
      "Iteration 41, loss = 0.25084374\n",
      "Iteration 39, loss = 0.24270697\n",
      "Iteration 42, loss = 0.24861215\n",
      "Iteration 40, loss = 0.24310555\n",
      "Iteration 43, loss = 0.24857571\n",
      "Iteration 41, loss = 0.24298136\n",
      "Iteration 44, loss = 0.24795396\n",
      "Iteration 42, loss = 0.24145579\n",
      "Iteration 45, loss = 0.24859267\n",
      "Iteration 43, loss = 0.24194483\n",
      "Iteration 46, loss = 0.24748050\n",
      "Iteration 47, loss = 0.24821426\n",
      "Iteration 44, loss = 0.23999592\n",
      "Iteration 48, loss = 0.25026317\n",
      "Iteration 45, loss = 0.24104494\n",
      "Iteration 49, loss = 0.24868399\n",
      "Iteration 46, loss = 0.23943653\n",
      "Iteration 50, loss = 0.24714055\n",
      "Iteration 47, loss = 0.23920711\n",
      "Iteration 51, loss = 0.24747333\n",
      "Iteration 48, loss = 0.24127669\n",
      "Iteration 52, loss = 0.24630707\n",
      "Iteration 49, loss = 0.23981595\n",
      "Iteration 53, loss = 0.24637078\n",
      "Iteration 50, loss = 0.24076066\n",
      "Iteration 54, loss = 0.24555442\n",
      "Iteration 51, loss = 0.23937034\n",
      "Iteration 55, loss = 0.24700597\n",
      "Iteration 52, loss = 0.23953971\n",
      "Iteration 56, loss = 0.24812364\n",
      "Iteration 53, loss = 0.23962300\n",
      "Iteration 57, loss = 0.24639623\n",
      "Iteration 54, loss = 0.23926285\n",
      "Iteration 58, loss = 0.24691765\n",
      "Iteration 55, loss = 0.23899103\n",
      "Iteration 59, loss = 0.24661791\n",
      "Iteration 56, loss = 0.23829876\n",
      "Iteration 60, loss = 0.24712750\n",
      "Iteration 61, loss = 0.24645748\n",
      "Iteration 57, loss = 0.24117009\n",
      "Iteration 62, loss = 0.24617981\n",
      "Iteration 58, loss = 0.23936907\n",
      "Iteration 63, loss = 0.24513937\n",
      "Iteration 59, loss = 0.23967387\n",
      "Iteration 64, loss = 0.24736445\n",
      "Iteration 60, loss = 0.23723222\n",
      "Iteration 65, loss = 0.24599008\n",
      "Iteration 61, loss = 0.23842612\n",
      "Iteration 66, loss = 0.24680451\n",
      "Iteration 67, loss = 0.24491945\n",
      "Iteration 62, loss = 0.23726086\n",
      "Iteration 68, loss = 0.24540490\n",
      "Iteration 63, loss = 0.23689151\n",
      "Iteration 69, loss = 0.24524764\n",
      "Iteration 64, loss = 0.23929781\n",
      "Iteration 70, loss = 0.24423475\n",
      "Iteration 65, loss = 0.23839298\n",
      "Iteration 71, loss = 0.24551739\n",
      "Iteration 66, loss = 0.23935110\n",
      "Iteration 72, loss = 0.24504535\n",
      "Iteration 67, loss = 0.23881298\n",
      "Iteration 73, loss = 0.24604550\n",
      "Iteration 68, loss = 0.23902133\n",
      "Iteration 74, loss = 0.24631558\n",
      "Iteration 69, loss = 0.23860142\n",
      "Iteration 75, loss = 0.24503004\n",
      "Iteration 70, loss = 0.23941510\n",
      "Iteration 76, loss = 0.24393634\n",
      "Iteration 71, loss = 0.23829719\n",
      "Iteration 77, loss = 0.24494260\n",
      "Iteration 78, loss = 0.24607861\n",
      "Iteration 72, loss = 0.23862157\n",
      "Iteration 79, loss = 0.24441285\n",
      "Iteration 73, loss = 0.23920999\n",
      "Iteration 80, loss = 0.24482739\n",
      "Iteration 74, loss = 0.23801862\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 81, loss = 0.24476631\n",
      "Iteration 82, loss = 0.24407629\n",
      "Iteration 83, loss = 0.25047286\n",
      "Iteration 1, loss = 0.33840263\n",
      "Iteration 84, loss = 0.24666396\n",
      "Iteration 2, loss = 0.28724976\n",
      "Iteration 85, loss = 0.24506339\n",
      "Iteration 3, loss = 0.28163709\n",
      "Iteration 86, loss = 0.24459553\n",
      "Iteration 4, loss = 0.27651767\n",
      "Iteration 5, loss = 0.27362795\n",
      "Iteration 87, loss = 0.24569227\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 6, loss = 0.27237684\n",
      "Iteration 7, loss = 0.26873378\n",
      "Iteration 1, loss = 0.33603557\n",
      "Iteration 8, loss = 0.26731408\n",
      "Iteration 2, loss = 0.28692724\n",
      "Iteration 3, loss = 0.28128911\n",
      "Iteration 9, loss = 0.26577159Iteration 4, loss = 0.27336373\n",
      "\n",
      "Iteration 5, loss = 0.27000616\n",
      "Iteration 10, loss = 0.26446613\n",
      "Iteration 6, loss = 0.26969446\n",
      "Iteration 11, loss = 0.26197222\n",
      "Iteration 12, loss = 0.26152510\n",
      "Iteration 7, loss = 0.26690272\n",
      "Iteration 13, loss = 0.26093873\n",
      "Iteration 8, loss = 0.26549389\n",
      "Iteration 14, loss = 0.26153387\n",
      "Iteration 9, loss = 0.26214438\n",
      "Iteration 15, loss = 0.25773964\n",
      "Iteration 10, loss = 0.26182060\n",
      "Iteration 16, loss = 0.25678672\n",
      "Iteration 11, loss = 0.25960352\n",
      "Iteration 17, loss = 0.25672666\n",
      "Iteration 12, loss = 0.25772943\n",
      "Iteration 18, loss = 0.25654894\n",
      "Iteration 13, loss = 0.25788428\n",
      "Iteration 19, loss = 0.25369401\n",
      "Iteration 14, loss = 0.25687555\n",
      "Iteration 20, loss = 0.25663186\n",
      "Iteration 15, loss = 0.25542209\n",
      "Iteration 21, loss = 0.25217777\n",
      "Iteration 16, loss = 0.25604913\n",
      "Iteration 22, loss = 0.25595349\n",
      "Iteration 17, loss = 0.25443809\n",
      "Iteration 23, loss = 0.25216494\n",
      "Iteration 18, loss = 0.25469758\n",
      "Iteration 24, loss = 0.25252130\n",
      "Iteration 19, loss = 0.25310195\n",
      "Iteration 25, loss = 0.25118013\n",
      "Iteration 20, loss = 0.25437976\n",
      "Iteration 26, loss = 0.25265540\n",
      "Iteration 21, loss = 0.25190098\n",
      "Iteration 22, loss = 0.25294208\n",
      "Iteration 27, loss = 0.25101352\n",
      "Iteration 23, loss = 0.25161122\n",
      "Iteration 28, loss = 0.25107481\n",
      "Iteration 24, loss = 0.25210403\n",
      "Iteration 29, loss = 0.25034128\n",
      "Iteration 25, loss = 0.24867902\n",
      "Iteration 30, loss = 0.25040630\n",
      "Iteration 31, loss = 0.25062155\n",
      "Iteration 26, loss = 0.24974954\n",
      "Iteration 32, loss = 0.25123201\n",
      "Iteration 27, loss = 0.24719328\n",
      "Iteration 33, loss = 0.24865306\n",
      "Iteration 28, loss = 0.24762449\n",
      "Iteration 34, loss = 0.24749850\n",
      "Iteration 29, loss = 0.24751139\n",
      "Iteration 35, loss = 0.24798630\n",
      "Iteration 30, loss = 0.24561691\n",
      "Iteration 36, loss = 0.24708346\n",
      "Iteration 31, loss = 0.24481333\n",
      "Iteration 37, loss = 0.24881976\n",
      "Iteration 32, loss = 0.24484532\n",
      "Iteration 38, loss = 0.25012188\n",
      "Iteration 33, loss = 0.24506315\n",
      "Iteration 39, loss = 0.24761324\n",
      "Iteration 34, loss = 0.24380200\n",
      "Iteration 40, loss = 0.24675245\n",
      "Iteration 35, loss = 0.24548259\n",
      "Iteration 41, loss = 0.24655777\n",
      "Iteration 36, loss = 0.24220033\n",
      "Iteration 42, loss = 0.24554258\n",
      "Iteration 43, loss = 0.24674135\n",
      "Iteration 44, loss = 0.24576789\n",
      "Iteration 45, loss = 0.24634691Iteration 37, loss = 0.24387313\n",
      "\n",
      "Iteration 38, loss = 0.24352121\n",
      "Iteration 39, loss = 0.24205669\n",
      "Iteration 46, loss = 0.24642322\n",
      "Iteration 40, loss = 0.24326722\n",
      "Iteration 41, loss = 0.24233949\n",
      "Iteration 47, loss = 0.24563135\n",
      "Iteration 42, loss = 0.24262941\n",
      "Iteration 43, loss = 0.24391023\n",
      "Iteration 48, loss = 0.24769555\n",
      "Iteration 49, loss = 0.24769794\n",
      "Iteration 44, loss = 0.24254253\n",
      "Iteration 50, loss = 0.24478600\n",
      "Iteration 45, loss = 0.24239607\n",
      "Iteration 46, loss = 0.24147394\n",
      "Iteration 47, loss = 0.23970366\n",
      "Iteration 48, loss = 0.24013338\n",
      "Iteration 51, loss = 0.24490040\n",
      "Iteration 49, loss = 0.24095809\n",
      "Iteration 52, loss = 0.24534402\n",
      "Iteration 53, loss = 0.24564490\n",
      "Iteration 50, loss = 0.23974873\n",
      "Iteration 54, loss = 0.24420893\n",
      "Iteration 51, loss = 0.24132508Iteration 55, loss = 0.24449197\n",
      "\n",
      "Iteration 52, loss = 0.24031587\n",
      "Iteration 56, loss = 0.24539765\n",
      "Iteration 53, loss = 0.23980492\n",
      "Iteration 57, loss = 0.24467226\n",
      "Iteration 54, loss = 0.23974823\n",
      "Iteration 55, loss = 0.24372547\n",
      "Iteration 56, loss = 0.24060517\n",
      "Iteration 58, loss = 0.24456155\n",
      "Iteration 59, loss = 0.24579221\n",
      "Iteration 57, loss = 0.24000077Iteration 60, loss = 0.24703136\n",
      "\n",
      "Iteration 61, loss = 0.24401884\n",
      "Iteration 58, loss = 0.24108724\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 62, loss = 0.24424741\n",
      "Iteration 63, loss = 0.24382503\n",
      "Iteration 64, loss = 0.24310784\n",
      "Iteration 1, loss = 0.33479995\n",
      "Iteration 65, loss = 0.24184369\n",
      "Iteration 2, loss = 0.28668198\n",
      "Iteration 66, loss = 0.24347337\n",
      "Iteration 3, loss = 0.27966856\n",
      "Iteration 67, loss = 0.24272751\n",
      "Iteration 4, loss = 0.27392283\n",
      "Iteration 68, loss = 0.24252686\n",
      "Iteration 5, loss = 0.27019863\n",
      "Iteration 69, loss = 0.24311981\n",
      "Iteration 6, loss = 0.26798584\n",
      "Iteration 70, loss = 0.24174673\n",
      "Iteration 7, loss = 0.26653284\n",
      "Iteration 71, loss = 0.24291532\n",
      "Iteration 8, loss = 0.26591638\n",
      "Iteration 72, loss = 0.24244835\n",
      "Iteration 9, loss = 0.26153763\n",
      "Iteration 73, loss = 0.24198876\n",
      "Iteration 10, loss = 0.26269488\n",
      "Iteration 74, loss = 0.24244863\n",
      "Iteration 11, loss = 0.26204137\n",
      "Iteration 75, loss = 0.24258535\n",
      "Iteration 12, loss = 0.26054057\n",
      "Iteration 76, loss = 0.24232410\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 13, loss = 0.26041849\n",
      "Iteration 14, loss = 0.25860685\n",
      "Iteration 15, loss = 0.25899881\n",
      "Iteration 1, loss = 0.33034902\n",
      "Iteration 16, loss = 0.25776756\n",
      "Iteration 2, loss = 0.28340317\n",
      "Iteration 17, loss = 0.25717030\n",
      "Iteration 3, loss = 0.27658922\n",
      "Iteration 18, loss = 0.25775797\n",
      "Iteration 4, loss = 0.27059268\n",
      "Iteration 19, loss = 0.25614606\n",
      "Iteration 5, loss = 0.26661573\n",
      "Iteration 6, loss = 0.26299576\n",
      "Iteration 20, loss = 0.25739320\n",
      "Iteration 7, loss = 0.26311512\n",
      "Iteration 21, loss = 0.25516759\n",
      "Iteration 8, loss = 0.26029869\n",
      "Iteration 22, loss = 0.25412818\n",
      "Iteration 9, loss = 0.25724622\n",
      "Iteration 23, loss = 0.25541048\n",
      "Iteration 10, loss = 0.25581903\n",
      "Iteration 24, loss = 0.25464568\n",
      "Iteration 11, loss = 0.25731813\n",
      "Iteration 25, loss = 0.25246375\n",
      "Iteration 12, loss = 0.25507991\n",
      "Iteration 26, loss = 0.25372311\n",
      "Iteration 13, loss = 0.25452605\n",
      "Iteration 27, loss = 0.25337362\n",
      "Iteration 14, loss = 0.25146944\n",
      "Iteration 28, loss = 0.25028228\n",
      "Iteration 15, loss = 0.25627081\n",
      "Iteration 29, loss = 0.25041056\n",
      "Iteration 16, loss = 0.25216755\n",
      "Iteration 30, loss = 0.24794677\n",
      "Iteration 17, loss = 0.25251727\n",
      "Iteration 31, loss = 0.24822761\n",
      "Iteration 18, loss = 0.25015888\n",
      "Iteration 32, loss = 0.24834111\n",
      "Iteration 19, loss = 0.25058236\n",
      "Iteration 33, loss = 0.24807474\n",
      "Iteration 20, loss = 0.25083769\n",
      "Iteration 34, loss = 0.24835222\n",
      "Iteration 21, loss = 0.25144558\n",
      "Iteration 35, loss = 0.24963111\n",
      "Iteration 22, loss = 0.24801305\n",
      "Iteration 36, loss = 0.24813799\n",
      "Iteration 23, loss = 0.24680143\n",
      "Iteration 37, loss = 0.24791597\n",
      "Iteration 24, loss = 0.24603986\n",
      "Iteration 25, loss = 0.24395478\n",
      "Iteration 38, loss = 0.25020768\n",
      "Iteration 26, loss = 0.24573859\n",
      "Iteration 39, loss = 0.24539452\n",
      "Iteration 27, loss = 0.24397062\n",
      "Iteration 40, loss = 0.24403696\n",
      "Iteration 41, loss = 0.24395303\n",
      "Iteration 28, loss = 0.24311875\n",
      "Iteration 42, loss = 0.24700723Iteration 29, loss = 0.24267730\n",
      "\n",
      "Iteration 30, loss = 0.24071722\n",
      "Iteration 43, loss = 0.25014961\n",
      "Iteration 31, loss = 0.24040696\n",
      "Iteration 44, loss = 0.24754376\n",
      "Iteration 32, loss = 0.24154591\n",
      "Iteration 45, loss = 0.24414107\n",
      "Iteration 33, loss = 0.24182303\n",
      "Iteration 46, loss = 0.24442603\n",
      "Iteration 47, loss = 0.24584186\n",
      "Iteration 34, loss = 0.23914814\n",
      "Iteration 35, loss = 0.24062318\n",
      "Iteration 48, loss = 0.24357564\n",
      "Iteration 36, loss = 0.23803793\n",
      "Iteration 49, loss = 0.24237344\n",
      "Iteration 37, loss = 0.24146932\n",
      "Iteration 50, loss = 0.24283547\n",
      "Iteration 38, loss = 0.23876094\n",
      "Iteration 51, loss = 0.24206482\n",
      "Iteration 39, loss = 0.23571820\n",
      "Iteration 52, loss = 0.24129972\n",
      "Iteration 40, loss = 0.23903476\n",
      "Iteration 53, loss = 0.24177172\n",
      "Iteration 41, loss = 0.23750521\n",
      "Iteration 54, loss = 0.24242999\n",
      "Iteration 42, loss = 0.24102491\n",
      "Iteration 55, loss = 0.24208593\n",
      "Iteration 43, loss = 0.24017049\n",
      "Iteration 56, loss = 0.24254747\n",
      "Iteration 44, loss = 0.23643654\n",
      "Iteration 57, loss = 0.24117570\n",
      "Iteration 45, loss = 0.23608433\n",
      "Iteration 58, loss = 0.24407912\n",
      "Iteration 46, loss = 0.23541778\n",
      "Iteration 59, loss = 0.24640806\n",
      "Iteration 47, loss = 0.23612595\n",
      "Iteration 60, loss = 0.24423239\n",
      "Iteration 48, loss = 0.23754563\n",
      "Iteration 61, loss = 0.24280697\n",
      "Iteration 49, loss = 0.23475610\n",
      "Iteration 62, loss = 0.24031380\n",
      "Iteration 50, loss = 0.23496521\n",
      "Iteration 51, loss = 0.23493363\n",
      "Iteration 52, loss = 0.23500618\n",
      "Iteration 63, loss = 0.24006098\n",
      "Iteration 53, loss = 0.23689334\n",
      "Iteration 64, loss = 0.24016800\n",
      "Iteration 54, loss = 0.23526408\n",
      "Iteration 65, loss = 0.24104881\n",
      "Iteration 66, loss = 0.24061931\n",
      "Iteration 55, loss = 0.23652635\n",
      "Iteration 67, loss = 0.24050438\n",
      "Iteration 56, loss = 0.23467265\n",
      "Iteration 68, loss = 0.24116128\n",
      "Iteration 57, loss = 0.23269007\n",
      "Iteration 69, loss = 0.24201616\n",
      "Iteration 58, loss = 0.23306137\n",
      "Iteration 70, loss = 0.23984467\n",
      "Iteration 59, loss = 0.23857506\n",
      "Iteration 71, loss = 0.24111653\n",
      "Iteration 60, loss = 0.23652594\n",
      "Iteration 72, loss = 0.23864956\n",
      "Iteration 61, loss = 0.23593913\n",
      "Iteration 73, loss = 0.24088630\n",
      "Iteration 62, loss = 0.23517163\n",
      "Iteration 63, loss = 0.23309335\n",
      "Iteration 74, loss = 0.23834521\n",
      "Iteration 64, loss = 0.23216037\n",
      "Iteration 75, loss = 0.23953828\n",
      "Iteration 65, loss = 0.23308402\n",
      "Iteration 76, loss = 0.23948612\n",
      "Iteration 66, loss = 0.23324436\n",
      "Iteration 77, loss = 0.24045489\n",
      "Iteration 67, loss = 0.23277727\n",
      "Iteration 78, loss = 0.23886144\n",
      "Iteration 68, loss = 0.23237152\n",
      "Iteration 79, loss = 0.23912355\n",
      "Iteration 69, loss = 0.23369753\n",
      "Iteration 80, loss = 0.24004533\n",
      "Iteration 70, loss = 0.23217628\n",
      "Iteration 81, loss = 0.23975766\n",
      "Iteration 71, loss = 0.23392310\n",
      "Iteration 82, loss = 0.23878614\n",
      "Iteration 72, loss = 0.23405060\n",
      "Iteration 73, loss = 0.23511513\n",
      "Iteration 83, loss = 0.24160930\n",
      "Iteration 74, loss = 0.23335092\n",
      "Iteration 84, loss = 0.24125474\n",
      "Iteration 85, loss = 0.23895378\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 75, loss = 0.23159841\n",
      "Iteration 76, loss = 0.23496921\n",
      "Iteration 77, loss = 0.23373497\n",
      "Iteration 78, loss = 0.23303164\n",
      "Iteration 1, loss = 0.32971409\n",
      "Iteration 2, loss = 0.28438932\n",
      "Iteration 79, loss = 0.23097492\n",
      "Iteration 3, loss = 0.27632224\n",
      "Iteration 4, loss = 0.27354643\n",
      "Iteration 80, loss = 0.23283950\n",
      "Iteration 81, loss = 0.23162566\n",
      "Iteration 82, loss = 0.23036468\n",
      "Iteration 5, loss = 0.26893243\n",
      "Iteration 6, loss = 0.26586706\n",
      "Iteration 83, loss = 0.23425778\n",
      "Iteration 7, loss = 0.26307800\n",
      "Iteration 8, loss = 0.26342834\n",
      "Iteration 84, loss = 0.23424615\n",
      "Iteration 9, loss = 0.25961237\n",
      "Iteration 85, loss = 0.23205327\n",
      "Iteration 10, loss = 0.25859538\n",
      "Iteration 86, loss = 0.23018243\n",
      "Iteration 11, loss = 0.25848132\n",
      "Iteration 87, loss = 0.23151764\n",
      "Iteration 12, loss = 0.25884679\n",
      "Iteration 88, loss = 0.23153804\n",
      "Iteration 13, loss = 0.25752994\n",
      "Iteration 89, loss = 0.23222200\n",
      "Iteration 14, loss = 0.25481012\n",
      "Iteration 15, loss = 0.25626892\n",
      "Iteration 90, loss = 0.23147889\n",
      "Iteration 91, loss = 0.23207160\n",
      "Iteration 16, loss = 0.25446486\n",
      "Iteration 92, loss = 0.23183877\n",
      "Iteration 17, loss = 0.25872709\n",
      "Iteration 93, loss = 0.23267551\n",
      "Iteration 18, loss = 0.25389079\n",
      "Iteration 94, loss = 0.23532012\n",
      "Iteration 19, loss = 0.25276587\n",
      "Iteration 95, loss = 0.23290606\n",
      "Iteration 20, loss = 0.25440307\n",
      "Iteration 96, loss = 0.23196516\n",
      "Iteration 21, loss = 0.25335365\n",
      "Iteration 97, loss = 0.23310748\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 22, loss = 0.24955921\n",
      "Iteration 23, loss = 0.24721346\n",
      "Iteration 24, loss = 0.24813830\n",
      "Iteration 1, loss = 0.32832944\n",
      "Iteration 25, loss = 0.24538243\n",
      "Iteration 2, loss = 0.28403981\n",
      "Iteration 26, loss = 0.24483937\n",
      "Iteration 3, loss = 0.27814341\n",
      "Iteration 27, loss = 0.24548795\n",
      "Iteration 4, loss = 0.27499541\n",
      "Iteration 28, loss = 0.24533785\n",
      "Iteration 5, loss = 0.27414330\n",
      "Iteration 29, loss = 0.24556144\n",
      "Iteration 6, loss = 0.26861373\n",
      "Iteration 30, loss = 0.24455545\n",
      "Iteration 7, loss = 0.26907183\n",
      "Iteration 31, loss = 0.24341221\n",
      "Iteration 8, loss = 0.26673782\n",
      "Iteration 32, loss = 0.24317412\n",
      "Iteration 9, loss = 0.26286600\n",
      "Iteration 33, loss = 0.24317155\n",
      "Iteration 10, loss = 0.26257427\n",
      "Iteration 34, loss = 0.24404434\n",
      "Iteration 11, loss = 0.26078526\n",
      "Iteration 35, loss = 0.24331619\n",
      "Iteration 12, loss = 0.26180647\n",
      "Iteration 36, loss = 0.24254016\n",
      "Iteration 13, loss = 0.25850235\n",
      "Iteration 37, loss = 0.24252363\n",
      "Iteration 14, loss = 0.25767184\n",
      "Iteration 38, loss = 0.24172005\n",
      "Iteration 15, loss = 0.25913194\n",
      "Iteration 39, loss = 0.23991167\n",
      "Iteration 16, loss = 0.25625475\n",
      "Iteration 40, loss = 0.24298821\n",
      "Iteration 17, loss = 0.25977199\n",
      "Iteration 41, loss = 0.24418313\n",
      "Iteration 18, loss = 0.25584730\n",
      "Iteration 42, loss = 0.24194764\n",
      "Iteration 19, loss = 0.25416616\n",
      "Iteration 43, loss = 0.24111643\n",
      "Iteration 20, loss = 0.25380953\n",
      "Iteration 44, loss = 0.24032384\n",
      "Iteration 21, loss = 0.25340266\n",
      "Iteration 45, loss = 0.24064733\n",
      "Iteration 22, loss = 0.25339925\n",
      "Iteration 46, loss = 0.24226875\n",
      "Iteration 23, loss = 0.25151702\n",
      "Iteration 47, loss = 0.24007445\n",
      "Iteration 24, loss = 0.25249649\n",
      "Iteration 48, loss = 0.23999463\n",
      "Iteration 25, loss = 0.25110723\n",
      "Iteration 49, loss = 0.24051050\n",
      "Iteration 26, loss = 0.25046190\n",
      "Iteration 50, loss = 0.23960689\n",
      "Iteration 27, loss = 0.24965193\n",
      "Iteration 51, loss = 0.24066802\n",
      "Iteration 28, loss = 0.24977057\n",
      "Iteration 52, loss = 0.24059361\n",
      "Iteration 29, loss = 0.24962420\n",
      "Iteration 53, loss = 0.24101966\n",
      "Iteration 30, loss = 0.24966272\n",
      "Iteration 54, loss = 0.24096001\n",
      "Iteration 31, loss = 0.24830849\n",
      "Iteration 55, loss = 0.24197261\n",
      "Iteration 32, loss = 0.24837980\n",
      "Iteration 56, loss = 0.24178888\n",
      "Iteration 33, loss = 0.24846493\n",
      "Iteration 57, loss = 0.24026966\n",
      "Iteration 34, loss = 0.24794991\n",
      "Iteration 58, loss = 0.23964277\n",
      "Iteration 35, loss = 0.24834199\n",
      "Iteration 59, loss = 0.24309856\n",
      "Iteration 36, loss = 0.24825969\n",
      "Iteration 60, loss = 0.24548203\n",
      "Iteration 37, loss = 0.24870643\n",
      "Iteration 61, loss = 0.24002394\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 38, loss = 0.24735765\n",
      "Iteration 39, loss = 0.24670495\n",
      "Iteration 40, loss = 0.24913947\n",
      "Iteration 1, loss = 0.33508149\n",
      "Iteration 41, loss = 0.24843433\n",
      "Iteration 2, loss = 0.28693621\n",
      "Iteration 42, loss = 0.24925351\n",
      "Iteration 3, loss = 0.27822798\n",
      "Iteration 43, loss = 0.24758976\n",
      "Iteration 4, loss = 0.27727580\n",
      "Iteration 44, loss = 0.24642486\n",
      "Iteration 5, loss = 0.27252586\n",
      "Iteration 45, loss = 0.24665190\n",
      "Iteration 6, loss = 0.26833543\n",
      "Iteration 46, loss = 0.24609459\n",
      "Iteration 7, loss = 0.26924732\n",
      "Iteration 47, loss = 0.24563871\n",
      "Iteration 8, loss = 0.26727915\n",
      "Iteration 48, loss = 0.24646975\n",
      "Iteration 9, loss = 0.26417891\n",
      "Iteration 49, loss = 0.24563946\n",
      "Iteration 10, loss = 0.26658253\n",
      "Iteration 50, loss = 0.24610567\n",
      "Iteration 11, loss = 0.26312847\n",
      "Iteration 51, loss = 0.24627611\n",
      "Iteration 12, loss = 0.26454554\n",
      "Iteration 52, loss = 0.24584409\n",
      "Iteration 13, loss = 0.26250239\n",
      "Iteration 53, loss = 0.24464181\n",
      "Iteration 14, loss = 0.26243180\n",
      "Iteration 54, loss = 0.24526578\n",
      "Iteration 15, loss = 0.26311850\n",
      "Iteration 55, loss = 0.24610150\n",
      "Iteration 56, loss = 0.24696710\n",
      "Iteration 16, loss = 0.26082592\n",
      "Iteration 57, loss = 0.24550274\n",
      "Iteration 17, loss = 0.26134663\n",
      "Iteration 58, loss = 0.24473030\n",
      "Iteration 18, loss = 0.25753437\n",
      "Iteration 59, loss = 0.24717315\n",
      "Iteration 19, loss = 0.25766098Iteration 60, loss = 0.24947471\n",
      "\n",
      "Iteration 20, loss = 0.25651856\n",
      "Iteration 61, loss = 0.24510707\n",
      "Iteration 21, loss = 0.25731090\n",
      "Iteration 62, loss = 0.24392294\n",
      "Iteration 63, loss = 0.24366454\n",
      "Iteration 64, loss = 0.24492757\n",
      "Iteration 22, loss = 0.25613571\n",
      "Iteration 23, loss = 0.25573469Iteration 65, loss = 0.24536360\n",
      "\n",
      "Iteration 66, loss = 0.24501213\n",
      "Iteration 24, loss = 0.25452895\n",
      "Iteration 67, loss = 0.24404570\n",
      "Iteration 25, loss = 0.25583713\n",
      "Iteration 68, loss = 0.24402476\n",
      "Iteration 26, loss = 0.25540121\n",
      "Iteration 69, loss = 0.24326679\n",
      "Iteration 27, loss = 0.25405743\n",
      "Iteration 70, loss = 0.24266610\n",
      "Iteration 28, loss = 0.25345269\n",
      "Iteration 71, loss = 0.24367658\n",
      "Iteration 29, loss = 0.25351811\n",
      "Iteration 72, loss = 0.24426706\n",
      "Iteration 30, loss = 0.25337368\n",
      "Iteration 73, loss = 0.24351859\n",
      "Iteration 31, loss = 0.25226070\n",
      "Iteration 74, loss = 0.24419458\n",
      "Iteration 75, loss = 0.24334472\n",
      "Iteration 32, loss = 0.25211762\n",
      "Iteration 76, loss = 0.24409834\n",
      "Iteration 77, loss = 0.24491950\n",
      "Iteration 33, loss = 0.25163281\n",
      "Iteration 34, loss = 0.25140502\n",
      "Iteration 78, loss = 0.24294860\n",
      "Iteration 35, loss = 0.25039575\n",
      "Iteration 79, loss = 0.24238497\n",
      "Iteration 36, loss = 0.25058269Iteration 80, loss = 0.24440408\n",
      "\n",
      "Iteration 81, loss = 0.24498322\n",
      "Iteration 37, loss = 0.25085327\n",
      "Iteration 82, loss = 0.24192604\n",
      "Iteration 38, loss = 0.25037222\n",
      "Iteration 83, loss = 0.24531568\n",
      "Iteration 39, loss = 0.25071378\n",
      "Iteration 84, loss = 0.24735090\n",
      "Iteration 40, loss = 0.25063218\n",
      "Iteration 85, loss = 0.24276643\n",
      "Iteration 41, loss = 0.25130519\n",
      "Iteration 86, loss = 0.24198041\n",
      "Iteration 42, loss = 0.24898173\n",
      "Iteration 87, loss = 0.24203325\n",
      "Iteration 43, loss = 0.25022470\n",
      "Iteration 88, loss = 0.24418973\n",
      "Iteration 44, loss = 0.24696675\n",
      "Iteration 89, loss = 0.24287930\n",
      "Iteration 45, loss = 0.24613098\n",
      "Iteration 90, loss = 0.24223964\n",
      "Iteration 91, loss = 0.24167267\n",
      "Iteration 92, loss = 0.24136220\n",
      "Iteration 46, loss = 0.24749231\n",
      "Iteration 47, loss = 0.24833955\n",
      "Iteration 93, loss = 0.24330768\n",
      "Iteration 48, loss = 0.24649488\n",
      "Iteration 49, loss = 0.24747982\n",
      "Iteration 94, loss = 0.24406565\n",
      "Iteration 50, loss = 0.24594500\n",
      "Iteration 95, loss = 0.24298570\n",
      "Iteration 51, loss = 0.24683320\n",
      "Iteration 96, loss = 0.24240823\n",
      "Iteration 52, loss = 0.24712498\n",
      "Iteration 97, loss = 0.24303877\n",
      "Iteration 53, loss = 0.25003701\n",
      "Iteration 98, loss = 0.24398284\n",
      "Iteration 54, loss = 0.24682591\n",
      "Iteration 99, loss = 0.24330245\n",
      "Iteration 55, loss = 0.24720306\n",
      "Iteration 100, loss = 0.24120380\n",
      "Iteration 56, loss = 0.24841759\n",
      "Iteration 101, loss = 0.24238212\n",
      "Iteration 57, loss = 0.24681933\n",
      "Iteration 102, loss = 0.24290144\n",
      "Iteration 58, loss = 0.24489200\n",
      "Iteration 103, loss = 0.24212576\n",
      "Iteration 59, loss = 0.24836735\n",
      "Iteration 104, loss = 0.24157862\n",
      "Iteration 60, loss = 0.24747893\n",
      "Iteration 105, loss = 0.24201171\n",
      "Iteration 61, loss = 0.24648686Iteration 106, loss = 0.24215163\n",
      "\n",
      "Iteration 62, loss = 0.24541909Iteration 107, loss = 0.24250905\n",
      "\n",
      "Iteration 108, loss = 0.24139230\n",
      "Iteration 109, loss = 0.24099897\n",
      "Iteration 110, loss = 0.24266806\n",
      "Iteration 111, loss = 0.24098721\n",
      "Iteration 63, loss = 0.24323283\n",
      "Iteration 112, loss = 0.24366219\n",
      "Iteration 113, loss = 0.24307136\n",
      "Iteration 64, loss = 0.24408297\n",
      "Iteration 65, loss = 0.24643707\n",
      "Iteration 66, loss = 0.24492253\n",
      "Iteration 114, loss = 0.24094531\n",
      "Iteration 67, loss = 0.24419954\n",
      "Iteration 115, loss = 0.24209156\n",
      "Iteration 68, loss = 0.24511344\n",
      "Iteration 69, loss = 0.24564418\n",
      "Iteration 116, loss = 0.24429718\n",
      "Iteration 70, loss = 0.24326093\n",
      "Iteration 117, loss = 0.24240536\n",
      "Iteration 71, loss = 0.24535377\n",
      "Iteration 118, loss = 0.24343140\n",
      "Iteration 72, loss = 0.24499642\n",
      "Iteration 73, loss = 0.24371172\n",
      "Iteration 119, loss = 0.24249042\n",
      "Iteration 74, loss = 0.24281971\n",
      "Iteration 75, loss = 0.24326863\n",
      "Iteration 120, loss = 0.24152066\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 76, loss = 0.24741445\n",
      "Iteration 1, loss = 0.33663611\n",
      "Iteration 2, loss = 0.28812857\n",
      "Iteration 3, loss = 0.27836035\n",
      "Iteration 77, loss = 0.24484892\n",
      "Iteration 4, loss = 0.27413190\n",
      "Iteration 78, loss = 0.24227802Iteration 5, loss = 0.27200502\n",
      "\n",
      "Iteration 79, loss = 0.24222534Iteration 6, loss = 0.26960438\n",
      "\n",
      "Iteration 7, loss = 0.26951447\n",
      "Iteration 8, loss = 0.26648618\n",
      "Iteration 80, loss = 0.24386257\n",
      "Iteration 9, loss = 0.26425748\n",
      "Iteration 81, loss = 0.24192947\n",
      "Iteration 82, loss = 0.24278343\n",
      "Iteration 83, loss = 0.24334131\n",
      "Iteration 10, loss = 0.26613154Iteration 84, loss = 0.24351271\n",
      "\n",
      "Iteration 11, loss = 0.26350449\n",
      "Iteration 85, loss = 0.24214995\n",
      "Iteration 12, loss = 0.26080167Iteration 86, loss = 0.24318953\n",
      "\n",
      "Iteration 13, loss = 0.25935601Iteration 87, loss = 0.24168136\n",
      "\n",
      "Iteration 88, loss = 0.24377547\n",
      "Iteration 14, loss = 0.25964900\n",
      "Iteration 89, loss = 0.24340257\n",
      "Iteration 15, loss = 0.25849944\n",
      "Iteration 90, loss = 0.24211671\n",
      "Iteration 16, loss = 0.25646199\n",
      "Iteration 91, loss = 0.24420168\n",
      "Iteration 17, loss = 0.25951721\n",
      "Iteration 92, loss = 0.24314183\n",
      "Iteration 18, loss = 0.25797405\n",
      "Iteration 93, loss = 0.24396640\n",
      "Iteration 19, loss = 0.25648500\n",
      "Iteration 94, loss = 0.24563036\n",
      "Iteration 20, loss = 0.25580225\n",
      "Iteration 95, loss = 0.24342132\n",
      "Iteration 21, loss = 0.25572314\n",
      "Iteration 96, loss = 0.24414149\n",
      "Iteration 22, loss = 0.25560770\n",
      "Iteration 97, loss = 0.24265411\n",
      "Iteration 23, loss = 0.25423808\n",
      "Iteration 98, loss = 0.24134168\n",
      "Iteration 99, loss = 0.24311208\n",
      "Iteration 24, loss = 0.25399313\n",
      "Iteration 100, loss = 0.24157882\n",
      "Iteration 101, loss = 0.24174208\n",
      "Iteration 25, loss = 0.25439141\n",
      "Iteration 102, loss = 0.24252945\n",
      "Iteration 26, loss = 0.25291929\n",
      "Iteration 103, loss = 0.24226593\n",
      "Iteration 27, loss = 0.25177433\n",
      "Iteration 104, loss = 0.24170811\n",
      "Iteration 28, loss = 0.24998988\n",
      "Iteration 105, loss = 0.24133736\n",
      "Iteration 29, loss = 0.24992504\n",
      "Iteration 106, loss = 0.24167024\n",
      "Iteration 30, loss = 0.25234031\n",
      "Iteration 107, loss = 0.24271908\n",
      "Iteration 31, loss = 0.25098242\n",
      "Iteration 108, loss = 0.24106584\n",
      "Iteration 109, loss = 0.24189863\n",
      "Iteration 32, loss = 0.24840397\n",
      "Iteration 110, loss = 0.24249601\n",
      "Iteration 33, loss = 0.24841030\n",
      "Iteration 111, loss = 0.24061926\n",
      "Iteration 34, loss = 0.25043251\n",
      "Iteration 112, loss = 0.24322064\n",
      "Iteration 35, loss = 0.24724416\n",
      "Iteration 113, loss = 0.24240551\n",
      "Iteration 114, loss = 0.24011206\n",
      "Iteration 36, loss = 0.24638323\n",
      "Iteration 115, loss = 0.24088091\n",
      "Iteration 37, loss = 0.24607299\n",
      "Iteration 116, loss = 0.24129302\n",
      "Iteration 38, loss = 0.24491883\n",
      "Iteration 117, loss = 0.24208718\n",
      "Iteration 39, loss = 0.24708367\n",
      "Iteration 118, loss = 0.24317829\n",
      "Iteration 40, loss = 0.24920961\n",
      "Iteration 119, loss = 0.24151075\n",
      "Iteration 41, loss = 0.25010145\n",
      "Iteration 120, loss = 0.24276190\n",
      "Iteration 42, loss = 0.25029379\n",
      "Iteration 121, loss = 0.24070491\n",
      "Iteration 43, loss = 0.24994617\n",
      "Iteration 122, loss = 0.24265211\n",
      "Iteration 44, loss = 0.24749529\n",
      "Iteration 123, loss = 0.24390406\n",
      "Iteration 124, loss = 0.24168956\n",
      "Iteration 45, loss = 0.24476781\n",
      "Iteration 125, loss = 0.24219716\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 46, loss = 0.24464599\n",
      "Iteration 47, loss = 0.24587661\n",
      "Iteration 48, loss = 0.24491587\n",
      "Iteration 49, loss = 0.24376329\n",
      "Iteration 50, loss = 0.24574894\n",
      "Iteration 51, loss = 0.24416558\n",
      "Iteration 52, loss = 0.24225176\n",
      "Iteration 53, loss = 0.24371447\n",
      "Iteration 54, loss = 0.24297439\n",
      "Iteration 55, loss = 0.24206632\n",
      "Iteration 56, loss = 0.24468680\n",
      "Iteration 57, loss = 0.24430470\n",
      "Iteration 58, loss = 0.24283733\n",
      "Iteration 59, loss = 0.24238862\n",
      "Iteration 60, loss = 0.24241563\n",
      "Iteration 61, loss = 0.24220708\n",
      "Iteration 62, loss = 0.24276677\n",
      "Iteration 63, loss = 0.24175006\n",
      "Iteration 64, loss = 0.24323600\n",
      "Iteration 65, loss = 0.24406960\n",
      "Iteration 66, loss = 0.24292834\n",
      "Iteration 67, loss = 0.24251140\n",
      "Iteration 68, loss = 0.24263094\n",
      "Iteration 69, loss = 0.24242848\n",
      "Iteration 70, loss = 0.24172960\n",
      "Iteration 71, loss = 0.24172374\n",
      "Iteration 72, loss = 0.24111553\n",
      "Iteration 73, loss = 0.24426247\n",
      "Iteration 74, loss = 0.24403299\n",
      "Iteration 75, loss = 0.24429023\n",
      "Iteration 76, loss = 0.24374309\n",
      "Iteration 77, loss = 0.24086236\n",
      "Iteration 78, loss = 0.24115994\n",
      "Iteration 79, loss = 0.24044295\n",
      "Iteration 80, loss = 0.24281806\n",
      "Iteration 81, loss = 0.24104011\n",
      "Iteration 82, loss = 0.24071735\n",
      "Iteration 83, loss = 0.23990958\n",
      "Iteration 84, loss = 0.24139720\n",
      "Iteration 85, loss = 0.24237444\n",
      "Iteration 86, loss = 0.23977223\n",
      "Iteration 87, loss = 0.24026197\n",
      "Iteration 88, loss = 0.23970684\n",
      "Iteration 89, loss = 0.24182023\n",
      "Iteration 90, loss = 0.24375326\n",
      "Iteration 91, loss = 0.24102310\n",
      "Iteration 92, loss = 0.24101322\n",
      "Iteration 93, loss = 0.24192385\n",
      "Iteration 94, loss = 0.24257799\n",
      "Iteration 95, loss = 0.24153007\n",
      "Iteration 96, loss = 0.23876978\n",
      "Iteration 97, loss = 0.24098478\n",
      "Iteration 98, loss = 0.24408022\n",
      "Iteration 99, loss = 0.24145880\n",
      "Iteration 100, loss = 0.24038578\n",
      "Iteration 101, loss = 0.24062207\n",
      "Iteration 102, loss = 0.24039554\n",
      "Iteration 103, loss = 0.23883190\n",
      "Iteration 104, loss = 0.23918645\n",
      "Iteration 105, loss = 0.23889433\n",
      "Iteration 106, loss = 0.23884087\n",
      "Iteration 107, loss = 0.23987042\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.32345540\n",
      "Iteration 1, loss = 0.32513877\n",
      "Iteration 2, loss = 0.28766289\n",
      "Iteration 3, loss = 0.28252036\n",
      "Iteration 2, loss = 0.29055917\n",
      "Iteration 3, loss = 0.28219470\n",
      "Iteration 4, loss = 0.27721598\n",
      "Iteration 4, loss = 0.27794063\n",
      "Iteration 5, loss = 0.27649062\n",
      "Iteration 5, loss = 0.27638611\n",
      "Iteration 6, loss = 0.27479312\n",
      "Iteration 6, loss = 0.27459531\n",
      "Iteration 7, loss = 0.27274570\n",
      "Iteration 7, loss = 0.27401791\n",
      "Iteration 8, loss = 0.27152209\n",
      "Iteration 8, loss = 0.27465619\n",
      "Iteration 9, loss = 0.27010491\n",
      "Iteration 9, loss = 0.27268120\n",
      "Iteration 10, loss = 0.26967102\n",
      "Iteration 10, loss = 0.27132942\n",
      "Iteration 11, loss = 0.26888854\n",
      "Iteration 11, loss = 0.27123532\n",
      "Iteration 12, loss = 0.26904018\n",
      "Iteration 12, loss = 0.26822051\n",
      "Iteration 13, loss = 0.26824603\n",
      "Iteration 13, loss = 0.26806207\n",
      "Iteration 14, loss = 0.26815502\n",
      "Iteration 14, loss = 0.26839241\n",
      "Iteration 15, loss = 0.26738178\n",
      "Iteration 15, loss = 0.26677869\n",
      "Iteration 16, loss = 0.26650267\n",
      "Iteration 16, loss = 0.26616662\n",
      "Iteration 17, loss = 0.26748959\n",
      "Iteration 17, loss = 0.26756872\n",
      "Iteration 18, loss = 0.26491210\n",
      "Iteration 18, loss = 0.26615027\n",
      "Iteration 19, loss = 0.26741249\n",
      "Iteration 19, loss = 0.26517483\n",
      "Iteration 20, loss = 0.26521118\n",
      "Iteration 20, loss = 0.26710078Iteration 21, loss = 0.26524676\n",
      "\n",
      "Iteration 22, loss = 0.26326417\n",
      "Iteration 21, loss = 0.26318849\n",
      "Iteration 23, loss = 0.26305491\n",
      "Iteration 22, loss = 0.26331357\n",
      "Iteration 24, loss = 0.26131204\n",
      "Iteration 23, loss = 0.26451201\n",
      "Iteration 25, loss = 0.26082629\n",
      "Iteration 24, loss = 0.26407462\n",
      "Iteration 26, loss = 0.26097276\n",
      "Iteration 25, loss = 0.26472236\n",
      "Iteration 27, loss = 0.25968607\n",
      "Iteration 26, loss = 0.26301203\n",
      "Iteration 28, loss = 0.25938180\n",
      "Iteration 27, loss = 0.26152773\n",
      "Iteration 29, loss = 0.25965724\n",
      "Iteration 28, loss = 0.26268064\n",
      "Iteration 30, loss = 0.26258928\n",
      "Iteration 31, loss = 0.25858064\n",
      "Iteration 29, loss = 0.26075986\n",
      "Iteration 32, loss = 0.25738413\n",
      "Iteration 30, loss = 0.26066571\n",
      "Iteration 31, loss = 0.26092530Iteration 33, loss = 0.25788512\n",
      "\n",
      "Iteration 34, loss = 0.25799736\n",
      "Iteration 35, loss = 0.25806412\n",
      "Iteration 32, loss = 0.25946725\n",
      "Iteration 33, loss = 0.25943178\n",
      "Iteration 34, loss = 0.26055433\n",
      "Iteration 36, loss = 0.25578260\n",
      "Iteration 35, loss = 0.26048976\n",
      "Iteration 36, loss = 0.26038881\n",
      "Iteration 37, loss = 0.25794856\n",
      "Iteration 38, loss = 0.25578910\n",
      "Iteration 37, loss = 0.25978915\n",
      "Iteration 39, loss = 0.25790529\n",
      "Iteration 38, loss = 0.25908376\n",
      "Iteration 40, loss = 0.25665803\n",
      "Iteration 39, loss = 0.25928628\n",
      "Iteration 41, loss = 0.25562016\n",
      "Iteration 40, loss = 0.25835432\n",
      "Iteration 42, loss = 0.25627896\n",
      "Iteration 41, loss = 0.25868263\n",
      "Iteration 43, loss = 0.25709700\n",
      "Iteration 42, loss = 0.25906719\n",
      "Iteration 44, loss = 0.25532292\n",
      "Iteration 43, loss = 0.25682505\n",
      "Iteration 45, loss = 0.25505900\n",
      "Iteration 44, loss = 0.25765810\n",
      "Iteration 46, loss = 0.25576317\n",
      "Iteration 45, loss = 0.26155209\n",
      "Iteration 47, loss = 0.25419170\n",
      "Iteration 46, loss = 0.25765168\n",
      "Iteration 48, loss = 0.25515346\n",
      "Iteration 47, loss = 0.25736379\n",
      "Iteration 48, loss = 0.25845768\n",
      "Iteration 49, loss = 0.25515287\n",
      "Iteration 49, loss = 0.25700690\n",
      "Iteration 50, loss = 0.25619017\n",
      "Iteration 50, loss = 0.25691006\n",
      "Iteration 51, loss = 0.25654777\n",
      "Iteration 51, loss = 0.25659883\n",
      "Iteration 52, loss = 0.25478865\n",
      "Iteration 52, loss = 0.25474652\n",
      "Iteration 53, loss = 0.25711676\n",
      "Iteration 53, loss = 0.25568363\n",
      "Iteration 54, loss = 0.25613443\n",
      "Iteration 54, loss = 0.25662016\n",
      "Iteration 55, loss = 0.25503545\n",
      "Iteration 55, loss = 0.25639493\n",
      "Iteration 56, loss = 0.25525290\n",
      "Iteration 56, loss = 0.25552043\n",
      "Iteration 57, loss = 0.25478355\n",
      "Iteration 57, loss = 0.25489542\n",
      "Iteration 58, loss = 0.25443510\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 58, loss = 0.25557936\n",
      "Iteration 59, loss = 0.25566324\n",
      "Iteration 60, loss = 0.25596486\n",
      "Iteration 1, loss = 0.32790022\n",
      "Iteration 61, loss = 0.25458721\n",
      "Iteration 2, loss = 0.28682351\n",
      "Iteration 62, loss = 0.25560123\n",
      "Iteration 3, loss = 0.27961429\n",
      "Iteration 63, loss = 0.25519819\n",
      "Iteration 4, loss = 0.27732705\n",
      "Iteration 64, loss = 0.25421663\n",
      "Iteration 5, loss = 0.27566312\n",
      "Iteration 65, loss = 0.25582762\n",
      "Iteration 6, loss = 0.27376695\n",
      "Iteration 66, loss = 0.25525229\n",
      "Iteration 7, loss = 0.27257495\n",
      "Iteration 67, loss = 0.25518250\n",
      "Iteration 8, loss = 0.27135655\n",
      "Iteration 68, loss = 0.25479626\n",
      "Iteration 69, loss = 0.25546003\n",
      "Iteration 9, loss = 0.26908739\n",
      "Iteration 10, loss = 0.27016373\n",
      "Iteration 70, loss = 0.25430852\n",
      "Iteration 11, loss = 0.27100925\n",
      "Iteration 71, loss = 0.25624845\n",
      "Iteration 72, loss = 0.25678756\n",
      "Iteration 12, loss = 0.26859129\n",
      "Iteration 13, loss = 0.26805286\n",
      "Iteration 73, loss = 0.25556309\n",
      "Iteration 14, loss = 0.26628395\n",
      "Iteration 15, loss = 0.26636733\n",
      "Iteration 74, loss = 0.25446088\n",
      "Iteration 75, loss = 0.25530707\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 16, loss = 0.26633088\n",
      "Iteration 17, loss = 0.26584233\n",
      "Iteration 18, loss = 0.26463164\n",
      "Iteration 1, loss = 0.32848810\n",
      "Iteration 19, loss = 0.26335396\n",
      "Iteration 2, loss = 0.28809617\n",
      "Iteration 20, loss = 0.26287967\n",
      "Iteration 3, loss = 0.28163517\n",
      "Iteration 21, loss = 0.26316855\n",
      "Iteration 4, loss = 0.27866886\n",
      "Iteration 22, loss = 0.26156932\n",
      "Iteration 5, loss = 0.27589730\n",
      "Iteration 23, loss = 0.26262042\n",
      "Iteration 6, loss = 0.27288034\n",
      "Iteration 24, loss = 0.26254006\n",
      "Iteration 7, loss = 0.27192263\n",
      "Iteration 25, loss = 0.26100246\n",
      "Iteration 8, loss = 0.27088882\n",
      "Iteration 26, loss = 0.26079023\n",
      "Iteration 9, loss = 0.26802372\n",
      "Iteration 10, loss = 0.26915831\n",
      "Iteration 11, loss = 0.26767209\n",
      "Iteration 12, loss = 0.26653778\n",
      "Iteration 27, loss = 0.25902966\n",
      "Iteration 13, loss = 0.26544989\n",
      "Iteration 14, loss = 0.26446580\n",
      "Iteration 15, loss = 0.26579891\n",
      "Iteration 28, loss = 0.26024760\n",
      "Iteration 16, loss = 0.26586244\n",
      "Iteration 29, loss = 0.25880597\n",
      "Iteration 17, loss = 0.26367838\n",
      "Iteration 30, loss = 0.25921153\n",
      "Iteration 18, loss = 0.26276988\n",
      "Iteration 31, loss = 0.25911277\n",
      "Iteration 19, loss = 0.26247562\n",
      "Iteration 32, loss = 0.25711070\n",
      "Iteration 20, loss = 0.26376628\n",
      "Iteration 33, loss = 0.25784384\n",
      "Iteration 21, loss = 0.26263446\n",
      "Iteration 34, loss = 0.25718106\n",
      "Iteration 35, loss = 0.25722090\n",
      "Iteration 36, loss = 0.25637034\n",
      "Iteration 22, loss = 0.26197552\n",
      "Iteration 37, loss = 0.25573336\n",
      "Iteration 38, loss = 0.25580816\n",
      "Iteration 39, loss = 0.25643945\n",
      "Iteration 23, loss = 0.26156595\n",
      "Iteration 40, loss = 0.25542658\n",
      "Iteration 24, loss = 0.26126941\n",
      "Iteration 41, loss = 0.25636711\n",
      "Iteration 25, loss = 0.26119450\n",
      "Iteration 42, loss = 0.25554314\n",
      "Iteration 26, loss = 0.26073109\n",
      "Iteration 43, loss = 0.25495691\n",
      "Iteration 27, loss = 0.26076172\n",
      "Iteration 44, loss = 0.25555518\n",
      "Iteration 28, loss = 0.26011397\n",
      "Iteration 45, loss = 0.25884844\n",
      "Iteration 29, loss = 0.25966462\n",
      "Iteration 46, loss = 0.25545807\n",
      "Iteration 30, loss = 0.25951012\n",
      "Iteration 47, loss = 0.25567761\n",
      "Iteration 31, loss = 0.25917153\n",
      "Iteration 48, loss = 0.25688832\n",
      "Iteration 32, loss = 0.25857921\n",
      "Iteration 49, loss = 0.25430058\n",
      "Iteration 33, loss = 0.25921026\n",
      "Iteration 50, loss = 0.25523452\n",
      "Iteration 34, loss = 0.25888494\n",
      "Iteration 51, loss = 0.25577621\n",
      "Iteration 35, loss = 0.26229981\n",
      "Iteration 52, loss = 0.25377173\n",
      "Iteration 36, loss = 0.25862465\n",
      "Iteration 53, loss = 0.25645767\n",
      "Iteration 37, loss = 0.25816194\n",
      "Iteration 54, loss = 0.25462944\n",
      "Iteration 38, loss = 0.25812209\n",
      "Iteration 55, loss = 0.25606820\n",
      "Iteration 39, loss = 0.25817918\n",
      "Iteration 56, loss = 0.25503928\n",
      "Iteration 40, loss = 0.25746738\n",
      "Iteration 57, loss = 0.25358101\n",
      "Iteration 41, loss = 0.25967810\n",
      "Iteration 58, loss = 0.25529046\n",
      "Iteration 42, loss = 0.25630913\n",
      "Iteration 59, loss = 0.25601296\n",
      "Iteration 43, loss = 0.25600154\n",
      "Iteration 60, loss = 0.25534686\n",
      "Iteration 44, loss = 0.25544484\n",
      "Iteration 61, loss = 0.25384525\n",
      "Iteration 45, loss = 0.25981603\n",
      "Iteration 62, loss = 0.25580407\n",
      "Iteration 46, loss = 0.25680023\n",
      "Iteration 47, loss = 0.25554317\n",
      "Iteration 63, loss = 0.25526517\n",
      "Iteration 48, loss = 0.25660755\n",
      "Iteration 49, loss = 0.25495748\n",
      "Iteration 64, loss = 0.25342327\n",
      "Iteration 50, loss = 0.25513534\n",
      "Iteration 65, loss = 0.25735237\n",
      "Iteration 51, loss = 0.25869319\n",
      "Iteration 66, loss = 0.25453153\n",
      "Iteration 52, loss = 0.25461754\n",
      "Iteration 67, loss = 0.25404320\n",
      "Iteration 53, loss = 0.25558200\n",
      "Iteration 68, loss = 0.25290339\n",
      "Iteration 54, loss = 0.25332695\n",
      "Iteration 69, loss = 0.25282966\n",
      "Iteration 55, loss = 0.25450512\n",
      "Iteration 70, loss = 0.25392834\n",
      "Iteration 56, loss = 0.25328557\n",
      "Iteration 71, loss = 0.25506027\n",
      "Iteration 57, loss = 0.25413859\n",
      "Iteration 72, loss = 0.25543315\n",
      "Iteration 58, loss = 0.25370774\n",
      "Iteration 73, loss = 0.25620053\n",
      "Iteration 74, loss = 0.25320691\n",
      "Iteration 59, loss = 0.25313907\n",
      "Iteration 75, loss = 0.25368293\n",
      "Iteration 76, loss = 0.25366256\n",
      "Iteration 60, loss = 0.25344575\n",
      "Iteration 77, loss = 0.25469957\n",
      "Iteration 61, loss = 0.25285063\n",
      "Iteration 78, loss = 0.25510177\n",
      "Iteration 62, loss = 0.25653587\n",
      "Iteration 79, loss = 0.25342897\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 63, loss = 0.25377574\n",
      "Iteration 64, loss = 0.25182604\n",
      "Iteration 65, loss = 0.25258640\n",
      "Iteration 1, loss = 0.32845094\n",
      "Iteration 66, loss = 0.25181840\n",
      "Iteration 2, loss = 0.28637110\n",
      "Iteration 67, loss = 0.25256076\n",
      "Iteration 3, loss = 0.27912439\n",
      "Iteration 68, loss = 0.25217492\n",
      "Iteration 4, loss = 0.27571084\n",
      "Iteration 69, loss = 0.25394440\n",
      "Iteration 5, loss = 0.27359512\n",
      "Iteration 70, loss = 0.25387327\n",
      "Iteration 6, loss = 0.27154717\n",
      "Iteration 71, loss = 0.25379752\n",
      "Iteration 7, loss = 0.27058570\n",
      "Iteration 8, loss = 0.26842282\n",
      "Iteration 72, loss = 0.25296749Iteration 9, loss = 0.26774999\n",
      "\n",
      "Iteration 73, loss = 0.25316313\n",
      "Iteration 10, loss = 0.26950030\n",
      "Iteration 74, loss = 0.25154372\n",
      "Iteration 11, loss = 0.26731848\n",
      "Iteration 75, loss = 0.25098271\n",
      "Iteration 12, loss = 0.26688241\n",
      "Iteration 76, loss = 0.25171651\n",
      "Iteration 13, loss = 0.26639903\n",
      "Iteration 77, loss = 0.25208499\n",
      "Iteration 14, loss = 0.26568410\n",
      "Iteration 78, loss = 0.25320616\n",
      "Iteration 15, loss = 0.26579836\n",
      "Iteration 79, loss = 0.25234267\n",
      "Iteration 16, loss = 0.26540568\n",
      "Iteration 80, loss = 0.25116804\n",
      "Iteration 17, loss = 0.26402647\n",
      "Iteration 81, loss = 0.25195578\n",
      "Iteration 18, loss = 0.26415646\n",
      "Iteration 82, loss = 0.25003108\n",
      "Iteration 19, loss = 0.26292874\n",
      "Iteration 83, loss = 0.25118095\n",
      "Iteration 20, loss = 0.26343136\n",
      "Iteration 84, loss = 0.25004753\n",
      "Iteration 21, loss = 0.26187780\n",
      "Iteration 85, loss = 0.25020262\n",
      "Iteration 86, loss = 0.25096033\n",
      "Iteration 87, loss = 0.25077193\n",
      "Iteration 22, loss = 0.26294118\n",
      "Iteration 88, loss = 0.24993369\n",
      "Iteration 89, loss = 0.25017647\n",
      "Iteration 23, loss = 0.26020630\n",
      "Iteration 24, loss = 0.26169375\n",
      "Iteration 90, loss = 0.24927528\n",
      "Iteration 25, loss = 0.25971697\n",
      "Iteration 26, loss = 0.25999866\n",
      "Iteration 91, loss = 0.25308517\n",
      "Iteration 27, loss = 0.25967245\n",
      "Iteration 28, loss = 0.25866137\n",
      "Iteration 92, loss = 0.24982427\n",
      "Iteration 29, loss = 0.25749779\n",
      "Iteration 93, loss = 0.25009698\n",
      "Iteration 30, loss = 0.25590328\n",
      "Iteration 94, loss = 0.25142923\n",
      "Iteration 31, loss = 0.25651296\n",
      "Iteration 95, loss = 0.25026592\n",
      "Iteration 32, loss = 0.25710447\n",
      "Iteration 96, loss = 0.25117261\n",
      "Iteration 33, loss = 0.25614310\n",
      "Iteration 97, loss = 0.25033130\n",
      "Iteration 34, loss = 0.25674604\n",
      "Iteration 98, loss = 0.25072333\n",
      "Iteration 35, loss = 0.25731642\n",
      "Iteration 99, loss = 0.25076324\n",
      "Iteration 36, loss = 0.25547153\n",
      "Iteration 100, loss = 0.25141631\n",
      "Iteration 37, loss = 0.25752011\n",
      "Iteration 101, loss = 0.25014862\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 38, loss = 0.25505195\n",
      "Iteration 39, loss = 0.25425415\n",
      "Iteration 40, loss = 0.25448734\n",
      "Iteration 1, loss = 0.32654193\n",
      "Iteration 41, loss = 0.25528663\n",
      "Iteration 2, loss = 0.28589793\n",
      "Iteration 42, loss = 0.25462314\n",
      "Iteration 3, loss = 0.27917593\n",
      "Iteration 43, loss = 0.25379554\n",
      "Iteration 4, loss = 0.27649886\n",
      "Iteration 44, loss = 0.25336673\n",
      "Iteration 5, loss = 0.27364488\n",
      "Iteration 45, loss = 0.25642479\n",
      "Iteration 6, loss = 0.27270619\n",
      "Iteration 46, loss = 0.25475823\n",
      "Iteration 7, loss = 0.27122941\n",
      "Iteration 47, loss = 0.25433790\n",
      "Iteration 8, loss = 0.27073388\n",
      "Iteration 48, loss = 0.25472946\n",
      "Iteration 9, loss = 0.26834341\n",
      "Iteration 49, loss = 0.25292210\n",
      "Iteration 10, loss = 0.26955683\n",
      "Iteration 50, loss = 0.25396462\n",
      "Iteration 11, loss = 0.27005681\n",
      "Iteration 51, loss = 0.25319311\n",
      "Iteration 12, loss = 0.26610134\n",
      "Iteration 52, loss = 0.25345575\n",
      "Iteration 13, loss = 0.26709291\n",
      "Iteration 14, loss = 0.26531470\n",
      "Iteration 53, loss = 0.25312849\n",
      "Iteration 15, loss = 0.26594171\n",
      "Iteration 54, loss = 0.25302715\n",
      "Iteration 16, loss = 0.26534735\n",
      "Iteration 55, loss = 0.25219791\n",
      "Iteration 56, loss = 0.25255843\n",
      "Iteration 17, loss = 0.26390590\n",
      "Iteration 57, loss = 0.25409916\n",
      "Iteration 18, loss = 0.26395751\n",
      "Iteration 58, loss = 0.25256401\n",
      "Iteration 19, loss = 0.26272246\n",
      "Iteration 20, loss = 0.26292647\n",
      "Iteration 59, loss = 0.25255457\n",
      "Iteration 21, loss = 0.26119625\n",
      "Iteration 60, loss = 0.25287040\n",
      "Iteration 22, loss = 0.26204623\n",
      "Iteration 61, loss = 0.25311470\n",
      "Iteration 23, loss = 0.25856081\n",
      "Iteration 62, loss = 0.25308329\n",
      "Iteration 24, loss = 0.25929402\n",
      "Iteration 25, loss = 0.25945807\n",
      "Iteration 63, loss = 0.25297192\n",
      "Iteration 26, loss = 0.25924291\n",
      "Iteration 64, loss = 0.25333846\n",
      "Iteration 27, loss = 0.25769758\n",
      "Iteration 65, loss = 0.25193248\n",
      "Iteration 66, loss = 0.25198439\n",
      "Iteration 28, loss = 0.25671424\n",
      "Iteration 67, loss = 0.25497985\n",
      "Iteration 29, loss = 0.25705068\n",
      "Iteration 68, loss = 0.25439386\n",
      "Iteration 30, loss = 0.25599166\n",
      "Iteration 31, loss = 0.25526918\n",
      "Iteration 69, loss = 0.25361813\n",
      "Iteration 70, loss = 0.25457259\n",
      "Iteration 32, loss = 0.25626943\n",
      "Iteration 71, loss = 0.25261713\n",
      "Iteration 33, loss = 0.25455511\n",
      "Iteration 72, loss = 0.25204230\n",
      "Iteration 34, loss = 0.25486128\n",
      "Iteration 73, loss = 0.25419038\n",
      "Iteration 35, loss = 0.25730348\n",
      "Iteration 74, loss = 0.25203253\n",
      "Iteration 36, loss = 0.25632835\n",
      "Iteration 75, loss = 0.25092576\n",
      "Iteration 37, loss = 0.25315192\n",
      "Iteration 76, loss = 0.25165864\n",
      "Iteration 38, loss = 0.25404897\n",
      "Iteration 77, loss = 0.25252906\n",
      "Iteration 39, loss = 0.25288316\n",
      "Iteration 78, loss = 0.25469220\n",
      "Iteration 40, loss = 0.25314839\n",
      "Iteration 79, loss = 0.25198936\n",
      "Iteration 41, loss = 0.25367900\n",
      "Iteration 80, loss = 0.25215219\n",
      "Iteration 42, loss = 0.25398056\n",
      "Iteration 81, loss = 0.25229574\n",
      "Iteration 43, loss = 0.25252978\n",
      "Iteration 82, loss = 0.25107237\n",
      "Iteration 44, loss = 0.25249400\n",
      "Iteration 83, loss = 0.25274219\n",
      "Iteration 45, loss = 0.25653848\n",
      "Iteration 84, loss = 0.25091066\n",
      "Iteration 46, loss = 0.25324614\n",
      "Iteration 85, loss = 0.25169792\n",
      "Iteration 47, loss = 0.25604957\n",
      "Iteration 86, loss = 0.25196926\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 48, loss = 0.25430605\n",
      "Iteration 49, loss = 0.25277804\n",
      "Iteration 50, loss = 0.25175764\n",
      "Iteration 1, loss = 0.32560338\n",
      "Iteration 51, loss = 0.25040447\n",
      "Iteration 2, loss = 0.28480795\n",
      "Iteration 52, loss = 0.25170266\n",
      "Iteration 3, loss = 0.27813431\n",
      "Iteration 53, loss = 0.25099194\n",
      "Iteration 4, loss = 0.27490083\n",
      "Iteration 54, loss = 0.25095021\n",
      "Iteration 5, loss = 0.27376063\n",
      "Iteration 55, loss = 0.25157761\n",
      "Iteration 6, loss = 0.27329010\n",
      "Iteration 56, loss = 0.25191769\n",
      "Iteration 7, loss = 0.27241693Iteration 57, loss = 0.25100947\n",
      "\n",
      "Iteration 58, loss = 0.25211574\n",
      "Iteration 8, loss = 0.26901768\n",
      "Iteration 59, loss = 0.25035328\n",
      "Iteration 60, loss = 0.25063920\n",
      "Iteration 9, loss = 0.26830654\n",
      "Iteration 10, loss = 0.26621279\n",
      "Iteration 61, loss = 0.25049690\n",
      "Iteration 11, loss = 0.26703388\n",
      "Iteration 62, loss = 0.25258413\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 12, loss = 0.26390878\n",
      "Iteration 13, loss = 0.26447477\n",
      "Iteration 14, loss = 0.26265486\n",
      "Iteration 1, loss = 0.32553165\n",
      "Iteration 2, loss = 0.28575737\n",
      "Iteration 15, loss = 0.26424328\n",
      "Iteration 3, loss = 0.27939170\n",
      "Iteration 16, loss = 0.26292569\n",
      "Iteration 4, loss = 0.27651770\n",
      "Iteration 17, loss = 0.26084768\n",
      "Iteration 18, loss = 0.26069131\n",
      "Iteration 19, loss = 0.25864070\n",
      "Iteration 5, loss = 0.27460403\n",
      "Iteration 20, loss = 0.25990908\n",
      "Iteration 6, loss = 0.27325607\n",
      "Iteration 7, loss = 0.27384550\n",
      "Iteration 21, loss = 0.25765060\n",
      "Iteration 8, loss = 0.27304060\n",
      "Iteration 22, loss = 0.25934705\n",
      "Iteration 9, loss = 0.26966244\n",
      "Iteration 23, loss = 0.25599353\n",
      "Iteration 10, loss = 0.26907199\n",
      "Iteration 24, loss = 0.25587380\n",
      "Iteration 11, loss = 0.27080361\n",
      "Iteration 25, loss = 0.25473002\n",
      "Iteration 12, loss = 0.26695472\n",
      "Iteration 26, loss = 0.25823637\n",
      "Iteration 13, loss = 0.26602335\n",
      "Iteration 27, loss = 0.25563161\n",
      "Iteration 14, loss = 0.26568436\n",
      "Iteration 28, loss = 0.25442899\n",
      "Iteration 15, loss = 0.26711916\n",
      "Iteration 29, loss = 0.25467567\n",
      "Iteration 16, loss = 0.26352887\n",
      "Iteration 30, loss = 0.25466786\n",
      "Iteration 17, loss = 0.26339135\n",
      "Iteration 31, loss = 0.25205842\n",
      "Iteration 18, loss = 0.26384932\n",
      "Iteration 19, loss = 0.26317903\n",
      "Iteration 32, loss = 0.25357217\n",
      "Iteration 20, loss = 0.26555674\n",
      "Iteration 33, loss = 0.25206318\n",
      "Iteration 34, loss = 0.25287074\n",
      "Iteration 21, loss = 0.26091922\n",
      "Iteration 35, loss = 0.25308345\n",
      "Iteration 22, loss = 0.26122507\n",
      "Iteration 36, loss = 0.25280299\n",
      "Iteration 23, loss = 0.25977064\n",
      "Iteration 37, loss = 0.25157314\n",
      "Iteration 24, loss = 0.25927874\n",
      "Iteration 25, loss = 0.26100550\n",
      "Iteration 38, loss = 0.25256775\n",
      "Iteration 26, loss = 0.25965080\n",
      "Iteration 39, loss = 0.25271652\n",
      "Iteration 27, loss = 0.25926579\n",
      "Iteration 40, loss = 0.25101211\n",
      "Iteration 41, loss = 0.25046227\n",
      "Iteration 28, loss = 0.25865596\n",
      "Iteration 42, loss = 0.25102381\n",
      "Iteration 29, loss = 0.25769182\n",
      "Iteration 43, loss = 0.25139302\n",
      "Iteration 30, loss = 0.25830742\n",
      "Iteration 44, loss = 0.25230052\n",
      "Iteration 31, loss = 0.25674281\n",
      "Iteration 45, loss = 0.25155586\n",
      "Iteration 32, loss = 0.25749741\n",
      "Iteration 46, loss = 0.25079624\n",
      "Iteration 33, loss = 0.25733975\n",
      "Iteration 47, loss = 0.25040149\n",
      "Iteration 34, loss = 0.25939660\n",
      "Iteration 48, loss = 0.25029121\n",
      "Iteration 35, loss = 0.25624189\n",
      "Iteration 49, loss = 0.25100036\n",
      "Iteration 36, loss = 0.25556534\n",
      "Iteration 50, loss = 0.24972209\n",
      "Iteration 37, loss = 0.25774345\n",
      "Iteration 51, loss = 0.24808615\n",
      "Iteration 38, loss = 0.25587842\n",
      "Iteration 52, loss = 0.24798212\n",
      "Iteration 39, loss = 0.25651049\n",
      "Iteration 53, loss = 0.25144427\n",
      "Iteration 40, loss = 0.25556452\n",
      "Iteration 54, loss = 0.24916454\n",
      "Iteration 41, loss = 0.25632402\n",
      "Iteration 55, loss = 0.24835605\n",
      "Iteration 42, loss = 0.25492992\n",
      "Iteration 56, loss = 0.24931125\n",
      "Iteration 43, loss = 0.25506718\n",
      "Iteration 57, loss = 0.24983994\n",
      "Iteration 44, loss = 0.25392175\n",
      "Iteration 58, loss = 0.24941401\n",
      "Iteration 45, loss = 0.25501277\n",
      "Iteration 59, loss = 0.24988099\n",
      "Iteration 46, loss = 0.25517491\n",
      "Iteration 60, loss = 0.24771849\n",
      "Iteration 47, loss = 0.25498157\n",
      "Iteration 61, loss = 0.24753670\n",
      "Iteration 48, loss = 0.25512735\n",
      "Iteration 62, loss = 0.25091636\n",
      "Iteration 49, loss = 0.25471031\n",
      "Iteration 63, loss = 0.24988879\n",
      "Iteration 50, loss = 0.25488699\n",
      "Iteration 64, loss = 0.24845075\n",
      "Iteration 51, loss = 0.25371973\n",
      "Iteration 65, loss = 0.24756406\n",
      "Iteration 52, loss = 0.25458662\n",
      "Iteration 66, loss = 0.24739315\n",
      "Iteration 53, loss = 0.25402577\n",
      "Iteration 67, loss = 0.24876911\n",
      "Iteration 68, loss = 0.24744750\n",
      "Iteration 54, loss = 0.25424401\n",
      "Iteration 69, loss = 0.24741437\n",
      "Iteration 55, loss = 0.25322237\n",
      "Iteration 70, loss = 0.24863885\n",
      "Iteration 56, loss = 0.25450865\n",
      "Iteration 71, loss = 0.24734614\n",
      "Iteration 57, loss = 0.25478469\n",
      "Iteration 72, loss = 0.24851795\n",
      "Iteration 58, loss = 0.25629103\n",
      "Iteration 73, loss = 0.25117437\n",
      "Iteration 59, loss = 0.25583622\n",
      "Iteration 74, loss = 0.24735144\n",
      "Iteration 60, loss = 0.25376386\n",
      "Iteration 75, loss = 0.24746195\n",
      "Iteration 61, loss = 0.25510034\n",
      "Iteration 76, loss = 0.24715689\n",
      "Iteration 62, loss = 0.25627977\n",
      "Iteration 77, loss = 0.24815365\n",
      "Iteration 63, loss = 0.25351935\n",
      "Iteration 78, loss = 0.24799546\n",
      "Iteration 64, loss = 0.25383401\n",
      "Iteration 79, loss = 0.24747535\n",
      "Iteration 65, loss = 0.25337672\n",
      "Iteration 80, loss = 0.24711325\n",
      "Iteration 66, loss = 0.25316200\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 81, loss = 0.24804736\n",
      "Iteration 82, loss = 0.24806916\n",
      "Iteration 1, loss = 0.33155301\n",
      "Iteration 83, loss = 0.24951367\n",
      "Iteration 2, loss = 0.28592092\n",
      "Iteration 84, loss = 0.24706679\n",
      "Iteration 3, loss = 0.27862715\n",
      "Iteration 85, loss = 0.24743488\n",
      "Iteration 4, loss = 0.27588696\n",
      "Iteration 86, loss = 0.24599083\n",
      "Iteration 5, loss = 0.27437180\n",
      "Iteration 87, loss = 0.24738189\n",
      "Iteration 6, loss = 0.27099571\n",
      "Iteration 88, loss = 0.24769040\n",
      "Iteration 7, loss = 0.27093139\n",
      "Iteration 89, loss = 0.24792766\n",
      "Iteration 8, loss = 0.27083554\n",
      "Iteration 90, loss = 0.24627307\n",
      "Iteration 9, loss = 0.26748607\n",
      "Iteration 91, loss = 0.24842820\n",
      "Iteration 10, loss = 0.26694214\n",
      "Iteration 11, loss = 0.27176688\n",
      "Iteration 92, loss = 0.24762012\n",
      "Iteration 12, loss = 0.26635610\n",
      "Iteration 93, loss = 0.24722705\n",
      "Iteration 13, loss = 0.26430810\n",
      "Iteration 94, loss = 0.24698805\n",
      "Iteration 14, loss = 0.26359218\n",
      "Iteration 95, loss = 0.24589290\n",
      "Iteration 15, loss = 0.26376969\n",
      "Iteration 96, loss = 0.24705157\n",
      "Iteration 16, loss = 0.26126042\n",
      "Iteration 97, loss = 0.24615352\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 17, loss = 0.26224351\n",
      "Iteration 18, loss = 0.26219401\n",
      "Iteration 19, loss = 0.26026625\n",
      "Iteration 1, loss = 0.33436361\n",
      "Iteration 20, loss = 0.26238408\n",
      "Iteration 2, loss = 0.28640585\n",
      "Iteration 21, loss = 0.25855669\n",
      "Iteration 3, loss = 0.27933906\n",
      "Iteration 22, loss = 0.25895483\n",
      "Iteration 4, loss = 0.27581911\n",
      "Iteration 23, loss = 0.25791707\n",
      "Iteration 5, loss = 0.27392979\n",
      "Iteration 24, loss = 0.25932987\n",
      "Iteration 6, loss = 0.27170608\n",
      "Iteration 25, loss = 0.25861193\n",
      "Iteration 7, loss = 0.26948336\n",
      "Iteration 26, loss = 0.25632846\n",
      "Iteration 8, loss = 0.27343853\n",
      "Iteration 27, loss = 0.25567984\n",
      "Iteration 9, loss = 0.27035369\n",
      "Iteration 28, loss = 0.25575198\n",
      "Iteration 10, loss = 0.26917190\n",
      "Iteration 29, loss = 0.25622659\n",
      "Iteration 11, loss = 0.27012561\n",
      "Iteration 30, loss = 0.25572205\n",
      "Iteration 12, loss = 0.26636760\n",
      "Iteration 31, loss = 0.25444357\n",
      "Iteration 13, loss = 0.26491931\n",
      "Iteration 32, loss = 0.25466657\n",
      "Iteration 14, loss = 0.26509944\n",
      "Iteration 33, loss = 0.25334225\n",
      "Iteration 15, loss = 0.26446031\n",
      "Iteration 34, loss = 0.26032419\n",
      "Iteration 16, loss = 0.26277716\n",
      "Iteration 35, loss = 0.25416421\n",
      "Iteration 17, loss = 0.26226176\n",
      "Iteration 36, loss = 0.25377831\n",
      "Iteration 18, loss = 0.26242003\n",
      "Iteration 19, loss = 0.26296873\n",
      "Iteration 37, loss = 0.25339824\n",
      "Iteration 38, loss = 0.25432335Iteration 20, loss = 0.26206023\n",
      "\n",
      "Iteration 21, loss = 0.25925849\n",
      "Iteration 39, loss = 0.25158783\n",
      "Iteration 22, loss = 0.25971863\n",
      "Iteration 23, loss = 0.25978136\n",
      "Iteration 40, loss = 0.25211042\n",
      "Iteration 24, loss = 0.26280679\n",
      "Iteration 41, loss = 0.25300381\n",
      "Iteration 42, loss = 0.25244128\n",
      "Iteration 25, loss = 0.26079728\n",
      "Iteration 43, loss = 0.25409661\n",
      "Iteration 26, loss = 0.25940374\n",
      "Iteration 44, loss = 0.25214309\n",
      "Iteration 27, loss = 0.25892327\n",
      "Iteration 45, loss = 0.25251682\n",
      "Iteration 28, loss = 0.25703086\n",
      "Iteration 46, loss = 0.25242071\n",
      "Iteration 29, loss = 0.25757271\n",
      "Iteration 47, loss = 0.25447167\n",
      "Iteration 30, loss = 0.25687145\n",
      "Iteration 48, loss = 0.25258438\n",
      "Iteration 31, loss = 0.25678124\n",
      "Iteration 49, loss = 0.25154887\n",
      "Iteration 32, loss = 0.25610707\n",
      "Iteration 50, loss = 0.25025274\n",
      "Iteration 33, loss = 0.25653460\n",
      "Iteration 51, loss = 0.25075468\n",
      "Iteration 34, loss = 0.25726013\n",
      "Iteration 52, loss = 0.25251409\n",
      "Iteration 35, loss = 0.25374554\n",
      "Iteration 53, loss = 0.25176650\n",
      "Iteration 36, loss = 0.25372465\n",
      "Iteration 54, loss = 0.25139128\n",
      "Iteration 37, loss = 0.25464620\n",
      "Iteration 55, loss = 0.24989512\n",
      "Iteration 38, loss = 0.25381134\n",
      "Iteration 56, loss = 0.25012713\n",
      "Iteration 39, loss = 0.25283806\n",
      "Iteration 57, loss = 0.25108866\n",
      "Iteration 40, loss = 0.25393659\n",
      "Iteration 58, loss = 0.25551989\n",
      "Iteration 41, loss = 0.25481652\n",
      "Iteration 59, loss = 0.25345251\n",
      "Iteration 42, loss = 0.25293555\n",
      "Iteration 60, loss = 0.25132761\n",
      "Iteration 43, loss = 0.25353065\n",
      "Iteration 61, loss = 0.25160908\n",
      "Iteration 44, loss = 0.25237924\n",
      "Iteration 62, loss = 0.25123915\n",
      "Iteration 45, loss = 0.25471671\n",
      "Iteration 63, loss = 0.25193501\n",
      "Iteration 46, loss = 0.25288040\n",
      "Iteration 64, loss = 0.25114991\n",
      "Iteration 47, loss = 0.25337569\n",
      "Iteration 65, loss = 0.25035679\n",
      "Iteration 48, loss = 0.25288484\n",
      "Iteration 66, loss = 0.24999430\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 49, loss = 0.25163887\n",
      "Iteration 50, loss = 0.25143562\n",
      "Iteration 51, loss = 0.25109819\n",
      "Iteration 52, loss = 0.25325262\n",
      "Iteration 53, loss = 0.25081559\n",
      "Iteration 54, loss = 0.25399729\n",
      "Iteration 55, loss = 0.25387679\n",
      "Iteration 56, loss = 0.25202765\n",
      "Iteration 57, loss = 0.25209057\n",
      "Iteration 58, loss = 0.25334375\n",
      "Iteration 59, loss = 0.24994509\n",
      "Iteration 60, loss = 0.25170066\n",
      "Iteration 61, loss = 0.25174238\n",
      "Iteration 62, loss = 0.25378913\n",
      "Iteration 63, loss = 0.25127500\n",
      "Iteration 64, loss = 0.25129762\n",
      "Iteration 65, loss = 0.25058347\n",
      "Iteration 66, loss = 0.25033429\n",
      "Iteration 67, loss = 0.25168997\n",
      "Iteration 68, loss = 0.25194853\n",
      "Iteration 69, loss = 0.25039015\n",
      "Iteration 70, loss = 0.25192323\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33558283\n",
      "Iteration 1, loss = 0.33629244\n",
      "Iteration 2, loss = 0.29694562\n",
      "Iteration 2, loss = 0.30108213\n",
      "Iteration 3, loss = 0.29603147\n",
      "Iteration 3, loss = 0.29115329\n",
      "Iteration 4, loss = 0.29298405\n",
      "Iteration 5, loss = 0.28926341\n",
      "Iteration 4, loss = 0.28887454\n",
      "Iteration 6, loss = 0.28851443\n",
      "Iteration 5, loss = 0.28649072\n",
      "Iteration 7, loss = 0.28687568\n",
      "Iteration 6, loss = 0.28434522\n",
      "Iteration 8, loss = 0.28330741\n",
      "Iteration 7, loss = 0.28458161\n",
      "Iteration 9, loss = 0.28319847\n",
      "Iteration 8, loss = 0.28172106\n",
      "Iteration 10, loss = 0.28044210\n",
      "Iteration 9, loss = 0.28099795\n",
      "Iteration 11, loss = 0.27925628\n",
      "Iteration 10, loss = 0.28064508\n",
      "Iteration 11, loss = 0.28210879\n",
      "Iteration 12, loss = 0.28405350\n",
      "Iteration 12, loss = 0.28371489\n",
      "Iteration 13, loss = 0.27844164\n",
      "Iteration 13, loss = 0.27787260\n",
      "Iteration 14, loss = 0.27705448\n",
      "Iteration 15, loss = 0.27721537\n",
      "Iteration 14, loss = 0.27801647\n",
      "Iteration 16, loss = 0.27567118\n",
      "Iteration 15, loss = 0.27649973\n",
      "Iteration 17, loss = 0.27587168\n",
      "Iteration 16, loss = 0.27615022\n",
      "Iteration 18, loss = 0.27522345\n",
      "Iteration 17, loss = 0.27675547\n",
      "Iteration 19, loss = 0.27651007\n",
      "Iteration 18, loss = 0.27507493\n",
      "Iteration 20, loss = 0.27585676\n",
      "Iteration 19, loss = 0.27750807\n",
      "Iteration 21, loss = 0.27415537\n",
      "Iteration 20, loss = 0.27520972\n",
      "Iteration 22, loss = 0.27337316\n",
      "Iteration 21, loss = 0.27514004\n",
      "Iteration 23, loss = 0.27396567Iteration 22, loss = 0.27360298\n",
      "\n",
      "Iteration 24, loss = 0.27505992\n",
      "Iteration 23, loss = 0.27318143\n",
      "Iteration 25, loss = 0.27299609\n",
      "Iteration 24, loss = 0.27379872\n",
      "Iteration 25, loss = 0.27193159\n",
      "Iteration 26, loss = 0.27296866\n",
      "Iteration 26, loss = 0.27220846\n",
      "Iteration 27, loss = 0.27245825\n",
      "Iteration 27, loss = 0.27165876\n",
      "Iteration 28, loss = 0.27096264\n",
      "Iteration 28, loss = 0.27278060\n",
      "Iteration 29, loss = 0.27005465\n",
      "Iteration 30, loss = 0.26982766\n",
      "Iteration 31, loss = 0.27070738Iteration 29, loss = 0.27315401\n",
      "\n",
      "Iteration 32, loss = 0.27280034\n",
      "Iteration 30, loss = 0.27180584\n",
      "Iteration 33, loss = 0.27012756\n",
      "Iteration 31, loss = 0.27099550\n",
      "Iteration 34, loss = 0.26968400\n",
      "Iteration 32, loss = 0.27162416\n",
      "Iteration 35, loss = 0.27021353\n",
      "Iteration 33, loss = 0.27210435\n",
      "Iteration 36, loss = 0.26888019\n",
      "Iteration 34, loss = 0.27082506\n",
      "Iteration 37, loss = 0.27065985\n",
      "Iteration 35, loss = 0.27057185\n",
      "Iteration 38, loss = 0.27031140\n",
      "Iteration 36, loss = 0.27086448\n",
      "Iteration 39, loss = 0.27147740\n",
      "Iteration 37, loss = 0.27296529\n",
      "Iteration 40, loss = 0.26926531\n",
      "Iteration 38, loss = 0.27156193\n",
      "Iteration 41, loss = 0.26882137\n",
      "Iteration 39, loss = 0.27164476\n",
      "Iteration 42, loss = 0.26813406\n",
      "Iteration 43, loss = 0.26817465\n",
      "Iteration 40, loss = 0.27030211\n",
      "Iteration 44, loss = 0.26814303\n",
      "Iteration 41, loss = 0.26977635\n",
      "Iteration 45, loss = 0.26791379\n",
      "Iteration 42, loss = 0.26997090\n",
      "Iteration 46, loss = 0.26857685\n",
      "Iteration 43, loss = 0.26981686\n",
      "Iteration 47, loss = 0.26943550\n",
      "Iteration 44, loss = 0.27110348\n",
      "Iteration 48, loss = 0.26820909\n",
      "Iteration 45, loss = 0.26843562\n",
      "Iteration 49, loss = 0.26847609\n",
      "Iteration 46, loss = 0.27102446\n",
      "Iteration 50, loss = 0.26930586\n",
      "Iteration 51, loss = 0.26799054\n",
      "Iteration 47, loss = 0.27032862\n",
      "Iteration 52, loss = 0.26849344\n",
      "Iteration 53, loss = 0.26740339\n",
      "Iteration 48, loss = 0.26913059\n",
      "Iteration 54, loss = 0.26967135\n",
      "Iteration 55, loss = 0.26755442\n",
      "Iteration 49, loss = 0.27056444\n",
      "Iteration 56, loss = 0.26797936\n",
      "Iteration 50, loss = 0.27057282\n",
      "Iteration 57, loss = 0.26818918\n",
      "Iteration 51, loss = 0.27022243\n",
      "Iteration 58, loss = 0.26886109\n",
      "Iteration 52, loss = 0.27115496\n",
      "Iteration 59, loss = 0.26875046\n",
      "Iteration 53, loss = 0.27006123\n",
      "Iteration 60, loss = 0.26863754\n",
      "Iteration 54, loss = 0.27146792\n",
      "Iteration 61, loss = 0.26994883\n",
      "Iteration 55, loss = 0.26766047\n",
      "Iteration 62, loss = 0.26801254\n",
      "Iteration 56, loss = 0.27119517\n",
      "Iteration 63, loss = 0.26904216\n",
      "Iteration 57, loss = 0.26865430\n",
      "Iteration 64, loss = 0.26812850\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 58, loss = 0.26954282\n",
      "Iteration 59, loss = 0.26954266\n",
      "Iteration 1, loss = 0.32911003\n",
      "Iteration 60, loss = 0.26977439\n",
      "Iteration 2, loss = 0.29796785\n",
      "Iteration 61, loss = 0.26865821\n",
      "Iteration 3, loss = 0.29219360\n",
      "Iteration 62, loss = 0.27160042\n",
      "Iteration 4, loss = 0.28908099\n",
      "Iteration 63, loss = 0.27255986\n",
      "Iteration 5, loss = 0.28473107\n",
      "Iteration 64, loss = 0.27037868\n",
      "Iteration 6, loss = 0.28325795\n",
      "Iteration 65, loss = 0.26792209\n",
      "Iteration 66, loss = 0.26950049\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 7, loss = 0.28226298\n",
      "Iteration 8, loss = 0.27938671\n",
      "Iteration 9, loss = 0.27838331\n",
      "Iteration 1, loss = 0.32682965\n",
      "Iteration 10, loss = 0.27676840\n",
      "Iteration 2, loss = 0.29874726\n",
      "Iteration 11, loss = 0.27537643\n",
      "Iteration 3, loss = 0.29282034\n",
      "Iteration 12, loss = 0.27895462\n",
      "Iteration 13, loss = 0.27523978\n",
      "Iteration 4, loss = 0.28886445\n",
      "Iteration 5, loss = 0.28660653\n",
      "Iteration 14, loss = 0.27344724\n",
      "Iteration 6, loss = 0.28536928\n",
      "Iteration 15, loss = 0.27295853\n",
      "Iteration 7, loss = 0.28598468\n",
      "Iteration 16, loss = 0.27204002\n",
      "Iteration 8, loss = 0.28434586\n",
      "Iteration 17, loss = 0.27145087\n",
      "Iteration 9, loss = 0.28387666\n",
      "Iteration 18, loss = 0.27109615\n",
      "Iteration 10, loss = 0.28084172\n",
      "Iteration 19, loss = 0.27093337\n",
      "Iteration 11, loss = 0.27963836\n",
      "Iteration 20, loss = 0.27053422\n",
      "Iteration 12, loss = 0.27983432\n",
      "Iteration 21, loss = 0.26893565\n",
      "Iteration 13, loss = 0.27991903\n",
      "Iteration 22, loss = 0.26851426\n",
      "Iteration 14, loss = 0.27661813Iteration 23, loss = 0.26777398\n",
      "\n",
      "Iteration 15, loss = 0.27818376\n",
      "Iteration 24, loss = 0.26904929\n",
      "Iteration 16, loss = 0.27610988\n",
      "Iteration 25, loss = 0.26871462\n",
      "Iteration 17, loss = 0.27583916\n",
      "Iteration 26, loss = 0.26769892\n",
      "Iteration 27, loss = 0.26796018\n",
      "Iteration 18, loss = 0.27467764\n",
      "Iteration 28, loss = 0.26769734\n",
      "Iteration 19, loss = 0.27437736\n",
      "Iteration 20, loss = 0.27456706\n",
      "Iteration 29, loss = 0.26812406\n",
      "Iteration 30, loss = 0.26742852\n",
      "Iteration 31, loss = 0.26570913\n",
      "Iteration 21, loss = 0.27483283\n",
      "Iteration 32, loss = 0.26680000\n",
      "Iteration 22, loss = 0.27377661Iteration 33, loss = 0.26914658\n",
      "\n",
      "Iteration 34, loss = 0.26672489\n",
      "Iteration 23, loss = 0.27634665\n",
      "Iteration 35, loss = 0.26689132\n",
      "Iteration 24, loss = 0.27640427\n",
      "Iteration 36, loss = 0.26620664\n",
      "Iteration 25, loss = 0.27440473\n",
      "Iteration 37, loss = 0.26770973\n",
      "Iteration 26, loss = 0.27178508\n",
      "Iteration 38, loss = 0.26850640\n",
      "Iteration 27, loss = 0.27525454\n",
      "Iteration 39, loss = 0.26552993\n",
      "Iteration 28, loss = 0.27239231\n",
      "Iteration 40, loss = 0.26527342\n",
      "Iteration 29, loss = 0.27417255\n",
      "Iteration 41, loss = 0.26521581\n",
      "Iteration 30, loss = 0.27198931\n",
      "Iteration 42, loss = 0.26613208\n",
      "Iteration 31, loss = 0.27078030\n",
      "Iteration 43, loss = 0.26661349\n",
      "Iteration 32, loss = 0.27096758\n",
      "Iteration 33, loss = 0.27117298\n",
      "Iteration 34, loss = 0.27146383\n",
      "Iteration 44, loss = 0.26663144\n",
      "Iteration 35, loss = 0.27310597\n",
      "Iteration 45, loss = 0.26519751\n",
      "Iteration 36, loss = 0.26966371\n",
      "Iteration 46, loss = 0.26574710\n",
      "Iteration 37, loss = 0.27004664\n",
      "Iteration 47, loss = 0.26491202\n",
      "Iteration 38, loss = 0.27088648\n",
      "Iteration 48, loss = 0.26375774\n",
      "Iteration 39, loss = 0.27166414\n",
      "Iteration 49, loss = 0.26485929\n",
      "Iteration 40, loss = 0.27086637\n",
      "Iteration 50, loss = 0.26484477\n",
      "Iteration 41, loss = 0.26957184\n",
      "Iteration 51, loss = 0.26532572\n",
      "Iteration 42, loss = 0.26974658\n",
      "Iteration 52, loss = 0.26409644\n",
      "Iteration 43, loss = 0.27069795\n",
      "Iteration 53, loss = 0.26657950\n",
      "Iteration 44, loss = 0.27088773\n",
      "Iteration 54, loss = 0.26569032\n",
      "Iteration 45, loss = 0.26993735\n",
      "Iteration 55, loss = 0.26518405\n",
      "Iteration 46, loss = 0.27307210\n",
      "Iteration 47, loss = 0.27103723\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 56, loss = 0.26784972\n",
      "Iteration 57, loss = 0.26543101\n",
      "Iteration 1, loss = 0.32582080\n",
      "Iteration 58, loss = 0.26494853\n",
      "Iteration 2, loss = 0.29847554\n",
      "Iteration 59, loss = 0.26414956\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 3, loss = 0.29437636\n",
      "Iteration 4, loss = 0.28859670\n",
      "Iteration 1, loss = 0.32886883\n",
      "Iteration 5, loss = 0.28616717\n",
      "Iteration 2, loss = 0.29941564\n",
      "Iteration 6, loss = 0.28447172\n",
      "Iteration 3, loss = 0.29273524\n",
      "Iteration 7, loss = 0.28353363\n",
      "Iteration 8, loss = 0.28314356\n",
      "Iteration 4, loss = 0.28822353\n",
      "Iteration 9, loss = 0.28117121\n",
      "Iteration 5, loss = 0.28706816\n",
      "Iteration 10, loss = 0.28025291\n",
      "Iteration 6, loss = 0.28612446\n",
      "Iteration 11, loss = 0.28012294\n",
      "Iteration 7, loss = 0.28425843\n",
      "Iteration 12, loss = 0.28096582\n",
      "Iteration 8, loss = 0.28402271\n",
      "Iteration 13, loss = 0.28045417\n",
      "Iteration 9, loss = 0.28183817\n",
      "Iteration 14, loss = 0.27824598\n",
      "Iteration 10, loss = 0.28228676\n",
      "Iteration 15, loss = 0.27754674\n",
      "Iteration 11, loss = 0.28041165\n",
      "Iteration 16, loss = 0.27661361\n",
      "Iteration 12, loss = 0.28125823\n",
      "Iteration 17, loss = 0.27841779\n",
      "Iteration 13, loss = 0.27968576\n",
      "Iteration 18, loss = 0.27423953\n",
      "Iteration 14, loss = 0.27854343\n",
      "Iteration 19, loss = 0.27487285\n",
      "Iteration 15, loss = 0.28073657\n",
      "Iteration 20, loss = 0.27533614\n",
      "Iteration 16, loss = 0.27789694\n",
      "Iteration 21, loss = 0.27338918\n",
      "Iteration 17, loss = 0.27901478\n",
      "Iteration 22, loss = 0.27316063\n",
      "Iteration 18, loss = 0.27669812\n",
      "Iteration 23, loss = 0.27313398\n",
      "Iteration 19, loss = 0.27674403\n",
      "Iteration 24, loss = 0.27348807\n",
      "Iteration 20, loss = 0.27694995\n",
      "Iteration 25, loss = 0.27267534\n",
      "Iteration 21, loss = 0.27625719\n",
      "Iteration 26, loss = 0.27062441\n",
      "Iteration 22, loss = 0.27572644\n",
      "Iteration 27, loss = 0.27381724\n",
      "Iteration 23, loss = 0.27718202\n",
      "Iteration 28, loss = 0.27059845\n",
      "Iteration 24, loss = 0.27635601\n",
      "Iteration 29, loss = 0.26959968\n",
      "Iteration 25, loss = 0.27337431\n",
      "Iteration 30, loss = 0.26864329\n",
      "Iteration 26, loss = 0.27275441\n",
      "Iteration 27, loss = 0.27696959\n",
      "Iteration 31, loss = 0.26816908\n",
      "Iteration 28, loss = 0.27524429\n",
      "Iteration 29, loss = 0.27254305\n",
      "Iteration 32, loss = 0.26773075\n",
      "Iteration 30, loss = 0.27167736\n",
      "Iteration 33, loss = 0.26946518\n",
      "Iteration 31, loss = 0.27122881\n",
      "Iteration 34, loss = 0.26914823\n",
      "Iteration 32, loss = 0.27047567\n",
      "Iteration 35, loss = 0.26985746\n",
      "Iteration 33, loss = 0.27035776\n",
      "Iteration 36, loss = 0.26768776\n",
      "Iteration 34, loss = 0.27193549\n",
      "Iteration 37, loss = 0.26680583\n",
      "Iteration 35, loss = 0.27195926\n",
      "Iteration 38, loss = 0.26638274\n",
      "Iteration 39, loss = 0.26771997\n",
      "Iteration 36, loss = 0.27054160\n",
      "Iteration 40, loss = 0.26634037\n",
      "Iteration 41, loss = 0.26552547\n",
      "Iteration 37, loss = 0.26935534\n",
      "Iteration 42, loss = 0.26619906\n",
      "Iteration 38, loss = 0.26888157\n",
      "Iteration 43, loss = 0.26604485\n",
      "Iteration 39, loss = 0.26999471\n",
      "Iteration 44, loss = 0.26732865\n",
      "Iteration 40, loss = 0.27008667\n",
      "Iteration 45, loss = 0.26590110\n",
      "Iteration 46, loss = 0.26868698\n",
      "Iteration 41, loss = 0.26965725\n",
      "Iteration 47, loss = 0.26528275\n",
      "Iteration 48, loss = 0.26533595\n",
      "Iteration 42, loss = 0.26995778\n",
      "Iteration 49, loss = 0.26632962\n",
      "Iteration 43, loss = 0.27118524\n",
      "Iteration 50, loss = 0.26587631\n",
      "Iteration 44, loss = 0.27000678\n",
      "Iteration 51, loss = 0.26588785\n",
      "Iteration 45, loss = 0.26889766\n",
      "Iteration 52, loss = 0.26547406\n",
      "Iteration 46, loss = 0.27044048\n",
      "Iteration 53, loss = 0.26929154\n",
      "Iteration 47, loss = 0.27091972\n",
      "Iteration 48, loss = 0.26914731Iteration 54, loss = 0.26612349\n",
      "\n",
      "Iteration 49, loss = 0.26851702\n",
      "Iteration 50, loss = 0.26872496\n",
      "Iteration 55, loss = 0.26717502\n",
      "Iteration 51, loss = 0.27058531\n",
      "Iteration 52, loss = 0.26904867\n",
      "Iteration 56, loss = 0.26692660Iteration 53, loss = 0.26822723\n",
      "\n",
      "Iteration 57, loss = 0.26714768\n",
      "Iteration 54, loss = 0.26971095\n",
      "Iteration 55, loss = 0.26994817Iteration 58, loss = 0.26529066\n",
      "\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 56, loss = 0.27013748\n",
      "Iteration 57, loss = 0.27065962\n",
      "Iteration 1, loss = 0.32411669\n",
      "Iteration 58, loss = 0.26854927\n",
      "Iteration 2, loss = 0.29624283\n",
      "Iteration 59, loss = 0.26816224\n",
      "Iteration 60, loss = 0.27032854\n",
      "Iteration 3, loss = 0.29058530\n",
      "Iteration 61, loss = 0.26723405\n",
      "Iteration 4, loss = 0.28468501\n",
      "Iteration 5, loss = 0.28453801\n",
      "Iteration 62, loss = 0.26929462\n",
      "Iteration 6, loss = 0.28278391\n",
      "Iteration 63, loss = 0.26976612\n",
      "Iteration 64, loss = 0.26987279\n",
      "Iteration 7, loss = 0.28040997\n",
      "Iteration 65, loss = 0.27060093\n",
      "Iteration 8, loss = 0.28000727\n",
      "Iteration 66, loss = 0.26765455\n",
      "Iteration 9, loss = 0.27873941\n",
      "Iteration 67, loss = 0.26803563\n",
      "Iteration 10, loss = 0.27845689\n",
      "Iteration 68, loss = 0.26996877\n",
      "Iteration 69, loss = 0.26864918\n",
      "Iteration 11, loss = 0.27731111\n",
      "Iteration 70, loss = 0.26818965\n",
      "Iteration 71, loss = 0.26816377\n",
      "Iteration 12, loss = 0.27453157\n",
      "Iteration 13, loss = 0.27482875\n",
      "Iteration 72, loss = 0.26868115\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 14, loss = 0.27547893\n",
      "Iteration 1, loss = 0.32980879\n",
      "Iteration 15, loss = 0.27340270\n",
      "Iteration 2, loss = 0.29723633\n",
      "Iteration 16, loss = 0.27317964\n",
      "Iteration 17, loss = 0.27246046Iteration 3, loss = 0.29106906\n",
      "\n",
      "Iteration 4, loss = 0.28758731\n",
      "Iteration 18, loss = 0.27300054\n",
      "Iteration 5, loss = 0.28684329\n",
      "Iteration 19, loss = 0.27255519\n",
      "Iteration 6, loss = 0.28332357\n",
      "Iteration 20, loss = 0.27354078\n",
      "Iteration 7, loss = 0.28210083\n",
      "Iteration 21, loss = 0.27042448\n",
      "Iteration 8, loss = 0.28069163\n",
      "Iteration 9, loss = 0.28073487\n",
      "Iteration 22, loss = 0.27158923\n",
      "Iteration 10, loss = 0.28044862\n",
      "Iteration 23, loss = 0.27078788\n",
      "Iteration 11, loss = 0.27894461\n",
      "Iteration 24, loss = 0.27095546\n",
      "Iteration 12, loss = 0.27630440\n",
      "Iteration 25, loss = 0.26823837\n",
      "Iteration 13, loss = 0.27559456\n",
      "Iteration 26, loss = 0.26901857\n",
      "Iteration 14, loss = 0.27556865\n",
      "Iteration 27, loss = 0.27136640\n",
      "Iteration 15, loss = 0.27462111\n",
      "Iteration 28, loss = 0.26911714\n",
      "Iteration 16, loss = 0.27339600\n",
      "Iteration 17, loss = 0.27399516\n",
      "Iteration 29, loss = 0.26714628\n",
      "Iteration 30, loss = 0.26846883\n",
      "Iteration 31, loss = 0.26799297\n",
      "Iteration 18, loss = 0.27294539\n",
      "Iteration 32, loss = 0.26609706\n",
      "Iteration 19, loss = 0.27392890\n",
      "Iteration 33, loss = 0.26685450\n",
      "Iteration 20, loss = 0.27239075\n",
      "Iteration 21, loss = 0.27267354\n",
      "Iteration 34, loss = 0.26907041\n",
      "Iteration 35, loss = 0.26716095\n",
      "Iteration 22, loss = 0.27168489\n",
      "Iteration 36, loss = 0.26669402\n",
      "Iteration 23, loss = 0.27139282\n",
      "Iteration 37, loss = 0.26678782\n",
      "Iteration 24, loss = 0.27182424\n",
      "Iteration 38, loss = 0.26638069\n",
      "Iteration 25, loss = 0.27081677\n",
      "Iteration 39, loss = 0.26762314\n",
      "Iteration 26, loss = 0.27020954\n",
      "Iteration 40, loss = 0.26789452\n",
      "Iteration 27, loss = 0.27389305\n",
      "Iteration 41, loss = 0.26625599\n",
      "Iteration 28, loss = 0.27202622\n",
      "Iteration 42, loss = 0.26699844\n",
      "Iteration 29, loss = 0.27021111\n",
      "Iteration 43, loss = 0.26717590\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 30, loss = 0.27052529\n",
      "Iteration 31, loss = 0.26887218\n",
      "Iteration 32, loss = 0.26919006\n",
      "Iteration 1, loss = 0.32746874\n",
      "Iteration 33, loss = 0.27015903\n",
      "Iteration 2, loss = 0.29726578\n",
      "Iteration 34, loss = 0.27056753\n",
      "Iteration 3, loss = 0.29248343\n",
      "Iteration 35, loss = 0.26863886\n",
      "Iteration 4, loss = 0.28954192\n",
      "Iteration 36, loss = 0.26961909\n",
      "Iteration 5, loss = 0.28881612\n",
      "Iteration 37, loss = 0.26785268\n",
      "Iteration 6, loss = 0.28403092\n",
      "Iteration 38, loss = 0.26726013\n",
      "Iteration 7, loss = 0.28167092\n",
      "Iteration 39, loss = 0.26924773\n",
      "Iteration 8, loss = 0.27959719\n",
      "Iteration 40, loss = 0.26810970\n",
      "Iteration 9, loss = 0.28060127\n",
      "Iteration 41, loss = 0.26705708\n",
      "Iteration 10, loss = 0.28124943\n",
      "Iteration 42, loss = 0.26795428\n",
      "Iteration 11, loss = 0.27803906\n",
      "Iteration 43, loss = 0.26898497\n",
      "Iteration 12, loss = 0.27512350\n",
      "Iteration 44, loss = 0.26920634\n",
      "Iteration 13, loss = 0.27508282\n",
      "Iteration 45, loss = 0.26780646\n",
      "Iteration 14, loss = 0.27436480\n",
      "Iteration 46, loss = 0.26713888\n",
      "Iteration 15, loss = 0.27231460\n",
      "Iteration 47, loss = 0.26871688\n",
      "Iteration 48, loss = 0.26694430\n",
      "Iteration 16, loss = 0.27365941\n",
      "Iteration 49, loss = 0.26719128\n",
      "Iteration 17, loss = 0.27465515\n",
      "Iteration 50, loss = 0.26744763\n",
      "Iteration 18, loss = 0.27333286\n",
      "Iteration 51, loss = 0.26699659\n",
      "Iteration 19, loss = 0.27378135\n",
      "Iteration 52, loss = 0.26813861\n",
      "Iteration 20, loss = 0.27336347\n",
      "Iteration 53, loss = 0.26687923\n",
      "Iteration 21, loss = 0.27124650\n",
      "Iteration 54, loss = 0.26850081\n",
      "Iteration 22, loss = 0.27114048\n",
      "Iteration 55, loss = 0.26888971\n",
      "Iteration 23, loss = 0.27150019\n",
      "Iteration 56, loss = 0.27021309\n",
      "Iteration 24, loss = 0.27203125\n",
      "Iteration 57, loss = 0.26618540\n",
      "Iteration 25, loss = 0.27089814\n",
      "Iteration 58, loss = 0.26717499\n",
      "Iteration 26, loss = 0.27087498\n",
      "Iteration 59, loss = 0.26869383\n",
      "Iteration 27, loss = 0.27174543\n",
      "Iteration 60, loss = 0.26761352\n",
      "Iteration 61, loss = 0.26695297\n",
      "Iteration 62, loss = 0.26683542Iteration 28, loss = 0.27020350\n",
      "\n",
      "Iteration 29, loss = 0.26839182\n",
      "Iteration 63, loss = 0.26660528\n",
      "Iteration 30, loss = 0.26848476Iteration 64, loss = 0.26772709\n",
      "\n",
      "Iteration 65, loss = 0.26660440Iteration 31, loss = 0.26801165\n",
      "\n",
      "Iteration 66, loss = 0.26716873Iteration 32, loss = 0.26903474\n",
      "\n",
      "Iteration 33, loss = 0.26812191\n",
      "Iteration 67, loss = 0.26564642\n",
      "Iteration 34, loss = 0.26909200\n",
      "Iteration 68, loss = 0.26592744\n",
      "Iteration 35, loss = 0.26839233\n",
      "Iteration 69, loss = 0.26706385\n",
      "Iteration 36, loss = 0.26889909\n",
      "Iteration 70, loss = 0.26663421\n",
      "Iteration 37, loss = 0.26747387\n",
      "Iteration 71, loss = 0.26618281\n",
      "Iteration 38, loss = 0.26825624\n",
      "Iteration 72, loss = 0.26689061\n",
      "Iteration 39, loss = 0.26830514\n",
      "Iteration 73, loss = 0.26714535\n",
      "Iteration 40, loss = 0.26790273\n",
      "Iteration 74, loss = 0.26690478\n",
      "Iteration 41, loss = 0.26843455\n",
      "Iteration 75, loss = 0.26750235\n",
      "Iteration 42, loss = 0.26819267\n",
      "Iteration 76, loss = 0.26631300\n",
      "Iteration 43, loss = 0.26743419\n",
      "Iteration 77, loss = 0.26620154\n",
      "Iteration 44, loss = 0.26769042\n",
      "Iteration 78, loss = 0.26712864\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 45, loss = 0.26716305\n",
      "Iteration 46, loss = 0.26770381\n",
      "Iteration 47, loss = 0.26745872\n",
      "Iteration 1, loss = 0.33034269\n",
      "Iteration 48, loss = 0.26716197\n",
      "Iteration 2, loss = 0.29571246\n",
      "Iteration 49, loss = 0.26631267\n",
      "Iteration 3, loss = 0.29124416\n",
      "Iteration 4, loss = 0.28820838\n",
      "Iteration 50, loss = 0.26643640\n",
      "Iteration 51, loss = 0.26655937\n",
      "Iteration 5, loss = 0.28733318\n",
      "Iteration 52, loss = 0.26613270\n",
      "Iteration 6, loss = 0.28345103\n",
      "Iteration 53, loss = 0.26602905\n",
      "Iteration 7, loss = 0.28319438\n",
      "Iteration 8, loss = 0.27992179\n",
      "Iteration 54, loss = 0.26771019\n",
      "Iteration 9, loss = 0.28228405\n",
      "Iteration 10, loss = 0.28247825Iteration 55, loss = 0.26623838\n",
      "\n",
      "Iteration 56, loss = 0.26706560\n",
      "Iteration 11, loss = 0.27804748\n",
      "Iteration 57, loss = 0.26577265\n",
      "Iteration 12, loss = 0.27667106\n",
      "Iteration 58, loss = 0.26686927\n",
      "Iteration 59, loss = 0.26870957Iteration 13, loss = 0.27603398\n",
      "\n",
      "Iteration 60, loss = 0.26745805Iteration 14, loss = 0.27496328\n",
      "\n",
      "Iteration 61, loss = 0.26516712\n",
      "Iteration 15, loss = 0.27373002\n",
      "Iteration 62, loss = 0.26690608\n",
      "Iteration 16, loss = 0.27318732\n",
      "Iteration 63, loss = 0.26873187\n",
      "Iteration 17, loss = 0.27464406\n",
      "Iteration 64, loss = 0.26800614\n",
      "Iteration 18, loss = 0.27428536\n",
      "Iteration 65, loss = 0.26567998\n",
      "Iteration 19, loss = 0.27395708\n",
      "Iteration 66, loss = 0.26600291\n",
      "Iteration 20, loss = 0.27320836\n",
      "Iteration 67, loss = 0.26442839\n",
      "Iteration 21, loss = 0.27159282\n",
      "Iteration 68, loss = 0.26608831\n",
      "Iteration 69, loss = 0.26647037\n",
      "Iteration 22, loss = 0.27142337\n",
      "Iteration 70, loss = 0.26566994\n",
      "Iteration 23, loss = 0.27016027\n",
      "Iteration 71, loss = 0.26596109\n",
      "Iteration 24, loss = 0.27054140\n",
      "Iteration 72, loss = 0.26652413\n",
      "Iteration 25, loss = 0.26943193\n",
      "Iteration 73, loss = 0.26594471\n",
      "Iteration 26, loss = 0.26843477\n",
      "Iteration 74, loss = 0.26489013\n",
      "Iteration 27, loss = 0.26837207\n",
      "Iteration 75, loss = 0.26676247\n",
      "Iteration 28, loss = 0.26836939\n",
      "Iteration 76, loss = 0.26674696\n",
      "Iteration 29, loss = 0.26774492\n",
      "Iteration 77, loss = 0.26588845\n",
      "Iteration 78, loss = 0.26609140\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 30, loss = 0.26963084\n",
      "Iteration 31, loss = 0.26846731\n",
      "Iteration 32, loss = 0.26821044\n",
      "Iteration 33, loss = 0.26883528\n",
      "Iteration 34, loss = 0.26766146\n",
      "Iteration 35, loss = 0.26576218\n",
      "Iteration 36, loss = 0.26817732\n",
      "Iteration 37, loss = 0.26653463\n",
      "Iteration 38, loss = 0.26713318\n",
      "Iteration 39, loss = 0.26785817\n",
      "Iteration 40, loss = 0.26645344\n",
      "Iteration 41, loss = 0.26667225\n",
      "Iteration 42, loss = 0.26603528\n",
      "Iteration 43, loss = 0.26619317\n",
      "Iteration 44, loss = 0.26664635\n",
      "Iteration 45, loss = 0.26572820\n",
      "Iteration 46, loss = 0.26594071\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.31762773\n",
      "Iteration 1, loss = 0.31284657\n",
      "Iteration 2, loss = 0.28807361\n",
      "Iteration 2, loss = 0.28337388\n",
      "Iteration 3, loss = 0.28555080\n",
      "Iteration 3, loss = 0.28045529\n",
      "Iteration 4, loss = 0.28245914\n",
      "Iteration 4, loss = 0.27684696\n",
      "Iteration 5, loss = 0.28034149\n",
      "Iteration 5, loss = 0.27380904\n",
      "Iteration 6, loss = 0.27685677\n",
      "Iteration 6, loss = 0.27161285\n",
      "Iteration 7, loss = 0.27810690\n",
      "Iteration 7, loss = 0.27286772\n",
      "Iteration 8, loss = 0.27511237\n",
      "Iteration 8, loss = 0.27002690\n",
      "Iteration 9, loss = 0.27620173\n",
      "Iteration 9, loss = 0.26799737\n",
      "Iteration 10, loss = 0.27399207\n",
      "Iteration 10, loss = 0.26927066\n",
      "Iteration 11, loss = 0.27245668\n",
      "Iteration 11, loss = 0.26776108\n",
      "Iteration 12, loss = 0.27334901\n",
      "Iteration 12, loss = 0.26703785\n",
      "Iteration 13, loss = 0.27353960\n",
      "Iteration 13, loss = 0.26763365\n",
      "Iteration 14, loss = 0.27169212\n",
      "Iteration 14, loss = 0.26968620\n",
      "Iteration 15, loss = 0.26978576\n",
      "Iteration 16, loss = 0.27177063\n",
      "Iteration 15, loss = 0.26619776\n",
      "Iteration 17, loss = 0.26820392\n",
      "Iteration 18, loss = 0.26924252\n",
      "Iteration 16, loss = 0.26732012\n",
      "Iteration 19, loss = 0.26693339\n",
      "Iteration 17, loss = 0.26545579\n",
      "Iteration 20, loss = 0.26736737\n",
      "Iteration 21, loss = 0.27001668\n",
      "Iteration 18, loss = 0.26503238\n",
      "Iteration 22, loss = 0.26663202\n",
      "Iteration 19, loss = 0.26428709\n",
      "Iteration 23, loss = 0.26660861\n",
      "Iteration 20, loss = 0.26372541\n",
      "Iteration 24, loss = 0.26607346\n",
      "Iteration 21, loss = 0.26458957\n",
      "Iteration 25, loss = 0.26700445\n",
      "Iteration 22, loss = 0.26420922\n",
      "Iteration 26, loss = 0.27144628\n",
      "Iteration 23, loss = 0.26439818\n",
      "Iteration 27, loss = 0.26797689\n",
      "Iteration 24, loss = 0.26407889\n",
      "Iteration 28, loss = 0.26578355\n",
      "Iteration 25, loss = 0.26526462\n",
      "Iteration 29, loss = 0.26533690\n",
      "Iteration 26, loss = 0.27000992Iteration 30, loss = 0.26442387\n",
      "\n",
      "Iteration 31, loss = 0.26458523\n",
      "Iteration 27, loss = 0.26539986\n",
      "Iteration 32, loss = 0.26813906\n",
      "Iteration 28, loss = 0.26436562\n",
      "Iteration 33, loss = 0.26643928\n",
      "Iteration 29, loss = 0.26497549\n",
      "Iteration 34, loss = 0.26542838\n",
      "Iteration 30, loss = 0.26273352\n",
      "Iteration 31, loss = 0.26231398\n",
      "Iteration 35, loss = 0.26417264\n",
      "Iteration 36, loss = 0.26546646\n",
      "Iteration 32, loss = 0.26422919\n",
      "Iteration 37, loss = 0.26367400\n",
      "Iteration 33, loss = 0.26343704\n",
      "Iteration 34, loss = 0.26346314\n",
      "Iteration 38, loss = 0.26595614\n",
      "Iteration 39, loss = 0.26415645\n",
      "Iteration 35, loss = 0.26251817\n",
      "Iteration 40, loss = 0.26592955\n",
      "Iteration 36, loss = 0.26159045\n",
      "Iteration 41, loss = 0.26272309\n",
      "Iteration 37, loss = 0.26127263\n",
      "Iteration 42, loss = 0.26347221\n",
      "Iteration 38, loss = 0.26126285\n",
      "Iteration 43, loss = 0.26377856\n",
      "Iteration 39, loss = 0.26092380\n",
      "Iteration 44, loss = 0.26352223\n",
      "Iteration 40, loss = 0.26059623\n",
      "Iteration 45, loss = 0.26828093\n",
      "Iteration 41, loss = 0.26026413\n",
      "Iteration 46, loss = 0.26710434\n",
      "Iteration 42, loss = 0.26065917\n",
      "Iteration 47, loss = 0.26344935\n",
      "Iteration 43, loss = 0.26069584\n",
      "Iteration 48, loss = 0.26246842\n",
      "Iteration 44, loss = 0.26150002\n",
      "Iteration 49, loss = 0.26388140\n",
      "Iteration 45, loss = 0.26746038\n",
      "Iteration 50, loss = 0.26252999\n",
      "Iteration 46, loss = 0.26419753\n",
      "Iteration 51, loss = 0.26603044\n",
      "Iteration 47, loss = 0.26130085\n",
      "Iteration 52, loss = 0.26498028\n",
      "Iteration 48, loss = 0.26124450\n",
      "Iteration 53, loss = 0.26340968\n",
      "Iteration 49, loss = 0.26243517\n",
      "Iteration 50, loss = 0.26151433\n",
      "Iteration 54, loss = 0.26341726\n",
      "Iteration 51, loss = 0.26521534\n",
      "Iteration 52, loss = 0.26252515\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 55, loss = 0.26316184\n",
      "Iteration 56, loss = 0.26468836\n",
      "Iteration 1, loss = 0.31888857\n",
      "Iteration 57, loss = 0.26638623\n",
      "Iteration 2, loss = 0.28783691\n",
      "Iteration 58, loss = 0.26287448\n",
      "Iteration 3, loss = 0.28548947\n",
      "Iteration 59, loss = 0.26338354\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 4, loss = 0.28254488\n",
      "Iteration 5, loss = 0.28023194\n",
      "Iteration 1, loss = 0.31855388\n",
      "Iteration 6, loss = 0.27782824\n",
      "Iteration 2, loss = 0.28594952\n",
      "Iteration 7, loss = 0.27834801\n",
      "Iteration 3, loss = 0.28457616\n",
      "Iteration 8, loss = 0.27794429\n",
      "Iteration 4, loss = 0.28273295\n",
      "Iteration 9, loss = 0.27631825\n",
      "Iteration 5, loss = 0.27874054\n",
      "Iteration 10, loss = 0.27411893\n",
      "Iteration 6, loss = 0.27680974\n",
      "Iteration 11, loss = 0.27431713\n",
      "Iteration 7, loss = 0.27520830\n",
      "Iteration 12, loss = 0.27313959\n",
      "Iteration 8, loss = 0.27463566\n",
      "Iteration 13, loss = 0.27356804\n",
      "Iteration 9, loss = 0.27881783\n",
      "Iteration 14, loss = 0.27243617\n",
      "Iteration 10, loss = 0.27219675\n",
      "Iteration 15, loss = 0.27245971\n",
      "Iteration 11, loss = 0.27283447\n",
      "Iteration 16, loss = 0.27224308\n",
      "Iteration 12, loss = 0.27286367\n",
      "Iteration 17, loss = 0.27137549\n",
      "Iteration 13, loss = 0.26976627\n",
      "Iteration 14, loss = 0.26812107\n",
      "Iteration 15, loss = 0.26913708\n",
      "Iteration 18, loss = 0.27291282\n",
      "Iteration 16, loss = 0.26868823\n",
      "Iteration 17, loss = 0.26653473\n",
      "Iteration 19, loss = 0.27139710\n",
      "Iteration 18, loss = 0.26681328\n",
      "Iteration 20, loss = 0.26974071\n",
      "Iteration 19, loss = 0.26580986\n",
      "Iteration 21, loss = 0.27164532\n",
      "Iteration 20, loss = 0.26565008\n",
      "Iteration 22, loss = 0.27142358\n",
      "Iteration 21, loss = 0.26718758\n",
      "Iteration 23, loss = 0.26987287\n",
      "Iteration 22, loss = 0.27107942\n",
      "Iteration 24, loss = 0.27002395\n",
      "Iteration 23, loss = 0.26640173\n",
      "Iteration 24, loss = 0.26501582\n",
      "Iteration 25, loss = 0.26995526\n",
      "Iteration 25, loss = 0.26488429\n",
      "Iteration 26, loss = 0.27166753Iteration 26, loss = 0.26781582\n",
      "\n",
      "Iteration 27, loss = 0.26741654\n",
      "Iteration 27, loss = 0.26948634\n",
      "Iteration 28, loss = 0.26497174\n",
      "Iteration 28, loss = 0.26848325\n",
      "Iteration 29, loss = 0.26450675\n",
      "Iteration 29, loss = 0.26875103\n",
      "Iteration 30, loss = 0.26436453\n",
      "Iteration 30, loss = 0.26755323\n",
      "Iteration 31, loss = 0.26491966\n",
      "Iteration 31, loss = 0.26807444\n",
      "Iteration 32, loss = 0.26936387\n",
      "Iteration 32, loss = 0.27112418\n",
      "Iteration 33, loss = 0.26671427\n",
      "Iteration 34, loss = 0.26485276\n",
      "Iteration 33, loss = 0.26996149\n",
      "Iteration 35, loss = 0.26599646\n",
      "Iteration 34, loss = 0.26830065\n",
      "Iteration 36, loss = 0.26559196\n",
      "Iteration 37, loss = 0.26467473\n",
      "Iteration 35, loss = 0.26865727\n",
      "Iteration 38, loss = 0.26390738\n",
      "Iteration 36, loss = 0.26833455\n",
      "Iteration 39, loss = 0.26518765\n",
      "Iteration 40, loss = 0.26512034\n",
      "Iteration 37, loss = 0.26694928\n",
      "Iteration 41, loss = 0.26549668\n",
      "Iteration 42, loss = 0.26345785\n",
      "Iteration 38, loss = 0.26828506\n",
      "Iteration 43, loss = 0.26661148\n",
      "Iteration 44, loss = 0.26286777\n",
      "Iteration 39, loss = 0.26663859\n",
      "Iteration 40, loss = 0.26827247\n",
      "Iteration 45, loss = 0.26804349\n",
      "Iteration 41, loss = 0.26700708\n",
      "Iteration 46, loss = 0.26652696Iteration 42, loss = 0.27038255\n",
      "\n",
      "Iteration 43, loss = 0.27074661\n",
      "Iteration 44, loss = 0.26825238\n",
      "Iteration 47, loss = 0.26387793\n",
      "Iteration 45, loss = 0.27150503\n",
      "Iteration 46, loss = 0.26985979\n",
      "Iteration 47, loss = 0.26788908\n",
      "Iteration 48, loss = 0.26351074\n",
      "Iteration 48, loss = 0.26686912\n",
      "Iteration 49, loss = 0.26461708\n",
      "Iteration 49, loss = 0.26767040\n",
      "Iteration 50, loss = 0.26404675\n",
      "Iteration 50, loss = 0.26682054\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 51, loss = 0.26598067\n",
      "Iteration 52, loss = 0.26485840\n",
      "Iteration 53, loss = 0.26318330Iteration 1, loss = 0.32039205\n",
      "\n",
      "Iteration 54, loss = 0.26291231Iteration 2, loss = 0.28675723\n",
      "\n",
      "Iteration 3, loss = 0.28437360\n",
      "Iteration 55, loss = 0.26300941\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 4, loss = 0.28086055\n",
      "Iteration 5, loss = 0.27987791\n",
      "Iteration 1, loss = 0.31759071\n",
      "Iteration 6, loss = 0.27982124\n",
      "Iteration 2, loss = 0.28434007\n",
      "Iteration 7, loss = 0.27859989\n",
      "Iteration 3, loss = 0.27928884\n",
      "Iteration 4, loss = 0.27802599\n",
      "Iteration 8, loss = 0.27761598\n",
      "Iteration 5, loss = 0.27711261\n",
      "Iteration 9, loss = 0.28112599\n",
      "Iteration 6, loss = 0.27578675\n",
      "Iteration 10, loss = 0.27577128\n",
      "Iteration 11, loss = 0.27653256\n",
      "Iteration 7, loss = 0.27526956\n",
      "Iteration 12, loss = 0.27662192\n",
      "Iteration 8, loss = 0.27290357\n",
      "Iteration 9, loss = 0.27354366\n",
      "Iteration 13, loss = 0.27396439\n",
      "Iteration 10, loss = 0.27264157\n",
      "Iteration 11, loss = 0.27139407\n",
      "Iteration 14, loss = 0.27345913\n",
      "Iteration 12, loss = 0.27332174\n",
      "Iteration 15, loss = 0.27327217\n",
      "Iteration 13, loss = 0.27052238\n",
      "Iteration 16, loss = 0.27399079\n",
      "Iteration 14, loss = 0.26973088\n",
      "Iteration 17, loss = 0.27093473\n",
      "Iteration 15, loss = 0.26787082\n",
      "Iteration 18, loss = 0.27249621\n",
      "Iteration 16, loss = 0.26730882\n",
      "Iteration 19, loss = 0.27175475\n",
      "Iteration 17, loss = 0.26716387\n",
      "Iteration 20, loss = 0.27230347\n",
      "Iteration 18, loss = 0.26760900\n",
      "Iteration 19, loss = 0.26767581\n",
      "Iteration 21, loss = 0.27273225\n",
      "Iteration 20, loss = 0.26700666\n",
      "Iteration 22, loss = 0.27626585\n",
      "Iteration 21, loss = 0.26643235\n",
      "Iteration 23, loss = 0.27111263\n",
      "Iteration 22, loss = 0.26479964\n",
      "Iteration 24, loss = 0.27083769\n",
      "Iteration 23, loss = 0.26534304\n",
      "Iteration 25, loss = 0.27083266\n",
      "Iteration 24, loss = 0.26577572\n",
      "Iteration 26, loss = 0.27324361\n",
      "Iteration 25, loss = 0.26409075\n",
      "Iteration 27, loss = 0.27182777\n",
      "Iteration 26, loss = 0.26491116\n",
      "Iteration 28, loss = 0.27195626\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 27, loss = 0.26417172\n",
      "Iteration 28, loss = 0.26414123\n",
      "Iteration 1, loss = 0.32051036\n",
      "Iteration 29, loss = 0.26512532\n",
      "Iteration 2, loss = 0.28464216\n",
      "Iteration 30, loss = 0.26394937\n",
      "Iteration 3, loss = 0.27935823\n",
      "Iteration 31, loss = 0.26364893\n",
      "Iteration 4, loss = 0.27754665Iteration 32, loss = 0.26324325\n",
      "\n",
      "Iteration 33, loss = 0.26426432\n",
      "Iteration 5, loss = 0.27476397\n",
      "Iteration 34, loss = 0.26327608\n",
      "Iteration 6, loss = 0.27442958\n",
      "Iteration 35, loss = 0.26417783\n",
      "Iteration 7, loss = 0.27365105\n",
      "Iteration 36, loss = 0.26350491\n",
      "Iteration 8, loss = 0.27187145\n",
      "Iteration 37, loss = 0.26254986\n",
      "Iteration 9, loss = 0.27449424\n",
      "Iteration 38, loss = 0.26328523\n",
      "Iteration 10, loss = 0.27290597\n",
      "Iteration 39, loss = 0.26325015\n",
      "Iteration 11, loss = 0.27153631\n",
      "Iteration 40, loss = 0.26234563\n",
      "Iteration 12, loss = 0.27346812\n",
      "Iteration 41, loss = 0.26199650\n",
      "Iteration 13, loss = 0.26985393\n",
      "Iteration 42, loss = 0.26231431\n",
      "Iteration 14, loss = 0.26923218\n",
      "Iteration 43, loss = 0.26139983\n",
      "Iteration 15, loss = 0.26905465\n",
      "Iteration 44, loss = 0.26143612\n",
      "Iteration 16, loss = 0.27001211\n",
      "Iteration 45, loss = 0.26175946\n",
      "Iteration 17, loss = 0.26819501\n",
      "Iteration 46, loss = 0.26110918\n",
      "Iteration 18, loss = 0.26852717\n",
      "Iteration 47, loss = 0.26227807\n",
      "Iteration 19, loss = 0.26724616\n",
      "Iteration 48, loss = 0.26475791\n",
      "Iteration 20, loss = 0.26705091\n",
      "Iteration 49, loss = 0.26121947\n",
      "Iteration 21, loss = 0.26534215\n",
      "Iteration 50, loss = 0.26167860\n",
      "Iteration 22, loss = 0.26576747\n",
      "Iteration 51, loss = 0.26164000\n",
      "Iteration 52, loss = 0.26157875\n",
      "Iteration 23, loss = 0.26720428\n",
      "Iteration 53, loss = 0.26133924\n",
      "Iteration 24, loss = 0.26612631\n",
      "Iteration 54, loss = 0.26060062\n",
      "Iteration 25, loss = 0.26543280\n",
      "Iteration 55, loss = 0.26077556\n",
      "Iteration 26, loss = 0.26357379\n",
      "Iteration 56, loss = 0.26315112\n",
      "Iteration 27, loss = 0.26441536\n",
      "Iteration 57, loss = 0.26156005\n",
      "Iteration 28, loss = 0.26521907\n",
      "Iteration 58, loss = 0.26034617\n",
      "Iteration 29, loss = 0.26447642\n",
      "Iteration 59, loss = 0.26095554\n",
      "Iteration 30, loss = 0.26205719\n",
      "Iteration 60, loss = 0.26286905\n",
      "Iteration 31, loss = 0.26206047\n",
      "Iteration 61, loss = 0.26088810\n",
      "Iteration 32, loss = 0.26334788\n",
      "Iteration 62, loss = 0.26004878\n",
      "Iteration 33, loss = 0.26521687\n",
      "Iteration 63, loss = 0.26336012\n",
      "Iteration 34, loss = 0.26294543\n",
      "Iteration 35, loss = 0.26412375\n",
      "Iteration 64, loss = 0.26177018\n",
      "Iteration 36, loss = 0.26224945\n",
      "Iteration 37, loss = 0.26338930\n",
      "Iteration 65, loss = 0.26111168\n",
      "Iteration 38, loss = 0.26376444\n",
      "Iteration 66, loss = 0.26098933\n",
      "Iteration 39, loss = 0.26253190\n",
      "Iteration 67, loss = 0.26090435\n",
      "Iteration 40, loss = 0.26313886\n",
      "Iteration 68, loss = 0.26024880\n",
      "Iteration 41, loss = 0.26315246\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 69, loss = 0.26040926\n",
      "Iteration 70, loss = 0.26337649\n",
      "Iteration 1, loss = 0.31836048\n",
      "Iteration 71, loss = 0.26273507\n",
      "Iteration 2, loss = 0.28310853\n",
      "Iteration 72, loss = 0.26095985\n",
      "Iteration 3, loss = 0.27930979\n",
      "Iteration 73, loss = 0.26270700\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 4, loss = 0.27945850\n",
      "Iteration 1, loss = 0.31861541\n",
      "Iteration 5, loss = 0.27483232\n",
      "Iteration 6, loss = 0.27400476\n",
      "Iteration 2, loss = 0.28429291\n",
      "Iteration 7, loss = 0.27251549\n",
      "Iteration 3, loss = 0.28089193\n",
      "Iteration 8, loss = 0.27118455\n",
      "Iteration 4, loss = 0.27922543\n",
      "Iteration 5, loss = 0.27630495\n",
      "Iteration 9, loss = 0.27108331\n",
      "Iteration 6, loss = 0.27574103\n",
      "Iteration 10, loss = 0.26871926\n",
      "Iteration 7, loss = 0.27605950\n",
      "Iteration 11, loss = 0.27113267\n",
      "Iteration 12, loss = 0.27446006\n",
      "Iteration 8, loss = 0.27339467\n",
      "Iteration 9, loss = 0.27197147Iteration 13, loss = 0.27199000\n",
      "\n",
      "Iteration 10, loss = 0.27299708\n",
      "Iteration 14, loss = 0.27083496\n",
      "Iteration 11, loss = 0.27195869\n",
      "Iteration 15, loss = 0.26887466\n",
      "Iteration 12, loss = 0.27350971\n",
      "Iteration 16, loss = 0.26863675\n",
      "Iteration 13, loss = 0.27203569\n",
      "Iteration 17, loss = 0.26729430\n",
      "Iteration 14, loss = 0.27317905\n",
      "Iteration 18, loss = 0.26661442\n",
      "Iteration 15, loss = 0.27161179\n",
      "Iteration 19, loss = 0.26648584\n",
      "Iteration 16, loss = 0.27089429\n",
      "Iteration 20, loss = 0.26682058\n",
      "Iteration 17, loss = 0.27034419\n",
      "Iteration 21, loss = 0.26829917\n",
      "Iteration 18, loss = 0.26958163\n",
      "Iteration 22, loss = 0.26579535\n",
      "Iteration 19, loss = 0.26838875\n",
      "Iteration 23, loss = 0.26677358\n",
      "Iteration 20, loss = 0.26833463\n",
      "Iteration 24, loss = 0.26703730\n",
      "Iteration 21, loss = 0.27038329\n",
      "Iteration 25, loss = 0.26634653\n",
      "Iteration 22, loss = 0.26913616\n",
      "Iteration 23, loss = 0.26717359\n",
      "Iteration 26, loss = 0.26533740\n",
      "Iteration 27, loss = 0.26667760Iteration 24, loss = 0.26685483\n",
      "\n",
      "Iteration 25, loss = 0.26685040\n",
      "Iteration 26, loss = 0.26578710\n",
      "Iteration 28, loss = 0.26583118\n",
      "Iteration 27, loss = 0.26603854\n",
      "Iteration 29, loss = 0.26521455\n",
      "Iteration 30, loss = 0.26524180\n",
      "Iteration 28, loss = 0.26636597\n",
      "Iteration 31, loss = 0.26536192\n",
      "Iteration 29, loss = 0.26423763\n",
      "Iteration 32, loss = 0.26488728\n",
      "Iteration 30, loss = 0.26358471\n",
      "Iteration 33, loss = 0.26799688\n",
      "Iteration 31, loss = 0.26415797\n",
      "Iteration 34, loss = 0.26652638\n",
      "Iteration 32, loss = 0.26384932\n",
      "Iteration 35, loss = 0.26846246\n",
      "Iteration 33, loss = 0.26535229\n",
      "Iteration 36, loss = 0.26727109\n",
      "Iteration 34, loss = 0.26436242\n",
      "Iteration 37, loss = 0.26596442\n",
      "Iteration 35, loss = 0.26444663\n",
      "Iteration 38, loss = 0.26562798\n",
      "Iteration 36, loss = 0.26413285\n",
      "Iteration 39, loss = 0.26505003\n",
      "Iteration 37, loss = 0.26626400\n",
      "Iteration 40, loss = 0.26619001\n",
      "Iteration 38, loss = 0.26341625\n",
      "Iteration 41, loss = 0.26693332\n",
      "Iteration 39, loss = 0.26240146\n",
      "Iteration 42, loss = 0.26518447\n",
      "Iteration 40, loss = 0.26362021\n",
      "Iteration 43, loss = 0.26572096\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 41, loss = 0.26194606\n",
      "Iteration 42, loss = 0.26253747\n",
      "Iteration 43, loss = 0.26401495Iteration 1, loss = 0.31560265\n",
      "\n",
      "Iteration 44, loss = 0.26701945\n",
      "Iteration 2, loss = 0.28362682\n",
      "Iteration 45, loss = 0.26333357\n",
      "Iteration 3, loss = 0.27980905\n",
      "Iteration 46, loss = 0.26261150\n",
      "Iteration 4, loss = 0.28066348\n",
      "Iteration 47, loss = 0.26133745\n",
      "Iteration 5, loss = 0.27652797\n",
      "Iteration 48, loss = 0.26408069\n",
      "Iteration 6, loss = 0.27480725\n",
      "Iteration 49, loss = 0.26225323\n",
      "Iteration 7, loss = 0.27573241\n",
      "Iteration 50, loss = 0.26248360\n",
      "Iteration 8, loss = 0.27190700\n",
      "Iteration 51, loss = 0.26320775\n",
      "Iteration 9, loss = 0.26991473\n",
      "Iteration 52, loss = 0.26128826\n",
      "Iteration 10, loss = 0.27019266\n",
      "Iteration 53, loss = 0.26140338\n",
      "Iteration 11, loss = 0.27030660\n",
      "Iteration 54, loss = 0.26070254\n",
      "Iteration 12, loss = 0.27060271\n",
      "Iteration 13, loss = 0.27002948\n",
      "Iteration 55, loss = 0.26190755\n",
      "Iteration 14, loss = 0.26891350\n",
      "Iteration 15, loss = 0.26699498\n",
      "Iteration 56, loss = 0.26195943\n",
      "Iteration 16, loss = 0.26693940\n",
      "Iteration 17, loss = 0.26533232\n",
      "Iteration 57, loss = 0.26057663\n",
      "Iteration 18, loss = 0.26584641\n",
      "Iteration 58, loss = 0.26097880\n",
      "Iteration 19, loss = 0.26449780\n",
      "Iteration 59, loss = 0.26176319\n",
      "Iteration 20, loss = 0.26453454\n",
      "Iteration 60, loss = 0.26134177\n",
      "Iteration 21, loss = 0.26429863\n",
      "Iteration 61, loss = 0.26035503\n",
      "Iteration 22, loss = 0.26386305\n",
      "Iteration 62, loss = 0.26027039\n",
      "Iteration 23, loss = 0.26337859\n",
      "Iteration 63, loss = 0.26091664\n",
      "Iteration 24, loss = 0.26498957\n",
      "Iteration 64, loss = 0.26092166\n",
      "Iteration 25, loss = 0.26518567\n",
      "Iteration 65, loss = 0.26223668\n",
      "Iteration 26, loss = 0.26225680\n",
      "Iteration 66, loss = 0.25971810\n",
      "Iteration 27, loss = 0.26411514\n",
      "Iteration 67, loss = 0.26210061\n",
      "Iteration 28, loss = 0.26395845\n",
      "Iteration 68, loss = 0.26132470\n",
      "Iteration 29, loss = 0.26279955\n",
      "Iteration 69, loss = 0.25973939\n",
      "Iteration 30, loss = 0.26146301\n",
      "Iteration 70, loss = 0.26062359\n",
      "Iteration 71, loss = 0.26001424\n",
      "Iteration 31, loss = 0.26174574\n",
      "Iteration 72, loss = 0.26062013\n",
      "Iteration 32, loss = 0.26150567\n",
      "Iteration 73, loss = 0.26201942\n",
      "Iteration 33, loss = 0.26452616\n",
      "Iteration 74, loss = 0.26264316\n",
      "Iteration 34, loss = 0.26144497\n",
      "Iteration 75, loss = 0.26178976\n",
      "Iteration 35, loss = 0.26454514\n",
      "Iteration 76, loss = 0.26093652\n",
      "Iteration 36, loss = 0.26348054\n",
      "Iteration 77, loss = 0.25987905\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 37, loss = 0.26200284\n",
      "Iteration 38, loss = 0.26205138\n",
      "Iteration 39, loss = 0.26035939\n",
      "Iteration 40, loss = 0.26398515\n",
      "Iteration 41, loss = 0.26102129\n",
      "Iteration 42, loss = 0.26070075\n",
      "Iteration 43, loss = 0.26165034\n",
      "Iteration 44, loss = 0.26248721\n",
      "Iteration 45, loss = 0.26161165\n",
      "Iteration 46, loss = 0.26203611\n",
      "Iteration 47, loss = 0.26054394\n",
      "Iteration 48, loss = 0.26373068\n",
      "Iteration 49, loss = 0.26217891\n",
      "Iteration 50, loss = 0.26084518\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.32059711\n",
      "Iteration 1, loss = 0.31744781\n",
      "Iteration 2, loss = 0.28855890\n",
      "Iteration 2, loss = 0.28660440\n",
      "Iteration 3, loss = 0.28770031\n",
      "Iteration 3, loss = 0.28492702\n",
      "Iteration 4, loss = 0.28121080\n",
      "Iteration 5, loss = 0.27863141\n",
      "Iteration 4, loss = 0.28379612\n",
      "Iteration 6, loss = 0.27707011\n",
      "Iteration 5, loss = 0.28161332\n",
      "Iteration 7, loss = 0.27683968\n",
      "Iteration 6, loss = 0.27904360\n",
      "Iteration 7, loss = 0.27875079\n",
      "Iteration 8, loss = 0.27593906\n",
      "Iteration 8, loss = 0.27817089\n",
      "Iteration 9, loss = 0.27599164\n",
      "Iteration 9, loss = 0.27586208\n",
      "Iteration 10, loss = 0.27374979\n",
      "Iteration 10, loss = 0.27303120\n",
      "Iteration 11, loss = 0.27582705\n",
      "Iteration 11, loss = 0.27472442\n",
      "Iteration 12, loss = 0.27370295\n",
      "Iteration 12, loss = 0.27283075\n",
      "Iteration 13, loss = 0.27444899\n",
      "Iteration 13, loss = 0.27323834\n",
      "Iteration 14, loss = 0.27319271\n",
      "Iteration 14, loss = 0.27281832\n",
      "Iteration 15, loss = 0.27311845\n",
      "Iteration 15, loss = 0.27093354\n",
      "Iteration 16, loss = 0.27372544\n",
      "Iteration 16, loss = 0.27356322\n",
      "Iteration 17, loss = 0.27155326\n",
      "Iteration 17, loss = 0.27009409\n",
      "Iteration 18, loss = 0.27193279\n",
      "Iteration 18, loss = 0.27124663\n",
      "Iteration 19, loss = 0.27101220\n",
      "Iteration 19, loss = 0.26981081\n",
      "Iteration 20, loss = 0.27148837\n",
      "Iteration 20, loss = 0.26987085\n",
      "Iteration 21, loss = 0.27130939\n",
      "Iteration 21, loss = 0.26965707\n",
      "Iteration 22, loss = 0.27081892\n",
      "Iteration 22, loss = 0.26948292\n",
      "Iteration 23, loss = 0.27172650\n",
      "Iteration 23, loss = 0.26991009\n",
      "Iteration 24, loss = 0.27140145\n",
      "Iteration 24, loss = 0.26857956\n",
      "Iteration 25, loss = 0.27079402\n",
      "Iteration 25, loss = 0.26689775\n",
      "Iteration 26, loss = 0.27117981\n",
      "Iteration 26, loss = 0.26873930\n",
      "Iteration 27, loss = 0.26920045\n",
      "Iteration 27, loss = 0.26698990\n",
      "Iteration 28, loss = 0.26993655\n",
      "Iteration 28, loss = 0.26739737\n",
      "Iteration 29, loss = 0.26920139\n",
      "Iteration 29, loss = 0.26659285\n",
      "Iteration 30, loss = 0.26977185\n",
      "Iteration 30, loss = 0.26747101\n",
      "Iteration 31, loss = 0.26865485\n",
      "Iteration 31, loss = 0.26691758\n",
      "Iteration 32, loss = 0.26957984\n",
      "Iteration 32, loss = 0.26590393\n",
      "Iteration 33, loss = 0.26776653\n",
      "Iteration 33, loss = 0.26511019\n",
      "Iteration 34, loss = 0.26759303\n",
      "Iteration 34, loss = 0.26491838\n",
      "Iteration 35, loss = 0.26808581\n",
      "Iteration 35, loss = 0.26619177\n",
      "Iteration 36, loss = 0.26998169\n",
      "Iteration 36, loss = 0.26723539\n",
      "Iteration 37, loss = 0.27004923\n",
      "Iteration 37, loss = 0.26579071\n",
      "Iteration 38, loss = 0.26852738\n",
      "Iteration 38, loss = 0.26487496\n",
      "Iteration 39, loss = 0.26665584\n",
      "Iteration 39, loss = 0.26510340\n",
      "Iteration 40, loss = 0.26701404\n",
      "Iteration 40, loss = 0.26469613\n",
      "Iteration 41, loss = 0.26638453\n",
      "Iteration 41, loss = 0.26442576\n",
      "Iteration 42, loss = 0.26769166\n",
      "Iteration 42, loss = 0.26514344\n",
      "Iteration 43, loss = 0.26787477\n",
      "Iteration 43, loss = 0.26561012\n",
      "Iteration 44, loss = 0.26672569\n",
      "Iteration 44, loss = 0.26541358\n",
      "Iteration 45, loss = 0.26654267\n",
      "Iteration 45, loss = 0.26580563\n",
      "Iteration 46, loss = 0.26831046\n",
      "Iteration 46, loss = 0.26525961\n",
      "Iteration 47, loss = 0.26759904\n",
      "Iteration 48, loss = 0.26812845\n",
      "Iteration 47, loss = 0.26417595\n",
      "Iteration 49, loss = 0.26640252\n",
      "Iteration 48, loss = 0.26392650\n",
      "Iteration 50, loss = 0.26578467\n",
      "Iteration 49, loss = 0.26364679\n",
      "Iteration 51, loss = 0.26658717\n",
      "Iteration 50, loss = 0.26435034\n",
      "Iteration 52, loss = 0.26673432\n",
      "Iteration 51, loss = 0.26461749\n",
      "Iteration 53, loss = 0.26847873\n",
      "Iteration 52, loss = 0.26495124\n",
      "Iteration 54, loss = 0.26914779\n",
      "Iteration 55, loss = 0.26706288\n",
      "Iteration 53, loss = 0.26453739\n",
      "Iteration 56, loss = 0.26666369\n",
      "Iteration 54, loss = 0.26653091\n",
      "Iteration 57, loss = 0.26578309\n",
      "Iteration 55, loss = 0.26614020\n",
      "Iteration 58, loss = 0.26646275\n",
      "Iteration 56, loss = 0.26487897\n",
      "Iteration 59, loss = 0.26632567\n",
      "Iteration 57, loss = 0.26405562\n",
      "Iteration 60, loss = 0.26706641\n",
      "Iteration 61, loss = 0.26542615\n",
      "Iteration 58, loss = 0.26367345\n",
      "Iteration 62, loss = 0.26754841\n",
      "Iteration 59, loss = 0.26537471\n",
      "Iteration 63, loss = 0.26607664\n",
      "Iteration 64, loss = 0.26933526\n",
      "Iteration 65, loss = 0.26697620\n",
      "Iteration 60, loss = 0.26567001\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 66, loss = 0.26609025\n",
      "Iteration 67, loss = 0.26692765\n",
      "Iteration 1, loss = 0.31628991\n",
      "Iteration 68, loss = 0.26766808\n",
      "Iteration 2, loss = 0.28882003\n",
      "Iteration 69, loss = 0.26618496\n",
      "Iteration 3, loss = 0.28833901\n",
      "Iteration 70, loss = 0.26853055\n",
      "Iteration 4, loss = 0.28442030\n",
      "Iteration 71, loss = 0.26677880\n",
      "Iteration 72, loss = 0.26545209\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 5, loss = 0.28458475\n",
      "Iteration 6, loss = 0.28037748\n",
      "Iteration 1, loss = 0.31687760\n",
      "Iteration 2, loss = 0.28876809\n",
      "Iteration 7, loss = 0.28051429\n",
      "Iteration 3, loss = 0.28634825\n",
      "Iteration 8, loss = 0.28040046\n",
      "Iteration 4, loss = 0.28407809\n",
      "Iteration 9, loss = 0.27922940\n",
      "Iteration 10, loss = 0.27639842\n",
      "Iteration 5, loss = 0.28252466\n",
      "Iteration 6, loss = 0.28104218\n",
      "Iteration 11, loss = 0.27710908\n",
      "Iteration 7, loss = 0.28034026\n",
      "Iteration 12, loss = 0.27671844Iteration 8, loss = 0.28078992\n",
      "\n",
      "Iteration 9, loss = 0.27822151\n",
      "Iteration 13, loss = 0.27546248\n",
      "Iteration 10, loss = 0.27619161\n",
      "Iteration 14, loss = 0.27597344\n",
      "Iteration 11, loss = 0.27679983\n",
      "Iteration 15, loss = 0.27463390\n",
      "Iteration 12, loss = 0.27654446\n",
      "Iteration 16, loss = 0.27843833\n",
      "Iteration 13, loss = 0.27479198\n",
      "Iteration 17, loss = 0.27421821\n",
      "Iteration 14, loss = 0.27352540\n",
      "Iteration 18, loss = 0.27344302\n",
      "Iteration 15, loss = 0.27445417\n",
      "Iteration 19, loss = 0.27229596\n",
      "Iteration 16, loss = 0.27437717\n",
      "Iteration 20, loss = 0.27248300\n",
      "Iteration 17, loss = 0.27284894\n",
      "Iteration 21, loss = 0.27226499\n",
      "Iteration 18, loss = 0.27400953\n",
      "Iteration 22, loss = 0.27211806\n",
      "Iteration 19, loss = 0.27205605\n",
      "Iteration 20, loss = 0.27236574\n",
      "Iteration 23, loss = 0.27023453\n",
      "Iteration 24, loss = 0.27246472\n",
      "Iteration 21, loss = 0.27218383\n",
      "Iteration 22, loss = 0.27348815\n",
      "Iteration 25, loss = 0.27053188\n",
      "Iteration 23, loss = 0.27123618\n",
      "Iteration 26, loss = 0.27192777\n",
      "Iteration 24, loss = 0.27466605\n",
      "Iteration 27, loss = 0.27043158\n",
      "Iteration 25, loss = 0.27102145\n",
      "Iteration 28, loss = 0.26949278\n",
      "Iteration 26, loss = 0.27179481\n",
      "Iteration 29, loss = 0.26966144\n",
      "Iteration 27, loss = 0.27071441\n",
      "Iteration 30, loss = 0.27019885\n",
      "Iteration 28, loss = 0.26966903\n",
      "Iteration 31, loss = 0.27018494\n",
      "Iteration 29, loss = 0.26952992\n",
      "Iteration 32, loss = 0.27000880\n",
      "Iteration 30, loss = 0.27095758\n",
      "Iteration 33, loss = 0.26893593\n",
      "Iteration 31, loss = 0.27051729\n",
      "Iteration 34, loss = 0.26925988\n",
      "Iteration 32, loss = 0.27073881\n",
      "Iteration 35, loss = 0.26920502\n",
      "Iteration 33, loss = 0.27064601\n",
      "Iteration 36, loss = 0.27022138\n",
      "Iteration 34, loss = 0.26951862\n",
      "Iteration 37, loss = 0.27326258\n",
      "Iteration 35, loss = 0.26923763\n",
      "Iteration 38, loss = 0.26966008\n",
      "Iteration 36, loss = 0.27125003\n",
      "Iteration 39, loss = 0.26891708\n",
      "Iteration 37, loss = 0.27066732\n",
      "Iteration 40, loss = 0.26885467\n",
      "Iteration 41, loss = 0.26857342\n",
      "Iteration 38, loss = 0.27049474\n",
      "Iteration 39, loss = 0.26930276\n",
      "Iteration 42, loss = 0.26905112\n",
      "Iteration 40, loss = 0.26947147\n",
      "Iteration 43, loss = 0.26882819\n",
      "Iteration 41, loss = 0.26896768\n",
      "Iteration 44, loss = 0.26807508\n",
      "Iteration 42, loss = 0.26927209\n",
      "Iteration 45, loss = 0.26795206\n",
      "Iteration 43, loss = 0.26865485\n",
      "Iteration 46, loss = 0.27164394\n",
      "Iteration 44, loss = 0.26861102\n",
      "Iteration 47, loss = 0.26939629\n",
      "Iteration 45, loss = 0.26778005\n",
      "Iteration 48, loss = 0.26868566\n",
      "Iteration 46, loss = 0.27075419\n",
      "Iteration 49, loss = 0.26888793\n",
      "Iteration 47, loss = 0.26939213\n",
      "Iteration 50, loss = 0.26754741\n",
      "Iteration 48, loss = 0.26948190\n",
      "Iteration 51, loss = 0.26873499\n",
      "Iteration 49, loss = 0.27080284\n",
      "Iteration 52, loss = 0.26828435\n",
      "Iteration 50, loss = 0.26906552\n",
      "Iteration 53, loss = 0.26784783\n",
      "Iteration 51, loss = 0.26937162\n",
      "Iteration 54, loss = 0.26928946\n",
      "Iteration 52, loss = 0.26759150\n",
      "Iteration 55, loss = 0.26906789\n",
      "Iteration 53, loss = 0.26938196\n",
      "Iteration 56, loss = 0.26794803\n",
      "Iteration 54, loss = 0.26947447\n",
      "Iteration 57, loss = 0.26816616\n",
      "Iteration 55, loss = 0.26967861\n",
      "Iteration 58, loss = 0.26997499\n",
      "Iteration 56, loss = 0.26831833\n",
      "Iteration 59, loss = 0.26920934\n",
      "Iteration 57, loss = 0.26783340\n",
      "Iteration 60, loss = 0.26881197\n",
      "Iteration 58, loss = 0.26876740\n",
      "Iteration 61, loss = 0.26715996\n",
      "Iteration 59, loss = 0.26876112\n",
      "Iteration 62, loss = 0.26676500\n",
      "Iteration 60, loss = 0.26960693\n",
      "Iteration 63, loss = 0.26672484\n",
      "Iteration 61, loss = 0.26780271\n",
      "Iteration 64, loss = 0.26962502\n",
      "Iteration 62, loss = 0.26774112\n",
      "Iteration 65, loss = 0.26725344\n",
      "Iteration 63, loss = 0.26781543\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 66, loss = 0.26786262\n",
      "Iteration 67, loss = 0.26843182\n",
      "Iteration 68, loss = 0.26681274\n",
      "Iteration 69, loss = 0.26872779\n",
      "Iteration 70, loss = 0.26997465\n",
      "Iteration 1, loss = 0.31503196\n",
      "Iteration 71, loss = 0.26732999\n",
      "Iteration 2, loss = 0.28821308\n",
      "Iteration 72, loss = 0.26788372\n",
      "Iteration 73, loss = 0.26718800\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 3, loss = 0.28591662\n",
      "Iteration 4, loss = 0.28414010\n",
      "Iteration 5, loss = 0.28165348\n",
      "Iteration 1, loss = 0.31307206\n",
      "Iteration 6, loss = 0.28102734\n",
      "Iteration 2, loss = 0.28719830\n",
      "Iteration 7, loss = 0.28009549\n",
      "Iteration 3, loss = 0.28362618\n",
      "Iteration 8, loss = 0.27884643\n",
      "Iteration 4, loss = 0.28145216\n",
      "Iteration 9, loss = 0.27828248\n",
      "Iteration 5, loss = 0.27904934\n",
      "Iteration 10, loss = 0.27390206\n",
      "Iteration 6, loss = 0.28262444\n",
      "Iteration 11, loss = 0.27417682\n",
      "Iteration 7, loss = 0.27775636\n",
      "Iteration 12, loss = 0.27344092\n",
      "Iteration 8, loss = 0.27732168\n",
      "Iteration 13, loss = 0.27285175Iteration 9, loss = 0.27542697\n",
      "\n",
      "Iteration 14, loss = 0.27225480\n",
      "Iteration 10, loss = 0.27377438\n",
      "Iteration 11, loss = 0.27424232\n",
      "Iteration 15, loss = 0.27284218\n",
      "Iteration 12, loss = 0.27270782\n",
      "Iteration 16, loss = 0.27100311\n",
      "Iteration 13, loss = 0.27202144\n",
      "Iteration 17, loss = 0.27135142\n",
      "Iteration 14, loss = 0.27180192\n",
      "Iteration 15, loss = 0.27213042\n",
      "Iteration 18, loss = 0.27044778\n",
      "Iteration 19, loss = 0.26999700\n",
      "Iteration 16, loss = 0.27209027\n",
      "Iteration 17, loss = 0.27083087\n",
      "Iteration 20, loss = 0.27011619\n",
      "Iteration 18, loss = 0.27055130\n",
      "Iteration 19, loss = 0.27007203\n",
      "Iteration 21, loss = 0.27003649\n",
      "Iteration 20, loss = 0.26921565\n",
      "Iteration 22, loss = 0.27012446\n",
      "Iteration 21, loss = 0.26845290\n",
      "Iteration 23, loss = 0.26866003\n",
      "Iteration 22, loss = 0.27194964\n",
      "Iteration 24, loss = 0.26957978\n",
      "Iteration 23, loss = 0.27001212\n",
      "Iteration 25, loss = 0.26894525\n",
      "Iteration 24, loss = 0.26897398\n",
      "Iteration 26, loss = 0.26924994\n",
      "Iteration 25, loss = 0.26901573\n",
      "Iteration 27, loss = 0.26745761\n",
      "Iteration 28, loss = 0.26720274\n",
      "Iteration 26, loss = 0.26978584\n",
      "Iteration 29, loss = 0.26692156\n",
      "Iteration 30, loss = 0.26695229\n",
      "Iteration 27, loss = 0.26807789\n",
      "Iteration 31, loss = 0.26760292\n",
      "Iteration 28, loss = 0.26941625\n",
      "Iteration 29, loss = 0.26834980\n",
      "Iteration 32, loss = 0.26682433Iteration 30, loss = 0.26675465\n",
      "\n",
      "Iteration 31, loss = 0.26835232\n",
      "Iteration 33, loss = 0.26815659\n",
      "Iteration 32, loss = 0.26835242\n",
      "Iteration 34, loss = 0.26729484\n",
      "Iteration 33, loss = 0.26941403\n",
      "Iteration 35, loss = 0.26662453\n",
      "Iteration 34, loss = 0.26825438\n",
      "Iteration 36, loss = 0.26673262\n",
      "Iteration 35, loss = 0.26788478\n",
      "Iteration 37, loss = 0.26744359\n",
      "Iteration 36, loss = 0.26809025\n",
      "Iteration 38, loss = 0.26575205\n",
      "Iteration 37, loss = 0.26861433\n",
      "Iteration 38, loss = 0.26817747\n",
      "Iteration 39, loss = 0.26708837\n",
      "Iteration 39, loss = 0.26733664\n",
      "Iteration 40, loss = 0.26621233\n",
      "Iteration 40, loss = 0.26775041\n",
      "Iteration 41, loss = 0.26652411\n",
      "Iteration 41, loss = 0.26803009\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 42, loss = 0.26638382\n",
      "Iteration 43, loss = 0.26523009\n",
      "Iteration 44, loss = 0.26519430\n",
      "Iteration 1, loss = 0.31388443\n",
      "Iteration 45, loss = 0.26563148\n",
      "Iteration 2, loss = 0.28883436\n",
      "Iteration 46, loss = 0.26800142\n",
      "Iteration 3, loss = 0.28512110\n",
      "Iteration 47, loss = 0.26639115\n",
      "Iteration 4, loss = 0.28235085\n",
      "Iteration 48, loss = 0.26592352\n",
      "Iteration 5, loss = 0.28164222\n",
      "Iteration 49, loss = 0.26722436\n",
      "Iteration 6, loss = 0.28380061\n",
      "Iteration 50, loss = 0.26441018\n",
      "Iteration 7, loss = 0.28020974\n",
      "Iteration 51, loss = 0.26686028\n",
      "Iteration 8, loss = 0.27964093\n",
      "Iteration 52, loss = 0.26502221\n",
      "Iteration 53, loss = 0.26543680\n",
      "Iteration 9, loss = 0.27751877\n",
      "Iteration 54, loss = 0.26630712\n",
      "Iteration 10, loss = 0.27534891\n",
      "Iteration 55, loss = 0.26519160\n",
      "Iteration 11, loss = 0.27623596\n",
      "Iteration 56, loss = 0.26498206\n",
      "Iteration 12, loss = 0.27387291\n",
      "Iteration 57, loss = 0.26437433\n",
      "Iteration 13, loss = 0.27434205\n",
      "Iteration 58, loss = 0.26686401\n",
      "Iteration 14, loss = 0.27271580\n",
      "Iteration 59, loss = 0.26583765\n",
      "Iteration 15, loss = 0.27530178\n",
      "Iteration 16, loss = 0.27379479\n",
      "Iteration 60, loss = 0.26751842\n",
      "Iteration 17, loss = 0.27153744\n",
      "Iteration 61, loss = 0.26511054\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 18, loss = 0.27199682\n",
      "Iteration 19, loss = 0.27164301\n",
      "Iteration 1, loss = 0.31103315\n",
      "Iteration 20, loss = 0.27027186\n",
      "Iteration 2, loss = 0.28748544\n",
      "Iteration 21, loss = 0.27033858\n",
      "Iteration 3, loss = 0.28598311\n",
      "Iteration 22, loss = 0.27095236\n",
      "Iteration 4, loss = 0.28132248\n",
      "Iteration 23, loss = 0.26898797\n",
      "Iteration 5, loss = 0.27748054\n",
      "Iteration 24, loss = 0.26936827\n",
      "Iteration 6, loss = 0.27784365\n",
      "Iteration 25, loss = 0.26975636\n",
      "Iteration 7, loss = 0.27643258\n",
      "Iteration 26, loss = 0.26950329\n",
      "Iteration 8, loss = 0.27496910\n",
      "Iteration 27, loss = 0.26723797\n",
      "Iteration 9, loss = 0.27579292\n",
      "Iteration 28, loss = 0.26776165\n",
      "Iteration 10, loss = 0.27355266\n",
      "Iteration 29, loss = 0.26825866\n",
      "Iteration 11, loss = 0.27305687\n",
      "Iteration 30, loss = 0.26688908\n",
      "Iteration 12, loss = 0.27252231\n",
      "Iteration 31, loss = 0.26864273\n",
      "Iteration 13, loss = 0.27448747\n",
      "Iteration 32, loss = 0.26837911\n",
      "Iteration 14, loss = 0.27189155\n",
      "Iteration 33, loss = 0.26850863\n",
      "Iteration 15, loss = 0.27198829\n",
      "Iteration 34, loss = 0.26792672\n",
      "Iteration 16, loss = 0.27156159\n",
      "Iteration 35, loss = 0.26695024\n",
      "Iteration 17, loss = 0.27015504\n",
      "Iteration 18, loss = 0.26998469\n",
      "Iteration 36, loss = 0.26740853\n",
      "Iteration 19, loss = 0.27061086\n",
      "Iteration 37, loss = 0.26960990\n",
      "Iteration 20, loss = 0.26882424\n",
      "Iteration 38, loss = 0.26780099\n",
      "Iteration 21, loss = 0.26995999\n",
      "Iteration 39, loss = 0.26762186\n",
      "Iteration 40, loss = 0.26842930\n",
      "Iteration 22, loss = 0.27116643\n",
      "Iteration 41, loss = 0.26704339\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 23, loss = 0.27116355\n",
      "Iteration 24, loss = 0.26924198\n",
      "Iteration 1, loss = 0.31138306\n",
      "Iteration 25, loss = 0.26916101\n",
      "Iteration 2, loss = 0.28599614\n",
      "Iteration 3, loss = 0.28408268\n",
      "Iteration 26, loss = 0.26832950\n",
      "Iteration 4, loss = 0.28008927\n",
      "Iteration 27, loss = 0.26809101\n",
      "Iteration 5, loss = 0.27690969\n",
      "Iteration 6, loss = 0.27525146\n",
      "Iteration 28, loss = 0.26913725\n",
      "Iteration 7, loss = 0.27485435\n",
      "Iteration 29, loss = 0.26770055\n",
      "Iteration 8, loss = 0.27216489\n",
      "Iteration 30, loss = 0.26816723\n",
      "Iteration 9, loss = 0.27257683\n",
      "Iteration 31, loss = 0.26762402\n",
      "Iteration 10, loss = 0.27081088\n",
      "Iteration 32, loss = 0.26799973\n",
      "Iteration 11, loss = 0.27195339\n",
      "Iteration 33, loss = 0.26863848\n",
      "Iteration 12, loss = 0.27059896\n",
      "Iteration 34, loss = 0.26797928\n",
      "Iteration 13, loss = 0.27218630\n",
      "Iteration 35, loss = 0.26801762\n",
      "Iteration 14, loss = 0.26929201\n",
      "Iteration 36, loss = 0.26650714\n",
      "Iteration 15, loss = 0.26852396\n",
      "Iteration 37, loss = 0.26619288\n",
      "Iteration 16, loss = 0.26917844\n",
      "Iteration 38, loss = 0.26830096\n",
      "Iteration 17, loss = 0.26660603\n",
      "Iteration 39, loss = 0.26845994\n",
      "Iteration 18, loss = 0.26725242\n",
      "Iteration 40, loss = 0.26876768\n",
      "Iteration 19, loss = 0.26815881\n",
      "Iteration 41, loss = 0.26733796\n",
      "Iteration 20, loss = 0.26673697\n",
      "Iteration 42, loss = 0.26748534\n",
      "Iteration 21, loss = 0.26800044\n",
      "Iteration 43, loss = 0.26951256\n",
      "Iteration 22, loss = 0.26907263\n",
      "Iteration 44, loss = 0.26799921\n",
      "Iteration 23, loss = 0.26605726\n",
      "Iteration 45, loss = 0.26573767\n",
      "Iteration 24, loss = 0.26560837\n",
      "Iteration 46, loss = 0.26625427\n",
      "Iteration 25, loss = 0.26735586\n",
      "Iteration 47, loss = 0.26643957\n",
      "Iteration 26, loss = 0.26685417\n",
      "Iteration 48, loss = 0.26595540\n",
      "Iteration 27, loss = 0.26623054\n",
      "Iteration 49, loss = 0.26650895\n",
      "Iteration 28, loss = 0.26632777\n",
      "Iteration 50, loss = 0.26731375\n",
      "Iteration 29, loss = 0.26560059\n",
      "Iteration 51, loss = 0.26768314\n",
      "Iteration 30, loss = 0.26587914\n",
      "Iteration 52, loss = 0.26611976\n",
      "Iteration 31, loss = 0.26496771\n",
      "Iteration 53, loss = 0.26713675\n",
      "Iteration 32, loss = 0.26535428\n",
      "Iteration 54, loss = 0.26810247\n",
      "Iteration 33, loss = 0.26707854\n",
      "Iteration 55, loss = 0.26713353\n",
      "Iteration 34, loss = 0.26493821\n",
      "Iteration 56, loss = 0.26578175\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 35, loss = 0.26512663\n",
      "Iteration 36, loss = 0.26442939\n",
      "Iteration 37, loss = 0.26433354\n",
      "Iteration 1, loss = 0.31680257\n",
      "Iteration 38, loss = 0.26396994\n",
      "Iteration 2, loss = 0.28927567\n",
      "Iteration 39, loss = 0.26452049\n",
      "Iteration 3, loss = 0.28525548\n",
      "Iteration 40, loss = 0.26404935\n",
      "Iteration 4, loss = 0.28313907\n",
      "Iteration 41, loss = 0.26384897\n",
      "Iteration 5, loss = 0.28049368\n",
      "Iteration 42, loss = 0.26553483\n",
      "Iteration 6, loss = 0.27932451\n",
      "Iteration 43, loss = 0.26684541\n",
      "Iteration 7, loss = 0.27781862\n",
      "Iteration 44, loss = 0.26489157\n",
      "Iteration 8, loss = 0.27561341\n",
      "Iteration 45, loss = 0.26428260\n",
      "Iteration 9, loss = 0.27453386\n",
      "Iteration 46, loss = 0.26466096\n",
      "Iteration 10, loss = 0.27522499\n",
      "Iteration 47, loss = 0.26344642\n",
      "Iteration 11, loss = 0.27376928\n",
      "Iteration 48, loss = 0.26405444\n",
      "Iteration 49, loss = 0.26557562\n",
      "Iteration 12, loss = 0.27355215\n",
      "Iteration 50, loss = 0.26444132\n",
      "Iteration 13, loss = 0.27636026\n",
      "Iteration 51, loss = 0.26520875\n",
      "Iteration 14, loss = 0.27256585\n",
      "Iteration 52, loss = 0.26599111\n",
      "Iteration 15, loss = 0.27212180\n",
      "Iteration 53, loss = 0.26419454\n",
      "Iteration 16, loss = 0.27280759\n",
      "Iteration 54, loss = 0.26461364\n",
      "Iteration 17, loss = 0.27072278\n",
      "Iteration 55, loss = 0.26316522\n",
      "Iteration 18, loss = 0.27041330\n",
      "Iteration 56, loss = 0.26400513\n",
      "Iteration 19, loss = 0.27133454\n",
      "Iteration 57, loss = 0.26340985\n",
      "Iteration 20, loss = 0.27084728\n",
      "Iteration 58, loss = 0.26215933\n",
      "Iteration 21, loss = 0.27210370\n",
      "Iteration 59, loss = 0.26405469\n",
      "Iteration 22, loss = 0.27310750\n",
      "Iteration 60, loss = 0.26456049\n",
      "Iteration 23, loss = 0.27255551\n",
      "Iteration 61, loss = 0.26282766\n",
      "Iteration 24, loss = 0.26920529\n",
      "Iteration 62, loss = 0.26320037\n",
      "Iteration 25, loss = 0.27218837\n",
      "Iteration 63, loss = 0.26306189\n",
      "Iteration 26, loss = 0.27032163\n",
      "Iteration 64, loss = 0.26607516\n",
      "Iteration 27, loss = 0.26932670\n",
      "Iteration 28, loss = 0.27084799\n",
      "Iteration 65, loss = 0.26357592\n",
      "Iteration 66, loss = 0.26377118\n",
      "Iteration 29, loss = 0.26944897\n",
      "Iteration 67, loss = 0.26363470\n",
      "Iteration 30, loss = 0.27012189\n",
      "Iteration 68, loss = 0.26242418\n",
      "Iteration 31, loss = 0.26966719\n",
      "Iteration 32, loss = 0.27071254\n",
      "Iteration 69, loss = 0.26594516\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 33, loss = 0.26977292\n",
      "Iteration 34, loss = 0.26917827\n",
      "Iteration 35, loss = 0.26955308\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.31474437\n",
      "Iteration 1, loss = 0.31833889\n",
      "Iteration 2, loss = 0.28785804\n",
      "Iteration 2, loss = 0.28932934\n",
      "Iteration 3, loss = 0.28536457Iteration 3, loss = 0.28672253\n",
      "\n",
      "Iteration 4, loss = 0.28577507\n",
      "Iteration 4, loss = 0.28451836\n",
      "Iteration 5, loss = 0.28443879\n",
      "Iteration 5, loss = 0.28481356\n",
      "Iteration 6, loss = 0.28346858\n",
      "Iteration 6, loss = 0.28201138\n",
      "Iteration 7, loss = 0.28214251\n",
      "Iteration 7, loss = 0.28157006\n",
      "Iteration 8, loss = 0.28096280\n",
      "Iteration 8, loss = 0.28114269\n",
      "Iteration 9, loss = 0.27967135\n",
      "Iteration 9, loss = 0.27989334\n",
      "Iteration 10, loss = 0.27767502\n",
      "Iteration 10, loss = 0.27950245\n",
      "Iteration 11, loss = 0.27851824\n",
      "Iteration 11, loss = 0.27910624\n",
      "Iteration 12, loss = 0.27906706\n",
      "Iteration 12, loss = 0.27810365\n",
      "Iteration 13, loss = 0.27691305\n",
      "Iteration 13, loss = 0.27666687\n",
      "Iteration 14, loss = 0.27675039\n",
      "Iteration 14, loss = 0.27716962\n",
      "Iteration 15, loss = 0.27388941\n",
      "Iteration 15, loss = 0.27609437\n",
      "Iteration 16, loss = 0.27501268\n",
      "Iteration 16, loss = 0.27544946\n",
      "Iteration 17, loss = 0.27660641\n",
      "Iteration 17, loss = 0.27604003\n",
      "Iteration 18, loss = 0.27417203\n",
      "Iteration 18, loss = 0.27523313\n",
      "Iteration 19, loss = 0.27367800\n",
      "Iteration 19, loss = 0.27395231\n",
      "Iteration 20, loss = 0.27335628\n",
      "Iteration 20, loss = 0.27415277\n",
      "Iteration 21, loss = 0.27261883\n",
      "Iteration 21, loss = 0.27507184\n",
      "Iteration 22, loss = 0.27327132\n",
      "Iteration 22, loss = 0.27438782\n",
      "Iteration 23, loss = 0.27296788\n",
      "Iteration 23, loss = 0.27508333\n",
      "Iteration 24, loss = 0.27200182\n",
      "Iteration 24, loss = 0.27450769\n",
      "Iteration 25, loss = 0.27239396\n",
      "Iteration 25, loss = 0.27530198\n",
      "Iteration 26, loss = 0.27508731\n",
      "Iteration 26, loss = 0.27501891\n",
      "Iteration 27, loss = 0.27177479\n",
      "Iteration 27, loss = 0.27321525\n",
      "Iteration 28, loss = 0.27250485\n",
      "Iteration 28, loss = 0.27404500\n",
      "Iteration 29, loss = 0.27268128\n",
      "Iteration 29, loss = 0.27432376\n",
      "Iteration 30, loss = 0.27349339\n",
      "Iteration 30, loss = 0.27447645\n",
      "Iteration 31, loss = 0.27311656\n",
      "Iteration 31, loss = 0.27461857\n",
      "Iteration 32, loss = 0.27238350\n",
      "Iteration 32, loss = 0.27406251\n",
      "Iteration 33, loss = 0.27292814\n",
      "Iteration 33, loss = 0.27375225\n",
      "Iteration 34, loss = 0.27104939\n",
      "Iteration 34, loss = 0.27255710\n",
      "Iteration 35, loss = 0.27290722\n",
      "Iteration 35, loss = 0.27292159\n",
      "Iteration 36, loss = 0.27066025\n",
      "Iteration 36, loss = 0.27261881\n",
      "Iteration 37, loss = 0.27015025\n",
      "Iteration 37, loss = 0.27202652\n",
      "Iteration 38, loss = 0.27081042\n",
      "Iteration 38, loss = 0.27348187\n",
      "Iteration 39, loss = 0.27067680\n",
      "Iteration 39, loss = 0.27139061\n",
      "Iteration 40, loss = 0.27103741\n",
      "Iteration 40, loss = 0.27174004\n",
      "Iteration 41, loss = 0.27147171\n",
      "Iteration 42, loss = 0.27308489\n",
      "Iteration 41, loss = 0.27259212\n",
      "Iteration 43, loss = 0.27261821\n",
      "Iteration 42, loss = 0.27395128\n",
      "Iteration 44, loss = 0.27078232\n",
      "Iteration 43, loss = 0.27311957\n",
      "Iteration 45, loss = 0.27076194\n",
      "Iteration 44, loss = 0.27193718\n",
      "Iteration 46, loss = 0.27079068\n",
      "Iteration 45, loss = 0.27244341\n",
      "Iteration 47, loss = 0.27152496\n",
      "Iteration 46, loss = 0.27220885\n",
      "Iteration 47, loss = 0.27348083\n",
      "Iteration 48, loss = 0.27181054\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 48, loss = 0.27182914\n",
      "Iteration 49, loss = 0.27292369\n",
      "Iteration 1, loss = 0.31658356\n",
      "Iteration 50, loss = 0.27099745\n",
      "Iteration 2, loss = 0.28599958\n",
      "Iteration 51, loss = 0.27148652\n",
      "Iteration 3, loss = 0.28403000\n",
      "Iteration 52, loss = 0.27232012\n",
      "Iteration 4, loss = 0.28393916\n",
      "Iteration 5, loss = 0.28108986\n",
      "Iteration 53, loss = 0.27188702\n",
      "Iteration 6, loss = 0.27931013\n",
      "Iteration 7, loss = 0.27797092\n",
      "Iteration 54, loss = 0.27096049\n",
      "Iteration 8, loss = 0.27813029\n",
      "Iteration 55, loss = 0.27190344\n",
      "Iteration 9, loss = 0.27787567\n",
      "Iteration 56, loss = 0.27250965\n",
      "Iteration 10, loss = 0.27641909\n",
      "Iteration 57, loss = 0.27329541\n",
      "Iteration 11, loss = 0.27527394\n",
      "Iteration 12, loss = 0.27768849Iteration 58, loss = 0.27204611\n",
      "\n",
      "Iteration 59, loss = 0.27070155\n",
      "Iteration 13, loss = 0.27521157\n",
      "Iteration 60, loss = 0.27072208\n",
      "Iteration 14, loss = 0.27596003\n",
      "Iteration 61, loss = 0.27121918\n",
      "Iteration 15, loss = 0.27408831\n",
      "Iteration 62, loss = 0.27150813\n",
      "Iteration 16, loss = 0.27530301\n",
      "Iteration 63, loss = 0.27121449\n",
      "Iteration 17, loss = 0.27548188\n",
      "Iteration 64, loss = 0.27266200\n",
      "Iteration 18, loss = 0.27379221\n",
      "Iteration 65, loss = 0.27207210\n",
      "Iteration 19, loss = 0.27325278\n",
      "Iteration 20, loss = 0.27317092Iteration 66, loss = 0.27240913\n",
      "\n",
      "Iteration 21, loss = 0.27295929\n",
      "Iteration 67, loss = 0.27127733\n",
      "Iteration 68, loss = 0.27169348Iteration 22, loss = 0.27309374\n",
      "\n",
      "Iteration 23, loss = 0.27294640\n",
      "Iteration 69, loss = 0.27249470\n",
      "Iteration 24, loss = 0.27259695\n",
      "Iteration 70, loss = 0.27290192\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 25, loss = 0.27297832\n",
      "Iteration 26, loss = 0.27432882\n",
      "Iteration 1, loss = 0.31804505\n",
      "Iteration 27, loss = 0.27137068\n",
      "Iteration 2, loss = 0.28830873\n",
      "Iteration 28, loss = 0.27244374\n",
      "Iteration 3, loss = 0.28540649\n",
      "Iteration 29, loss = 0.27205724\n",
      "Iteration 4, loss = 0.28353896\n",
      "Iteration 30, loss = 0.27254762\n",
      "Iteration 5, loss = 0.28281897\n",
      "Iteration 31, loss = 0.27246697\n",
      "Iteration 6, loss = 0.28221908\n",
      "Iteration 32, loss = 0.27231776\n",
      "Iteration 33, loss = 0.27227684\n",
      "Iteration 7, loss = 0.27902487\n",
      "Iteration 8, loss = 0.27947326\n",
      "Iteration 34, loss = 0.27164347\n",
      "Iteration 9, loss = 0.27854709\n",
      "Iteration 35, loss = 0.27278240\n",
      "Iteration 36, loss = 0.27175496\n",
      "Iteration 10, loss = 0.27853699\n",
      "Iteration 37, loss = 0.27125995\n",
      "Iteration 11, loss = 0.27903760\n",
      "Iteration 12, loss = 0.27802711\n",
      "Iteration 38, loss = 0.27138679\n",
      "Iteration 39, loss = 0.27034911\n",
      "Iteration 13, loss = 0.27512142\n",
      "Iteration 40, loss = 0.27117652\n",
      "Iteration 14, loss = 0.27681700\n",
      "Iteration 41, loss = 0.27134866\n",
      "Iteration 15, loss = 0.27644821\n",
      "Iteration 42, loss = 0.27252444\n",
      "Iteration 16, loss = 0.27575845\n",
      "Iteration 43, loss = 0.27221062\n",
      "Iteration 17, loss = 0.27670690\n",
      "Iteration 44, loss = 0.27067354\n",
      "Iteration 18, loss = 0.27560818\n",
      "Iteration 19, loss = 0.27435036\n",
      "Iteration 45, loss = 0.27099396\n",
      "Iteration 46, loss = 0.27230876\n",
      "Iteration 20, loss = 0.27472792\n",
      "Iteration 47, loss = 0.27258435\n",
      "Iteration 21, loss = 0.27365768\n",
      "Iteration 48, loss = 0.27100069\n",
      "Iteration 22, loss = 0.27476681\n",
      "Iteration 49, loss = 0.27234024\n",
      "Iteration 23, loss = 0.27445667\n",
      "Iteration 50, loss = 0.27035260\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 24, loss = 0.27396883\n",
      "Iteration 25, loss = 0.27343132\n",
      "Iteration 1, loss = 0.31814392\n",
      "Iteration 26, loss = 0.27335930\n",
      "Iteration 2, loss = 0.28997500\n",
      "Iteration 27, loss = 0.27210247\n",
      "Iteration 28, loss = 0.27279411\n",
      "Iteration 3, loss = 0.28768779\n",
      "Iteration 29, loss = 0.27284124\n",
      "Iteration 4, loss = 0.28564479\n",
      "Iteration 30, loss = 0.27237772\n",
      "Iteration 5, loss = 0.28300420\n",
      "Iteration 31, loss = 0.27191175\n",
      "Iteration 6, loss = 0.28122218\n",
      "Iteration 32, loss = 0.27243913\n",
      "Iteration 7, loss = 0.27998671\n",
      "Iteration 33, loss = 0.27161931\n",
      "Iteration 8, loss = 0.27863095\n",
      "Iteration 34, loss = 0.27141443\n",
      "Iteration 9, loss = 0.27876501\n",
      "Iteration 35, loss = 0.27219618\n",
      "Iteration 10, loss = 0.27725931\n",
      "Iteration 36, loss = 0.27063757\n",
      "Iteration 11, loss = 0.27740775\n",
      "Iteration 37, loss = 0.27143367\n",
      "Iteration 12, loss = 0.27869532\n",
      "Iteration 38, loss = 0.27201638\n",
      "Iteration 13, loss = 0.27626081\n",
      "Iteration 39, loss = 0.27112587\n",
      "Iteration 14, loss = 0.27629044\n",
      "Iteration 40, loss = 0.27094104\n",
      "Iteration 15, loss = 0.27555844\n",
      "Iteration 41, loss = 0.27047996\n",
      "Iteration 16, loss = 0.27673746\n",
      "Iteration 42, loss = 0.27163348\n",
      "Iteration 17, loss = 0.27616163\n",
      "Iteration 43, loss = 0.27142895\n",
      "Iteration 18, loss = 0.27557023\n",
      "Iteration 44, loss = 0.27123073\n",
      "Iteration 19, loss = 0.27396604\n",
      "Iteration 45, loss = 0.27123188\n",
      "Iteration 20, loss = 0.27397274\n",
      "Iteration 46, loss = 0.27268167\n",
      "Iteration 21, loss = 0.27414461\n",
      "Iteration 47, loss = 0.27255353\n",
      "Iteration 22, loss = 0.27392440\n",
      "Iteration 48, loss = 0.27009938\n",
      "Iteration 23, loss = 0.27469665\n",
      "Iteration 49, loss = 0.27158846\n",
      "Iteration 24, loss = 0.27359671\n",
      "Iteration 50, loss = 0.27075747\n",
      "Iteration 25, loss = 0.27392884\n",
      "Iteration 51, loss = 0.26969839\n",
      "Iteration 26, loss = 0.27390538\n",
      "Iteration 52, loss = 0.27198530\n",
      "Iteration 27, loss = 0.27287656\n",
      "Iteration 53, loss = 0.27050037\n",
      "Iteration 28, loss = 0.27466588\n",
      "Iteration 54, loss = 0.26908690\n",
      "Iteration 29, loss = 0.27446658\n",
      "Iteration 55, loss = 0.26929189\n",
      "Iteration 30, loss = 0.27320445\n",
      "Iteration 56, loss = 0.27023013\n",
      "Iteration 31, loss = 0.27331502\n",
      "Iteration 57, loss = 0.26898072\n",
      "Iteration 32, loss = 0.27365526\n",
      "Iteration 58, loss = 0.26929129\n",
      "Iteration 33, loss = 0.27382818\n",
      "Iteration 59, loss = 0.27005323\n",
      "Iteration 34, loss = 0.27275828\n",
      "Iteration 60, loss = 0.26989878\n",
      "Iteration 35, loss = 0.27314618\n",
      "Iteration 61, loss = 0.27038689\n",
      "Iteration 36, loss = 0.27223490\n",
      "Iteration 62, loss = 0.26956168\n",
      "Iteration 37, loss = 0.27207267\n",
      "Iteration 63, loss = 0.27184914\n",
      "Iteration 38, loss = 0.27217117\n",
      "Iteration 64, loss = 0.27111657\n",
      "Iteration 39, loss = 0.27244286\n",
      "Iteration 65, loss = 0.27022672\n",
      "Iteration 40, loss = 0.27188733\n",
      "Iteration 66, loss = 0.26903949\n",
      "Iteration 67, loss = 0.27077321\n",
      "Iteration 41, loss = 0.27356669\n",
      "Iteration 42, loss = 0.27453274\n",
      "Iteration 68, loss = 0.26987887\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 43, loss = 0.27225426\n",
      "Iteration 44, loss = 0.27165119\n",
      "Iteration 1, loss = 0.31685691\n",
      "Iteration 45, loss = 0.27216333\n",
      "Iteration 2, loss = 0.28720081\n",
      "Iteration 46, loss = 0.27279982\n",
      "Iteration 3, loss = 0.28523723\n",
      "Iteration 47, loss = 0.27392189\n",
      "Iteration 4, loss = 0.28182446\n",
      "Iteration 48, loss = 0.27244742\n",
      "Iteration 5, loss = 0.28106823\n",
      "Iteration 49, loss = 0.27333886\n",
      "Iteration 6, loss = 0.27915543\n",
      "Iteration 50, loss = 0.27248169\n",
      "Iteration 7, loss = 0.27804806\n",
      "Iteration 51, loss = 0.27267253\n",
      "Iteration 8, loss = 0.27697698\n",
      "Iteration 52, loss = 0.27434565\n",
      "Iteration 9, loss = 0.27564215\n",
      "Iteration 10, loss = 0.27364245\n",
      "Iteration 53, loss = 0.27256840\n",
      "Iteration 11, loss = 0.27446423\n",
      "Iteration 54, loss = 0.27093577\n",
      "Iteration 12, loss = 0.27423430\n",
      "Iteration 55, loss = 0.27071904\n",
      "Iteration 13, loss = 0.27191853\n",
      "Iteration 56, loss = 0.27190572\n",
      "Iteration 14, loss = 0.27206402\n",
      "Iteration 57, loss = 0.27121222\n",
      "Iteration 15, loss = 0.27037828\n",
      "Iteration 58, loss = 0.27121335\n",
      "Iteration 16, loss = 0.27163947\n",
      "Iteration 59, loss = 0.27125415\n",
      "Iteration 17, loss = 0.27059362\n",
      "Iteration 60, loss = 0.27183391\n",
      "Iteration 18, loss = 0.27096063\n",
      "Iteration 61, loss = 0.27263509\n",
      "Iteration 19, loss = 0.26979685\n",
      "Iteration 62, loss = 0.27136431\n",
      "Iteration 20, loss = 0.27012540\n",
      "Iteration 63, loss = 0.27188534\n",
      "Iteration 21, loss = 0.26975570\n",
      "Iteration 64, loss = 0.27250275\n",
      "Iteration 22, loss = 0.26942860\n",
      "Iteration 65, loss = 0.27308138\n",
      "Iteration 23, loss = 0.26993685\n",
      "Iteration 66, loss = 0.27196184\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 24, loss = 0.26908575\n",
      "Iteration 25, loss = 0.26924712\n",
      "Iteration 1, loss = 0.31841108\n",
      "Iteration 26, loss = 0.26948792\n",
      "Iteration 2, loss = 0.28986716\n",
      "Iteration 27, loss = 0.26782901\n",
      "Iteration 3, loss = 0.28775621\n",
      "Iteration 28, loss = 0.26935948\n",
      "Iteration 4, loss = 0.28372380\n",
      "Iteration 29, loss = 0.26957477\n",
      "Iteration 5, loss = 0.28322595\n",
      "Iteration 30, loss = 0.26808904\n",
      "Iteration 6, loss = 0.28297101\n",
      "Iteration 31, loss = 0.26806771\n",
      "Iteration 7, loss = 0.28143474\n",
      "Iteration 32, loss = 0.26840789\n",
      "Iteration 8, loss = 0.28102287\n",
      "Iteration 33, loss = 0.26754668\n",
      "Iteration 9, loss = 0.28020217\n",
      "Iteration 34, loss = 0.26821079\n",
      "Iteration 10, loss = 0.27779120\n",
      "Iteration 35, loss = 0.26818405\n",
      "Iteration 36, loss = 0.26789620\n",
      "Iteration 11, loss = 0.27871497\n",
      "Iteration 37, loss = 0.26771401\n",
      "Iteration 12, loss = 0.28137957\n",
      "Iteration 38, loss = 0.26804467\n",
      "Iteration 13, loss = 0.27660771\n",
      "Iteration 39, loss = 0.26750128\n",
      "Iteration 14, loss = 0.27627485\n",
      "Iteration 40, loss = 0.26728559\n",
      "Iteration 15, loss = 0.27584769\n",
      "Iteration 41, loss = 0.26793231\n",
      "Iteration 16, loss = 0.27661715\n",
      "Iteration 42, loss = 0.26854775\n",
      "Iteration 17, loss = 0.27628972\n",
      "Iteration 43, loss = 0.26777885\n",
      "Iteration 18, loss = 0.27672838\n",
      "Iteration 44, loss = 0.26816815\n",
      "Iteration 19, loss = 0.27612669\n",
      "Iteration 45, loss = 0.26904528\n",
      "Iteration 20, loss = 0.27546223\n",
      "Iteration 46, loss = 0.26805917\n",
      "Iteration 21, loss = 0.27583739\n",
      "Iteration 47, loss = 0.26836900\n",
      "Iteration 22, loss = 0.27443897\n",
      "Iteration 48, loss = 0.26683911\n",
      "Iteration 23, loss = 0.27477483\n",
      "Iteration 49, loss = 0.26884159\n",
      "Iteration 24, loss = 0.27448331\n",
      "Iteration 50, loss = 0.26780321\n",
      "Iteration 25, loss = 0.27501962\n",
      "Iteration 51, loss = 0.26711507\n",
      "Iteration 26, loss = 0.27297737\n",
      "Iteration 52, loss = 0.26911782\n",
      "Iteration 27, loss = 0.27299733\n",
      "Iteration 53, loss = 0.26774334\n",
      "Iteration 28, loss = 0.27288543\n",
      "Iteration 54, loss = 0.26691299\n",
      "Iteration 29, loss = 0.27477812\n",
      "Iteration 55, loss = 0.26814678\n",
      "Iteration 30, loss = 0.27327402\n",
      "Iteration 56, loss = 0.26735535\n",
      "Iteration 31, loss = 0.27292349\n",
      "Iteration 57, loss = 0.26773897\n",
      "Iteration 32, loss = 0.27247776\n",
      "Iteration 33, loss = 0.27321863\n",
      "Iteration 58, loss = 0.26737806\n",
      "Iteration 34, loss = 0.27378422\n",
      "Iteration 35, loss = 0.27282238\n",
      "Iteration 59, loss = 0.26745234\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 36, loss = 0.27242769\n",
      "Iteration 1, loss = 0.31881992\n",
      "Iteration 37, loss = 0.27283603\n",
      "Iteration 2, loss = 0.28942625\n",
      "Iteration 38, loss = 0.27258013\n",
      "Iteration 3, loss = 0.28603392\n",
      "Iteration 39, loss = 0.27376342\n",
      "Iteration 4, loss = 0.28219504\n",
      "Iteration 40, loss = 0.27234733\n",
      "Iteration 5, loss = 0.28106809\n",
      "Iteration 41, loss = 0.27464802\n",
      "Iteration 6, loss = 0.28098178\n",
      "Iteration 42, loss = 0.27359532\n",
      "Iteration 7, loss = 0.27872270\n",
      "Iteration 8, loss = 0.28025931\n",
      "Iteration 43, loss = 0.27292698\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 9, loss = 0.27844688\n",
      "Iteration 10, loss = 0.27671224\n",
      "Iteration 1, loss = 0.31832613\n",
      "Iteration 11, loss = 0.27921418\n",
      "Iteration 2, loss = 0.28953501\n",
      "Iteration 12, loss = 0.27906398\n",
      "Iteration 3, loss = 0.28701093\n",
      "Iteration 13, loss = 0.27595612\n",
      "Iteration 4, loss = 0.28243406\n",
      "Iteration 14, loss = 0.27601758\n",
      "Iteration 15, loss = 0.27519765Iteration 5, loss = 0.28169887\n",
      "\n",
      "Iteration 16, loss = 0.27592572\n",
      "Iteration 6, loss = 0.28120982\n",
      "Iteration 7, loss = 0.27983364\n",
      "Iteration 17, loss = 0.27574232\n",
      "Iteration 8, loss = 0.28034897\n",
      "Iteration 18, loss = 0.27612163\n",
      "Iteration 9, loss = 0.27927062\n",
      "Iteration 19, loss = 0.27553361\n",
      "Iteration 10, loss = 0.27762759\n",
      "Iteration 11, loss = 0.27853097\n",
      "Iteration 20, loss = 0.27430657\n",
      "Iteration 12, loss = 0.28109608\n",
      "Iteration 21, loss = 0.27347825\n",
      "Iteration 13, loss = 0.27660695\n",
      "Iteration 22, loss = 0.27302617\n",
      "Iteration 14, loss = 0.27666296\n",
      "Iteration 23, loss = 0.27389695\n",
      "Iteration 15, loss = 0.27610885\n",
      "Iteration 24, loss = 0.27324018\n",
      "Iteration 16, loss = 0.27604492\n",
      "Iteration 25, loss = 0.27271324\n",
      "Iteration 17, loss = 0.27586248\n",
      "Iteration 18, loss = 0.27566128\n",
      "Iteration 26, loss = 0.27195470\n",
      "Iteration 19, loss = 0.27535926\n",
      "Iteration 27, loss = 0.27078439\n",
      "Iteration 20, loss = 0.27555869\n",
      "Iteration 28, loss = 0.27182241\n",
      "Iteration 21, loss = 0.27395059\n",
      "Iteration 29, loss = 0.27494001\n",
      "Iteration 22, loss = 0.27442578\n",
      "Iteration 30, loss = 0.27270449\n",
      "Iteration 23, loss = 0.27393099\n",
      "Iteration 31, loss = 0.27125230\n",
      "Iteration 24, loss = 0.27330675\n",
      "Iteration 32, loss = 0.27099842\n",
      "Iteration 25, loss = 0.27329053\n",
      "Iteration 33, loss = 0.27191359\n",
      "Iteration 26, loss = 0.27119730\n",
      "Iteration 34, loss = 0.27054948\n",
      "Iteration 27, loss = 0.27242244\n",
      "Iteration 35, loss = 0.27129416\n",
      "Iteration 28, loss = 0.27215718\n",
      "Iteration 36, loss = 0.27055320\n",
      "Iteration 29, loss = 0.27318637\n",
      "Iteration 37, loss = 0.27048251\n",
      "Iteration 30, loss = 0.27187123\n",
      "Iteration 38, loss = 0.27098139\n",
      "Iteration 31, loss = 0.27211659\n",
      "Iteration 39, loss = 0.27055995\n",
      "Iteration 32, loss = 0.27154535\n",
      "Iteration 33, loss = 0.27250315Iteration 40, loss = 0.27006392\n",
      "\n",
      "Iteration 34, loss = 0.27139319\n",
      "Iteration 41, loss = 0.27063732\n",
      "Iteration 35, loss = 0.27198809\n",
      "Iteration 42, loss = 0.27134341\n",
      "Iteration 36, loss = 0.27036549\n",
      "Iteration 43, loss = 0.27069841\n",
      "Iteration 37, loss = 0.27126314\n",
      "Iteration 44, loss = 0.26983222\n",
      "Iteration 38, loss = 0.27025445\n",
      "Iteration 45, loss = 0.27070881\n",
      "Iteration 39, loss = 0.27067931\n",
      "Iteration 46, loss = 0.27086305\n",
      "Iteration 40, loss = 0.27074258\n",
      "Iteration 47, loss = 0.27027965\n",
      "Iteration 41, loss = 0.27253799\n",
      "Iteration 48, loss = 0.27041049\n",
      "Iteration 42, loss = 0.27106614\n",
      "Iteration 49, loss = 0.27443281\n",
      "Iteration 43, loss = 0.27150783\n",
      "Iteration 50, loss = 0.27018330\n",
      "Iteration 44, loss = 0.26950995\n",
      "Iteration 51, loss = 0.26958790\n",
      "Iteration 45, loss = 0.27190949\n",
      "Iteration 52, loss = 0.26976286\n",
      "Iteration 53, loss = 0.27122069\n",
      "Iteration 46, loss = 0.27062072\n",
      "Iteration 47, loss = 0.27114352\n",
      "Iteration 48, loss = 0.27109359\n",
      "Iteration 54, loss = 0.26986517\n",
      "Iteration 49, loss = 0.27435307\n",
      "Iteration 55, loss = 0.26991986\n",
      "Iteration 50, loss = 0.27119256\n",
      "Iteration 56, loss = 0.26986714\n",
      "Iteration 51, loss = 0.27007223\n",
      "Iteration 57, loss = 0.26906317\n",
      "Iteration 52, loss = 0.27035677\n",
      "Iteration 58, loss = 0.26994271\n",
      "Iteration 53, loss = 0.27086405\n",
      "Iteration 59, loss = 0.26926733\n",
      "Iteration 54, loss = 0.27062056\n",
      "Iteration 55, loss = 0.27001409\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 60, loss = 0.27268808\n",
      "Iteration 61, loss = 0.26903621\n",
      "Iteration 1, loss = 0.31865354\n",
      "Iteration 62, loss = 0.26920464\n",
      "Iteration 2, loss = 0.29021019\n",
      "Iteration 63, loss = 0.27043777\n",
      "Iteration 3, loss = 0.28804393\n",
      "Iteration 64, loss = 0.26987877\n",
      "Iteration 4, loss = 0.28510538\n",
      "Iteration 65, loss = 0.26881090\n",
      "Iteration 66, loss = 0.26900233\n",
      "Iteration 5, loss = 0.28438820\n",
      "Iteration 6, loss = 0.28436738\n",
      "Iteration 67, loss = 0.27141227\n",
      "Iteration 7, loss = 0.28183424\n",
      "Iteration 68, loss = 0.27007788\n",
      "Iteration 8, loss = 0.28156657\n",
      "Iteration 69, loss = 0.26899520\n",
      "Iteration 9, loss = 0.28189204\n",
      "Iteration 70, loss = 0.26821123Iteration 10, loss = 0.28062271\n",
      "\n",
      "Iteration 11, loss = 0.28145137\n",
      "Iteration 71, loss = 0.26914429\n",
      "Iteration 12, loss = 0.28080772\n",
      "Iteration 72, loss = 0.27310699\n",
      "Iteration 13, loss = 0.27948624\n",
      "Iteration 73, loss = 0.26935863\n",
      "Iteration 14, loss = 0.27886848\n",
      "Iteration 74, loss = 0.26851354\n",
      "Iteration 15, loss = 0.27922883\n",
      "Iteration 75, loss = 0.27031485\n",
      "Iteration 16, loss = 0.27619251\n",
      "Iteration 76, loss = 0.27013983\n",
      "Iteration 17, loss = 0.27711573\n",
      "Iteration 77, loss = 0.27069313\n",
      "Iteration 18, loss = 0.27727968\n",
      "Iteration 19, loss = 0.27985559\n",
      "Iteration 78, loss = 0.27026428\n",
      "Iteration 20, loss = 0.27767033\n",
      "Iteration 79, loss = 0.26863717Iteration 21, loss = 0.27715246\n",
      "\n",
      "Iteration 22, loss = 0.27646387\n",
      "Iteration 23, loss = 0.27583889\n",
      "Iteration 80, loss = 0.26875334\n",
      "Iteration 24, loss = 0.27607777\n",
      "Iteration 81, loss = 0.26939192\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 25, loss = 0.27630709\n",
      "Iteration 26, loss = 0.27452794\n",
      "Iteration 27, loss = 0.27587957\n",
      "Iteration 28, loss = 0.27432465\n",
      "Iteration 29, loss = 0.27614401\n",
      "Iteration 30, loss = 0.27456921\n",
      "Iteration 31, loss = 0.27464712\n",
      "Iteration 32, loss = 0.27435157\n",
      "Iteration 33, loss = 0.27543896\n",
      "Iteration 34, loss = 0.27494943\n",
      "Iteration 35, loss = 0.27430763\n",
      "Iteration 36, loss = 0.27484746\n",
      "Iteration 37, loss = 0.27555964\n",
      "Iteration 38, loss = 0.27378010\n",
      "Iteration 39, loss = 0.27468356\n",
      "Iteration 40, loss = 0.27405856\n",
      "Iteration 41, loss = 0.27411248\n",
      "Iteration 42, loss = 0.27361282\n",
      "Iteration 43, loss = 0.27333411\n",
      "Iteration 44, loss = 0.27415377\n",
      "Iteration 45, loss = 0.27535506\n",
      "Iteration 46, loss = 0.27466368\n",
      "Iteration 47, loss = 0.27385835\n",
      "Iteration 48, loss = 0.27372172\n",
      "Iteration 49, loss = 0.27788655\n",
      "Iteration 50, loss = 0.27426191\n",
      "Iteration 51, loss = 0.27261627\n",
      "Iteration 52, loss = 0.27174604\n",
      "Iteration 53, loss = 0.27309114\n",
      "Iteration 54, loss = 0.27332145\n",
      "Iteration 55, loss = 0.27273965\n",
      "Iteration 56, loss = 0.27266271\n",
      "Iteration 57, loss = 0.27263786\n",
      "Iteration 58, loss = 0.27300056\n",
      "Iteration 59, loss = 0.27260976\n",
      "Iteration 60, loss = 0.27273115\n",
      "Iteration 61, loss = 0.27437322\n",
      "Iteration 62, loss = 0.27302419\n",
      "Iteration 63, loss = 0.27438890\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.31611417\n",
      "Iteration 1, loss = 0.31835579\n",
      "Iteration 2, loss = 0.28881939\n",
      "Iteration 2, loss = 0.29009877\n",
      "Iteration 3, loss = 0.28421882\n",
      "Iteration 3, loss = 0.28603655\n",
      "Iteration 4, loss = 0.28120228\n",
      "Iteration 4, loss = 0.28365905\n",
      "Iteration 5, loss = 0.28139833\n",
      "Iteration 5, loss = 0.28425517\n",
      "Iteration 6, loss = 0.28056108\n",
      "Iteration 6, loss = 0.28378331\n",
      "Iteration 7, loss = 0.27928340\n",
      "Iteration 7, loss = 0.28082220\n",
      "Iteration 8, loss = 0.27860363\n",
      "Iteration 8, loss = 0.28063842\n",
      "Iteration 9, loss = 0.27674424\n",
      "Iteration 9, loss = 0.27894483\n",
      "Iteration 10, loss = 0.27602123\n",
      "Iteration 10, loss = 0.27841905\n",
      "Iteration 11, loss = 0.27626166\n",
      "Iteration 11, loss = 0.27782031\n",
      "Iteration 12, loss = 0.27563007\n",
      "Iteration 12, loss = 0.27701406\n",
      "Iteration 13, loss = 0.27506408\n",
      "Iteration 13, loss = 0.27732240\n",
      "Iteration 14, loss = 0.27554874\n",
      "Iteration 14, loss = 0.27617159\n",
      "Iteration 15, loss = 0.27685072\n",
      "Iteration 15, loss = 0.27635966\n",
      "Iteration 16, loss = 0.27539920\n",
      "Iteration 16, loss = 0.27580101\n",
      "Iteration 17, loss = 0.27403381\n",
      "Iteration 17, loss = 0.27524974\n",
      "Iteration 18, loss = 0.27444439\n",
      "Iteration 18, loss = 0.27548349\n",
      "Iteration 19, loss = 0.27489592\n",
      "Iteration 19, loss = 0.27534035\n",
      "Iteration 20, loss = 0.27308323\n",
      "Iteration 20, loss = 0.27290581\n",
      "Iteration 21, loss = 0.27387914\n",
      "Iteration 21, loss = 0.27350998\n",
      "Iteration 22, loss = 0.27445270\n",
      "Iteration 22, loss = 0.27463384\n",
      "Iteration 23, loss = 0.27303456\n",
      "Iteration 23, loss = 0.27177667\n",
      "Iteration 24, loss = 0.27329052\n",
      "Iteration 24, loss = 0.27343800\n",
      "Iteration 25, loss = 0.27417419\n",
      "Iteration 25, loss = 0.27363833\n",
      "Iteration 26, loss = 0.27294918\n",
      "Iteration 26, loss = 0.27168974\n",
      "Iteration 27, loss = 0.27273752\n",
      "Iteration 27, loss = 0.27245992\n",
      "Iteration 28, loss = 0.27162581\n",
      "Iteration 28, loss = 0.27176289\n",
      "Iteration 29, loss = 0.27236357\n",
      "Iteration 29, loss = 0.27210552\n",
      "Iteration 30, loss = 0.27218573\n",
      "Iteration 30, loss = 0.27129573\n",
      "Iteration 31, loss = 0.27150187\n",
      "Iteration 31, loss = 0.27151161\n",
      "Iteration 32, loss = 0.27201065\n",
      "Iteration 32, loss = 0.27151689\n",
      "Iteration 33, loss = 0.27127359\n",
      "Iteration 33, loss = 0.27162946\n",
      "Iteration 34, loss = 0.27159179\n",
      "Iteration 34, loss = 0.27173906\n",
      "Iteration 35, loss = 0.27138833\n",
      "Iteration 35, loss = 0.27161581\n",
      "Iteration 36, loss = 0.27023450Iteration 36, loss = 0.27122036\n",
      "\n",
      "Iteration 37, loss = 0.26988839\n",
      "Iteration 37, loss = 0.27030275\n",
      "Iteration 38, loss = 0.27040965\n",
      "Iteration 38, loss = 0.27168251\n",
      "Iteration 39, loss = 0.27167872\n",
      "Iteration 39, loss = 0.27335537\n",
      "Iteration 40, loss = 0.27185702\n",
      "Iteration 40, loss = 0.27135517\n",
      "Iteration 41, loss = 0.27210587\n",
      "Iteration 41, loss = 0.27271651\n",
      "Iteration 42, loss = 0.27067816\n",
      "Iteration 42, loss = 0.27069702\n",
      "Iteration 43, loss = 0.27158172\n",
      "Iteration 43, loss = 0.27217809\n",
      "Iteration 44, loss = 0.27095462\n",
      "Iteration 44, loss = 0.27038680\n",
      "Iteration 45, loss = 0.27151896\n",
      "Iteration 45, loss = 0.27041912\n",
      "Iteration 46, loss = 0.27091651\n",
      "Iteration 46, loss = 0.27143161\n",
      "Iteration 47, loss = 0.27042819\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 47, loss = 0.27129545\n",
      "Iteration 48, loss = 0.27204770\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.31753875\n",
      "Iteration 2, loss = 0.29095357\n",
      "Iteration 1, loss = 0.31429496\n",
      "Iteration 3, loss = 0.29207166\n",
      "Iteration 4, loss = 0.28591168\n",
      "Iteration 2, loss = 0.28815992\n",
      "Iteration 5, loss = 0.28295095\n",
      "Iteration 3, loss = 0.28969681\n",
      "Iteration 6, loss = 0.28364516\n",
      "Iteration 4, loss = 0.28322183\n",
      "Iteration 7, loss = 0.28307850\n",
      "Iteration 5, loss = 0.28183655\n",
      "Iteration 8, loss = 0.28235431\n",
      "Iteration 6, loss = 0.28220352\n",
      "Iteration 9, loss = 0.27973916\n",
      "Iteration 7, loss = 0.28057954\n",
      "Iteration 10, loss = 0.28062617\n",
      "Iteration 8, loss = 0.27983879\n",
      "Iteration 11, loss = 0.27884124\n",
      "Iteration 9, loss = 0.27819169\n",
      "Iteration 12, loss = 0.27841422\n",
      "Iteration 10, loss = 0.27973118\n",
      "Iteration 13, loss = 0.27823143\n",
      "Iteration 11, loss = 0.27893123\n",
      "Iteration 14, loss = 0.27875597\n",
      "Iteration 12, loss = 0.27758886\n",
      "Iteration 15, loss = 0.27778221\n",
      "Iteration 13, loss = 0.27671029\n",
      "Iteration 16, loss = 0.27864839\n",
      "Iteration 14, loss = 0.28019845\n",
      "Iteration 17, loss = 0.27821897\n",
      "Iteration 15, loss = 0.27755221\n",
      "Iteration 18, loss = 0.27698747\n",
      "Iteration 16, loss = 0.27707633\n",
      "Iteration 19, loss = 0.27897370\n",
      "Iteration 17, loss = 0.27702262\n",
      "Iteration 20, loss = 0.27701686\n",
      "Iteration 18, loss = 0.27732354\n",
      "Iteration 21, loss = 0.27750383\n",
      "Iteration 19, loss = 0.27748047\n",
      "Iteration 22, loss = 0.27710825\n",
      "Iteration 20, loss = 0.27620049\n",
      "Iteration 23, loss = 0.27797197\n",
      "Iteration 24, loss = 0.27678944\n",
      "Iteration 21, loss = 0.27742537\n",
      "Iteration 25, loss = 0.27668378\n",
      "Iteration 22, loss = 0.27686253\n",
      "Iteration 26, loss = 0.27690619\n",
      "Iteration 23, loss = 0.27662050\n",
      "Iteration 27, loss = 0.27499457\n",
      "Iteration 24, loss = 0.27671560\n",
      "Iteration 28, loss = 0.27489794\n",
      "Iteration 25, loss = 0.27590951\n",
      "Iteration 29, loss = 0.27449064\n",
      "Iteration 26, loss = 0.27723097\n",
      "Iteration 30, loss = 0.27499913\n",
      "Iteration 27, loss = 0.27497373\n",
      "Iteration 31, loss = 0.27526884\n",
      "Iteration 28, loss = 0.27503708\n",
      "Iteration 32, loss = 0.27614607\n",
      "Iteration 29, loss = 0.27614073\n",
      "Iteration 33, loss = 0.27590997\n",
      "Iteration 34, loss = 0.27548762\n",
      "Iteration 30, loss = 0.27558414\n",
      "Iteration 35, loss = 0.27411110\n",
      "Iteration 31, loss = 0.27480976\n",
      "Iteration 36, loss = 0.27523353\n",
      "Iteration 32, loss = 0.27474226\n",
      "Iteration 37, loss = 0.27512776\n",
      "Iteration 33, loss = 0.27510573\n",
      "Iteration 38, loss = 0.27539848\n",
      "Iteration 34, loss = 0.27669834\n",
      "Iteration 39, loss = 0.27380675\n",
      "Iteration 35, loss = 0.27402430\n",
      "Iteration 40, loss = 0.27390279\n",
      "Iteration 36, loss = 0.27532012\n",
      "Iteration 41, loss = 0.27529516\n",
      "Iteration 37, loss = 0.27558538\n",
      "Iteration 42, loss = 0.27490571\n",
      "Iteration 43, loss = 0.27399065\n",
      "Iteration 38, loss = 0.27395945\n",
      "Iteration 44, loss = 0.27475222\n",
      "Iteration 39, loss = 0.27379675\n",
      "Iteration 45, loss = 0.27502024\n",
      "Iteration 40, loss = 0.27365224\n",
      "Iteration 46, loss = 0.27415324\n",
      "Iteration 41, loss = 0.27574074\n",
      "Iteration 47, loss = 0.27369452\n",
      "Iteration 42, loss = 0.27452367\n",
      "Iteration 48, loss = 0.27399419\n",
      "Iteration 43, loss = 0.27364466\n",
      "Iteration 49, loss = 0.27559252\n",
      "Iteration 44, loss = 0.27574470\n",
      "Iteration 50, loss = 0.27519016\n",
      "Iteration 45, loss = 0.27473350\n",
      "Iteration 51, loss = 0.27495373\n",
      "Iteration 46, loss = 0.27354445\n",
      "Iteration 52, loss = 0.27428606\n",
      "Iteration 47, loss = 0.27274378\n",
      "Iteration 53, loss = 0.27342190\n",
      "Iteration 48, loss = 0.27337093\n",
      "Iteration 54, loss = 0.27477458\n",
      "Iteration 49, loss = 0.27535785\n",
      "Iteration 55, loss = 0.27420001\n",
      "Iteration 50, loss = 0.27409060\n",
      "Iteration 56, loss = 0.27371270\n",
      "Iteration 51, loss = 0.27376547\n",
      "Iteration 57, loss = 0.27484561\n",
      "Iteration 52, loss = 0.27285016\n",
      "Iteration 58, loss = 0.27373730\n",
      "Iteration 53, loss = 0.27306920\n",
      "Iteration 59, loss = 0.27344520\n",
      "Iteration 54, loss = 0.27422357\n",
      "Iteration 60, loss = 0.27373375\n",
      "Iteration 61, loss = 0.27392123\n",
      "Iteration 55, loss = 0.27259316\n",
      "Iteration 56, loss = 0.27290292\n",
      "Iteration 62, loss = 0.27370240\n",
      "Iteration 57, loss = 0.27372767\n",
      "Iteration 63, loss = 0.27511857\n",
      "Iteration 58, loss = 0.27422151\n",
      "Iteration 64, loss = 0.27328896\n",
      "Iteration 59, loss = 0.27285046\n",
      "Iteration 65, loss = 0.27462489\n",
      "Iteration 60, loss = 0.27369116\n",
      "Iteration 66, loss = 0.27327770\n",
      "Iteration 61, loss = 0.27464167\n",
      "Iteration 67, loss = 0.27444303\n",
      "Iteration 62, loss = 0.27247091\n",
      "Iteration 68, loss = 0.27459630\n",
      "Iteration 63, loss = 0.27430829\n",
      "Iteration 69, loss = 0.27458014\n",
      "Iteration 64, loss = 0.27448199\n",
      "Iteration 70, loss = 0.27284672\n",
      "Iteration 65, loss = 0.27400178\n",
      "Iteration 71, loss = 0.27322135\n",
      "Iteration 66, loss = 0.27300510\n",
      "Iteration 72, loss = 0.27381612\n",
      "Iteration 67, loss = 0.27302553\n",
      "Iteration 73, loss = 0.27364294\n",
      "Iteration 68, loss = 0.27205579\n",
      "Iteration 74, loss = 0.27500659\n",
      "Iteration 69, loss = 0.27263535\n",
      "Iteration 75, loss = 0.27359416\n",
      "Iteration 70, loss = 0.27298780\n",
      "Iteration 76, loss = 0.27330997\n",
      "Iteration 71, loss = 0.27262869\n",
      "Iteration 77, loss = 0.27297783\n",
      "Iteration 72, loss = 0.27199642\n",
      "Iteration 78, loss = 0.27399392\n",
      "Iteration 73, loss = 0.27598479\n",
      "Iteration 79, loss = 0.27420046\n",
      "Iteration 74, loss = 0.27470644\n",
      "Iteration 80, loss = 0.27349965\n",
      "Iteration 75, loss = 0.27276135\n",
      "Iteration 81, loss = 0.27416053\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 76, loss = 0.27318628\n",
      "Iteration 77, loss = 0.27278426\n",
      "Iteration 78, loss = 0.27269318\n",
      "Iteration 1, loss = 0.31280090\n",
      "Iteration 79, loss = 0.27468163\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 2, loss = 0.28757237\n",
      "Iteration 3, loss = 0.28778943\n",
      "Iteration 1, loss = 0.31565573\n",
      "Iteration 4, loss = 0.28269699\n",
      "Iteration 2, loss = 0.28803472\n",
      "Iteration 5, loss = 0.28093668\n",
      "Iteration 3, loss = 0.28665939\n",
      "Iteration 6, loss = 0.28085623\n",
      "Iteration 4, loss = 0.28357959\n",
      "Iteration 7, loss = 0.27888771\n",
      "Iteration 5, loss = 0.28181254\n",
      "Iteration 8, loss = 0.27788639\n",
      "Iteration 6, loss = 0.28220141\n",
      "Iteration 9, loss = 0.27603933\n",
      "Iteration 7, loss = 0.28145575\n",
      "Iteration 8, loss = 0.27893722\n",
      "Iteration 10, loss = 0.27872029\n",
      "Iteration 9, loss = 0.27836901\n",
      "Iteration 11, loss = 0.27633893\n",
      "Iteration 10, loss = 0.27875256\n",
      "Iteration 12, loss = 0.27587174\n",
      "Iteration 11, loss = 0.27740972\n",
      "Iteration 13, loss = 0.27536180\n",
      "Iteration 12, loss = 0.27724560\n",
      "Iteration 14, loss = 0.27735660\n",
      "Iteration 13, loss = 0.27673750\n",
      "Iteration 15, loss = 0.27495178\n",
      "Iteration 14, loss = 0.27907280\n",
      "Iteration 16, loss = 0.27524627\n",
      "Iteration 15, loss = 0.27654532\n",
      "Iteration 17, loss = 0.27464008\n",
      "Iteration 16, loss = 0.27597070\n",
      "Iteration 18, loss = 0.27427175\n",
      "Iteration 17, loss = 0.27580066\n",
      "Iteration 19, loss = 0.27579766\n",
      "Iteration 18, loss = 0.27468424\n",
      "Iteration 20, loss = 0.27298829\n",
      "Iteration 19, loss = 0.27647076\n",
      "Iteration 21, loss = 0.27376857\n",
      "Iteration 20, loss = 0.27572275\n",
      "Iteration 22, loss = 0.27346186\n",
      "Iteration 21, loss = 0.27674216\n",
      "Iteration 23, loss = 0.27323905\n",
      "Iteration 22, loss = 0.27549883\n",
      "Iteration 24, loss = 0.27404173\n",
      "Iteration 23, loss = 0.27580679\n",
      "Iteration 25, loss = 0.27310498\n",
      "Iteration 24, loss = 0.27557331\n",
      "Iteration 26, loss = 0.27407314\n",
      "Iteration 25, loss = 0.27429908\n",
      "Iteration 27, loss = 0.27203505\n",
      "Iteration 26, loss = 0.27559408\n",
      "Iteration 28, loss = 0.27208748\n",
      "Iteration 27, loss = 0.27442530\n",
      "Iteration 29, loss = 0.27337687\n",
      "Iteration 28, loss = 0.27378216\n",
      "Iteration 30, loss = 0.27329176\n",
      "Iteration 29, loss = 0.27569187\n",
      "Iteration 31, loss = 0.27184645\n",
      "Iteration 30, loss = 0.27479588\n",
      "Iteration 32, loss = 0.27131023\n",
      "Iteration 31, loss = 0.27507660\n",
      "Iteration 33, loss = 0.27155568\n",
      "Iteration 32, loss = 0.27400344\n",
      "Iteration 34, loss = 0.27327452\n",
      "Iteration 33, loss = 0.27423670\n",
      "Iteration 35, loss = 0.27137962\n",
      "Iteration 34, loss = 0.27624198\n",
      "Iteration 36, loss = 0.27249279\n",
      "Iteration 35, loss = 0.27393313\n",
      "Iteration 37, loss = 0.27175104\n",
      "Iteration 36, loss = 0.27409544\n",
      "Iteration 38, loss = 0.27157324\n",
      "Iteration 37, loss = 0.27424089\n",
      "Iteration 39, loss = 0.27246027\n",
      "Iteration 38, loss = 0.27396591\n",
      "Iteration 40, loss = 0.27172260\n",
      "Iteration 41, loss = 0.27255263\n",
      "Iteration 39, loss = 0.27557147\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 42, loss = 0.27210841\n",
      "Iteration 43, loss = 0.27173568\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.31865929\n",
      "Iteration 2, loss = 0.28894584\n",
      "Iteration 1, loss = 0.31658654\n",
      "Iteration 3, loss = 0.28694578\n",
      "Iteration 2, loss = 0.28918879\n",
      "Iteration 4, loss = 0.28447228\n",
      "Iteration 5, loss = 0.28213581\n",
      "Iteration 3, loss = 0.28627450\n",
      "Iteration 6, loss = 0.28245953\n",
      "Iteration 4, loss = 0.28546419\n",
      "Iteration 5, loss = 0.28256339\n",
      "Iteration 7, loss = 0.28233078\n",
      "Iteration 6, loss = 0.28211659\n",
      "Iteration 8, loss = 0.27959565\n",
      "Iteration 7, loss = 0.28111200\n",
      "Iteration 9, loss = 0.28071372\n",
      "Iteration 10, loss = 0.28131173\n",
      "Iteration 8, loss = 0.27969036\n",
      "Iteration 9, loss = 0.28113648\n",
      "Iteration 11, loss = 0.27841528\n",
      "Iteration 10, loss = 0.28189356Iteration 12, loss = 0.27686436\n",
      "\n",
      "Iteration 11, loss = 0.28029274\n",
      "Iteration 13, loss = 0.27741988\n",
      "Iteration 12, loss = 0.27889213\n",
      "Iteration 14, loss = 0.27826139\n",
      "Iteration 13, loss = 0.27951367\n",
      "Iteration 15, loss = 0.27481700\n",
      "Iteration 14, loss = 0.28186662\n",
      "Iteration 16, loss = 0.27562355\n",
      "Iteration 15, loss = 0.27649988\n",
      "Iteration 16, loss = 0.27645825\n",
      "Iteration 17, loss = 0.27537761\n",
      "Iteration 17, loss = 0.27687393\n",
      "Iteration 18, loss = 0.27510117\n",
      "Iteration 18, loss = 0.27583557\n",
      "Iteration 19, loss = 0.27421847\n",
      "Iteration 19, loss = 0.27598170\n",
      "Iteration 20, loss = 0.27457594\n",
      "Iteration 20, loss = 0.27613057\n",
      "Iteration 21, loss = 0.27565036\n",
      "Iteration 21, loss = 0.27488167\n",
      "Iteration 22, loss = 0.27375192\n",
      "Iteration 22, loss = 0.27544732\n",
      "Iteration 23, loss = 0.27409398\n",
      "Iteration 23, loss = 0.27493307\n",
      "Iteration 24, loss = 0.27425595\n",
      "Iteration 24, loss = 0.27554177\n",
      "Iteration 25, loss = 0.27270281\n",
      "Iteration 25, loss = 0.27494390\n",
      "Iteration 26, loss = 0.27325668\n",
      "Iteration 26, loss = 0.27435423\n",
      "Iteration 27, loss = 0.27203736\n",
      "Iteration 27, loss = 0.27445747\n",
      "Iteration 28, loss = 0.27331985\n",
      "Iteration 28, loss = 0.27412655\n",
      "Iteration 29, loss = 0.27427479\n",
      "Iteration 29, loss = 0.27616517\n",
      "Iteration 30, loss = 0.27271047\n",
      "Iteration 30, loss = 0.27495679\n",
      "Iteration 31, loss = 0.27181852\n",
      "Iteration 31, loss = 0.27403177\n",
      "Iteration 32, loss = 0.27185559\n",
      "Iteration 32, loss = 0.27351374\n",
      "Iteration 33, loss = 0.27164003\n",
      "Iteration 33, loss = 0.27231811\n",
      "Iteration 34, loss = 0.27544289\n",
      "Iteration 34, loss = 0.27648866\n",
      "Iteration 35, loss = 0.27141101\n",
      "Iteration 35, loss = 0.27257528\n",
      "Iteration 36, loss = 0.27246174\n",
      "Iteration 36, loss = 0.27317608\n",
      "Iteration 37, loss = 0.27108904\n",
      "Iteration 37, loss = 0.27297964\n",
      "Iteration 38, loss = 0.27174942\n",
      "Iteration 38, loss = 0.27260659\n",
      "Iteration 39, loss = 0.27276621\n",
      "Iteration 39, loss = 0.27509730\n",
      "Iteration 40, loss = 0.27107681\n",
      "Iteration 40, loss = 0.27402332\n",
      "Iteration 41, loss = 0.27047732\n",
      "Iteration 41, loss = 0.27196655\n",
      "Iteration 42, loss = 0.27180694\n",
      "Iteration 43, loss = 0.27091669\n",
      "Iteration 42, loss = 0.27302808\n",
      "Iteration 44, loss = 0.27092954\n",
      "Iteration 43, loss = 0.27247525\n",
      "Iteration 45, loss = 0.27147428\n",
      "Iteration 44, loss = 0.27253548\n",
      "Iteration 46, loss = 0.27060694\n",
      "Iteration 45, loss = 0.27276343\n",
      "Iteration 47, loss = 0.27000416\n",
      "Iteration 46, loss = 0.27269264\n",
      "Iteration 48, loss = 0.27157889\n",
      "Iteration 47, loss = 0.27241728\n",
      "Iteration 49, loss = 0.27458477\n",
      "Iteration 48, loss = 0.27277242\n",
      "Iteration 50, loss = 0.27054456\n",
      "Iteration 49, loss = 0.27352442\n",
      "Iteration 51, loss = 0.27079716\n",
      "Iteration 50, loss = 0.27263420\n",
      "Iteration 52, loss = 0.27061508\n",
      "Iteration 51, loss = 0.27268077\n",
      "Iteration 53, loss = 0.27099657\n",
      "Iteration 52, loss = 0.27456317\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 54, loss = 0.27078481\n",
      "Iteration 55, loss = 0.27000199\n",
      "Iteration 1, loss = 0.31682070\n",
      "Iteration 56, loss = 0.26976024\n",
      "Iteration 2, loss = 0.28976350\n",
      "Iteration 57, loss = 0.27041546\n",
      "Iteration 3, loss = 0.28523570\n",
      "Iteration 58, loss = 0.27133026\n",
      "Iteration 4, loss = 0.28441424\n",
      "Iteration 59, loss = 0.27027293\n",
      "Iteration 60, loss = 0.27086409\n",
      "Iteration 5, loss = 0.28198467\n",
      "Iteration 61, loss = 0.27076185\n",
      "Iteration 6, loss = 0.28072904\n",
      "Iteration 62, loss = 0.27053701\n",
      "Iteration 63, loss = 0.27034465\n",
      "Iteration 7, loss = 0.28037874\n",
      "Iteration 64, loss = 0.27040384\n",
      "Iteration 8, loss = 0.27913545\n",
      "Iteration 65, loss = 0.27179152\n",
      "Iteration 9, loss = 0.28044989\n",
      "Iteration 66, loss = 0.26990680\n",
      "Iteration 10, loss = 0.28049153\n",
      "Iteration 67, loss = 0.27086612\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 11, loss = 0.27928319\n",
      "Iteration 12, loss = 0.27855854\n",
      "Iteration 1, loss = 0.31694161\n",
      "Iteration 13, loss = 0.27909668\n",
      "Iteration 2, loss = 0.28767867\n",
      "Iteration 14, loss = 0.28026244\n",
      "Iteration 3, loss = 0.28332451\n",
      "Iteration 15, loss = 0.27812519\n",
      "Iteration 4, loss = 0.28254707\n",
      "Iteration 16, loss = 0.27678410\n",
      "Iteration 5, loss = 0.28093212\n",
      "Iteration 17, loss = 0.27728154\n",
      "Iteration 6, loss = 0.28036755\n",
      "Iteration 18, loss = 0.27554186\n",
      "Iteration 7, loss = 0.27898923\n",
      "Iteration 19, loss = 0.27677958\n",
      "Iteration 8, loss = 0.27862872\n",
      "Iteration 20, loss = 0.27656960\n",
      "Iteration 9, loss = 0.27952073\n",
      "Iteration 21, loss = 0.27447867\n",
      "Iteration 10, loss = 0.27905098\n",
      "Iteration 22, loss = 0.27450123\n",
      "Iteration 11, loss = 0.27672530\n",
      "Iteration 23, loss = 0.27463069\n",
      "Iteration 12, loss = 0.27630046\n",
      "Iteration 24, loss = 0.27491702\n",
      "Iteration 13, loss = 0.27569644\n",
      "Iteration 25, loss = 0.27508456\n",
      "Iteration 14, loss = 0.27798209\n",
      "Iteration 26, loss = 0.27394556\n",
      "Iteration 15, loss = 0.27679072\n",
      "Iteration 27, loss = 0.27340679\n",
      "Iteration 16, loss = 0.27441864\n",
      "Iteration 17, loss = 0.27446282\n",
      "Iteration 28, loss = 0.27334403\n",
      "Iteration 29, loss = 0.27393775\n",
      "Iteration 18, loss = 0.27447566\n",
      "Iteration 30, loss = 0.27386096\n",
      "Iteration 31, loss = 0.27425253\n",
      "Iteration 19, loss = 0.27503788\n",
      "Iteration 32, loss = 0.27310095\n",
      "Iteration 20, loss = 0.27486906\n",
      "Iteration 21, loss = 0.27305284\n",
      "Iteration 22, loss = 0.27443020\n",
      "Iteration 33, loss = 0.27310111\n",
      "Iteration 23, loss = 0.27349097\n",
      "Iteration 34, loss = 0.27427039\n",
      "Iteration 24, loss = 0.27451509\n",
      "Iteration 35, loss = 0.27205544\n",
      "Iteration 36, loss = 0.27283178\n",
      "Iteration 25, loss = 0.27386904Iteration 37, loss = 0.27252251\n",
      "\n",
      "Iteration 38, loss = 0.27222411\n",
      "Iteration 39, loss = 0.27273846\n",
      "Iteration 26, loss = 0.27280035\n",
      "Iteration 40, loss = 0.27287844\n",
      "Iteration 27, loss = 0.27268548\n",
      "Iteration 41, loss = 0.27212103\n",
      "Iteration 28, loss = 0.27166657\n",
      "Iteration 42, loss = 0.27318085\n",
      "Iteration 29, loss = 0.27133368\n",
      "Iteration 43, loss = 0.27220775\n",
      "Iteration 30, loss = 0.27223257\n",
      "Iteration 44, loss = 0.27088278\n",
      "Iteration 31, loss = 0.27129113\n",
      "Iteration 45, loss = 0.27303792\n",
      "Iteration 32, loss = 0.27131740\n",
      "Iteration 33, loss = 0.27156354\n",
      "Iteration 46, loss = 0.27184539\n",
      "Iteration 34, loss = 0.27246168\n",
      "Iteration 47, loss = 0.27181455\n",
      "Iteration 35, loss = 0.27137694\n",
      "Iteration 48, loss = 0.27238302\n",
      "Iteration 36, loss = 0.27018353\n",
      "Iteration 49, loss = 0.27193219\n",
      "Iteration 37, loss = 0.26960165\n",
      "Iteration 50, loss = 0.27185503\n",
      "Iteration 38, loss = 0.27088496\n",
      "Iteration 51, loss = 0.27171103\n",
      "Iteration 39, loss = 0.27100353\n",
      "Iteration 52, loss = 0.27238121\n",
      "Iteration 40, loss = 0.27026854\n",
      "Iteration 53, loss = 0.27298121\n",
      "Iteration 41, loss = 0.26956416\n",
      "Iteration 54, loss = 0.27066591\n",
      "Iteration 42, loss = 0.27145352\n",
      "Iteration 55, loss = 0.27162392\n",
      "Iteration 43, loss = 0.26963312\n",
      "Iteration 56, loss = 0.27098990\n",
      "Iteration 57, loss = 0.27258886\n",
      "Iteration 44, loss = 0.26832049\n",
      "Iteration 58, loss = 0.27173421\n",
      "Iteration 45, loss = 0.26977552\n",
      "Iteration 59, loss = 0.27160168\n",
      "Iteration 46, loss = 0.26873508\n",
      "Iteration 60, loss = 0.27261283\n",
      "Iteration 47, loss = 0.27034621\n",
      "Iteration 61, loss = 0.27216164\n",
      "Iteration 48, loss = 0.26890712\n",
      "Iteration 62, loss = 0.27114477\n",
      "Iteration 49, loss = 0.26958632\n",
      "Iteration 50, loss = 0.27013989\n",
      "Iteration 63, loss = 0.27046129Iteration 51, loss = 0.27000656\n",
      "\n",
      "Iteration 52, loss = 0.27081987\n",
      "Iteration 64, loss = 0.27221175\n",
      "Iteration 53, loss = 0.27089234\n",
      "Iteration 65, loss = 0.27142132\n",
      "Iteration 66, loss = 0.27063441\n",
      "Iteration 67, loss = 0.27087944\n",
      "Iteration 54, loss = 0.26924100\n",
      "Iteration 68, loss = 0.27183967\n",
      "Iteration 55, loss = 0.26854964\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 69, loss = 0.27205941\n",
      "Iteration 70, loss = 0.27146661\n",
      "Iteration 71, loss = 0.27272782\n",
      "Iteration 72, loss = 0.27153118\n",
      "Iteration 73, loss = 0.27257642\n",
      "Iteration 74, loss = 0.27113449\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30984405\n",
      "Iteration 1, loss = 0.30987595\n",
      "Iteration 2, loss = 0.28832731\n",
      "Iteration 2, loss = 0.28850507\n",
      "Iteration 3, loss = 0.28785285\n",
      "Iteration 3, loss = 0.28740062\n",
      "Iteration 4, loss = 0.28416859\n",
      "Iteration 4, loss = 0.28270808\n",
      "Iteration 5, loss = 0.28191158\n",
      "Iteration 5, loss = 0.28188843\n",
      "Iteration 6, loss = 0.28300274\n",
      "Iteration 6, loss = 0.28325797\n",
      "Iteration 7, loss = 0.27892549\n",
      "Iteration 7, loss = 0.28010496\n",
      "Iteration 8, loss = 0.27791145\n",
      "Iteration 8, loss = 0.27882743\n",
      "Iteration 9, loss = 0.27801382\n",
      "Iteration 9, loss = 0.27788356\n",
      "Iteration 10, loss = 0.27840897\n",
      "Iteration 10, loss = 0.27710032\n",
      "Iteration 11, loss = 0.27642168\n",
      "Iteration 11, loss = 0.27698741\n",
      "Iteration 12, loss = 0.27662408\n",
      "Iteration 12, loss = 0.27720661\n",
      "Iteration 13, loss = 0.27710956\n",
      "Iteration 13, loss = 0.27785195\n",
      "Iteration 14, loss = 0.27558115\n",
      "Iteration 14, loss = 0.27509761\n",
      "Iteration 15, loss = 0.27575550\n",
      "Iteration 15, loss = 0.27613203\n",
      "Iteration 16, loss = 0.27398776\n",
      "Iteration 16, loss = 0.27655867\n",
      "Iteration 17, loss = 0.27403850\n",
      "Iteration 17, loss = 0.27503208\n",
      "Iteration 18, loss = 0.27410587\n",
      "Iteration 18, loss = 0.27367794\n",
      "Iteration 19, loss = 0.27627900\n",
      "Iteration 19, loss = 0.27535879\n",
      "Iteration 20, loss = 0.27447942\n",
      "Iteration 20, loss = 0.27383549\n",
      "Iteration 21, loss = 0.27360007\n",
      "Iteration 21, loss = 0.27398776\n",
      "Iteration 22, loss = 0.27412219\n",
      "Iteration 22, loss = 0.27359420\n",
      "Iteration 23, loss = 0.27238797\n",
      "Iteration 23, loss = 0.27389852\n",
      "Iteration 24, loss = 0.27215813\n",
      "Iteration 24, loss = 0.27242130\n",
      "Iteration 25, loss = 0.27535026\n",
      "Iteration 25, loss = 0.27544014\n",
      "Iteration 26, loss = 0.27278884\n",
      "Iteration 26, loss = 0.27326997\n",
      "Iteration 27, loss = 0.27309656\n",
      "Iteration 27, loss = 0.27453833\n",
      "Iteration 28, loss = 0.27226219\n",
      "Iteration 28, loss = 0.27336860\n",
      "Iteration 29, loss = 0.27413503\n",
      "Iteration 29, loss = 0.27258901\n",
      "Iteration 30, loss = 0.27449836\n",
      "Iteration 30, loss = 0.27239483\n",
      "Iteration 31, loss = 0.27241178\n",
      "Iteration 31, loss = 0.27181053\n",
      "Iteration 32, loss = 0.27200328\n",
      "Iteration 32, loss = 0.27206714\n",
      "Iteration 33, loss = 0.27205332\n",
      "Iteration 33, loss = 0.27187344\n",
      "Iteration 34, loss = 0.27335726\n",
      "Iteration 34, loss = 0.27116282\n",
      "Iteration 35, loss = 0.27204429\n",
      "Iteration 36, loss = 0.27221041\n",
      "Iteration 35, loss = 0.27158236\n",
      "Iteration 37, loss = 0.27290422\n",
      "Iteration 36, loss = 0.27026911\n",
      "Iteration 38, loss = 0.27512376\n",
      "Iteration 37, loss = 0.27171270\n",
      "Iteration 39, loss = 0.27299355\n",
      "Iteration 38, loss = 0.27364950\n",
      "Iteration 40, loss = 0.27252131\n",
      "Iteration 41, loss = 0.27331450\n",
      "Iteration 39, loss = 0.27303741\n",
      "Iteration 42, loss = 0.27179750\n",
      "Iteration 40, loss = 0.27200541\n",
      "Iteration 43, loss = 0.27347358\n",
      "Iteration 44, loss = 0.27292759\n",
      "Iteration 41, loss = 0.27109192\n",
      "Iteration 45, loss = 0.27243984Iteration 42, loss = 0.27188466\n",
      "\n",
      "Iteration 43, loss = 0.27051359\n",
      "Iteration 46, loss = 0.27309081\n",
      "Iteration 44, loss = 0.27058977\n",
      "Iteration 47, loss = 0.27296421\n",
      "Iteration 45, loss = 0.27139835\n",
      "Iteration 48, loss = 0.27190986\n",
      "Iteration 46, loss = 0.27035621\n",
      "Iteration 49, loss = 0.27228263\n",
      "Iteration 47, loss = 0.27228687\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 50, loss = 0.27191336\n",
      "Iteration 51, loss = 0.27198151\n",
      "Iteration 1, loss = 0.31158611\n",
      "Iteration 52, loss = 0.27357020\n",
      "Iteration 2, loss = 0.28845818\n",
      "Iteration 53, loss = 0.27205579\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 3, loss = 0.28743380\n",
      "Iteration 4, loss = 0.28335250\n",
      "Iteration 1, loss = 0.30802853\n",
      "Iteration 5, loss = 0.28195895\n",
      "Iteration 2, loss = 0.28792529\n",
      "Iteration 6, loss = 0.28090496\n",
      "Iteration 7, loss = 0.28093731\n",
      "Iteration 3, loss = 0.28613181\n",
      "Iteration 4, loss = 0.28207411\n",
      "Iteration 8, loss = 0.27926656\n",
      "Iteration 5, loss = 0.28119300Iteration 9, loss = 0.27783477\n",
      "\n",
      "Iteration 10, loss = 0.27757230\n",
      "Iteration 6, loss = 0.27915670\n",
      "Iteration 7, loss = 0.27834938\n",
      "Iteration 11, loss = 0.27764394\n",
      "Iteration 8, loss = 0.27749200\n",
      "Iteration 12, loss = 0.27610030\n",
      "Iteration 9, loss = 0.27577500\n",
      "Iteration 13, loss = 0.27642907\n",
      "Iteration 10, loss = 0.27584109\n",
      "Iteration 14, loss = 0.27696965\n",
      "Iteration 11, loss = 0.27684224\n",
      "Iteration 12, loss = 0.27599631\n",
      "Iteration 15, loss = 0.27596377\n",
      "Iteration 13, loss = 0.27446124\n",
      "Iteration 16, loss = 0.27526986\n",
      "Iteration 17, loss = 0.27549439\n",
      "Iteration 14, loss = 0.27320822\n",
      "Iteration 18, loss = 0.27251852\n",
      "Iteration 15, loss = 0.27336689\n",
      "Iteration 19, loss = 0.27509132\n",
      "Iteration 16, loss = 0.27278986\n",
      "Iteration 20, loss = 0.27395970\n",
      "Iteration 17, loss = 0.27275790\n",
      "Iteration 21, loss = 0.27309422\n",
      "Iteration 18, loss = 0.27250863\n",
      "Iteration 22, loss = 0.27198108\n",
      "Iteration 19, loss = 0.27394949\n",
      "Iteration 23, loss = 0.27239149\n",
      "Iteration 20, loss = 0.27258092\n",
      "Iteration 24, loss = 0.27242712\n",
      "Iteration 21, loss = 0.27248588\n",
      "Iteration 25, loss = 0.27299956\n",
      "Iteration 22, loss = 0.27171698\n",
      "Iteration 26, loss = 0.27065183\n",
      "Iteration 23, loss = 0.27208946\n",
      "Iteration 27, loss = 0.27167882\n",
      "Iteration 24, loss = 0.27169528\n",
      "Iteration 28, loss = 0.27163035\n",
      "Iteration 25, loss = 0.27212863\n",
      "Iteration 29, loss = 0.27146954\n",
      "Iteration 26, loss = 0.27125600\n",
      "Iteration 30, loss = 0.27223446\n",
      "Iteration 31, loss = 0.27235369\n",
      "Iteration 27, loss = 0.27225120\n",
      "Iteration 28, loss = 0.27228946\n",
      "Iteration 32, loss = 0.27047627\n",
      "Iteration 33, loss = 0.27022015\n",
      "Iteration 29, loss = 0.27272524\n",
      "Iteration 34, loss = 0.27045753\n",
      "Iteration 30, loss = 0.27420180\n",
      "Iteration 35, loss = 0.27005647\n",
      "Iteration 31, loss = 0.27300570\n",
      "Iteration 36, loss = 0.27006927\n",
      "Iteration 32, loss = 0.27075226\n",
      "Iteration 37, loss = 0.27024390\n",
      "Iteration 33, loss = 0.27041541\n",
      "Iteration 38, loss = 0.27321269\n",
      "Iteration 34, loss = 0.27136370\n",
      "Iteration 39, loss = 0.26996792\n",
      "Iteration 35, loss = 0.27011692\n",
      "Iteration 40, loss = 0.26941176\n",
      "Iteration 36, loss = 0.27045936\n",
      "Iteration 41, loss = 0.27177781\n",
      "Iteration 37, loss = 0.27074920\n",
      "Iteration 42, loss = 0.27013422\n",
      "Iteration 38, loss = 0.27213970\n",
      "Iteration 43, loss = 0.26991934\n",
      "Iteration 44, loss = 0.26958876\n",
      "Iteration 39, loss = 0.27079431\n",
      "Iteration 45, loss = 0.26984806\n",
      "Iteration 40, loss = 0.27082499\n",
      "Iteration 46, loss = 0.27095957\n",
      "Iteration 41, loss = 0.27082275\n",
      "Iteration 47, loss = 0.26996339\n",
      "Iteration 42, loss = 0.27108843\n",
      "Iteration 48, loss = 0.27007913\n",
      "Iteration 43, loss = 0.27095368\n",
      "Iteration 49, loss = 0.26964218\n",
      "Iteration 44, loss = 0.27091620\n",
      "Iteration 50, loss = 0.26990299\n",
      "Iteration 45, loss = 0.27041422\n",
      "Iteration 51, loss = 0.26952390\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 46, loss = 0.27086621\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.31050886\n",
      "Iteration 1, loss = 0.31135930\n",
      "Iteration 2, loss = 0.28770039\n",
      "Iteration 2, loss = 0.28820914\n",
      "Iteration 3, loss = 0.28252968\n",
      "Iteration 3, loss = 0.28519933\n",
      "Iteration 4, loss = 0.28204075\n",
      "Iteration 5, loss = 0.28241900\n",
      "Iteration 4, loss = 0.28409366\n",
      "Iteration 6, loss = 0.27991786\n",
      "Iteration 5, loss = 0.28226446\n",
      "Iteration 7, loss = 0.27829628\n",
      "Iteration 6, loss = 0.28180908\n",
      "Iteration 8, loss = 0.27920631\n",
      "Iteration 7, loss = 0.27979111\n",
      "Iteration 9, loss = 0.27769358\n",
      "Iteration 8, loss = 0.28031376\n",
      "Iteration 10, loss = 0.27865622\n",
      "Iteration 9, loss = 0.28066356\n",
      "Iteration 11, loss = 0.27779317\n",
      "Iteration 10, loss = 0.27941996\n",
      "Iteration 12, loss = 0.27547521\n",
      "Iteration 11, loss = 0.27938288\n",
      "Iteration 13, loss = 0.27481130\n",
      "Iteration 12, loss = 0.27768067\n",
      "Iteration 14, loss = 0.27449358\n",
      "Iteration 13, loss = 0.27732698\n",
      "Iteration 15, loss = 0.27478307\n",
      "Iteration 14, loss = 0.27795626\n",
      "Iteration 16, loss = 0.27554366\n",
      "Iteration 15, loss = 0.27791159\n",
      "Iteration 17, loss = 0.27276271\n",
      "Iteration 16, loss = 0.27697365\n",
      "Iteration 18, loss = 0.27370313\n",
      "Iteration 17, loss = 0.27727581\n",
      "Iteration 19, loss = 0.27306591\n",
      "Iteration 18, loss = 0.27655735\n",
      "Iteration 20, loss = 0.27310141\n",
      "Iteration 19, loss = 0.27622179\n",
      "Iteration 21, loss = 0.27326164\n",
      "Iteration 20, loss = 0.27675690\n",
      "Iteration 22, loss = 0.27132123\n",
      "Iteration 21, loss = 0.27687213\n",
      "Iteration 23, loss = 0.27235525\n",
      "Iteration 24, loss = 0.27407015\n",
      "Iteration 22, loss = 0.27513294\n",
      "Iteration 25, loss = 0.27142918\n",
      "Iteration 23, loss = 0.27532835\n",
      "Iteration 24, loss = 0.27433696\n",
      "Iteration 26, loss = 0.27156180\n",
      "Iteration 27, loss = 0.27123592\n",
      "Iteration 25, loss = 0.27376501\n",
      "Iteration 26, loss = 0.27378158\n",
      "Iteration 27, loss = 0.27434048\n",
      "Iteration 28, loss = 0.27049744\n",
      "Iteration 28, loss = 0.27458808\n",
      "Iteration 29, loss = 0.26974458\n",
      "Iteration 29, loss = 0.27233983\n",
      "Iteration 30, loss = 0.27194847\n",
      "Iteration 30, loss = 0.27353440\n",
      "Iteration 31, loss = 0.27101453\n",
      "Iteration 31, loss = 0.27231079\n",
      "Iteration 32, loss = 0.27321216\n",
      "Iteration 32, loss = 0.27201338\n",
      "Iteration 33, loss = 0.27261180\n",
      "Iteration 33, loss = 0.27095770\n",
      "Iteration 34, loss = 0.27146617\n",
      "Iteration 34, loss = 0.27028540\n",
      "Iteration 35, loss = 0.27311474\n",
      "Iteration 35, loss = 0.27020653\n",
      "Iteration 36, loss = 0.27341758\n",
      "Iteration 36, loss = 0.27168775\n",
      "Iteration 37, loss = 0.27095211\n",
      "Iteration 38, loss = 0.27311637\n",
      "Iteration 37, loss = 0.26986289\n",
      "Iteration 39, loss = 0.27234221\n",
      "Iteration 38, loss = 0.27231430\n",
      "Iteration 40, loss = 0.27239953\n",
      "Iteration 39, loss = 0.27178299\n",
      "Iteration 40, loss = 0.27107868\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 41, loss = 0.27192017\n",
      "Iteration 42, loss = 0.27169362\n",
      "Iteration 43, loss = 0.27204544\n",
      "Iteration 1, loss = 0.31019770\n",
      "Iteration 44, loss = 0.27136708\n",
      "Iteration 2, loss = 0.28684920\n",
      "Iteration 45, loss = 0.27305021\n",
      "Iteration 3, loss = 0.28390809\n",
      "Iteration 46, loss = 0.27189361\n",
      "Iteration 4, loss = 0.28115289\n",
      "Iteration 47, loss = 0.27311347\n",
      "Iteration 5, loss = 0.28065814\n",
      "Iteration 6, loss = 0.27941877\n",
      "Iteration 48, loss = 0.27115543\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 7, loss = 0.27836287\n",
      "Iteration 8, loss = 0.27886001\n",
      "Iteration 1, loss = 0.31003475\n",
      "Iteration 2, loss = 0.28921175\n",
      "Iteration 3, loss = 0.28604847Iteration 9, loss = 0.27820452\n",
      "\n",
      "Iteration 4, loss = 0.28405371\n",
      "Iteration 10, loss = 0.27772789\n",
      "Iteration 5, loss = 0.28239579\n",
      "Iteration 11, loss = 0.27772422\n",
      "Iteration 6, loss = 0.28224663\n",
      "Iteration 12, loss = 0.27647539\n",
      "Iteration 7, loss = 0.28161082\n",
      "Iteration 13, loss = 0.27610744\n",
      "Iteration 8, loss = 0.28057613\n",
      "Iteration 14, loss = 0.27602228\n",
      "Iteration 9, loss = 0.28055528\n",
      "Iteration 15, loss = 0.27694102\n",
      "Iteration 10, loss = 0.28165397\n",
      "Iteration 16, loss = 0.27470752\n",
      "Iteration 11, loss = 0.27919806\n",
      "Iteration 17, loss = 0.27455003\n",
      "Iteration 12, loss = 0.27817393\n",
      "Iteration 18, loss = 0.27376359\n",
      "Iteration 13, loss = 0.27745645\n",
      "Iteration 19, loss = 0.27259847\n",
      "Iteration 14, loss = 0.27792119\n",
      "Iteration 20, loss = 0.27340088\n",
      "Iteration 15, loss = 0.27886002\n",
      "Iteration 21, loss = 0.27344116\n",
      "Iteration 16, loss = 0.27768095\n",
      "Iteration 22, loss = 0.27315622\n",
      "Iteration 17, loss = 0.27691102\n",
      "Iteration 23, loss = 0.27120139\n",
      "Iteration 18, loss = 0.27696841\n",
      "Iteration 24, loss = 0.27207698\n",
      "Iteration 19, loss = 0.27584970\n",
      "Iteration 25, loss = 0.27086324\n",
      "Iteration 20, loss = 0.27863609\n",
      "Iteration 26, loss = 0.27130869\n",
      "Iteration 21, loss = 0.27594093\n",
      "Iteration 27, loss = 0.27143690\n",
      "Iteration 22, loss = 0.27496201\n",
      "Iteration 28, loss = 0.27115094\n",
      "Iteration 23, loss = 0.27505901\n",
      "Iteration 29, loss = 0.27097138\n",
      "Iteration 24, loss = 0.27662025\n",
      "Iteration 30, loss = 0.27198719\n",
      "Iteration 25, loss = 0.27372280\n",
      "Iteration 31, loss = 0.27146578\n",
      "Iteration 26, loss = 0.27429391\n",
      "Iteration 32, loss = 0.27122817\n",
      "Iteration 27, loss = 0.27518990\n",
      "Iteration 33, loss = 0.27194170\n",
      "Iteration 28, loss = 0.27522023\n",
      "Iteration 34, loss = 0.26967671\n",
      "Iteration 29, loss = 0.27425232\n",
      "Iteration 35, loss = 0.27036822\n",
      "Iteration 30, loss = 0.27487442\n",
      "Iteration 36, loss = 0.27187828\n",
      "Iteration 31, loss = 0.27460787\n",
      "Iteration 37, loss = 0.27093085\n",
      "Iteration 32, loss = 0.27441383\n",
      "Iteration 38, loss = 0.27060142\n",
      "Iteration 33, loss = 0.27419221\n",
      "Iteration 39, loss = 0.27035904\n",
      "Iteration 34, loss = 0.27362153\n",
      "Iteration 40, loss = 0.27058198\n",
      "Iteration 35, loss = 0.27475898\n",
      "Iteration 41, loss = 0.27066929\n",
      "Iteration 36, loss = 0.27441391\n",
      "Iteration 42, loss = 0.27007663\n",
      "Iteration 37, loss = 0.27281650\n",
      "Iteration 43, loss = 0.27067824\n",
      "Iteration 38, loss = 0.27299140\n",
      "Iteration 44, loss = 0.26980348\n",
      "Iteration 39, loss = 0.27279666\n",
      "Iteration 45, loss = 0.27231914\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 40, loss = 0.27376583\n",
      "Iteration 41, loss = 0.27463380\n",
      "Iteration 42, loss = 0.27318781\n",
      "Iteration 1, loss = 0.30714992\n",
      "Iteration 43, loss = 0.27364534\n",
      "Iteration 2, loss = 0.28539952\n",
      "Iteration 44, loss = 0.27303078\n",
      "Iteration 3, loss = 0.28280550\n",
      "Iteration 45, loss = 0.27489884\n",
      "Iteration 4, loss = 0.28232933\n",
      "Iteration 46, loss = 0.27221240\n",
      "Iteration 47, loss = 0.27335928Iteration 5, loss = 0.27929464\n",
      "\n",
      "Iteration 48, loss = 0.27367023\n",
      "Iteration 6, loss = 0.27874453\n",
      "Iteration 7, loss = 0.27723346\n",
      "Iteration 49, loss = 0.27316757\n",
      "Iteration 8, loss = 0.27787500\n",
      "Iteration 50, loss = 0.27195803\n",
      "Iteration 51, loss = 0.27254409\n",
      "Iteration 9, loss = 0.27892988\n",
      "Iteration 52, loss = 0.27306997\n",
      "Iteration 10, loss = 0.27813284\n",
      "Iteration 53, loss = 0.27185538\n",
      "Iteration 11, loss = 0.27662048\n",
      "Iteration 54, loss = 0.27253571\n",
      "Iteration 12, loss = 0.27642271\n",
      "Iteration 55, loss = 0.27419123\n",
      "Iteration 13, loss = 0.27463646\n",
      "Iteration 56, loss = 0.27246660\n",
      "Iteration 14, loss = 0.27576477\n",
      "Iteration 57, loss = 0.27287478\n",
      "Iteration 15, loss = 0.27449820\n",
      "Iteration 58, loss = 0.27285852\n",
      "Iteration 16, loss = 0.27359419\n",
      "Iteration 59, loss = 0.27123424\n",
      "Iteration 17, loss = 0.27440134\n",
      "Iteration 60, loss = 0.27297448\n",
      "Iteration 18, loss = 0.27300353\n",
      "Iteration 61, loss = 0.27339882\n",
      "Iteration 19, loss = 0.27424340\n",
      "Iteration 62, loss = 0.27232074\n",
      "Iteration 20, loss = 0.27540749\n",
      "Iteration 21, loss = 0.27354103\n",
      "Iteration 63, loss = 0.27408216\n",
      "Iteration 22, loss = 0.27235099\n",
      "Iteration 64, loss = 0.27257894\n",
      "Iteration 65, loss = 0.27289332\n",
      "Iteration 23, loss = 0.27229627\n",
      "Iteration 24, loss = 0.27305603\n",
      "Iteration 66, loss = 0.27295340\n",
      "Iteration 25, loss = 0.27201366\n",
      "Iteration 67, loss = 0.27443963\n",
      "Iteration 26, loss = 0.27272707\n",
      "Iteration 68, loss = 0.27230171\n",
      "Iteration 27, loss = 0.27209930\n",
      "Iteration 69, loss = 0.27180842\n",
      "Iteration 28, loss = 0.27167107\n",
      "Iteration 70, loss = 0.27278917\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 29, loss = 0.27158120\n",
      "Iteration 30, loss = 0.27235063\n",
      "Iteration 1, loss = 0.30706016\n",
      "Iteration 31, loss = 0.27274963\n",
      "Iteration 2, loss = 0.28660821\n",
      "Iteration 32, loss = 0.27200253\n",
      "Iteration 33, loss = 0.27147254\n",
      "Iteration 3, loss = 0.28403637\n",
      "Iteration 34, loss = 0.27144070\n",
      "Iteration 4, loss = 0.28196185\n",
      "Iteration 35, loss = 0.27204562\n",
      "Iteration 5, loss = 0.27910562\n",
      "Iteration 36, loss = 0.27175042\n",
      "Iteration 6, loss = 0.27810962\n",
      "Iteration 37, loss = 0.27195286\n",
      "Iteration 7, loss = 0.27878023\n",
      "Iteration 38, loss = 0.27158906\n",
      "Iteration 8, loss = 0.27707969\n",
      "Iteration 39, loss = 0.27239672\n",
      "Iteration 9, loss = 0.27837640\n",
      "Iteration 40, loss = 0.27214373\n",
      "Iteration 10, loss = 0.27642556\n",
      "Iteration 41, loss = 0.27278830\n",
      "Iteration 11, loss = 0.27637963\n",
      "Iteration 42, loss = 0.27123251\n",
      "Iteration 12, loss = 0.27665925\n",
      "Iteration 43, loss = 0.27155503\n",
      "Iteration 13, loss = 0.27436394\n",
      "Iteration 44, loss = 0.27120312\n",
      "Iteration 14, loss = 0.27531798\n",
      "Iteration 45, loss = 0.27184870\n",
      "Iteration 15, loss = 0.27457602\n",
      "Iteration 46, loss = 0.27068703\n",
      "Iteration 16, loss = 0.27405089\n",
      "Iteration 47, loss = 0.27132164\n",
      "Iteration 17, loss = 0.27506472\n",
      "Iteration 48, loss = 0.27088370\n",
      "Iteration 18, loss = 0.27298607\n",
      "Iteration 49, loss = 0.27132559\n",
      "Iteration 19, loss = 0.27397214\n",
      "Iteration 50, loss = 0.27089670\n",
      "Iteration 20, loss = 0.27511956\n",
      "Iteration 51, loss = 0.27090687\n",
      "Iteration 21, loss = 0.27428299\n",
      "Iteration 52, loss = 0.27016785\n",
      "Iteration 22, loss = 0.27304858\n",
      "Iteration 53, loss = 0.27061126\n",
      "Iteration 23, loss = 0.27295687\n",
      "Iteration 54, loss = 0.27069382\n",
      "Iteration 24, loss = 0.27241083\n",
      "Iteration 55, loss = 0.27252656\n",
      "Iteration 25, loss = 0.27143429\n",
      "Iteration 56, loss = 0.27106404\n",
      "Iteration 26, loss = 0.27204227\n",
      "Iteration 57, loss = 0.27203429\n",
      "Iteration 27, loss = 0.27265337\n",
      "Iteration 58, loss = 0.27056801\n",
      "Iteration 28, loss = 0.27219391\n",
      "Iteration 59, loss = 0.27049992\n",
      "Iteration 29, loss = 0.27186211\n",
      "Iteration 60, loss = 0.27043954\n",
      "Iteration 30, loss = 0.27290312\n",
      "Iteration 61, loss = 0.27082960\n",
      "Iteration 31, loss = 0.27197307\n",
      "Iteration 62, loss = 0.27018986\n",
      "Iteration 32, loss = 0.27304399\n",
      "Iteration 33, loss = 0.27155694\n",
      "Iteration 63, loss = 0.27288969\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 34, loss = 0.27135870\n",
      "Iteration 35, loss = 0.27216687\n",
      "Iteration 36, loss = 0.27282240\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30735912\n",
      "Iteration 1, loss = 0.30618721\n",
      "Iteration 2, loss = 0.28335719\n",
      "Iteration 2, loss = 0.28427097\n",
      "Iteration 3, loss = 0.28266217\n",
      "Iteration 3, loss = 0.28111048\n",
      "Iteration 4, loss = 0.27969790\n",
      "Iteration 4, loss = 0.27926759\n",
      "Iteration 5, loss = 0.27877500\n",
      "Iteration 5, loss = 0.27668338\n",
      "Iteration 6, loss = 0.27867251\n",
      "Iteration 6, loss = 0.27734055\n",
      "Iteration 7, loss = 0.27610402\n",
      "Iteration 7, loss = 0.27408288\n",
      "Iteration 8, loss = 0.27675292\n",
      "Iteration 8, loss = 0.27446004\n",
      "Iteration 9, loss = 0.27531387\n",
      "Iteration 10, loss = 0.27600477\n",
      "Iteration 9, loss = 0.27390452\n",
      "Iteration 11, loss = 0.27574824\n",
      "Iteration 10, loss = 0.27386305\n",
      "Iteration 12, loss = 0.27468218\n",
      "Iteration 11, loss = 0.27361371\n",
      "Iteration 12, loss = 0.27308109\n",
      "Iteration 13, loss = 0.27437025\n",
      "Iteration 13, loss = 0.27231914\n",
      "Iteration 14, loss = 0.27463631\n",
      "Iteration 14, loss = 0.27252304\n",
      "Iteration 15, loss = 0.27317263\n",
      "Iteration 15, loss = 0.27144894\n",
      "Iteration 16, loss = 0.27386019\n",
      "Iteration 16, loss = 0.27243526\n",
      "Iteration 17, loss = 0.27128084\n",
      "Iteration 17, loss = 0.27321610\n",
      "Iteration 18, loss = 0.27095178\n",
      "Iteration 18, loss = 0.27369214\n",
      "Iteration 19, loss = 0.27050804\n",
      "Iteration 19, loss = 0.27408034\n",
      "Iteration 20, loss = 0.27055820\n",
      "Iteration 20, loss = 0.27391733\n",
      "Iteration 21, loss = 0.27080850\n",
      "Iteration 21, loss = 0.27324900\n",
      "Iteration 22, loss = 0.27020317\n",
      "Iteration 22, loss = 0.27381210\n",
      "Iteration 23, loss = 0.27112479\n",
      "Iteration 23, loss = 0.27316438\n",
      "Iteration 24, loss = 0.26997292\n",
      "Iteration 24, loss = 0.27245088\n",
      "Iteration 25, loss = 0.27015865\n",
      "Iteration 25, loss = 0.27301385\n",
      "Iteration 26, loss = 0.27061944\n",
      "Iteration 26, loss = 0.27355255\n",
      "Iteration 27, loss = 0.27213444\n",
      "Iteration 27, loss = 0.27582577\n",
      "Iteration 28, loss = 0.27015012\n",
      "Iteration 28, loss = 0.27307158\n",
      "Iteration 29, loss = 0.26930577\n",
      "Iteration 29, loss = 0.27223860\n",
      "Iteration 30, loss = 0.27147519\n",
      "Iteration 30, loss = 0.27330661\n",
      "Iteration 31, loss = 0.27036644\n",
      "Iteration 31, loss = 0.27329735\n",
      "Iteration 32, loss = 0.27057132\n",
      "Iteration 32, loss = 0.27377374\n",
      "Iteration 33, loss = 0.27023398\n",
      "Iteration 33, loss = 0.27241610\n",
      "Iteration 34, loss = 0.26994829\n",
      "Iteration 35, loss = 0.26918907\n",
      "Iteration 34, loss = 0.27402315\n",
      "Iteration 36, loss = 0.26981348\n",
      "Iteration 35, loss = 0.27216702\n",
      "Iteration 37, loss = 0.26926203\n",
      "Iteration 38, loss = 0.26972797\n",
      "Iteration 36, loss = 0.27251165\n",
      "Iteration 39, loss = 0.27068807\n",
      "Iteration 37, loss = 0.27230756\n",
      "Iteration 40, loss = 0.26968291\n",
      "Iteration 38, loss = 0.27248534\n",
      "Iteration 41, loss = 0.26973088\n",
      "Iteration 39, loss = 0.27372194\n",
      "Iteration 42, loss = 0.27008101\n",
      "Iteration 40, loss = 0.27205560\n",
      "Iteration 43, loss = 0.27005691\n",
      "Iteration 41, loss = 0.27085491\n",
      "Iteration 42, loss = 0.27241011\n",
      "Iteration 44, loss = 0.26909168\n",
      "Iteration 43, loss = 0.27309477\n",
      "Iteration 44, loss = 0.27281536\n",
      "Iteration 45, loss = 0.26961263\n",
      "Iteration 46, loss = 0.26993712\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 45, loss = 0.27221837\n",
      "Iteration 46, loss = 0.27280395\n",
      "Iteration 1, loss = 0.30455930\n",
      "Iteration 47, loss = 0.27205837\n",
      "Iteration 2, loss = 0.28281024\n",
      "Iteration 48, loss = 0.27184232\n",
      "Iteration 3, loss = 0.28011463\n",
      "Iteration 49, loss = 0.27320513\n",
      "Iteration 50, loss = 0.27201671\n",
      "Iteration 4, loss = 0.27905852\n",
      "Iteration 51, loss = 0.27197773\n",
      "Iteration 5, loss = 0.27630218\n",
      "Iteration 52, loss = 0.27159799\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 6, loss = 0.27691313\n",
      "Iteration 7, loss = 0.27445849\n",
      "Iteration 8, loss = 0.27397103\n",
      "Iteration 1, loss = 0.30449597\n",
      "Iteration 9, loss = 0.27483452\n",
      "Iteration 2, loss = 0.28188165\n",
      "Iteration 10, loss = 0.27323898\n",
      "Iteration 3, loss = 0.28062593\n",
      "Iteration 11, loss = 0.27462753\n",
      "Iteration 4, loss = 0.27941363\n",
      "Iteration 12, loss = 0.27315548\n",
      "Iteration 5, loss = 0.27609570\n",
      "Iteration 13, loss = 0.27167668\n",
      "Iteration 6, loss = 0.27705566\n",
      "Iteration 14, loss = 0.27254710\n",
      "Iteration 7, loss = 0.27400422\n",
      "Iteration 15, loss = 0.27128411\n",
      "Iteration 8, loss = 0.27457090\n",
      "Iteration 16, loss = 0.27215459\n",
      "Iteration 9, loss = 0.27405615\n",
      "Iteration 17, loss = 0.27090643\n",
      "Iteration 10, loss = 0.27269230\n",
      "Iteration 18, loss = 0.27062576\n",
      "Iteration 11, loss = 0.27264595\n",
      "Iteration 12, loss = 0.27143560\n",
      "Iteration 19, loss = 0.27124337\n",
      "Iteration 13, loss = 0.27180493\n",
      "Iteration 20, loss = 0.27194153\n",
      "Iteration 14, loss = 0.27247300\n",
      "Iteration 21, loss = 0.27087728\n",
      "Iteration 15, loss = 0.27043506\n",
      "Iteration 22, loss = 0.27015025\n",
      "Iteration 16, loss = 0.27058375\n",
      "Iteration 23, loss = 0.26957488\n",
      "Iteration 17, loss = 0.27021972\n",
      "Iteration 24, loss = 0.27004060\n",
      "Iteration 18, loss = 0.26994995\n",
      "Iteration 25, loss = 0.27078779\n",
      "Iteration 19, loss = 0.27015286\n",
      "Iteration 20, loss = 0.27056892\n",
      "Iteration 26, loss = 0.27042429\n",
      "Iteration 21, loss = 0.26861282\n",
      "Iteration 27, loss = 0.27168828\n",
      "Iteration 22, loss = 0.27022186\n",
      "Iteration 28, loss = 0.26944211\n",
      "Iteration 23, loss = 0.27034717\n",
      "Iteration 29, loss = 0.26995226\n",
      "Iteration 24, loss = 0.26995839\n",
      "Iteration 30, loss = 0.27156627\n",
      "Iteration 25, loss = 0.27016325\n",
      "Iteration 31, loss = 0.26995332\n",
      "Iteration 26, loss = 0.26896957\n",
      "Iteration 32, loss = 0.27070940\n",
      "Iteration 27, loss = 0.27190740\n",
      "Iteration 33, loss = 0.26925899\n",
      "Iteration 28, loss = 0.26892411\n",
      "Iteration 34, loss = 0.26896835\n",
      "Iteration 29, loss = 0.27053015\n",
      "Iteration 35, loss = 0.26926851\n",
      "Iteration 30, loss = 0.27073160\n",
      "Iteration 36, loss = 0.26924018\n",
      "Iteration 31, loss = 0.27008322\n",
      "Iteration 37, loss = 0.27081644\n",
      "Iteration 32, loss = 0.26966178\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 38, loss = 0.26933334\n",
      "Iteration 39, loss = 0.26982558\n",
      "Iteration 1, loss = 0.30319786\n",
      "Iteration 40, loss = 0.26984745\n",
      "Iteration 2, loss = 0.28159878\n",
      "Iteration 41, loss = 0.26883268\n",
      "Iteration 3, loss = 0.28117702\n",
      "Iteration 42, loss = 0.26973020\n",
      "Iteration 4, loss = 0.27994129\n",
      "Iteration 43, loss = 0.27005799\n",
      "Iteration 5, loss = 0.27659337\n",
      "Iteration 44, loss = 0.26903519\n",
      "Iteration 6, loss = 0.27630391\n",
      "Iteration 45, loss = 0.26924340\n",
      "Iteration 7, loss = 0.27464334\n",
      "Iteration 46, loss = 0.26870434\n",
      "Iteration 8, loss = 0.27376873\n",
      "Iteration 47, loss = 0.26957122\n",
      "Iteration 9, loss = 0.27335986\n",
      "Iteration 48, loss = 0.27012445\n",
      "Iteration 10, loss = 0.27185122\n",
      "Iteration 49, loss = 0.27000183\n",
      "Iteration 11, loss = 0.27318433\n",
      "Iteration 12, loss = 0.27098526\n",
      "Iteration 50, loss = 0.26935965\n",
      "Iteration 13, loss = 0.27044692\n",
      "Iteration 51, loss = 0.27011651\n",
      "Iteration 14, loss = 0.27108981\n",
      "Iteration 52, loss = 0.27011235\n",
      "Iteration 15, loss = 0.27018134\n",
      "Iteration 53, loss = 0.26904634\n",
      "Iteration 16, loss = 0.27081334\n",
      "Iteration 54, loss = 0.26920792\n",
      "Iteration 17, loss = 0.26913699\n",
      "Iteration 55, loss = 0.26824867\n",
      "Iteration 18, loss = 0.26827468\n",
      "Iteration 56, loss = 0.26861984\n",
      "Iteration 19, loss = 0.26897695\n",
      "Iteration 20, loss = 0.27002026\n",
      "Iteration 21, loss = 0.26768036\n",
      "Iteration 57, loss = 0.26958026\n",
      "Iteration 22, loss = 0.26867884\n",
      "Iteration 58, loss = 0.26888537\n",
      "Iteration 59, loss = 0.26959016\n",
      "Iteration 60, loss = 0.26930800\n",
      "Iteration 23, loss = 0.26905061\n",
      "Iteration 61, loss = 0.26937876\n",
      "Iteration 24, loss = 0.26902472\n",
      "Iteration 62, loss = 0.27002998\n",
      "Iteration 25, loss = 0.26906289\n",
      "Iteration 63, loss = 0.26866859\n",
      "Iteration 26, loss = 0.26878501\n",
      "Iteration 27, loss = 0.27045044\n",
      "Iteration 28, loss = 0.26799226\n",
      "Iteration 64, loss = 0.27020361\n",
      "Iteration 29, loss = 0.26800797\n",
      "Iteration 65, loss = 0.27113951\n",
      "Iteration 30, loss = 0.26933592\n",
      "Iteration 66, loss = 0.26978333\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 31, loss = 0.26816657\n",
      "Iteration 32, loss = 0.26832237\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30742689\n",
      "Iteration 2, loss = 0.28396890\n",
      "Iteration 1, loss = 0.30565175\n",
      "Iteration 3, loss = 0.28281770\n",
      "Iteration 2, loss = 0.28589745\n",
      "Iteration 4, loss = 0.28028591\n",
      "Iteration 3, loss = 0.28429902\n",
      "Iteration 5, loss = 0.27849854\n",
      "Iteration 4, loss = 0.28353717\n",
      "Iteration 6, loss = 0.27858335\n",
      "Iteration 5, loss = 0.27911887\n",
      "Iteration 7, loss = 0.27653986\n",
      "Iteration 6, loss = 0.27986127\n",
      "Iteration 8, loss = 0.27708314\n",
      "Iteration 7, loss = 0.27802185\n",
      "Iteration 9, loss = 0.27699198\n",
      "Iteration 8, loss = 0.27772018\n",
      "Iteration 10, loss = 0.27509819\n",
      "Iteration 9, loss = 0.27826325\n",
      "Iteration 11, loss = 0.27590156\n",
      "Iteration 10, loss = 0.27639520\n",
      "Iteration 12, loss = 0.27495378\n",
      "Iteration 13, loss = 0.27413296\n",
      "Iteration 11, loss = 0.27615863\n",
      "Iteration 14, loss = 0.27449063\n",
      "Iteration 12, loss = 0.27607860\n",
      "Iteration 15, loss = 0.27265331\n",
      "Iteration 13, loss = 0.27550790\n",
      "Iteration 16, loss = 0.27368120\n",
      "Iteration 14, loss = 0.27739188\n",
      "Iteration 17, loss = 0.27252578\n",
      "Iteration 15, loss = 0.27534317\n",
      "Iteration 16, loss = 0.27334756\n",
      "Iteration 18, loss = 0.27176597\n",
      "Iteration 17, loss = 0.27236681\n",
      "Iteration 19, loss = 0.27184712\n",
      "Iteration 18, loss = 0.27260304\n",
      "Iteration 20, loss = 0.27279400\n",
      "Iteration 19, loss = 0.27273347\n",
      "Iteration 21, loss = 0.27157164\n",
      "Iteration 20, loss = 0.27401116\n",
      "Iteration 22, loss = 0.27292409\n",
      "Iteration 21, loss = 0.27106154\n",
      "Iteration 23, loss = 0.27159750\n",
      "Iteration 22, loss = 0.27171621\n",
      "Iteration 24, loss = 0.27171962\n",
      "Iteration 23, loss = 0.27220992\n",
      "Iteration 25, loss = 0.27130754\n",
      "Iteration 24, loss = 0.27266344\n",
      "Iteration 26, loss = 0.27151048\n",
      "Iteration 25, loss = 0.27199964\n",
      "Iteration 27, loss = 0.27370385\n",
      "Iteration 26, loss = 0.27196782\n",
      "Iteration 28, loss = 0.27103160\n",
      "Iteration 27, loss = 0.27241752\n",
      "Iteration 29, loss = 0.27273337\n",
      "Iteration 28, loss = 0.27048714\n",
      "Iteration 29, loss = 0.27306990\n",
      "Iteration 30, loss = 0.27102254\n",
      "Iteration 31, loss = 0.27078600\n",
      "Iteration 30, loss = 0.27141268\n",
      "Iteration 31, loss = 0.27082067\n",
      "Iteration 32, loss = 0.27063182\n",
      "Iteration 32, loss = 0.27059111\n",
      "Iteration 33, loss = 0.27082736\n",
      "Iteration 33, loss = 0.27042917\n",
      "Iteration 34, loss = 0.27054292\n",
      "Iteration 34, loss = 0.27028862\n",
      "Iteration 35, loss = 0.27052564\n",
      "Iteration 35, loss = 0.26970766\n",
      "Iteration 36, loss = 0.27113775\n",
      "Iteration 36, loss = 0.27252129\n",
      "Iteration 37, loss = 0.27050626\n",
      "Iteration 37, loss = 0.26944994\n",
      "Iteration 38, loss = 0.26989993\n",
      "Iteration 38, loss = 0.27003178\n",
      "Iteration 39, loss = 0.27039692\n",
      "Iteration 40, loss = 0.27081846\n",
      "Iteration 39, loss = 0.26991006\n",
      "Iteration 41, loss = 0.27040540\n",
      "Iteration 40, loss = 0.27075408\n",
      "Iteration 42, loss = 0.27001075\n",
      "Iteration 41, loss = 0.27112067\n",
      "Iteration 43, loss = 0.27263729\n",
      "Iteration 42, loss = 0.27028827\n",
      "Iteration 44, loss = 0.26979866\n",
      "Iteration 43, loss = 0.27261116\n",
      "Iteration 45, loss = 0.26945197\n",
      "Iteration 44, loss = 0.27154783\n",
      "Iteration 45, loss = 0.26978542\n",
      "Iteration 46, loss = 0.26992802\n",
      "Iteration 46, loss = 0.26987062\n",
      "Iteration 47, loss = 0.27155651\n",
      "Iteration 47, loss = 0.27125723\n",
      "Iteration 48, loss = 0.27119969\n",
      "Iteration 49, loss = 0.27085288\n",
      "Iteration 48, loss = 0.27054065\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 50, loss = 0.27040345\n",
      "Iteration 51, loss = 0.26979793\n",
      "Iteration 1, loss = 0.30402928\n",
      "Iteration 52, loss = 0.27178424\n",
      "Iteration 2, loss = 0.28271355\n",
      "Iteration 3, loss = 0.27978302\n",
      "Iteration 53, loss = 0.27132433\n",
      "Iteration 54, loss = 0.27089130\n",
      "Iteration 4, loss = 0.27904710\n",
      "Iteration 55, loss = 0.27066344\n",
      "Iteration 5, loss = 0.27819131\n",
      "Iteration 56, loss = 0.27005923\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 6, loss = 0.27652323\n",
      "Iteration 7, loss = 0.27362644\n",
      "Iteration 1, loss = 0.30441268\n",
      "Iteration 8, loss = 0.27310504\n",
      "Iteration 2, loss = 0.28396855\n",
      "Iteration 9, loss = 0.27234225\n",
      "Iteration 3, loss = 0.28001224\n",
      "Iteration 10, loss = 0.27116001\n",
      "Iteration 4, loss = 0.27842505\n",
      "Iteration 11, loss = 0.27100906\n",
      "Iteration 5, loss = 0.27717991\n",
      "Iteration 12, loss = 0.27030272\n",
      "Iteration 6, loss = 0.27589988\n",
      "Iteration 13, loss = 0.26936496\n",
      "Iteration 7, loss = 0.27499545\n",
      "Iteration 14, loss = 0.27042662\n",
      "Iteration 8, loss = 0.27498418\n",
      "Iteration 15, loss = 0.26952520\n",
      "Iteration 9, loss = 0.27310261\n",
      "Iteration 16, loss = 0.26944887\n",
      "Iteration 10, loss = 0.27315920\n",
      "Iteration 17, loss = 0.26835618\n",
      "Iteration 11, loss = 0.27292017\n",
      "Iteration 18, loss = 0.26794068\n",
      "Iteration 12, loss = 0.27264683\n",
      "Iteration 19, loss = 0.27098443\n",
      "Iteration 13, loss = 0.27152002\n",
      "Iteration 20, loss = 0.26845787\n",
      "Iteration 14, loss = 0.27290820\n",
      "Iteration 21, loss = 0.26826302\n",
      "Iteration 15, loss = 0.27203118\n",
      "Iteration 22, loss = 0.26652648\n",
      "Iteration 16, loss = 0.26983912\n",
      "Iteration 23, loss = 0.26835391\n",
      "Iteration 17, loss = 0.27120427\n",
      "Iteration 24, loss = 0.26964347\n",
      "Iteration 18, loss = 0.27030492\n",
      "Iteration 25, loss = 0.26782949\n",
      "Iteration 19, loss = 0.27113920\n",
      "Iteration 26, loss = 0.26806621\n",
      "Iteration 20, loss = 0.26960514\n",
      "Iteration 27, loss = 0.26775386\n",
      "Iteration 28, loss = 0.26761956\n",
      "Iteration 21, loss = 0.26895785\n",
      "Iteration 29, loss = 0.26643535\n",
      "Iteration 22, loss = 0.27034635\n",
      "Iteration 30, loss = 0.26704134\n",
      "Iteration 23, loss = 0.27120337\n",
      "Iteration 31, loss = 0.26778199\n",
      "Iteration 24, loss = 0.26960350\n",
      "Iteration 32, loss = 0.26670290\n",
      "Iteration 25, loss = 0.26989166\n",
      "Iteration 33, loss = 0.26765390\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 26, loss = 0.26932081\n",
      "Iteration 27, loss = 0.26951493\n",
      "Iteration 1, loss = 0.30548707\n",
      "Iteration 28, loss = 0.26929247\n",
      "Iteration 2, loss = 0.28334117\n",
      "Iteration 3, loss = 0.28052589\n",
      "Iteration 29, loss = 0.26787884\n",
      "Iteration 4, loss = 0.27762255\n",
      "Iteration 30, loss = 0.26874914\n",
      "Iteration 5, loss = 0.27711650\n",
      "Iteration 31, loss = 0.26876487\n",
      "Iteration 6, loss = 0.27685951\n",
      "Iteration 32, loss = 0.26869753\n",
      "Iteration 7, loss = 0.27327705\n",
      "Iteration 33, loss = 0.26940326\n",
      "Iteration 8, loss = 0.27243070\n",
      "Iteration 34, loss = 0.27009814\n",
      "Iteration 9, loss = 0.27366873\n",
      "Iteration 35, loss = 0.26933679\n",
      "Iteration 10, loss = 0.27214119\n",
      "Iteration 11, loss = 0.27101507\n",
      "Iteration 36, loss = 0.26844153\n",
      "Iteration 12, loss = 0.27095639\n",
      "Iteration 37, loss = 0.26857570\n",
      "Iteration 13, loss = 0.27045465\n",
      "Iteration 38, loss = 0.26814036\n",
      "Iteration 14, loss = 0.27040207\n",
      "Iteration 39, loss = 0.26859015\n",
      "Iteration 15, loss = 0.26923519\n",
      "Iteration 40, loss = 0.26829786\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 16, loss = 0.26843708\n",
      "Iteration 17, loss = 0.26909408\n",
      "Iteration 18, loss = 0.26805470\n",
      "Iteration 19, loss = 0.26971767\n",
      "Iteration 20, loss = 0.26858470\n",
      "Iteration 21, loss = 0.26928560\n",
      "Iteration 22, loss = 0.26822481\n",
      "Iteration 23, loss = 0.26911469\n",
      "Iteration 24, loss = 0.26908270\n",
      "Iteration 25, loss = 0.26922264\n",
      "Iteration 26, loss = 0.26728145\n",
      "Iteration 27, loss = 0.26744019\n",
      "Iteration 28, loss = 0.26797953\n",
      "Iteration 29, loss = 0.26646083\n",
      "Iteration 30, loss = 0.26785750\n",
      "Iteration 31, loss = 0.26769301\n",
      "Iteration 32, loss = 0.26733020\n",
      "Iteration 33, loss = 0.26733710\n",
      "Iteration 34, loss = 0.26727502\n",
      "Iteration 35, loss = 0.26746787\n",
      "Iteration 36, loss = 0.26651571\n",
      "Iteration 37, loss = 0.26698342\n",
      "Iteration 38, loss = 0.26756327\n",
      "Iteration 39, loss = 0.26761790\n",
      "Iteration 40, loss = 0.26715403\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30506164\n",
      "Iteration 1, loss = 0.30312356\n",
      "Iteration 2, loss = 0.28618983\n",
      "Iteration 2, loss = 0.28426094\n",
      "Iteration 3, loss = 0.28485434\n",
      "Iteration 3, loss = 0.28313051\n",
      "Iteration 4, loss = 0.28067239\n",
      "Iteration 4, loss = 0.28014830\n",
      "Iteration 5, loss = 0.27959039\n",
      "Iteration 5, loss = 0.27805152\n",
      "Iteration 6, loss = 0.27890141\n",
      "Iteration 6, loss = 0.27690884\n",
      "Iteration 7, loss = 0.27921540\n",
      "Iteration 7, loss = 0.27740513\n",
      "Iteration 8, loss = 0.27840801\n",
      "Iteration 8, loss = 0.27646294\n",
      "Iteration 9, loss = 0.27978522\n",
      "Iteration 9, loss = 0.27668071\n",
      "Iteration 10, loss = 0.27678444\n",
      "Iteration 10, loss = 0.27711636\n",
      "Iteration 11, loss = 0.27662928\n",
      "Iteration 11, loss = 0.27543153\n",
      "Iteration 12, loss = 0.27516446\n",
      "Iteration 12, loss = 0.27393722\n",
      "Iteration 13, loss = 0.27487379\n",
      "Iteration 13, loss = 0.27675838\n",
      "Iteration 14, loss = 0.28055873\n",
      "Iteration 14, loss = 0.28034600\n",
      "Iteration 15, loss = 0.27631784\n",
      "Iteration 15, loss = 0.27469002\n",
      "Iteration 16, loss = 0.27560302\n",
      "Iteration 16, loss = 0.27410375\n",
      "Iteration 17, loss = 0.27466176\n",
      "Iteration 17, loss = 0.27393174\n",
      "Iteration 18, loss = 0.27374449\n",
      "Iteration 18, loss = 0.27216173\n",
      "Iteration 19, loss = 0.27540867\n",
      "Iteration 20, loss = 0.27349411\n",
      "Iteration 19, loss = 0.27442797\n",
      "Iteration 21, loss = 0.27489173\n",
      "Iteration 20, loss = 0.27225009\n",
      "Iteration 22, loss = 0.27357042\n",
      "Iteration 21, loss = 0.27350985\n",
      "Iteration 23, loss = 0.27302974\n",
      "Iteration 22, loss = 0.27133719\n",
      "Iteration 24, loss = 0.27499438\n",
      "Iteration 23, loss = 0.27232983\n",
      "Iteration 25, loss = 0.27475313\n",
      "Iteration 24, loss = 0.27217138\n",
      "Iteration 26, loss = 0.27335754\n",
      "Iteration 25, loss = 0.27284501\n",
      "Iteration 27, loss = 0.28184198\n",
      "Iteration 26, loss = 0.27157867\n",
      "Iteration 28, loss = 0.27559293\n",
      "Iteration 27, loss = 0.27315462\n",
      "Iteration 29, loss = 0.27460201\n",
      "Iteration 28, loss = 0.27278886\n",
      "Iteration 30, loss = 0.27422939\n",
      "Iteration 29, loss = 0.27388335\n",
      "Iteration 31, loss = 0.28153073\n",
      "Iteration 30, loss = 0.27085426\n",
      "Iteration 32, loss = 0.27670573\n",
      "Iteration 31, loss = 0.28997556\n",
      "Iteration 33, loss = 0.27791517\n",
      "Iteration 32, loss = 0.27530001\n",
      "Iteration 34, loss = 0.27476305\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 33, loss = 0.27293289\n",
      "Iteration 34, loss = 0.27150874\n",
      "Iteration 1, loss = 0.30409237\n",
      "Iteration 35, loss = 0.27055621\n",
      "Iteration 2, loss = 0.28596991\n",
      "Iteration 36, loss = 0.27259447\n",
      "Iteration 37, loss = 0.26996931\n",
      "Iteration 3, loss = 0.28412060\n",
      "Iteration 38, loss = 0.26993512\n",
      "Iteration 4, loss = 0.28068351\n",
      "Iteration 39, loss = 0.27041709\n",
      "Iteration 5, loss = 0.27959922\n",
      "Iteration 40, loss = 0.26997593\n",
      "Iteration 6, loss = 0.27839459\n",
      "Iteration 41, loss = 0.27097260\n",
      "Iteration 7, loss = 0.27701881\n",
      "Iteration 42, loss = 0.27076320\n",
      "Iteration 8, loss = 0.27752503\n",
      "Iteration 43, loss = 0.27142522\n",
      "Iteration 9, loss = 0.27665378\n",
      "Iteration 44, loss = 0.27049105\n",
      "Iteration 10, loss = 0.27745263\n",
      "Iteration 11, loss = 0.27535744\n",
      "Iteration 45, loss = 0.27045773\n",
      "Iteration 12, loss = 0.27437455\n",
      "Iteration 46, loss = 0.27027175\n",
      "Iteration 13, loss = 0.27494579\n",
      "Iteration 47, loss = 0.26958732\n",
      "Iteration 14, loss = 0.27864148\n",
      "Iteration 48, loss = 0.27161403\n",
      "Iteration 15, loss = 0.27408418\n",
      "Iteration 49, loss = 0.26995425\n",
      "Iteration 16, loss = 0.27600401\n",
      "Iteration 50, loss = 0.27284393\n",
      "Iteration 17, loss = 0.27381227\n",
      "Iteration 51, loss = 0.27096561\n",
      "Iteration 18, loss = 0.27532194\n",
      "Iteration 52, loss = 0.27000635\n",
      "Iteration 19, loss = 0.27550630\n",
      "Iteration 53, loss = 0.27142242\n",
      "Iteration 20, loss = 0.27355131\n",
      "Iteration 54, loss = 0.27735225\n",
      "Iteration 21, loss = 0.27377370\n",
      "Iteration 55, loss = 0.27233319\n",
      "Iteration 22, loss = 0.27314703\n",
      "Iteration 56, loss = 0.27741304\n",
      "Iteration 23, loss = 0.27276183\n",
      "Iteration 24, loss = 0.27412180\n",
      "Iteration 57, loss = 0.27557766\n",
      "Iteration 25, loss = 0.27409428\n",
      "Iteration 58, loss = 0.27547734\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 26, loss = 0.27278217\n",
      "Iteration 27, loss = 0.27397582\n",
      "Iteration 1, loss = 0.30276558\n",
      "Iteration 28, loss = 0.27363768\n",
      "Iteration 2, loss = 0.28535533\n",
      "Iteration 29, loss = 0.27279996\n",
      "Iteration 3, loss = 0.28385311\n",
      "Iteration 30, loss = 0.27202431\n",
      "Iteration 4, loss = 0.28087449\n",
      "Iteration 31, loss = 0.29602807\n",
      "Iteration 5, loss = 0.27909684\n",
      "Iteration 32, loss = 0.27759484\n",
      "Iteration 6, loss = 0.27853849\n",
      "Iteration 33, loss = 0.27398407\n",
      "Iteration 7, loss = 0.27861850\n",
      "Iteration 34, loss = 0.27326425\n",
      "Iteration 8, loss = 0.27873140\n",
      "Iteration 35, loss = 0.27167859\n",
      "Iteration 9, loss = 0.27767614\n",
      "Iteration 36, loss = 0.27214966\n",
      "Iteration 37, loss = 0.27000405\n",
      "Iteration 10, loss = 0.27678798\n",
      "Iteration 11, loss = 0.27547670\n",
      "Iteration 38, loss = 0.27108074\n",
      "Iteration 12, loss = 0.28325926\n",
      "Iteration 39, loss = 0.27174485\n",
      "Iteration 40, loss = 0.27087399\n",
      "Iteration 13, loss = 0.27830133\n",
      "Iteration 41, loss = 0.27174119\n",
      "Iteration 14, loss = 0.27670188\n",
      "Iteration 42, loss = 0.27091773\n",
      "Iteration 15, loss = 0.27396413\n",
      "Iteration 43, loss = 0.27150633\n",
      "Iteration 16, loss = 0.27584456\n",
      "Iteration 44, loss = 0.27050044\n",
      "Iteration 17, loss = 0.27456422\n",
      "Iteration 45, loss = 0.27162394\n",
      "Iteration 18, loss = 0.27407143\n",
      "Iteration 46, loss = 0.27231224\n",
      "Iteration 19, loss = 0.27468112\n",
      "Iteration 47, loss = 0.27012666\n",
      "Iteration 20, loss = 0.27384826\n",
      "Iteration 48, loss = 0.27081137\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 21, loss = 0.27436225\n",
      "Iteration 22, loss = 0.27346382\n",
      "Iteration 23, loss = 0.27315830\n",
      "Iteration 1, loss = 0.30411134\n",
      "Iteration 24, loss = 0.27386138\n",
      "Iteration 2, loss = 0.28383515\n",
      "Iteration 25, loss = 0.27374938\n",
      "Iteration 3, loss = 0.28263957\n",
      "Iteration 26, loss = 0.27252500\n",
      "Iteration 4, loss = 0.27991991\n",
      "Iteration 27, loss = 0.27340931\n",
      "Iteration 5, loss = 0.27881760\n",
      "Iteration 28, loss = 0.27364852\n",
      "Iteration 6, loss = 0.27647527\n",
      "Iteration 29, loss = 0.27443709\n",
      "Iteration 7, loss = 0.27627341\n",
      "Iteration 30, loss = 0.27323936\n",
      "Iteration 8, loss = 0.27536831\n",
      "Iteration 31, loss = 0.28057917\n",
      "Iteration 9, loss = 0.27524820\n",
      "Iteration 32, loss = 0.27626802\n",
      "Iteration 10, loss = 0.27455544\n",
      "Iteration 33, loss = 0.27553401\n",
      "Iteration 11, loss = 0.27361793\n",
      "Iteration 34, loss = 0.27278682\n",
      "Iteration 12, loss = 0.27483742\n",
      "Iteration 35, loss = 0.27174484\n",
      "Iteration 13, loss = 0.27583520\n",
      "Iteration 36, loss = 0.27296318\n",
      "Iteration 37, loss = 0.27077151\n",
      "Iteration 14, loss = 0.27830475\n",
      "Iteration 38, loss = 0.27203466\n",
      "Iteration 15, loss = 0.27325152\n",
      "Iteration 16, loss = 0.27358258\n",
      "Iteration 39, loss = 0.27281825\n",
      "Iteration 17, loss = 0.27856049\n",
      "Iteration 40, loss = 0.27209180\n",
      "Iteration 18, loss = 0.27582420\n",
      "Iteration 41, loss = 0.27302559\n",
      "Iteration 19, loss = 0.27799130\n",
      "Iteration 42, loss = 0.27312670\n",
      "Iteration 20, loss = 0.27338525\n",
      "Iteration 43, loss = 0.27139991\n",
      "Iteration 21, loss = 0.27295084\n",
      "Iteration 44, loss = 0.27180056\n",
      "Iteration 22, loss = 0.28179374\n",
      "Iteration 45, loss = 0.27326382\n",
      "Iteration 23, loss = 0.27515182\n",
      "Iteration 46, loss = 0.27345219\n",
      "Iteration 24, loss = 0.27617174\n",
      "Iteration 47, loss = 0.27145913\n",
      "Iteration 25, loss = 0.27494582\n",
      "Iteration 48, loss = 0.27342844\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 26, loss = 0.27383521\n",
      "Iteration 27, loss = 0.27287528\n",
      "Iteration 1, loss = 0.30446384\n",
      "Iteration 28, loss = 0.27334078\n",
      "Iteration 2, loss = 0.28606605\n",
      "Iteration 29, loss = 0.27274117\n",
      "Iteration 3, loss = 0.28417890\n",
      "Iteration 30, loss = 0.27192624\n",
      "Iteration 4, loss = 0.28252770\n",
      "Iteration 31, loss = 0.28361031\n",
      "Iteration 5, loss = 0.28247305\n",
      "Iteration 32, loss = 0.27620308\n",
      "Iteration 6, loss = 0.27933657\n",
      "Iteration 7, loss = 0.27942334\n",
      "Iteration 33, loss = 0.27260525\n",
      "Iteration 34, loss = 0.27166610\n",
      "Iteration 8, loss = 0.27951033\n",
      "Iteration 9, loss = 0.27925436\n",
      "Iteration 35, loss = 0.27083965\n",
      "Iteration 36, loss = 0.27215286\n",
      "Iteration 10, loss = 0.27954180\n",
      "Iteration 37, loss = 0.27080184\n",
      "Iteration 11, loss = 0.27786659\n",
      "Iteration 38, loss = 0.27122931\n",
      "Iteration 12, loss = 0.28011324\n",
      "Iteration 39, loss = 0.27123832\n",
      "Iteration 13, loss = 0.27835816\n",
      "Iteration 40, loss = 0.27090871\n",
      "Iteration 14, loss = 0.28181498\n",
      "Iteration 41, loss = 0.27107933\n",
      "Iteration 15, loss = 0.27755066\n",
      "Iteration 42, loss = 0.27109594\n",
      "Iteration 16, loss = 0.27564694\n",
      "Iteration 43, loss = 0.27168446\n",
      "Iteration 17, loss = 0.28226864\n",
      "Iteration 44, loss = 0.27050483\n",
      "Iteration 18, loss = 0.27635926\n",
      "Iteration 45, loss = 0.27030662\n",
      "Iteration 19, loss = 0.27762404\n",
      "Iteration 46, loss = 0.27233720\n",
      "Iteration 20, loss = 0.27616625\n",
      "Iteration 47, loss = 0.27036669\n",
      "Iteration 21, loss = 0.27468459\n",
      "Iteration 48, loss = 0.27252507\n",
      "Iteration 22, loss = 0.28398175\n",
      "Iteration 49, loss = 0.27020049\n",
      "Iteration 23, loss = 0.27585526\n",
      "Iteration 50, loss = 0.27130495\n",
      "Iteration 24, loss = 0.27582577\n",
      "Iteration 51, loss = 0.27148291\n",
      "Iteration 25, loss = 0.27763041\n",
      "Iteration 26, loss = 0.27652172\n",
      "Iteration 52, loss = 0.27023589\n",
      "Iteration 27, loss = 0.27445971\n",
      "Iteration 53, loss = 0.27109589\n",
      "Iteration 28, loss = 0.27419733\n",
      "Iteration 54, loss = 0.27481527\n",
      "Iteration 55, loss = 0.26971225\n",
      "Iteration 29, loss = 0.27664982\n",
      "Iteration 56, loss = 0.27219684\n",
      "Iteration 30, loss = 0.27595686\n",
      "Iteration 57, loss = 0.27035126\n",
      "Iteration 31, loss = 0.29144731\n",
      "Iteration 58, loss = 0.27355819\n",
      "Iteration 32, loss = 0.27963316\n",
      "Iteration 59, loss = 0.27020799\n",
      "Iteration 33, loss = 0.27730235\n",
      "Iteration 60, loss = 0.27634536\n",
      "Iteration 34, loss = 0.27952914\n",
      "Iteration 61, loss = 0.27212000\n",
      "Iteration 35, loss = 0.27583204\n",
      "Iteration 62, loss = 0.27141139\n",
      "Iteration 36, loss = 0.27755649\n",
      "Iteration 37, loss = 0.27445193\n",
      "Iteration 63, loss = 0.27264837\n",
      "Iteration 38, loss = 0.27499894\n",
      "Iteration 64, loss = 0.27503571\n",
      "Iteration 39, loss = 0.27488428\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 65, loss = 0.27019360\n",
      "Iteration 66, loss = 0.26967239\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30250754\n",
      "Iteration 2, loss = 0.28421333\n",
      "Iteration 1, loss = 0.30526829\n",
      "Iteration 3, loss = 0.28166229\n",
      "Iteration 2, loss = 0.28639529\n",
      "Iteration 4, loss = 0.27978811\n",
      "Iteration 3, loss = 0.28445693\n",
      "Iteration 5, loss = 0.27964166\n",
      "Iteration 4, loss = 0.28243187\n",
      "Iteration 6, loss = 0.27693322\n",
      "Iteration 5, loss = 0.28180689\n",
      "Iteration 7, loss = 0.27679692\n",
      "Iteration 6, loss = 0.28194973\n",
      "Iteration 8, loss = 0.27497757\n",
      "Iteration 9, loss = 0.27544201\n",
      "Iteration 7, loss = 0.28041785\n",
      "Iteration 10, loss = 0.27500400\n",
      "Iteration 8, loss = 0.27897258\n",
      "Iteration 11, loss = 0.27320483\n",
      "Iteration 9, loss = 0.27828899\n",
      "Iteration 12, loss = 0.27900834\n",
      "Iteration 10, loss = 0.27758394\n",
      "Iteration 13, loss = 0.27449738\n",
      "Iteration 11, loss = 0.27615007\n",
      "Iteration 14, loss = 0.27575743\n",
      "Iteration 12, loss = 0.27974325\n",
      "Iteration 15, loss = 0.27314824\n",
      "Iteration 13, loss = 0.27678176\n",
      "Iteration 16, loss = 0.27175628\n",
      "Iteration 14, loss = 0.27835837\n",
      "Iteration 17, loss = 0.27602410\n",
      "Iteration 15, loss = 0.27718839\n",
      "Iteration 18, loss = 0.27383275\n",
      "Iteration 16, loss = 0.27603204\n",
      "Iteration 19, loss = 0.27226311\n",
      "Iteration 17, loss = 0.28015073\n",
      "Iteration 20, loss = 0.27089050\n",
      "Iteration 18, loss = 0.27814346\n",
      "Iteration 19, loss = 0.27665390\n",
      "Iteration 21, loss = 0.27040045\n",
      "Iteration 22, loss = 0.27630371\n",
      "Iteration 20, loss = 0.27522857\n",
      "Iteration 23, loss = 0.27167282\n",
      "Iteration 21, loss = 0.27548990\n",
      "Iteration 24, loss = 0.27122612\n",
      "Iteration 22, loss = 0.28875861\n",
      "Iteration 25, loss = 0.27129556\n",
      "Iteration 23, loss = 0.28023527\n",
      "Iteration 26, loss = 0.27187109\n",
      "Iteration 24, loss = 0.27715255\n",
      "Iteration 27, loss = 0.27010170\n",
      "Iteration 28, loss = 0.27052304\n",
      "Iteration 25, loss = 0.27600687\n",
      "Iteration 29, loss = 0.27080797\n",
      "Iteration 26, loss = 0.27631603\n",
      "Iteration 27, loss = 0.27497414\n",
      "Iteration 30, loss = 0.27021657\n",
      "Iteration 28, loss = 0.27524319\n",
      "Iteration 31, loss = 0.28009160\n",
      "Iteration 29, loss = 0.27530109\n",
      "Iteration 32, loss = 0.27440035\n",
      "Iteration 30, loss = 0.27489447\n",
      "Iteration 33, loss = 0.27582130\n",
      "Iteration 31, loss = 0.28916699\n",
      "Iteration 34, loss = 0.27251435\n",
      "Iteration 32, loss = 0.27767465\n",
      "Iteration 35, loss = 0.27096443\n",
      "Iteration 33, loss = 0.27523157\n",
      "Iteration 36, loss = 0.27411455\n",
      "Iteration 34, loss = 0.27604932\n",
      "Iteration 37, loss = 0.27321326\n",
      "Iteration 35, loss = 0.27476462\n",
      "Iteration 38, loss = 0.27126410\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 36, loss = 0.27662793\n",
      "Iteration 1, loss = 0.30786429\n",
      "Iteration 37, loss = 0.27441325\n",
      "Iteration 2, loss = 0.28550041\n",
      "Iteration 38, loss = 0.27485677\n",
      "Iteration 3, loss = 0.28353578\n",
      "Iteration 39, loss = 0.27465764\n",
      "Iteration 4, loss = 0.28133402\n",
      "Iteration 40, loss = 0.27497717\n",
      "Iteration 5, loss = 0.28085044\n",
      "Iteration 41, loss = 0.27437037\n",
      "Iteration 6, loss = 0.28157868\n",
      "Iteration 42, loss = 0.27497729\n",
      "Iteration 7, loss = 0.27932933\n",
      "Iteration 43, loss = 0.27456158\n",
      "Iteration 8, loss = 0.27870162\n",
      "Iteration 44, loss = 0.27476000\n",
      "Iteration 9, loss = 0.27875364\n",
      "Iteration 10, loss = 0.27877414\n",
      "Iteration 45, loss = 0.27608243\n",
      "Iteration 11, loss = 0.27817185\n",
      "Iteration 46, loss = 0.27711002\n",
      "Iteration 12, loss = 0.28267723\n",
      "Iteration 47, loss = 0.27528320\n",
      "Iteration 13, loss = 0.27953300\n",
      "Iteration 48, loss = 0.27605098\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 14, loss = 0.28231152\n",
      "Iteration 15, loss = 0.27857687\n",
      "Iteration 1, loss = 0.30951035\n",
      "Iteration 16, loss = 0.27802967\n",
      "Iteration 2, loss = 0.28697803\n",
      "Iteration 17, loss = 0.28080391\n",
      "Iteration 3, loss = 0.28670307\n",
      "Iteration 18, loss = 0.27727826\n",
      "Iteration 4, loss = 0.28383840\n",
      "Iteration 19, loss = 0.27691645\n",
      "Iteration 5, loss = 0.28241814\n",
      "Iteration 20, loss = 0.27591039\n",
      "Iteration 6, loss = 0.28298950\n",
      "Iteration 21, loss = 0.27697731\n",
      "Iteration 7, loss = 0.28164277\n",
      "Iteration 22, loss = 0.28822541\n",
      "Iteration 8, loss = 0.28023818\n",
      "Iteration 23, loss = 0.27740670\n",
      "Iteration 9, loss = 0.27999011\n",
      "Iteration 24, loss = 0.27797024\n",
      "Iteration 10, loss = 0.27950946\n",
      "Iteration 25, loss = 0.27642989\n",
      "Iteration 26, loss = 0.27595022\n",
      "Iteration 11, loss = 0.28155233\n",
      "Iteration 27, loss = 0.27594091\n",
      "Iteration 12, loss = 0.27898706\n",
      "Iteration 28, loss = 0.27620184\n",
      "Iteration 13, loss = 0.27883007\n",
      "Iteration 14, loss = 0.28440386\n",
      "Iteration 29, loss = 0.27503028\n",
      "Iteration 15, loss = 0.27957334\n",
      "Iteration 30, loss = 0.27502157\n",
      "Iteration 16, loss = 0.27887881\n",
      "Iteration 31, loss = 0.27547307\n",
      "Iteration 17, loss = 0.27930454\n",
      "Iteration 32, loss = 0.28098263\n",
      "Iteration 18, loss = 0.27895876\n",
      "Iteration 33, loss = 0.27661720\n",
      "Iteration 19, loss = 0.27881004\n",
      "Iteration 34, loss = 0.27748771\n",
      "Iteration 20, loss = 0.27731187\n",
      "Iteration 35, loss = 0.27536666\n",
      "Iteration 21, loss = 0.27692878\n",
      "Iteration 36, loss = 0.27572522\n",
      "Iteration 37, loss = 0.27462994\n",
      "Iteration 22, loss = 0.27948122\n",
      "Iteration 38, loss = 0.27418724\n",
      "Iteration 23, loss = 0.27720243\n",
      "Iteration 39, loss = 0.27508333\n",
      "Iteration 24, loss = 0.27826964\n",
      "Iteration 25, loss = 0.27690359\n",
      "Iteration 40, loss = 0.27428531\n",
      "Iteration 26, loss = 0.27706655\n",
      "Iteration 41, loss = 0.27454931\n",
      "Iteration 27, loss = 0.27588726\n",
      "Iteration 42, loss = 0.27515565\n",
      "Iteration 28, loss = 0.27924369\n",
      "Iteration 43, loss = 0.27423127\n",
      "Iteration 29, loss = 0.27607054\n",
      "Iteration 44, loss = 0.27443762\n",
      "Iteration 30, loss = 0.27670710\n",
      "Iteration 45, loss = 0.27510560\n",
      "Iteration 31, loss = 0.27634592\n",
      "Iteration 46, loss = 0.27368890\n",
      "Iteration 32, loss = 0.27984720\n",
      "Iteration 47, loss = 0.27570067\n",
      "Iteration 33, loss = 0.27732423\n",
      "Iteration 48, loss = 0.27594475\n",
      "Iteration 34, loss = 0.27630715\n",
      "Iteration 49, loss = 0.27363133\n",
      "Iteration 35, loss = 0.27566342\n",
      "Iteration 50, loss = 0.27414255\n",
      "Iteration 36, loss = 0.27525310\n",
      "Iteration 51, loss = 0.27455114\n",
      "Iteration 37, loss = 0.27520473\n",
      "Iteration 52, loss = 0.27285489\n",
      "Iteration 38, loss = 0.29011695\n",
      "Iteration 53, loss = 0.27681552\n",
      "Iteration 39, loss = 0.27831769\n",
      "Iteration 54, loss = 0.27367259\n",
      "Iteration 55, loss = 0.27515942\n",
      "Iteration 40, loss = 0.27758376\n",
      "Iteration 56, loss = 0.27703363\n",
      "Iteration 41, loss = 0.27763462\n",
      "Iteration 42, loss = 0.27557104\n",
      "Iteration 57, loss = 0.27351038\n",
      "Iteration 43, loss = 0.27565158\n",
      "Iteration 58, loss = 0.27513062\n",
      "Iteration 44, loss = 0.27579900\n",
      "Iteration 59, loss = 0.27489712\n",
      "Iteration 45, loss = 0.27722298\n",
      "Iteration 60, loss = 0.27635712\n",
      "Iteration 46, loss = 0.27508258\n",
      "Iteration 61, loss = 0.27520222\n",
      "Iteration 47, loss = 0.27523514\n",
      "Iteration 62, loss = 0.27415796\n",
      "Iteration 48, loss = 0.27577833\n",
      "Iteration 63, loss = 0.27749300\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 49, loss = 0.27674070\n",
      "Iteration 50, loss = 0.27501789\n",
      "Iteration 51, loss = 0.27558482\n",
      "Iteration 52, loss = 0.27447817\n",
      "Iteration 53, loss = 0.27773899\n",
      "Iteration 54, loss = 0.27584366\n",
      "Iteration 55, loss = 0.27591757\n",
      "Iteration 56, loss = 0.27685810\n",
      "Iteration 57, loss = 0.27859987\n",
      "Iteration 58, loss = 0.27611402\n",
      "Iteration 59, loss = 0.27530723\n",
      "Iteration 60, loss = 0.27500423\n",
      "Iteration 61, loss = 0.27636655\n",
      "Iteration 62, loss = 0.27679721\n",
      "Iteration 63, loss = 0.27511686\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30315494\n",
      "Iteration 1, loss = 0.30628523\n",
      "Iteration 2, loss = 0.28128093\n",
      "Iteration 2, loss = 0.28240894\n",
      "Iteration 3, loss = 0.27670561\n",
      "Iteration 4, loss = 0.27610164\n",
      "Iteration 3, loss = 0.28004498\n",
      "Iteration 5, loss = 0.27493526\n",
      "Iteration 4, loss = 0.27748452\n",
      "Iteration 5, loss = 0.27726922\n",
      "Iteration 6, loss = 0.27479177\n",
      "Iteration 7, loss = 0.27466549\n",
      "Iteration 6, loss = 0.27585736\n",
      "Iteration 8, loss = 0.27271929\n",
      "Iteration 7, loss = 0.27572452\n",
      "Iteration 9, loss = 0.27258170\n",
      "Iteration 8, loss = 0.27539327\n",
      "Iteration 10, loss = 0.27141777\n",
      "Iteration 9, loss = 0.27539872\n",
      "Iteration 11, loss = 0.27209497\n",
      "Iteration 10, loss = 0.27385303\n",
      "Iteration 12, loss = 0.27121250\n",
      "Iteration 11, loss = 0.27454025\n",
      "Iteration 13, loss = 0.27223350\n",
      "Iteration 12, loss = 0.27377303\n",
      "Iteration 14, loss = 0.27240291\n",
      "Iteration 13, loss = 0.27426996\n",
      "Iteration 15, loss = 0.27141765\n",
      "Iteration 14, loss = 0.27310417\n",
      "Iteration 16, loss = 0.27134019\n",
      "Iteration 15, loss = 0.27360991\n",
      "Iteration 17, loss = 0.26984159\n",
      "Iteration 16, loss = 0.27201563\n",
      "Iteration 18, loss = 0.27166381\n",
      "Iteration 17, loss = 0.27246924\n",
      "Iteration 19, loss = 0.27120084\n",
      "Iteration 18, loss = 0.27303497\n",
      "Iteration 20, loss = 0.27083680\n",
      "Iteration 19, loss = 0.27386887\n",
      "Iteration 21, loss = 0.27040136\n",
      "Iteration 20, loss = 0.27290158\n",
      "Iteration 21, loss = 0.27240792\n",
      "Iteration 22, loss = 0.26949239\n",
      "Iteration 22, loss = 0.27246475\n",
      "Iteration 23, loss = 0.26885641\n",
      "Iteration 23, loss = 0.27273751\n",
      "Iteration 24, loss = 0.26830040\n",
      "Iteration 24, loss = 0.27237865\n",
      "Iteration 25, loss = 0.27118006\n",
      "Iteration 25, loss = 0.27225089\n",
      "Iteration 26, loss = 0.27127173\n",
      "Iteration 26, loss = 0.27148802\n",
      "Iteration 27, loss = 0.27086341\n",
      "Iteration 27, loss = 0.27089473\n",
      "Iteration 28, loss = 0.26932161\n",
      "Iteration 28, loss = 0.27228979\n",
      "Iteration 29, loss = 0.27011005\n",
      "Iteration 29, loss = 0.27202001\n",
      "Iteration 30, loss = 0.26921337\n",
      "Iteration 31, loss = 0.26852486\n",
      "Iteration 30, loss = 0.27183393\n",
      "Iteration 32, loss = 0.26836338\n",
      "Iteration 31, loss = 0.27131873\n",
      "Iteration 33, loss = 0.26874550\n",
      "Iteration 32, loss = 0.27164871\n",
      "Iteration 34, loss = 0.26818681\n",
      "Iteration 33, loss = 0.27002560\n",
      "Iteration 35, loss = 0.26855976\n",
      "Iteration 34, loss = 0.27216418\n",
      "Iteration 36, loss = 0.26892459\n",
      "Iteration 35, loss = 0.27195945\n",
      "Iteration 37, loss = 0.26953816\n",
      "Iteration 36, loss = 0.27018684\n",
      "Iteration 38, loss = 0.26918068\n",
      "Iteration 37, loss = 0.27075671\n",
      "Iteration 39, loss = 0.26874724\n",
      "Iteration 38, loss = 0.26971337\n",
      "Iteration 40, loss = 0.27004339\n",
      "Iteration 39, loss = 0.27105182\n",
      "Iteration 41, loss = 0.26778920\n",
      "Iteration 40, loss = 0.26986763\n",
      "Iteration 42, loss = 0.26815758\n",
      "Iteration 41, loss = 0.26988755\n",
      "Iteration 43, loss = 0.26811660\n",
      "Iteration 42, loss = 0.27141284\n",
      "Iteration 44, loss = 0.26830427\n",
      "Iteration 43, loss = 0.27144982\n",
      "Iteration 45, loss = 0.26957462\n",
      "Iteration 44, loss = 0.26940181\n",
      "Iteration 46, loss = 0.26825880\n",
      "Iteration 45, loss = 0.27118090\n",
      "Iteration 47, loss = 0.26814932\n",
      "Iteration 46, loss = 0.27143399\n",
      "Iteration 48, loss = 0.26844660\n",
      "Iteration 47, loss = 0.26939656\n",
      "Iteration 49, loss = 0.26844132\n",
      "Iteration 48, loss = 0.27129610\n",
      "Iteration 50, loss = 0.26880346\n",
      "Iteration 49, loss = 0.27029041\n",
      "Iteration 51, loss = 0.26862792\n",
      "Iteration 50, loss = 0.26980729\n",
      "Iteration 52, loss = 0.26848873\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 51, loss = 0.27049884\n",
      "Iteration 52, loss = 0.27050835\n",
      "Iteration 53, loss = 0.27053168\n",
      "Iteration 1, loss = 0.30466980\n",
      "Iteration 2, loss = 0.28130188\n",
      "Iteration 54, loss = 0.27045023\n",
      "Iteration 3, loss = 0.28029047\n",
      "Iteration 55, loss = 0.26985813\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 4, loss = 0.27732510\n",
      "Iteration 1, loss = 0.30620435\n",
      "Iteration 5, loss = 0.27661708\n",
      "Iteration 6, loss = 0.27472507\n",
      "Iteration 2, loss = 0.28060495\n",
      "Iteration 7, loss = 0.27484464\n",
      "Iteration 3, loss = 0.27812417\n",
      "Iteration 8, loss = 0.27323192Iteration 4, loss = 0.27462893\n",
      "\n",
      "Iteration 5, loss = 0.27412000\n",
      "Iteration 9, loss = 0.27425621\n",
      "Iteration 6, loss = 0.27275551\n",
      "Iteration 10, loss = 0.27261572\n",
      "Iteration 7, loss = 0.27291159\n",
      "Iteration 11, loss = 0.27320563\n",
      "Iteration 8, loss = 0.27291041\n",
      "Iteration 12, loss = 0.27301671\n",
      "Iteration 13, loss = 0.27367919\n",
      "Iteration 9, loss = 0.27236433\n",
      "Iteration 14, loss = 0.27180472\n",
      "Iteration 10, loss = 0.27179694\n",
      "Iteration 15, loss = 0.27266517\n",
      "Iteration 11, loss = 0.27151483\n",
      "Iteration 16, loss = 0.27131465\n",
      "Iteration 17, loss = 0.27110850\n",
      "Iteration 12, loss = 0.27051571\n",
      "Iteration 18, loss = 0.27106719\n",
      "Iteration 13, loss = 0.27246541\n",
      "Iteration 19, loss = 0.27417412\n",
      "Iteration 14, loss = 0.27000236\n",
      "Iteration 20, loss = 0.27064186\n",
      "Iteration 15, loss = 0.27092438\n",
      "Iteration 21, loss = 0.27073094\n",
      "Iteration 16, loss = 0.27003348\n",
      "Iteration 22, loss = 0.27277319\n",
      "Iteration 17, loss = 0.26949176\n",
      "Iteration 23, loss = 0.27192981\n",
      "Iteration 18, loss = 0.27003945\n",
      "Iteration 24, loss = 0.26999645\n",
      "Iteration 19, loss = 0.27043352\n",
      "Iteration 25, loss = 0.27014211\n",
      "Iteration 20, loss = 0.26919358\n",
      "Iteration 26, loss = 0.27009326\n",
      "Iteration 21, loss = 0.26871358\n",
      "Iteration 22, loss = 0.26880041\n",
      "Iteration 27, loss = 0.27009632\n",
      "Iteration 23, loss = 0.26968053\n",
      "Iteration 28, loss = 0.26993816\n",
      "Iteration 24, loss = 0.26735117\n",
      "Iteration 29, loss = 0.26888022\n",
      "Iteration 25, loss = 0.26805841\n",
      "Iteration 30, loss = 0.27123278\n",
      "Iteration 26, loss = 0.26756221\n",
      "Iteration 31, loss = 0.26917344\n",
      "Iteration 27, loss = 0.26822053\n",
      "Iteration 32, loss = 0.27158219\n",
      "Iteration 28, loss = 0.26831825\n",
      "Iteration 33, loss = 0.26801547\n",
      "Iteration 29, loss = 0.26693630\n",
      "Iteration 34, loss = 0.27069737\n",
      "Iteration 30, loss = 0.26870391\n",
      "Iteration 35, loss = 0.26888333\n",
      "Iteration 31, loss = 0.26622751\n",
      "Iteration 36, loss = 0.26918921\n",
      "Iteration 32, loss = 0.26789476\n",
      "Iteration 37, loss = 0.26871349\n",
      "Iteration 33, loss = 0.26592253\n",
      "Iteration 38, loss = 0.26906870\n",
      "Iteration 34, loss = 0.26696714\n",
      "Iteration 39, loss = 0.26926117\n",
      "Iteration 35, loss = 0.26718228\n",
      "Iteration 40, loss = 0.26870649\n",
      "Iteration 36, loss = 0.26654779\n",
      "Iteration 41, loss = 0.26786710\n",
      "Iteration 37, loss = 0.26686480\n",
      "Iteration 38, loss = 0.26705092\n",
      "Iteration 42, loss = 0.26938622\n",
      "Iteration 39, loss = 0.26698000\n",
      "Iteration 43, loss = 0.27044752\n",
      "Iteration 40, loss = 0.26647460\n",
      "Iteration 44, loss = 0.26815336\n",
      "Iteration 41, loss = 0.26560252\n",
      "Iteration 45, loss = 0.26948380\n",
      "Iteration 42, loss = 0.26602680\n",
      "Iteration 46, loss = 0.27033850\n",
      "Iteration 43, loss = 0.26849783\n",
      "Iteration 47, loss = 0.26848414\n",
      "Iteration 44, loss = 0.26563589\n",
      "Iteration 48, loss = 0.26968448\n",
      "Iteration 45, loss = 0.26536160\n",
      "Iteration 49, loss = 0.26959471\n",
      "Iteration 46, loss = 0.26651601\n",
      "Iteration 50, loss = 0.26856990\n",
      "Iteration 47, loss = 0.26545184\n",
      "Iteration 51, loss = 0.26889471\n",
      "Iteration 48, loss = 0.26643271\n",
      "Iteration 52, loss = 0.26854353\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 49, loss = 0.26679945\n",
      "Iteration 50, loss = 0.26645631\n",
      "Iteration 51, loss = 0.26708853\n",
      "Iteration 1, loss = 0.30401143\n",
      "Iteration 52, loss = 0.26688201\n",
      "Iteration 2, loss = 0.27966831\n",
      "Iteration 53, loss = 0.26618862\n",
      "Iteration 3, loss = 0.27741935\n",
      "Iteration 54, loss = 0.26560953\n",
      "Iteration 4, loss = 0.27498121\n",
      "Iteration 55, loss = 0.26670162\n",
      "Iteration 5, loss = 0.27347258\n",
      "Iteration 56, loss = 0.26558004\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 6, loss = 0.27280003\n",
      "Iteration 7, loss = 0.27286551\n",
      "Iteration 1, loss = 0.30319386\n",
      "Iteration 8, loss = 0.27240821\n",
      "Iteration 2, loss = 0.28187399\n",
      "Iteration 9, loss = 0.27215980\n",
      "Iteration 3, loss = 0.27888645\n",
      "Iteration 10, loss = 0.27167365\n",
      "Iteration 4, loss = 0.27712809\n",
      "Iteration 11, loss = 0.27157494\n",
      "Iteration 5, loss = 0.27533045\n",
      "Iteration 12, loss = 0.27126871\n",
      "Iteration 6, loss = 0.27415412\n",
      "Iteration 13, loss = 0.27210151\n",
      "Iteration 7, loss = 0.27319381\n",
      "Iteration 14, loss = 0.26911869\n",
      "Iteration 8, loss = 0.27329990\n",
      "Iteration 15, loss = 0.27054361\n",
      "Iteration 9, loss = 0.27372663\n",
      "Iteration 16, loss = 0.26870097\n",
      "Iteration 10, loss = 0.27332832\n",
      "Iteration 17, loss = 0.26880577\n",
      "Iteration 11, loss = 0.27272151\n",
      "Iteration 18, loss = 0.26826617\n",
      "Iteration 19, loss = 0.26852140\n",
      "Iteration 12, loss = 0.27280739\n",
      "Iteration 20, loss = 0.26829666\n",
      "Iteration 13, loss = 0.27382239\n",
      "Iteration 21, loss = 0.26846790\n",
      "Iteration 14, loss = 0.27180870\n",
      "Iteration 22, loss = 0.26907801\n",
      "Iteration 15, loss = 0.27207085\n",
      "Iteration 23, loss = 0.26948656\n",
      "Iteration 16, loss = 0.27178270\n",
      "Iteration 24, loss = 0.26752015\n",
      "Iteration 17, loss = 0.27194354\n",
      "Iteration 25, loss = 0.26784625\n",
      "Iteration 18, loss = 0.27194570\n",
      "Iteration 26, loss = 0.26706193\n",
      "Iteration 19, loss = 0.27232455\n",
      "Iteration 27, loss = 0.26716075\n",
      "Iteration 20, loss = 0.27093805\n",
      "Iteration 28, loss = 0.26763923\n",
      "Iteration 21, loss = 0.27181508\n",
      "Iteration 29, loss = 0.26631770\n",
      "Iteration 22, loss = 0.27368854\n",
      "Iteration 30, loss = 0.26858762\n",
      "Iteration 23, loss = 0.27209766\n",
      "Iteration 31, loss = 0.26669002\n",
      "Iteration 24, loss = 0.27173101\n",
      "Iteration 32, loss = 0.26735633\n",
      "Iteration 25, loss = 0.27106729\n",
      "Iteration 33, loss = 0.26708258\n",
      "Iteration 26, loss = 0.27052700\n",
      "Iteration 34, loss = 0.26648330\n",
      "Iteration 27, loss = 0.27052346\n",
      "Iteration 35, loss = 0.26662199\n",
      "Iteration 28, loss = 0.27055724\n",
      "Iteration 36, loss = 0.26734902\n",
      "Iteration 29, loss = 0.27004645\n",
      "Iteration 37, loss = 0.26648597\n",
      "Iteration 30, loss = 0.27027295\n",
      "Iteration 31, loss = 0.26991766\n",
      "Iteration 38, loss = 0.26803325\n",
      "Iteration 39, loss = 0.26672185\n",
      "Iteration 32, loss = 0.26986755\n",
      "Iteration 40, loss = 0.26611982\n",
      "Iteration 33, loss = 0.27042778\n",
      "Iteration 41, loss = 0.26527678\n",
      "Iteration 34, loss = 0.26981695\n",
      "Iteration 42, loss = 0.26523160\n",
      "Iteration 35, loss = 0.26980318\n",
      "Iteration 43, loss = 0.26734386\n",
      "Iteration 36, loss = 0.27082702\n",
      "Iteration 44, loss = 0.26513659\n",
      "Iteration 37, loss = 0.26904746\n",
      "Iteration 45, loss = 0.26585412\n",
      "Iteration 38, loss = 0.26905833\n",
      "Iteration 46, loss = 0.26742048\n",
      "Iteration 39, loss = 0.27043598\n",
      "Iteration 47, loss = 0.26486950\n",
      "Iteration 40, loss = 0.26937815\n",
      "Iteration 48, loss = 0.26691804\n",
      "Iteration 41, loss = 0.26886511\n",
      "Iteration 49, loss = 0.26693791\n",
      "Iteration 42, loss = 0.26934131\n",
      "Iteration 43, loss = 0.26957063\n",
      "Iteration 50, loss = 0.26826614\n",
      "Iteration 44, loss = 0.26887713\n",
      "Iteration 51, loss = 0.26745809\n",
      "Iteration 45, loss = 0.26890925\n",
      "Iteration 52, loss = 0.26553559\n",
      "Iteration 53, loss = 0.26668605Iteration 46, loss = 0.27136006\n",
      "\n",
      "Iteration 54, loss = 0.26627250\n",
      "Iteration 47, loss = 0.26821506\n",
      "Iteration 55, loss = 0.26595110\n",
      "Iteration 48, loss = 0.26914455\n",
      "Iteration 56, loss = 0.26539302\n",
      "Iteration 49, loss = 0.27021326\n",
      "Iteration 57, loss = 0.26616681\n",
      "Iteration 50, loss = 0.27096105\n",
      "Iteration 58, loss = 0.26685010\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 51, loss = 0.27049404\n",
      "Iteration 52, loss = 0.26900978\n",
      "Iteration 1, loss = 0.30208593\n",
      "Iteration 53, loss = 0.26850098\n",
      "Iteration 2, loss = 0.28117297\n",
      "Iteration 54, loss = 0.26904845\n",
      "Iteration 3, loss = 0.27932941\n",
      "Iteration 55, loss = 0.26862837\n",
      "Iteration 4, loss = 0.27749549\n",
      "Iteration 56, loss = 0.26849445\n",
      "Iteration 5, loss = 0.27652926\n",
      "Iteration 57, loss = 0.26807618\n",
      "Iteration 6, loss = 0.27545139\n",
      "Iteration 58, loss = 0.26965374\n",
      "Iteration 7, loss = 0.27501379\n",
      "Iteration 59, loss = 0.26857133\n",
      "Iteration 8, loss = 0.27294616\n",
      "Iteration 60, loss = 0.26864666\n",
      "Iteration 9, loss = 0.27514418\n",
      "Iteration 61, loss = 0.26865214\n",
      "Iteration 10, loss = 0.27315717\n",
      "Iteration 62, loss = 0.26777712\n",
      "Iteration 11, loss = 0.27234565\n",
      "Iteration 63, loss = 0.26840569\n",
      "Iteration 12, loss = 0.27244005\n",
      "Iteration 64, loss = 0.26845793\n",
      "Iteration 13, loss = 0.27475969\n",
      "Iteration 65, loss = 0.26934743\n",
      "Iteration 14, loss = 0.27173069\n",
      "Iteration 66, loss = 0.26891515\n",
      "Iteration 15, loss = 0.27214334\n",
      "Iteration 67, loss = 0.26841791\n",
      "Iteration 16, loss = 0.27151337\n",
      "Iteration 17, loss = 0.27139785\n",
      "Iteration 68, loss = 0.26897975\n",
      "Iteration 18, loss = 0.27216746\n",
      "Iteration 69, loss = 0.27161954\n",
      "Iteration 19, loss = 0.27200957\n",
      "Iteration 20, loss = 0.27045719\n",
      "Iteration 70, loss = 0.26836522\n",
      "Iteration 71, loss = 0.27055641\n",
      "Iteration 21, loss = 0.27207939\n",
      "Iteration 72, loss = 0.26784079\n",
      "Iteration 22, loss = 0.27210162\n",
      "Iteration 73, loss = 0.26960796\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 23, loss = 0.27226334\n",
      "Iteration 24, loss = 0.27005742\n",
      "Iteration 1, loss = 0.30196333\n",
      "Iteration 25, loss = 0.27045732\n",
      "Iteration 2, loss = 0.28056753\n",
      "Iteration 3, loss = 0.27656580\n",
      "Iteration 26, loss = 0.27078988\n",
      "Iteration 27, loss = 0.27019311\n",
      "Iteration 4, loss = 0.27587217\n",
      "Iteration 28, loss = 0.27059981\n",
      "Iteration 5, loss = 0.27408447\n",
      "Iteration 29, loss = 0.27130461\n",
      "Iteration 6, loss = 0.27336746\n",
      "Iteration 30, loss = 0.27192193\n",
      "Iteration 7, loss = 0.27239688\n",
      "Iteration 31, loss = 0.27098458\n",
      "Iteration 8, loss = 0.27139753\n",
      "Iteration 32, loss = 0.27097356\n",
      "Iteration 9, loss = 0.27258777\n",
      "Iteration 33, loss = 0.26993781\n",
      "Iteration 10, loss = 0.27209162\n",
      "Iteration 34, loss = 0.27163999\n",
      "Iteration 11, loss = 0.27017776\n",
      "Iteration 35, loss = 0.27036235\n",
      "Iteration 12, loss = 0.26985556\n",
      "Iteration 36, loss = 0.27218668\n",
      "Iteration 13, loss = 0.27049003\n",
      "Iteration 37, loss = 0.27115297\n",
      "Iteration 14, loss = 0.26898186\n",
      "Iteration 38, loss = 0.27011932\n",
      "Iteration 15, loss = 0.26834155\n",
      "Iteration 39, loss = 0.27025472\n",
      "Iteration 16, loss = 0.26883404\n",
      "Iteration 40, loss = 0.27032494\n",
      "Iteration 17, loss = 0.26903530\n",
      "Iteration 41, loss = 0.26982955\n",
      "Iteration 18, loss = 0.26814993\n",
      "Iteration 42, loss = 0.27053115\n",
      "Iteration 43, loss = 0.27187403\n",
      "Iteration 19, loss = 0.26858835\n",
      "Iteration 44, loss = 0.26995867\n",
      "Iteration 20, loss = 0.26726065\n",
      "Iteration 45, loss = 0.27038003\n",
      "Iteration 21, loss = 0.26885366\n",
      "Iteration 46, loss = 0.27202086\n",
      "Iteration 22, loss = 0.26723311\n",
      "Iteration 47, loss = 0.26935456\n",
      "Iteration 23, loss = 0.26810284\n",
      "Iteration 48, loss = 0.27093160\n",
      "Iteration 24, loss = 0.26757522\n",
      "Iteration 49, loss = 0.27041878\n",
      "Iteration 25, loss = 0.26715343\n",
      "Iteration 50, loss = 0.27348512\n",
      "Iteration 26, loss = 0.26698587\n",
      "Iteration 51, loss = 0.27122019\n",
      "Iteration 27, loss = 0.26620886\n",
      "Iteration 52, loss = 0.26984096\n",
      "Iteration 28, loss = 0.26656107\n",
      "Iteration 53, loss = 0.26962808\n",
      "Iteration 29, loss = 0.26603835\n",
      "Iteration 54, loss = 0.27084425\n",
      "Iteration 30, loss = 0.26724770\n",
      "Iteration 55, loss = 0.26874390\n",
      "Iteration 31, loss = 0.26806962\n",
      "Iteration 56, loss = 0.26974202\n",
      "Iteration 32, loss = 0.26768074\n",
      "Iteration 57, loss = 0.27041367\n",
      "Iteration 33, loss = 0.26708515\n",
      "Iteration 58, loss = 0.27153346\n",
      "Iteration 34, loss = 0.26668670\n",
      "Iteration 59, loss = 0.26982102\n",
      "Iteration 35, loss = 0.26624784\n",
      "Iteration 60, loss = 0.27083591\n",
      "Iteration 36, loss = 0.26686496\n",
      "Iteration 61, loss = 0.27091222\n",
      "Iteration 37, loss = 0.26725364\n",
      "Iteration 62, loss = 0.27011040\n",
      "Iteration 38, loss = 0.26572230\n",
      "Iteration 63, loss = 0.26999101\n",
      "Iteration 39, loss = 0.26657609\n",
      "Iteration 64, loss = 0.27044126\n",
      "Iteration 40, loss = 0.26583580\n",
      "Iteration 65, loss = 0.27058310\n",
      "Iteration 41, loss = 0.26492251\n",
      "Iteration 66, loss = 0.27029733\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 42, loss = 0.26571422\n",
      "Iteration 43, loss = 0.26702757\n",
      "Iteration 1, loss = 0.30331116\n",
      "Iteration 44, loss = 0.26621479\n",
      "Iteration 2, loss = 0.28040935\n",
      "Iteration 45, loss = 0.26558021\n",
      "Iteration 3, loss = 0.27800338\n",
      "Iteration 46, loss = 0.26749375\n",
      "Iteration 47, loss = 0.26522213\n",
      "Iteration 4, loss = 0.27622913\n",
      "Iteration 48, loss = 0.26572984\n",
      "Iteration 5, loss = 0.27585197\n",
      "Iteration 6, loss = 0.27583606\n",
      "Iteration 49, loss = 0.26555077\n",
      "Iteration 7, loss = 0.27490922\n",
      "Iteration 50, loss = 0.26898921\n",
      "Iteration 8, loss = 0.27343153\n",
      "Iteration 51, loss = 0.26692972\n",
      "Iteration 9, loss = 0.27448344\n",
      "Iteration 52, loss = 0.26542301\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 10, loss = 0.27444400\n",
      "Iteration 11, loss = 0.27243465\n",
      "Iteration 1, loss = 0.30303080\n",
      "Iteration 12, loss = 0.27285895\n",
      "Iteration 13, loss = 0.27376548\n",
      "Iteration 2, loss = 0.27913126\n",
      "Iteration 14, loss = 0.27264120\n",
      "Iteration 3, loss = 0.27633635\n",
      "Iteration 15, loss = 0.27282640\n",
      "Iteration 4, loss = 0.27538049\n",
      "Iteration 16, loss = 0.27252411\n",
      "Iteration 5, loss = 0.27408142\n",
      "Iteration 17, loss = 0.27284141\n",
      "Iteration 6, loss = 0.27353308\n",
      "Iteration 18, loss = 0.27285338\n",
      "Iteration 19, loss = 0.27234208\n",
      "Iteration 7, loss = 0.27241706\n",
      "Iteration 20, loss = 0.27102660\n",
      "Iteration 8, loss = 0.27182687\n",
      "Iteration 21, loss = 0.27199659\n",
      "Iteration 9, loss = 0.27147610\n",
      "Iteration 22, loss = 0.27071793\n",
      "Iteration 10, loss = 0.27170433\n",
      "Iteration 23, loss = 0.27071692\n",
      "Iteration 11, loss = 0.27044670\n",
      "Iteration 24, loss = 0.26925310\n",
      "Iteration 12, loss = 0.27018830\n",
      "Iteration 25, loss = 0.27012686\n",
      "Iteration 13, loss = 0.27088305\n",
      "Iteration 26, loss = 0.26984571\n",
      "Iteration 14, loss = 0.27051987\n",
      "Iteration 27, loss = 0.27011236\n",
      "Iteration 15, loss = 0.27108291\n",
      "Iteration 28, loss = 0.26928275\n",
      "Iteration 16, loss = 0.27119119\n",
      "Iteration 29, loss = 0.26959604\n",
      "Iteration 17, loss = 0.26955442\n",
      "Iteration 30, loss = 0.26991240\n",
      "Iteration 18, loss = 0.26911630\n",
      "Iteration 31, loss = 0.26859486\n",
      "Iteration 19, loss = 0.26963191\n",
      "Iteration 32, loss = 0.26977415\n",
      "Iteration 20, loss = 0.26898646\n",
      "Iteration 33, loss = 0.26851449\n",
      "Iteration 21, loss = 0.26907079\n",
      "Iteration 22, loss = 0.26797125\n",
      "Iteration 34, loss = 0.26881797\n",
      "Iteration 23, loss = 0.26910954\n",
      "Iteration 35, loss = 0.26943628\n",
      "Iteration 24, loss = 0.26789293\n",
      "Iteration 36, loss = 0.26818132\n",
      "Iteration 25, loss = 0.26789385\n",
      "Iteration 37, loss = 0.26893429\n",
      "Iteration 26, loss = 0.26776199\n",
      "Iteration 38, loss = 0.27008891\n",
      "Iteration 27, loss = 0.26706452\n",
      "Iteration 39, loss = 0.26936458\n",
      "Iteration 28, loss = 0.26755111\n",
      "Iteration 40, loss = 0.27050667\n",
      "Iteration 29, loss = 0.26803685\n",
      "Iteration 41, loss = 0.26881374\n",
      "Iteration 30, loss = 0.26836540\n",
      "Iteration 42, loss = 0.26922666\n",
      "Iteration 31, loss = 0.26688594\n",
      "Iteration 43, loss = 0.26887829\n",
      "Iteration 32, loss = 0.26654036\n",
      "Iteration 44, loss = 0.26846746\n",
      "Iteration 33, loss = 0.26619153\n",
      "Iteration 45, loss = 0.26988935\n",
      "Iteration 34, loss = 0.26716919\n",
      "Iteration 46, loss = 0.26950966\n",
      "Iteration 35, loss = 0.26689141\n",
      "Iteration 47, loss = 0.26839606\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 36, loss = 0.26765944\n",
      "Iteration 37, loss = 0.26699136\n",
      "Iteration 38, loss = 0.26650119\n",
      "Iteration 39, loss = 0.26687955\n",
      "Iteration 40, loss = 0.26592792\n",
      "Iteration 41, loss = 0.26583400\n",
      "Iteration 42, loss = 0.26586674\n",
      "Iteration 43, loss = 0.26605500\n",
      "Iteration 44, loss = 0.26504448\n",
      "Iteration 45, loss = 0.26559681\n",
      "Iteration 46, loss = 0.26627510\n",
      "Iteration 47, loss = 0.26558210\n",
      "Iteration 48, loss = 0.26569284\n",
      "Iteration 49, loss = 0.26579392\n",
      "Iteration 50, loss = 0.26838372\n",
      "Iteration 51, loss = 0.26487534\n",
      "Iteration 52, loss = 0.26493553\n",
      "Iteration 53, loss = 0.26467049\n",
      "Iteration 54, loss = 0.26876796\n",
      "Iteration 55, loss = 0.26536152\n",
      "Iteration 56, loss = 0.26559944\n",
      "Iteration 57, loss = 0.26574335\n",
      "Iteration 58, loss = 0.26678119\n",
      "Iteration 59, loss = 0.26519386\n",
      "Iteration 60, loss = 0.26565716\n",
      "Iteration 61, loss = 0.26561399\n",
      "Iteration 62, loss = 0.26413329\n",
      "Iteration 63, loss = 0.26425807\n",
      "Iteration 64, loss = 0.26454469\n",
      "Iteration 65, loss = 0.26568561\n",
      "Iteration 66, loss = 0.26405488\n",
      "Iteration 67, loss = 0.26497913\n",
      "Iteration 68, loss = 0.26488050\n",
      "Iteration 69, loss = 0.26516070\n",
      "Iteration 70, loss = 0.26473013\n",
      "Iteration 71, loss = 0.26582886\n",
      "Iteration 72, loss = 0.26535535\n",
      "Iteration 73, loss = 0.26498337\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29945505\n",
      "Iteration 1, loss = 0.30031038\n",
      "Iteration 2, loss = 0.27933041\n",
      "Iteration 2, loss = 0.27984937\n",
      "Iteration 3, loss = 0.27653374\n",
      "Iteration 3, loss = 0.27788768\n",
      "Iteration 4, loss = 0.27451622\n",
      "Iteration 4, loss = 0.27538422\n",
      "Iteration 5, loss = 0.27555552\n",
      "Iteration 5, loss = 0.27530521\n",
      "Iteration 6, loss = 0.27443297\n",
      "Iteration 6, loss = 0.27552699\n",
      "Iteration 7, loss = 0.27340857\n",
      "Iteration 7, loss = 0.27326466\n",
      "Iteration 8, loss = 0.27216871\n",
      "Iteration 8, loss = 0.27298475\n",
      "Iteration 9, loss = 0.27157148\n",
      "Iteration 9, loss = 0.27301719\n",
      "Iteration 10, loss = 0.27070710\n",
      "Iteration 10, loss = 0.27166039\n",
      "Iteration 11, loss = 0.27087776\n",
      "Iteration 12, loss = 0.27083907\n",
      "Iteration 11, loss = 0.27148017\n",
      "Iteration 13, loss = 0.27052155\n",
      "Iteration 12, loss = 0.27110135\n",
      "Iteration 14, loss = 0.27033719Iteration 13, loss = 0.27109172\n",
      "\n",
      "Iteration 14, loss = 0.27008240\n",
      "Iteration 15, loss = 0.27135714\n",
      "Iteration 15, loss = 0.27087478\n",
      "Iteration 16, loss = 0.27023876\n",
      "Iteration 16, loss = 0.27043531\n",
      "Iteration 17, loss = 0.26957926\n",
      "Iteration 18, loss = 0.26944382\n",
      "Iteration 17, loss = 0.27077560\n",
      "Iteration 18, loss = 0.26902399\n",
      "Iteration 19, loss = 0.27077648\n",
      "Iteration 19, loss = 0.27056076\n",
      "Iteration 20, loss = 0.26981903\n",
      "Iteration 20, loss = 0.27000620\n",
      "Iteration 21, loss = 0.26988304\n",
      "Iteration 21, loss = 0.26978974\n",
      "Iteration 22, loss = 0.27017887\n",
      "Iteration 22, loss = 0.27054761\n",
      "Iteration 23, loss = 0.27039400\n",
      "Iteration 23, loss = 0.26943988\n",
      "Iteration 24, loss = 0.26910253\n",
      "Iteration 24, loss = 0.26914082\n",
      "Iteration 25, loss = 0.26987608\n",
      "Iteration 25, loss = 0.27018894\n",
      "Iteration 26, loss = 0.26964113\n",
      "Iteration 26, loss = 0.26883331\n",
      "Iteration 27, loss = 0.26894183\n",
      "Iteration 27, loss = 0.26896818\n",
      "Iteration 28, loss = 0.26868173\n",
      "Iteration 28, loss = 0.26972278\n",
      "Iteration 29, loss = 0.26959604\n",
      "Iteration 29, loss = 0.27033363\n",
      "Iteration 30, loss = 0.26923231\n",
      "Iteration 30, loss = 0.26883075\n",
      "Iteration 31, loss = 0.26956736\n",
      "Iteration 31, loss = 0.26988770\n",
      "Iteration 32, loss = 0.26937509\n",
      "Iteration 32, loss = 0.26989913\n",
      "Iteration 33, loss = 0.26854431\n",
      "Iteration 33, loss = 0.26912669\n",
      "Iteration 34, loss = 0.26979298\n",
      "Iteration 34, loss = 0.27004907\n",
      "Iteration 35, loss = 0.26838849\n",
      "Iteration 35, loss = 0.26884281\n",
      "Iteration 36, loss = 0.26900794\n",
      "Iteration 36, loss = 0.27039477\n",
      "Iteration 37, loss = 0.26891650\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 37, loss = 0.26894123\n",
      "Iteration 38, loss = 0.26788955\n",
      "Iteration 1, loss = 0.29798196\n",
      "Iteration 39, loss = 0.26929119\n",
      "Iteration 2, loss = 0.28077066\n",
      "Iteration 40, loss = 0.26927187\n",
      "Iteration 3, loss = 0.27721779\n",
      "Iteration 41, loss = 0.26954458\n",
      "Iteration 4, loss = 0.27574418\n",
      "Iteration 42, loss = 0.26945644\n",
      "Iteration 5, loss = 0.27528564\n",
      "Iteration 6, loss = 0.27443085\n",
      "Iteration 43, loss = 0.26930562\n",
      "Iteration 7, loss = 0.27412425\n",
      "Iteration 44, loss = 0.26884568\n",
      "Iteration 8, loss = 0.27314389\n",
      "Iteration 45, loss = 0.26940820\n",
      "Iteration 9, loss = 0.27225099\n",
      "Iteration 46, loss = 0.26784420\n",
      "Iteration 10, loss = 0.27170445\n",
      "Iteration 47, loss = 0.26903792\n",
      "Iteration 11, loss = 0.27176691\n",
      "Iteration 48, loss = 0.26890097\n",
      "Iteration 12, loss = 0.27112845\n",
      "Iteration 49, loss = 0.26829565\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 13, loss = 0.27065271\n",
      "Iteration 14, loss = 0.27069533\n",
      "Iteration 1, loss = 0.30027487\n",
      "Iteration 15, loss = 0.27108862\n",
      "Iteration 2, loss = 0.28153633\n",
      "Iteration 16, loss = 0.27034223\n",
      "Iteration 3, loss = 0.28023069\n",
      "Iteration 17, loss = 0.27112061\n",
      "Iteration 4, loss = 0.27786203\n",
      "Iteration 18, loss = 0.26996382\n",
      "Iteration 5, loss = 0.27645464\n",
      "Iteration 19, loss = 0.26995906\n",
      "Iteration 6, loss = 0.27582083\n",
      "Iteration 20, loss = 0.26953229\n",
      "Iteration 7, loss = 0.27449751\n",
      "Iteration 21, loss = 0.26913850\n",
      "Iteration 8, loss = 0.27430742\n",
      "Iteration 22, loss = 0.27095541\n",
      "Iteration 9, loss = 0.27392923\n",
      "Iteration 23, loss = 0.26979410\n",
      "Iteration 10, loss = 0.27287612\n",
      "Iteration 24, loss = 0.26919021\n",
      "Iteration 11, loss = 0.27256062\n",
      "Iteration 25, loss = 0.26960125\n",
      "Iteration 12, loss = 0.27273715\n",
      "Iteration 26, loss = 0.26879611\n",
      "Iteration 13, loss = 0.27255648\n",
      "Iteration 27, loss = 0.26913717\n",
      "Iteration 14, loss = 0.27321750\n",
      "Iteration 28, loss = 0.26921533\n",
      "Iteration 15, loss = 0.27214316\n",
      "Iteration 29, loss = 0.27028552\n",
      "Iteration 16, loss = 0.27253055\n",
      "Iteration 30, loss = 0.26887775\n",
      "Iteration 31, loss = 0.27015029\n",
      "Iteration 17, loss = 0.27214107\n",
      "Iteration 32, loss = 0.26884691\n",
      "Iteration 18, loss = 0.27200429\n",
      "Iteration 33, loss = 0.26896584\n",
      "Iteration 19, loss = 0.27319634\n",
      "Iteration 34, loss = 0.26990743\n",
      "Iteration 20, loss = 0.27178081\n",
      "Iteration 35, loss = 0.26859507\n",
      "Iteration 21, loss = 0.27145739\n",
      "Iteration 36, loss = 0.26877096\n",
      "Iteration 22, loss = 0.27132732\n",
      "Iteration 37, loss = 0.26943676\n",
      "Iteration 23, loss = 0.27120221\n",
      "Iteration 38, loss = 0.26947204\n",
      "Iteration 24, loss = 0.27178658\n",
      "Iteration 39, loss = 0.27001810\n",
      "Iteration 25, loss = 0.27062414\n",
      "Iteration 40, loss = 0.26875829\n",
      "Iteration 26, loss = 0.27098023\n",
      "Iteration 41, loss = 0.26918844\n",
      "Iteration 27, loss = 0.27221083\n",
      "Iteration 42, loss = 0.26884283\n",
      "Iteration 43, loss = 0.26870994\n",
      "Iteration 28, loss = 0.26973941\n",
      "Iteration 44, loss = 0.26846618\n",
      "Iteration 29, loss = 0.27148903\n",
      "Iteration 45, loss = 0.26806758\n",
      "Iteration 30, loss = 0.27055739\n",
      "Iteration 46, loss = 0.26790796\n",
      "Iteration 31, loss = 0.27053285\n",
      "Iteration 47, loss = 0.26835135\n",
      "Iteration 48, loss = 0.26839011\n",
      "Iteration 32, loss = 0.27247206\n",
      "Iteration 49, loss = 0.26783259\n",
      "Iteration 33, loss = 0.27065930\n",
      "Iteration 50, loss = 0.26878305\n",
      "Iteration 34, loss = 0.27105136\n",
      "Iteration 51, loss = 0.26832431\n",
      "Iteration 35, loss = 0.27158967\n",
      "Iteration 52, loss = 0.26766642\n",
      "Iteration 36, loss = 0.27149075\n",
      "Iteration 53, loss = 0.26954584\n",
      "Iteration 37, loss = 0.27111540\n",
      "Iteration 54, loss = 0.26909739\n",
      "Iteration 38, loss = 0.27043209\n",
      "Iteration 55, loss = 0.26858985\n",
      "Iteration 39, loss = 0.27093931\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 56, loss = 0.26922561\n",
      "Iteration 57, loss = 0.26770553\n",
      "Iteration 58, loss = 0.27002991\n",
      "Iteration 1, loss = 0.29858262\n",
      "Iteration 59, loss = 0.26788329\n",
      "Iteration 2, loss = 0.28138542\n",
      "Iteration 60, loss = 0.26855600\n",
      "Iteration 3, loss = 0.27970727\n",
      "Iteration 61, loss = 0.26778772\n",
      "Iteration 4, loss = 0.27772304\n",
      "Iteration 62, loss = 0.26879257\n",
      "Iteration 5, loss = 0.27770503\n",
      "Iteration 63, loss = 0.26829285\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 6, loss = 0.27681069\n",
      "Iteration 7, loss = 0.27569581\n",
      "Iteration 1, loss = 0.29796517\n",
      "Iteration 8, loss = 0.27574271\n",
      "Iteration 2, loss = 0.28037226\n",
      "Iteration 9, loss = 0.27458781\n",
      "Iteration 3, loss = 0.27837660\n",
      "Iteration 10, loss = 0.27374220\n",
      "Iteration 4, loss = 0.27532321\n",
      "Iteration 11, loss = 0.27390367\n",
      "Iteration 5, loss = 0.27583994\n",
      "Iteration 12, loss = 0.27263822\n",
      "Iteration 6, loss = 0.27398891\n",
      "Iteration 13, loss = 0.27192573\n",
      "Iteration 7, loss = 0.27251465\n",
      "Iteration 14, loss = 0.27319214\n",
      "Iteration 8, loss = 0.27195024\n",
      "Iteration 15, loss = 0.27272447\n",
      "Iteration 16, loss = 0.27092989\n",
      "Iteration 9, loss = 0.27216007\n",
      "Iteration 10, loss = 0.27190155\n",
      "Iteration 17, loss = 0.27176633\n",
      "Iteration 11, loss = 0.27125411\n",
      "Iteration 18, loss = 0.27133394\n",
      "Iteration 19, loss = 0.27202041\n",
      "Iteration 12, loss = 0.27112431\n",
      "Iteration 20, loss = 0.27118646\n",
      "Iteration 13, loss = 0.27056289\n",
      "Iteration 21, loss = 0.27074545\n",
      "Iteration 14, loss = 0.27106016\n",
      "Iteration 22, loss = 0.27042695\n",
      "Iteration 15, loss = 0.27114652\n",
      "Iteration 23, loss = 0.27070987\n",
      "Iteration 16, loss = 0.26937193\n",
      "Iteration 24, loss = 0.27059492\n",
      "Iteration 17, loss = 0.26946479\n",
      "Iteration 25, loss = 0.26970394\n",
      "Iteration 18, loss = 0.26972054\n",
      "Iteration 26, loss = 0.26921543\n",
      "Iteration 19, loss = 0.26969977\n",
      "Iteration 27, loss = 0.27146528\n",
      "Iteration 20, loss = 0.27003205\n",
      "Iteration 28, loss = 0.26949253\n",
      "Iteration 21, loss = 0.27016813\n",
      "Iteration 29, loss = 0.27036721\n",
      "Iteration 22, loss = 0.26978957\n",
      "Iteration 30, loss = 0.26926453\n",
      "Iteration 23, loss = 0.26930762\n",
      "Iteration 31, loss = 0.26871426\n",
      "Iteration 24, loss = 0.26843659\n",
      "Iteration 32, loss = 0.26976337\n",
      "Iteration 25, loss = 0.26878757\n",
      "Iteration 33, loss = 0.27021557\n",
      "Iteration 26, loss = 0.26829148\n",
      "Iteration 34, loss = 0.26888359\n",
      "Iteration 27, loss = 0.26941449\n",
      "Iteration 35, loss = 0.27002149\n",
      "Iteration 28, loss = 0.26821240\n",
      "Iteration 29, loss = 0.26873637\n",
      "Iteration 36, loss = 0.27007508\n",
      "Iteration 30, loss = 0.26835256\n",
      "Iteration 37, loss = 0.27030036\n",
      "Iteration 31, loss = 0.26841918\n",
      "Iteration 38, loss = 0.27045346\n",
      "Iteration 32, loss = 0.26918123\n",
      "Iteration 33, loss = 0.26845529\n",
      "Iteration 39, loss = 0.26978457\n",
      "Iteration 34, loss = 0.26791195\n",
      "Iteration 40, loss = 0.26983613\n",
      "Iteration 41, loss = 0.26911330\n",
      "Iteration 35, loss = 0.26942414\n",
      "Iteration 36, loss = 0.26834622\n",
      "Iteration 42, loss = 0.26867099\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 37, loss = 0.26879279\n",
      "Iteration 38, loss = 0.26863640\n",
      "Iteration 1, loss = 0.29892634\n",
      "Iteration 2, loss = 0.28036253\n",
      "Iteration 39, loss = 0.26865478\n",
      "Iteration 3, loss = 0.27937978\n",
      "Iteration 40, loss = 0.26865172\n",
      "Iteration 4, loss = 0.27647526\n",
      "Iteration 41, loss = 0.26809302\n",
      "Iteration 5, loss = 0.27727316\n",
      "Iteration 42, loss = 0.26816110\n",
      "Iteration 6, loss = 0.27518280\n",
      "Iteration 43, loss = 0.26791303\n",
      "Iteration 7, loss = 0.27344228\n",
      "Iteration 44, loss = 0.26874439\n",
      "Iteration 8, loss = 0.27375755\n",
      "Iteration 45, loss = 0.26779163\n",
      "Iteration 9, loss = 0.27281169\n",
      "Iteration 46, loss = 0.26965231\n",
      "Iteration 10, loss = 0.27290208\n",
      "Iteration 47, loss = 0.26844348\n",
      "Iteration 11, loss = 0.27208936\n",
      "Iteration 48, loss = 0.26832874\n",
      "Iteration 12, loss = 0.27212498\n",
      "Iteration 49, loss = 0.26805575\n",
      "Iteration 13, loss = 0.27103860\n",
      "Iteration 50, loss = 0.26796730\n",
      "Iteration 14, loss = 0.27208152\n",
      "Iteration 51, loss = 0.26816102\n",
      "Iteration 15, loss = 0.27203051\n",
      "Iteration 52, loss = 0.26761949\n",
      "Iteration 16, loss = 0.27001753\n",
      "Iteration 53, loss = 0.26852731\n",
      "Iteration 17, loss = 0.27097307\n",
      "Iteration 54, loss = 0.26906447\n",
      "Iteration 18, loss = 0.26982846\n",
      "Iteration 55, loss = 0.26781288\n",
      "Iteration 19, loss = 0.26966945\n",
      "Iteration 56, loss = 0.26803999\n",
      "Iteration 20, loss = 0.26970836\n",
      "Iteration 57, loss = 0.26778213\n",
      "Iteration 21, loss = 0.27146034\n",
      "Iteration 58, loss = 0.26808650\n",
      "Iteration 22, loss = 0.26979122\n",
      "Iteration 59, loss = 0.26838554\n",
      "Iteration 23, loss = 0.26874137\n",
      "Iteration 60, loss = 0.26872312\n",
      "Iteration 24, loss = 0.26870359\n",
      "Iteration 61, loss = 0.26822733\n",
      "Iteration 25, loss = 0.26987651\n",
      "Iteration 62, loss = 0.26867951\n",
      "Iteration 26, loss = 0.26940896\n",
      "Iteration 63, loss = 0.26874259\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 27, loss = 0.26999979\n",
      "Iteration 28, loss = 0.26909733\n",
      "Iteration 1, loss = 0.29628192\n",
      "Iteration 29, loss = 0.26942497\n",
      "Iteration 2, loss = 0.27904053\n",
      "Iteration 30, loss = 0.26898030\n",
      "Iteration 31, loss = 0.26830748\n",
      "Iteration 3, loss = 0.27671669\n",
      "Iteration 32, loss = 0.26931409\n",
      "Iteration 4, loss = 0.27488436\n",
      "Iteration 33, loss = 0.27002418\n",
      "Iteration 5, loss = 0.27587117\n",
      "Iteration 34, loss = 0.26910954\n",
      "Iteration 6, loss = 0.27417822\n",
      "Iteration 35, loss = 0.26958239\n",
      "Iteration 7, loss = 0.27262096\n",
      "Iteration 36, loss = 0.26988609\n",
      "Iteration 8, loss = 0.27237697\n",
      "Iteration 37, loss = 0.26915940\n",
      "Iteration 9, loss = 0.27080167\n",
      "Iteration 38, loss = 0.26912359\n",
      "Iteration 10, loss = 0.27120565\n",
      "Iteration 39, loss = 0.26894096\n",
      "Iteration 11, loss = 0.27053263\n",
      "Iteration 40, loss = 0.26895829\n",
      "Iteration 12, loss = 0.27150658\n",
      "Iteration 41, loss = 0.26801515\n",
      "Iteration 13, loss = 0.27157394\n",
      "Iteration 42, loss = 0.26857483\n",
      "Iteration 14, loss = 0.27200368\n",
      "Iteration 43, loss = 0.26828165\n",
      "Iteration 44, loss = 0.26878679\n",
      "Iteration 15, loss = 0.27222813\n",
      "Iteration 45, loss = 0.26922286\n",
      "Iteration 16, loss = 0.27013401\n",
      "Iteration 46, loss = 0.26991566\n",
      "Iteration 17, loss = 0.26932979\n",
      "Iteration 47, loss = 0.26873253\n",
      "Iteration 18, loss = 0.27000505\n",
      "Iteration 48, loss = 0.26881240\n",
      "Iteration 19, loss = 0.26916364\n",
      "Iteration 49, loss = 0.26926684\n",
      "Iteration 20, loss = 0.26979477\n",
      "Iteration 50, loss = 0.26882208\n",
      "Iteration 21, loss = 0.27131579\n",
      "Iteration 51, loss = 0.26890908\n",
      "Iteration 22, loss = 0.26963692\n",
      "Iteration 52, loss = 0.26900217\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 23, loss = 0.26885348\n",
      "Iteration 24, loss = 0.26901443\n",
      "Iteration 25, loss = 0.26909885\n",
      "Iteration 1, loss = 0.29696332\n",
      "Iteration 26, loss = 0.26849801\n",
      "Iteration 2, loss = 0.28025964\n",
      "Iteration 27, loss = 0.26936773\n",
      "Iteration 3, loss = 0.27887498\n",
      "Iteration 28, loss = 0.26922031\n",
      "Iteration 4, loss = 0.27692276\n",
      "Iteration 29, loss = 0.26983208\n",
      "Iteration 5, loss = 0.27754695\n",
      "Iteration 30, loss = 0.26883129\n",
      "Iteration 6, loss = 0.27564022\n",
      "Iteration 31, loss = 0.26797571\n",
      "Iteration 7, loss = 0.27527090\n",
      "Iteration 32, loss = 0.27010912\n",
      "Iteration 8, loss = 0.27421247\n",
      "Iteration 33, loss = 0.26863182\n",
      "Iteration 9, loss = 0.27425914\n",
      "Iteration 34, loss = 0.26893025\n",
      "Iteration 35, loss = 0.26888633\n",
      "Iteration 10, loss = 0.27264849\n",
      "Iteration 36, loss = 0.26935361\n",
      "Iteration 11, loss = 0.27236581\n",
      "Iteration 37, loss = 0.26876822\n",
      "Iteration 12, loss = 0.27301006\n",
      "Iteration 38, loss = 0.26884159\n",
      "Iteration 13, loss = 0.27240909\n",
      "Iteration 39, loss = 0.26829829\n",
      "Iteration 14, loss = 0.27216405\n",
      "Iteration 40, loss = 0.26949417\n",
      "Iteration 15, loss = 0.27343117\n",
      "Iteration 41, loss = 0.26836163\n",
      "Iteration 16, loss = 0.27194027\n",
      "Iteration 17, loss = 0.27234273\n",
      "Iteration 42, loss = 0.26853386\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 18, loss = 0.27076112\n",
      "Iteration 19, loss = 0.27111963\n",
      "Iteration 1, loss = 0.29866395\n",
      "Iteration 20, loss = 0.27134562\n",
      "Iteration 2, loss = 0.28179245\n",
      "Iteration 21, loss = 0.27141729\n",
      "Iteration 3, loss = 0.27927198\n",
      "Iteration 22, loss = 0.27115739\n",
      "Iteration 4, loss = 0.27715413\n",
      "Iteration 23, loss = 0.26960983\n",
      "Iteration 5, loss = 0.27622012\n",
      "Iteration 24, loss = 0.27117223\n",
      "Iteration 6, loss = 0.27555389\n",
      "Iteration 25, loss = 0.27046415\n",
      "Iteration 7, loss = 0.27450073\n",
      "Iteration 26, loss = 0.27006990\n",
      "Iteration 8, loss = 0.27356991\n",
      "Iteration 27, loss = 0.27023632\n",
      "Iteration 9, loss = 0.27366710\n",
      "Iteration 28, loss = 0.27071508\n",
      "Iteration 10, loss = 0.27202558\n",
      "Iteration 29, loss = 0.27166012\n",
      "Iteration 30, loss = 0.26945620\n",
      "Iteration 11, loss = 0.27283293\n",
      "Iteration 31, loss = 0.26952697\n",
      "Iteration 12, loss = 0.27270680\n",
      "Iteration 32, loss = 0.27105101\n",
      "Iteration 13, loss = 0.27211890\n",
      "Iteration 33, loss = 0.26990515\n",
      "Iteration 14, loss = 0.27188823\n",
      "Iteration 34, loss = 0.26964141\n",
      "Iteration 15, loss = 0.27361475\n",
      "Iteration 35, loss = 0.26960869\n",
      "Iteration 16, loss = 0.27180197\n",
      "Iteration 36, loss = 0.27047426\n",
      "Iteration 17, loss = 0.27235572\n",
      "Iteration 37, loss = 0.26946631\n",
      "Iteration 18, loss = 0.27135846\n",
      "Iteration 38, loss = 0.26904826\n",
      "Iteration 19, loss = 0.27110249\n",
      "Iteration 39, loss = 0.27016194\n",
      "Iteration 40, loss = 0.27098306\n",
      "Iteration 20, loss = 0.27149175\n",
      "Iteration 41, loss = 0.26931762\n",
      "Iteration 21, loss = 0.27128216\n",
      "Iteration 42, loss = 0.26989753\n",
      "Iteration 22, loss = 0.27091778\n",
      "Iteration 43, loss = 0.27126428\n",
      "Iteration 23, loss = 0.26977646\n",
      "Iteration 44, loss = 0.27125174\n",
      "Iteration 24, loss = 0.27158366\n",
      "Iteration 45, loss = 0.27065658\n",
      "Iteration 25, loss = 0.27012242\n",
      "Iteration 46, loss = 0.27114563\n",
      "Iteration 26, loss = 0.27003457\n",
      "Iteration 47, loss = 0.27012240\n",
      "Iteration 48, loss = 0.26912364\n",
      "Iteration 27, loss = 0.27160288\n",
      "Iteration 49, loss = 0.27004269\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 28, loss = 0.27110239\n",
      "Iteration 29, loss = 0.27132459\n",
      "Iteration 30, loss = 0.27032918\n",
      "Iteration 31, loss = 0.27043317\n",
      "Iteration 32, loss = 0.27226623\n",
      "Iteration 33, loss = 0.26997000\n",
      "Iteration 34, loss = 0.27033890\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30302690\n",
      "Iteration 1, loss = 0.30450534\n",
      "Iteration 2, loss = 0.28599273\n",
      "Iteration 2, loss = 0.28444675\n",
      "Iteration 3, loss = 0.28364958\n",
      "Iteration 3, loss = 0.28152742\n",
      "Iteration 4, loss = 0.28223174\n",
      "Iteration 4, loss = 0.28099350\n",
      "Iteration 5, loss = 0.28233145\n",
      "Iteration 5, loss = 0.28077025\n",
      "Iteration 6, loss = 0.28118863\n",
      "Iteration 6, loss = 0.27970208\n",
      "Iteration 7, loss = 0.28178309\n",
      "Iteration 7, loss = 0.27850645\n",
      "Iteration 8, loss = 0.27896096\n",
      "Iteration 9, loss = 0.27917213\n",
      "Iteration 8, loss = 0.27751080\n",
      "Iteration 10, loss = 0.27946955\n",
      "Iteration 9, loss = 0.27759400\n",
      "Iteration 11, loss = 0.27995538\n",
      "Iteration 10, loss = 0.27776828\n",
      "Iteration 12, loss = 0.27837022\n",
      "Iteration 11, loss = 0.27741068\n",
      "Iteration 13, loss = 0.27781282\n",
      "Iteration 12, loss = 0.27721126\n",
      "Iteration 14, loss = 0.27783851\n",
      "Iteration 13, loss = 0.27599841\n",
      "Iteration 15, loss = 0.27920204\n",
      "Iteration 14, loss = 0.27709516\n",
      "Iteration 16, loss = 0.28008213\n",
      "Iteration 15, loss = 0.27662259\n",
      "Iteration 17, loss = 0.27819510\n",
      "Iteration 16, loss = 0.27837876\n",
      "Iteration 18, loss = 0.27802427\n",
      "Iteration 17, loss = 0.27670593\n",
      "Iteration 19, loss = 0.27819085\n",
      "Iteration 18, loss = 0.27563698\n",
      "Iteration 20, loss = 0.27958902\n",
      "Iteration 19, loss = 0.27583588\n",
      "Iteration 21, loss = 0.27725722\n",
      "Iteration 20, loss = 0.27780641\n",
      "Iteration 22, loss = 0.27798736\n",
      "Iteration 21, loss = 0.27457200\n",
      "Iteration 23, loss = 0.27608991\n",
      "Iteration 22, loss = 0.27578518\n",
      "Iteration 24, loss = 0.27720542\n",
      "Iteration 23, loss = 0.27413967\n",
      "Iteration 25, loss = 0.27724716\n",
      "Iteration 24, loss = 0.27462797\n",
      "Iteration 26, loss = 0.27710224\n",
      "Iteration 25, loss = 0.27616538\n",
      "Iteration 27, loss = 0.27587587\n",
      "Iteration 28, loss = 0.27672056\n",
      "Iteration 26, loss = 0.27543961\n",
      "Iteration 29, loss = 0.27667712\n",
      "Iteration 27, loss = 0.27379660\n",
      "Iteration 30, loss = 0.27645588\n",
      "Iteration 28, loss = 0.27526874\n",
      "Iteration 31, loss = 0.27678977\n",
      "Iteration 32, loss = 0.27680491\n",
      "Iteration 29, loss = 0.27498295\n",
      "Iteration 33, loss = 0.27727601\n",
      "Iteration 30, loss = 0.27431595\n",
      "Iteration 31, loss = 0.27512036\n",
      "Iteration 34, loss = 0.27610691\n",
      "Iteration 32, loss = 0.27480768\n",
      "Iteration 35, loss = 0.27558382\n",
      "Iteration 33, loss = 0.27593804\n",
      "Iteration 36, loss = 0.27746787\n",
      "Iteration 34, loss = 0.27443099\n",
      "Iteration 37, loss = 0.27660078\n",
      "Iteration 35, loss = 0.27448038\n",
      "Iteration 38, loss = 0.27691123\n",
      "Iteration 36, loss = 0.27608478\n",
      "Iteration 39, loss = 0.27666350\n",
      "Iteration 37, loss = 0.27440115\n",
      "Iteration 40, loss = 0.27820884\n",
      "Iteration 38, loss = 0.27471906\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 41, loss = 0.27651139\n",
      "Iteration 42, loss = 0.27631153\n",
      "Iteration 1, loss = 0.30701510\n",
      "Iteration 43, loss = 0.27702193\n",
      "Iteration 2, loss = 0.28677581\n",
      "Iteration 44, loss = 0.27659071\n",
      "Iteration 3, loss = 0.28443814\n",
      "Iteration 45, loss = 0.27649398\n",
      "Iteration 4, loss = 0.28253599\n",
      "Iteration 46, loss = 0.27709033\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 5, loss = 0.28256468\n",
      "Iteration 6, loss = 0.28119194\n",
      "Iteration 1, loss = 0.30546143\n",
      "Iteration 7, loss = 0.28150620\n",
      "Iteration 2, loss = 0.28708662\n",
      "Iteration 8, loss = 0.27999640\n",
      "Iteration 3, loss = 0.28380961\n",
      "Iteration 9, loss = 0.27994629\n",
      "Iteration 4, loss = 0.28147256\n",
      "Iteration 10, loss = 0.27995425\n",
      "Iteration 5, loss = 0.28247658\n",
      "Iteration 11, loss = 0.27956964\n",
      "Iteration 6, loss = 0.28012440\n",
      "Iteration 12, loss = 0.27937322\n",
      "Iteration 7, loss = 0.28155757\n",
      "Iteration 13, loss = 0.27890337\n",
      "Iteration 8, loss = 0.27969631\n",
      "Iteration 14, loss = 0.27972048\n",
      "Iteration 9, loss = 0.27942187\n",
      "Iteration 15, loss = 0.27982551\n",
      "Iteration 10, loss = 0.27863735\n",
      "Iteration 16, loss = 0.28021579\n",
      "Iteration 11, loss = 0.27887300\n",
      "Iteration 17, loss = 0.27873162\n",
      "Iteration 12, loss = 0.27775795\n",
      "Iteration 18, loss = 0.27859840\n",
      "Iteration 13, loss = 0.27813090\n",
      "Iteration 19, loss = 0.27943410\n",
      "Iteration 14, loss = 0.27768637\n",
      "Iteration 20, loss = 0.28024617\n",
      "Iteration 15, loss = 0.27791342\n",
      "Iteration 21, loss = 0.27784459\n",
      "Iteration 16, loss = 0.27886780\n",
      "Iteration 22, loss = 0.28008145\n",
      "Iteration 23, loss = 0.27718148\n",
      "Iteration 17, loss = 0.27776612\n",
      "Iteration 24, loss = 0.27780579\n",
      "Iteration 18, loss = 0.27685672\n",
      "Iteration 19, loss = 0.27781098\n",
      "Iteration 25, loss = 0.27843916\n",
      "Iteration 20, loss = 0.27782427\n",
      "Iteration 26, loss = 0.27734843\n",
      "Iteration 21, loss = 0.27640355\n",
      "Iteration 27, loss = 0.27753229\n",
      "Iteration 22, loss = 0.27686079\n",
      "Iteration 28, loss = 0.27766861\n",
      "Iteration 29, loss = 0.27813993\n",
      "Iteration 23, loss = 0.27557011\n",
      "Iteration 30, loss = 0.27709551\n",
      "Iteration 24, loss = 0.27584333\n",
      "Iteration 31, loss = 0.27723657\n",
      "Iteration 25, loss = 0.27827067\n",
      "Iteration 32, loss = 0.27806049\n",
      "Iteration 26, loss = 0.27686020\n",
      "Iteration 33, loss = 0.27816007\n",
      "Iteration 27, loss = 0.27575941\n",
      "Iteration 34, loss = 0.27698515\n",
      "Iteration 28, loss = 0.27638804\n",
      "Iteration 35, loss = 0.27699258\n",
      "Iteration 29, loss = 0.27668254\n",
      "Iteration 36, loss = 0.27744713\n",
      "Iteration 37, loss = 0.27632757\n",
      "Iteration 30, loss = 0.27613478\n",
      "Iteration 38, loss = 0.27674932\n",
      "Iteration 31, loss = 0.27665770\n",
      "Iteration 39, loss = 0.27714901\n",
      "Iteration 32, loss = 0.27795518\n",
      "Iteration 40, loss = 0.27772611\n",
      "Iteration 33, loss = 0.27696854\n",
      "Iteration 41, loss = 0.27671349\n",
      "Iteration 34, loss = 0.27620393\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 42, loss = 0.27688972\n",
      "Iteration 43, loss = 0.27807655\n",
      "Iteration 1, loss = 0.30470009\n",
      "Iteration 44, loss = 0.27643544\n",
      "Iteration 2, loss = 0.28664570\n",
      "Iteration 45, loss = 0.27744498\n",
      "Iteration 3, loss = 0.28518994\n",
      "Iteration 46, loss = 0.27552106\n",
      "Iteration 4, loss = 0.28268279\n",
      "Iteration 47, loss = 0.27704787\n",
      "Iteration 5, loss = 0.28273599\n",
      "Iteration 48, loss = 0.27682350\n",
      "Iteration 6, loss = 0.28101580\n",
      "Iteration 49, loss = 0.27724057\n",
      "Iteration 7, loss = 0.28088317\n",
      "Iteration 50, loss = 0.27693576\n",
      "Iteration 8, loss = 0.28042020\n",
      "Iteration 51, loss = 0.27636817\n",
      "Iteration 9, loss = 0.27962625\n",
      "Iteration 52, loss = 0.27597985\n",
      "Iteration 10, loss = 0.27998543\n",
      "Iteration 53, loss = 0.27688818\n",
      "Iteration 11, loss = 0.27967111\n",
      "Iteration 54, loss = 0.27670299\n",
      "Iteration 12, loss = 0.27940035\n",
      "Iteration 55, loss = 0.27675536\n",
      "Iteration 13, loss = 0.27894185\n",
      "Iteration 56, loss = 0.27715037\n",
      "Iteration 14, loss = 0.27859534\n",
      "Iteration 57, loss = 0.27699576\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 15, loss = 0.27939024\n",
      "Iteration 16, loss = 0.27947592\n",
      "Iteration 1, loss = 0.30898348\n",
      "Iteration 17, loss = 0.27892967\n",
      "Iteration 2, loss = 0.28684131\n",
      "Iteration 18, loss = 0.27807328\n",
      "Iteration 3, loss = 0.28409922\n",
      "Iteration 19, loss = 0.27872196\n",
      "Iteration 20, loss = 0.27856294\n",
      "Iteration 4, loss = 0.28333359\n",
      "Iteration 21, loss = 0.27841152\n",
      "Iteration 5, loss = 0.28123696\n",
      "Iteration 22, loss = 0.27805873\n",
      "Iteration 6, loss = 0.28263368\n",
      "Iteration 7, loss = 0.28238514\n",
      "Iteration 23, loss = 0.27728022\n",
      "Iteration 24, loss = 0.27731151\n",
      "Iteration 8, loss = 0.28011638\n",
      "Iteration 9, loss = 0.28009843\n",
      "Iteration 25, loss = 0.27823111\n",
      "Iteration 10, loss = 0.27964691\n",
      "Iteration 26, loss = 0.27743750\n",
      "Iteration 11, loss = 0.28049930\n",
      "Iteration 12, loss = 0.27957478\n",
      "Iteration 27, loss = 0.27694636\n",
      "Iteration 13, loss = 0.27969376\n",
      "Iteration 28, loss = 0.27757972\n",
      "Iteration 14, loss = 0.27892825\n",
      "Iteration 29, loss = 0.27795656\n",
      "Iteration 15, loss = 0.27905207\n",
      "Iteration 30, loss = 0.27819855\n",
      "Iteration 16, loss = 0.27904849\n",
      "Iteration 31, loss = 0.27717874\n",
      "Iteration 17, loss = 0.27953241\n",
      "Iteration 32, loss = 0.27853035\n",
      "Iteration 18, loss = 0.27861173\n",
      "Iteration 33, loss = 0.27787954\n",
      "Iteration 19, loss = 0.27942588\n",
      "Iteration 34, loss = 0.27827748\n",
      "Iteration 20, loss = 0.27866915\n",
      "Iteration 35, loss = 0.27626526\n",
      "Iteration 21, loss = 0.27870508\n",
      "Iteration 36, loss = 0.27761558\n",
      "Iteration 22, loss = 0.27841057\n",
      "Iteration 37, loss = 0.27663617\n",
      "Iteration 23, loss = 0.27771862\n",
      "Iteration 24, loss = 0.27978118\n",
      "Iteration 38, loss = 0.27723411\n",
      "Iteration 25, loss = 0.27859612\n",
      "Iteration 39, loss = 0.27743680\n",
      "Iteration 26, loss = 0.27761524\n",
      "Iteration 40, loss = 0.27798478\n",
      "Iteration 27, loss = 0.27849923\n",
      "Iteration 41, loss = 0.27728062\n",
      "Iteration 28, loss = 0.27833235\n",
      "Iteration 42, loss = 0.27663787\n",
      "Iteration 29, loss = 0.27766419\n",
      "Iteration 43, loss = 0.27821202\n",
      "Iteration 30, loss = 0.27822044\n",
      "Iteration 44, loss = 0.27709065\n",
      "Iteration 31, loss = 0.27773586\n",
      "Iteration 45, loss = 0.27656400\n",
      "Iteration 32, loss = 0.27873332\n",
      "Iteration 46, loss = 0.27620762\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 33, loss = 0.27831961\n",
      "Iteration 34, loss = 0.27751503\n",
      "Iteration 1, loss = 0.30854893\n",
      "Iteration 35, loss = 0.27740679\n",
      "Iteration 2, loss = 0.28676678\n",
      "Iteration 36, loss = 0.27752475\n",
      "Iteration 3, loss = 0.28397218\n",
      "Iteration 37, loss = 0.27726045\n",
      "Iteration 4, loss = 0.28444718\n",
      "Iteration 38, loss = 0.27752820\n",
      "Iteration 5, loss = 0.28125047\n",
      "Iteration 39, loss = 0.27802907\n",
      "Iteration 6, loss = 0.28226675\n",
      "Iteration 40, loss = 0.27869164\n",
      "Iteration 7, loss = 0.28115048\n",
      "Iteration 41, loss = 0.27779590\n",
      "Iteration 8, loss = 0.28005022\n",
      "Iteration 42, loss = 0.27798488\n",
      "Iteration 9, loss = 0.28072300\n",
      "Iteration 43, loss = 0.27705531\n",
      "Iteration 10, loss = 0.27926264\n",
      "Iteration 44, loss = 0.27907722\n",
      "Iteration 11, loss = 0.27925435\n",
      "Iteration 45, loss = 0.27835281\n",
      "Iteration 12, loss = 0.27920362\n",
      "Iteration 46, loss = 0.27755646\n",
      "Iteration 13, loss = 0.27949400\n",
      "Iteration 47, loss = 0.27763760\n",
      "Iteration 14, loss = 0.27821804\n",
      "Iteration 48, loss = 0.27757351\n",
      "Iteration 15, loss = 0.27938821\n",
      "Iteration 49, loss = 0.27740463\n",
      "Iteration 16, loss = 0.27814909\n",
      "Iteration 50, loss = 0.27906741\n",
      "Iteration 17, loss = 0.27862585\n",
      "Iteration 51, loss = 0.27734254\n",
      "Iteration 52, loss = 0.27782937\n",
      "Iteration 18, loss = 0.27844754\n",
      "Iteration 53, loss = 0.27762363\n",
      "Iteration 19, loss = 0.27779256\n",
      "Iteration 54, loss = 0.27776716\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 20, loss = 0.27877321\n",
      "Iteration 21, loss = 0.27705885\n",
      "Iteration 1, loss = 0.30777714\n",
      "Iteration 22, loss = 0.27669139\n",
      "Iteration 2, loss = 0.28739491\n",
      "Iteration 3, loss = 0.28513809\n",
      "Iteration 23, loss = 0.27601730\n",
      "Iteration 4, loss = 0.28392696\n",
      "Iteration 24, loss = 0.27835763\n",
      "Iteration 25, loss = 0.27590543\n",
      "Iteration 5, loss = 0.28263195\n",
      "Iteration 6, loss = 0.28232324\n",
      "Iteration 26, loss = 0.27551166\n",
      "Iteration 7, loss = 0.28169929\n",
      "Iteration 27, loss = 0.27567640\n",
      "Iteration 8, loss = 0.28073861\n",
      "Iteration 28, loss = 0.27587525\n",
      "Iteration 29, loss = 0.27656420Iteration 9, loss = 0.28136133\n",
      "\n",
      "Iteration 10, loss = 0.27919777Iteration 30, loss = 0.27570471\n",
      "\n",
      "Iteration 31, loss = 0.27617174\n",
      "Iteration 11, loss = 0.27993432\n",
      "Iteration 32, loss = 0.27717162\n",
      "Iteration 12, loss = 0.27939839\n",
      "Iteration 33, loss = 0.27641212\n",
      "Iteration 13, loss = 0.27930618\n",
      "Iteration 34, loss = 0.27576378\n",
      "Iteration 14, loss = 0.27991271\n",
      "Iteration 35, loss = 0.27645515\n",
      "Iteration 15, loss = 0.27898881\n",
      "Iteration 36, loss = 0.27591847\n",
      "Iteration 16, loss = 0.27852644\n",
      "Iteration 37, loss = 0.27575023\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 17, loss = 0.27877610\n",
      "Iteration 18, loss = 0.27847135\n",
      "Iteration 1, loss = 0.30760301\n",
      "Iteration 19, loss = 0.27765703\n",
      "Iteration 2, loss = 0.28756876\n",
      "Iteration 20, loss = 0.27866079\n",
      "Iteration 3, loss = 0.28452974\n",
      "Iteration 21, loss = 0.27794928\n",
      "Iteration 4, loss = 0.28439153\n",
      "Iteration 22, loss = 0.27703705\n",
      "Iteration 5, loss = 0.28207769\n",
      "Iteration 23, loss = 0.27618118\n",
      "Iteration 6, loss = 0.28177786\n",
      "Iteration 24, loss = 0.27825093\n",
      "Iteration 7, loss = 0.28104417\n",
      "Iteration 25, loss = 0.27715245\n",
      "Iteration 8, loss = 0.28058323\n",
      "Iteration 26, loss = 0.27572169\n",
      "Iteration 9, loss = 0.28274235\n",
      "Iteration 10, loss = 0.27960841\n",
      "Iteration 27, loss = 0.27713028\n",
      "Iteration 11, loss = 0.28043460\n",
      "Iteration 28, loss = 0.27634561\n",
      "Iteration 12, loss = 0.28015438\n",
      "Iteration 29, loss = 0.27599451\n",
      "Iteration 13, loss = 0.27954189\n",
      "Iteration 30, loss = 0.27709344\n",
      "Iteration 14, loss = 0.27871515\n",
      "Iteration 31, loss = 0.27615145\n",
      "Iteration 15, loss = 0.27912596\n",
      "Iteration 32, loss = 0.27812993\n",
      "Iteration 16, loss = 0.28038649\n",
      "Iteration 33, loss = 0.27639624\n",
      "Iteration 17, loss = 0.27873213\n",
      "Iteration 34, loss = 0.27604702\n",
      "Iteration 18, loss = 0.27827641\n",
      "Iteration 19, loss = 0.27730756\n",
      "Iteration 35, loss = 0.27665093\n",
      "Iteration 20, loss = 0.27761463\n",
      "Iteration 36, loss = 0.27647950\n",
      "Iteration 21, loss = 0.27706620\n",
      "Iteration 37, loss = 0.27606347\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 22, loss = 0.27679091\n",
      "Iteration 23, loss = 0.27746032\n",
      "Iteration 24, loss = 0.27837224\n",
      "Iteration 1, loss = 0.30520309\n",
      "Iteration 25, loss = 0.27708126\n",
      "Iteration 2, loss = 0.28583527\n",
      "Iteration 26, loss = 0.27670864\n",
      "Iteration 3, loss = 0.28456207\n",
      "Iteration 27, loss = 0.27785367\n",
      "Iteration 4, loss = 0.28450793\n",
      "Iteration 28, loss = 0.27677689\n",
      "Iteration 5, loss = 0.28257036\n",
      "Iteration 29, loss = 0.27742883\n",
      "Iteration 6, loss = 0.28173400\n",
      "Iteration 30, loss = 0.27700454\n",
      "Iteration 7, loss = 0.28146734\n",
      "Iteration 31, loss = 0.27680089\n",
      "Iteration 8, loss = 0.28043927\n",
      "Iteration 32, loss = 0.27887525\n",
      "Iteration 9, loss = 0.28120640\n",
      "Iteration 33, loss = 0.27749559\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 10, loss = 0.28042945\n",
      "Iteration 11, loss = 0.27938425\n",
      "Iteration 12, loss = 0.27998984\n",
      "Iteration 13, loss = 0.27911662\n",
      "Iteration 14, loss = 0.27966410\n",
      "Iteration 15, loss = 0.27916718\n",
      "Iteration 16, loss = 0.27985294\n",
      "Iteration 17, loss = 0.27952188\n",
      "Iteration 18, loss = 0.27877362\n",
      "Iteration 19, loss = 0.27880727\n",
      "Iteration 20, loss = 0.28006008\n",
      "Iteration 21, loss = 0.27830609\n",
      "Iteration 22, loss = 0.27848315\n",
      "Iteration 23, loss = 0.27815732\n",
      "Iteration 24, loss = 0.28076918\n",
      "Iteration 25, loss = 0.27730487\n",
      "Iteration 26, loss = 0.27726690\n",
      "Iteration 27, loss = 0.27917418\n",
      "Iteration 28, loss = 0.27733398\n",
      "Iteration 29, loss = 0.27751764\n",
      "Iteration 30, loss = 0.27794320\n",
      "Iteration 31, loss = 0.27694578\n",
      "Iteration 32, loss = 0.27809258\n",
      "Iteration 33, loss = 0.27823557\n",
      "Iteration 34, loss = 0.27740649\n",
      "Iteration 35, loss = 0.27797497\n",
      "Iteration 36, loss = 0.27805960\n",
      "Iteration 37, loss = 0.27710553\n",
      "Iteration 38, loss = 0.27645165\n",
      "Iteration 39, loss = 0.27747025\n",
      "Iteration 40, loss = 0.27924966\n",
      "Iteration 41, loss = 0.27605805\n",
      "Iteration 42, loss = 0.27561715\n",
      "Iteration 43, loss = 0.27531338\n",
      "Iteration 44, loss = 0.27824541\n",
      "Iteration 45, loss = 0.27647639\n",
      "Iteration 46, loss = 0.27618883\n",
      "Iteration 47, loss = 0.27533896\n",
      "Iteration 48, loss = 0.27565926\n",
      "Iteration 49, loss = 0.27508073\n",
      "Iteration 50, loss = 0.27488867\n",
      "Iteration 51, loss = 0.27549686\n",
      "Iteration 52, loss = 0.27494964\n",
      "Iteration 53, loss = 0.27542721\n",
      "Iteration 54, loss = 0.27600308\n",
      "Iteration 55, loss = 0.27575024\n",
      "Iteration 56, loss = 0.27538984\n",
      "Iteration 57, loss = 0.27497284\n",
      "Iteration 58, loss = 0.27643247\n",
      "Iteration 59, loss = 0.27516186\n",
      "Iteration 60, loss = 0.27555817\n",
      "Iteration 61, loss = 0.27521217\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29662643\n",
      "Iteration 1, loss = 0.29756679\n",
      "Iteration 2, loss = 0.28021744\n",
      "Iteration 2, loss = 0.27837017\n",
      "Iteration 3, loss = 0.27726808Iteration 3, loss = 0.27609804\n",
      "\n",
      "Iteration 4, loss = 0.27701962\n",
      "Iteration 4, loss = 0.27629933\n",
      "Iteration 5, loss = 0.27667501\n",
      "Iteration 5, loss = 0.27486649\n",
      "Iteration 6, loss = 0.27396811\n",
      "Iteration 6, loss = 0.27437988\n",
      "Iteration 7, loss = 0.27385764\n",
      "Iteration 7, loss = 0.27330221\n",
      "Iteration 8, loss = 0.27373390\n",
      "Iteration 8, loss = 0.27320934\n",
      "Iteration 9, loss = 0.27264921\n",
      "Iteration 9, loss = 0.27328073\n",
      "Iteration 10, loss = 0.27240379\n",
      "Iteration 10, loss = 0.27268950\n",
      "Iteration 11, loss = 0.27275156\n",
      "Iteration 11, loss = 0.27268479\n",
      "Iteration 12, loss = 0.27219101\n",
      "Iteration 13, loss = 0.27121721\n",
      "Iteration 12, loss = 0.27187775\n",
      "Iteration 14, loss = 0.27197997\n",
      "Iteration 13, loss = 0.27214614\n",
      "Iteration 14, loss = 0.27255518\n",
      "Iteration 15, loss = 0.27099214\n",
      "Iteration 15, loss = 0.27151883\n",
      "Iteration 16, loss = 0.26960461\n",
      "Iteration 17, loss = 0.26959537\n",
      "Iteration 16, loss = 0.27125835\n",
      "Iteration 18, loss = 0.27016661\n",
      "Iteration 17, loss = 0.27150628\n",
      "Iteration 19, loss = 0.26918284\n",
      "Iteration 18, loss = 0.27191760\n",
      "Iteration 20, loss = 0.26993763\n",
      "Iteration 19, loss = 0.26995552\n",
      "Iteration 21, loss = 0.26821440\n",
      "Iteration 20, loss = 0.27086082\n",
      "Iteration 22, loss = 0.26837149\n",
      "Iteration 21, loss = 0.26950915\n",
      "Iteration 23, loss = 0.26841377\n",
      "Iteration 22, loss = 0.27008992\n",
      "Iteration 24, loss = 0.26816262\n",
      "Iteration 23, loss = 0.27003809\n",
      "Iteration 25, loss = 0.26888879\n",
      "Iteration 24, loss = 0.26973118\n",
      "Iteration 26, loss = 0.26938766\n",
      "Iteration 25, loss = 0.26933268\n",
      "Iteration 27, loss = 0.26889322\n",
      "Iteration 26, loss = 0.26900585\n",
      "Iteration 28, loss = 0.26806136\n",
      "Iteration 27, loss = 0.27047818\n",
      "Iteration 29, loss = 0.26795480\n",
      "Iteration 28, loss = 0.26844327\n",
      "Iteration 30, loss = 0.26949402\n",
      "Iteration 29, loss = 0.26831319\n",
      "Iteration 31, loss = 0.26784026\n",
      "Iteration 30, loss = 0.26865581\n",
      "Iteration 32, loss = 0.26822299\n",
      "Iteration 31, loss = 0.26794801\n",
      "Iteration 33, loss = 0.26912876\n",
      "Iteration 34, loss = 0.26875511\n",
      "Iteration 32, loss = 0.26807999\n",
      "Iteration 35, loss = 0.26897311\n",
      "Iteration 33, loss = 0.26854303\n",
      "Iteration 34, loss = 0.26842446\n",
      "Iteration 36, loss = 0.26756681\n",
      "Iteration 35, loss = 0.26883196\n",
      "Iteration 37, loss = 0.26666462\n",
      "Iteration 36, loss = 0.26873179\n",
      "Iteration 38, loss = 0.26798177\n",
      "Iteration 37, loss = 0.26864044\n",
      "Iteration 39, loss = 0.26783346\n",
      "Iteration 38, loss = 0.26835398\n",
      "Iteration 40, loss = 0.26752772\n",
      "Iteration 39, loss = 0.26760396\n",
      "Iteration 41, loss = 0.26783573\n",
      "Iteration 40, loss = 0.26789116\n",
      "Iteration 42, loss = 0.26834141\n",
      "Iteration 41, loss = 0.26858246\n",
      "Iteration 43, loss = 0.26674559\n",
      "Iteration 42, loss = 0.26823969\n",
      "Iteration 44, loss = 0.26853993\n",
      "Iteration 43, loss = 0.26789207\n",
      "Iteration 45, loss = 0.27029342\n",
      "Iteration 44, loss = 0.26870233\n",
      "Iteration 46, loss = 0.26763971\n",
      "Iteration 45, loss = 0.27050294\n",
      "Iteration 47, loss = 0.26723278\n",
      "Iteration 46, loss = 0.26767974\n",
      "Iteration 48, loss = 0.26742542\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 47, loss = 0.26755948\n",
      "Iteration 48, loss = 0.26766633\n",
      "Iteration 1, loss = 0.29663383\n",
      "Iteration 49, loss = 0.26741805\n",
      "Iteration 50, loss = 0.26743264\n",
      "Iteration 2, loss = 0.27913599\n",
      "Iteration 3, loss = 0.27635382\n",
      "Iteration 51, loss = 0.26690837\n",
      "Iteration 52, loss = 0.26710250\n",
      "Iteration 4, loss = 0.27554605\n",
      "Iteration 53, loss = 0.26789723\n",
      "Iteration 5, loss = 0.27511092\n",
      "Iteration 54, loss = 0.26842487\n",
      "Iteration 6, loss = 0.27368709\n",
      "Iteration 55, loss = 0.26817076\n",
      "Iteration 7, loss = 0.27339533\n",
      "Iteration 56, loss = 0.26786753\n",
      "Iteration 8, loss = 0.27262816\n",
      "Iteration 57, loss = 0.26759097\n",
      "Iteration 9, loss = 0.27243793\n",
      "Iteration 58, loss = 0.26671083\n",
      "Iteration 10, loss = 0.27116453\n",
      "Iteration 59, loss = 0.26739028\n",
      "Iteration 11, loss = 0.27133771\n",
      "Iteration 60, loss = 0.26709804\n",
      "Iteration 12, loss = 0.27156803\n",
      "Iteration 61, loss = 0.26766453\n",
      "Iteration 13, loss = 0.27252055\n",
      "Iteration 62, loss = 0.26859193\n",
      "Iteration 14, loss = 0.27134777\n",
      "Iteration 63, loss = 0.26775578\n",
      "Iteration 15, loss = 0.27062657\n",
      "Iteration 64, loss = 0.26723105\n",
      "Iteration 65, loss = 0.26667282\n",
      "Iteration 16, loss = 0.27036356\n",
      "Iteration 66, loss = 0.26717953\n",
      "Iteration 17, loss = 0.27149265\n",
      "Iteration 67, loss = 0.26735196\n",
      "Iteration 18, loss = 0.27190672\n",
      "Iteration 68, loss = 0.26762660\n",
      "Iteration 19, loss = 0.27043946\n",
      "Iteration 69, loss = 0.26698911\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 20, loss = 0.27025460\n",
      "Iteration 21, loss = 0.26952995\n",
      "Iteration 1, loss = 0.29757603\n",
      "Iteration 22, loss = 0.27050854\n",
      "Iteration 2, loss = 0.28067306\n",
      "Iteration 23, loss = 0.26985372\n",
      "Iteration 3, loss = 0.27725895\n",
      "Iteration 24, loss = 0.26954712\n",
      "Iteration 4, loss = 0.27595997\n",
      "Iteration 25, loss = 0.26935196\n",
      "Iteration 5, loss = 0.27575587\n",
      "Iteration 26, loss = 0.26977338\n",
      "Iteration 6, loss = 0.27511928\n",
      "Iteration 7, loss = 0.27460571\n",
      "Iteration 27, loss = 0.26982892\n",
      "Iteration 8, loss = 0.27478899\n",
      "Iteration 28, loss = 0.26837539\n",
      "Iteration 9, loss = 0.27442662\n",
      "Iteration 29, loss = 0.26851412\n",
      "Iteration 10, loss = 0.27412493\n",
      "Iteration 30, loss = 0.26979123\n",
      "Iteration 11, loss = 0.27275613\n",
      "Iteration 31, loss = 0.26839045\n",
      "Iteration 12, loss = 0.27324878\n",
      "Iteration 32, loss = 0.26827163\n",
      "Iteration 13, loss = 0.27352477\n",
      "Iteration 33, loss = 0.26852533\n",
      "Iteration 14, loss = 0.27239610\n",
      "Iteration 34, loss = 0.26840972\n",
      "Iteration 15, loss = 0.27176299\n",
      "Iteration 35, loss = 0.26873593\n",
      "Iteration 16, loss = 0.27174370\n",
      "Iteration 36, loss = 0.26749724\n",
      "Iteration 17, loss = 0.27260151\n",
      "Iteration 37, loss = 0.26831453\n",
      "Iteration 18, loss = 0.27239960\n",
      "Iteration 38, loss = 0.26749408\n",
      "Iteration 19, loss = 0.27315068\n",
      "Iteration 39, loss = 0.26782684\n",
      "Iteration 20, loss = 0.27261422\n",
      "Iteration 40, loss = 0.26814846\n",
      "Iteration 21, loss = 0.27179764\n",
      "Iteration 41, loss = 0.26681678\n",
      "Iteration 22, loss = 0.27131420\n",
      "Iteration 42, loss = 0.26886555\n",
      "Iteration 23, loss = 0.27092020\n",
      "Iteration 43, loss = 0.26826752\n",
      "Iteration 24, loss = 0.27092610\n",
      "Iteration 44, loss = 0.26781464\n",
      "Iteration 25, loss = 0.27136249\n",
      "Iteration 45, loss = 0.26932853\n",
      "Iteration 26, loss = 0.27163858\n",
      "Iteration 46, loss = 0.26717008\n",
      "Iteration 27, loss = 0.27165632\n",
      "Iteration 47, loss = 0.26796740\n",
      "Iteration 28, loss = 0.27039953\n",
      "Iteration 48, loss = 0.26787433\n",
      "Iteration 29, loss = 0.27044295\n",
      "Iteration 49, loss = 0.26744173\n",
      "Iteration 30, loss = 0.27166124\n",
      "Iteration 31, loss = 0.27125925\n",
      "Iteration 50, loss = 0.26872527\n",
      "Iteration 32, loss = 0.27161816\n",
      "Iteration 51, loss = 0.26706196\n",
      "Iteration 33, loss = 0.27014820\n",
      "Iteration 52, loss = 0.26734410\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 34, loss = 0.26971658\n",
      "Iteration 35, loss = 0.27115520\n",
      "Iteration 1, loss = 0.29526547\n",
      "Iteration 36, loss = 0.27046200\n",
      "Iteration 2, loss = 0.28033033\n",
      "Iteration 37, loss = 0.27027406\n",
      "Iteration 3, loss = 0.27745276\n",
      "Iteration 38, loss = 0.26992888\n",
      "Iteration 4, loss = 0.27637521\n",
      "Iteration 39, loss = 0.26977728\n",
      "Iteration 5, loss = 0.27605959\n",
      "Iteration 40, loss = 0.27035316\n",
      "Iteration 6, loss = 0.27682871\n",
      "Iteration 41, loss = 0.27011461\n",
      "Iteration 7, loss = 0.27508352\n",
      "Iteration 42, loss = 0.27114639\n",
      "Iteration 8, loss = 0.27499249\n",
      "Iteration 43, loss = 0.26971897\n",
      "Iteration 9, loss = 0.27471646\n",
      "Iteration 44, loss = 0.26994083\n",
      "Iteration 10, loss = 0.27495131\n",
      "Iteration 45, loss = 0.27136842\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 11, loss = 0.27360731\n",
      "Iteration 12, loss = 0.27326764\n",
      "Iteration 1, loss = 0.29428167\n",
      "Iteration 13, loss = 0.27418906\n",
      "Iteration 2, loss = 0.28002192\n",
      "Iteration 14, loss = 0.27356728\n",
      "Iteration 3, loss = 0.27644368Iteration 15, loss = 0.27242655\n",
      "\n",
      "Iteration 16, loss = 0.27181399\n",
      "Iteration 4, loss = 0.27526544\n",
      "Iteration 17, loss = 0.27205015\n",
      "Iteration 5, loss = 0.27485054\n",
      "Iteration 18, loss = 0.27204507\n",
      "Iteration 6, loss = 0.27457107\n",
      "Iteration 19, loss = 0.27251263\n",
      "Iteration 7, loss = 0.27260890\n",
      "Iteration 20, loss = 0.27256797\n",
      "Iteration 8, loss = 0.27274351\n",
      "Iteration 21, loss = 0.27236398\n",
      "Iteration 9, loss = 0.27280109\n",
      "Iteration 22, loss = 0.27280774\n",
      "Iteration 10, loss = 0.27120600\n",
      "Iteration 23, loss = 0.27081536\n",
      "Iteration 11, loss = 0.27072055\n",
      "Iteration 12, loss = 0.27025417\n",
      "Iteration 24, loss = 0.27149206\n",
      "Iteration 13, loss = 0.27041586\n",
      "Iteration 25, loss = 0.27096204\n",
      "Iteration 14, loss = 0.26956133\n",
      "Iteration 26, loss = 0.27076775\n",
      "Iteration 15, loss = 0.26855551\n",
      "Iteration 27, loss = 0.27132524\n",
      "Iteration 16, loss = 0.26897878\n",
      "Iteration 28, loss = 0.26992170\n",
      "Iteration 17, loss = 0.26809660\n",
      "Iteration 29, loss = 0.27091526\n",
      "Iteration 18, loss = 0.26872095\n",
      "Iteration 30, loss = 0.27016638\n",
      "Iteration 19, loss = 0.27012889\n",
      "Iteration 31, loss = 0.26949405\n",
      "Iteration 20, loss = 0.26836344\n",
      "Iteration 32, loss = 0.27131130\n",
      "Iteration 21, loss = 0.26815492\n",
      "Iteration 33, loss = 0.27259422\n",
      "Iteration 22, loss = 0.26968353\n",
      "Iteration 34, loss = 0.27044935\n",
      "Iteration 23, loss = 0.26842542\n",
      "Iteration 35, loss = 0.27235383\n",
      "Iteration 36, loss = 0.27076050\n",
      "Iteration 24, loss = 0.26827611\n",
      "Iteration 37, loss = 0.27168724\n",
      "Iteration 25, loss = 0.26849053\n",
      "Iteration 38, loss = 0.26973744\n",
      "Iteration 26, loss = 0.26830324\n",
      "Iteration 39, loss = 0.27054270\n",
      "Iteration 27, loss = 0.26882991\n",
      "Iteration 40, loss = 0.27010179\n",
      "Iteration 28, loss = 0.26827929\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 41, loss = 0.26972881\n",
      "Iteration 42, loss = 0.27053066\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29747471\n",
      "Iteration 2, loss = 0.28086925\n",
      "Iteration 3, loss = 0.27764563\n",
      "Iteration 1, loss = 0.29659485\n",
      "Iteration 4, loss = 0.27619017\n",
      "Iteration 2, loss = 0.28091746\n",
      "Iteration 5, loss = 0.27634190\n",
      "Iteration 3, loss = 0.27835058\n",
      "Iteration 6, loss = 0.27646932\n",
      "Iteration 4, loss = 0.27675633\n",
      "Iteration 7, loss = 0.27494583\n",
      "Iteration 5, loss = 0.27496256\n",
      "Iteration 8, loss = 0.27460852\n",
      "Iteration 6, loss = 0.27626322\n",
      "Iteration 9, loss = 0.27474995\n",
      "Iteration 7, loss = 0.27464174\n",
      "Iteration 10, loss = 0.27442819\n",
      "Iteration 8, loss = 0.27487539\n",
      "Iteration 11, loss = 0.27355325\n",
      "Iteration 9, loss = 0.27521154\n",
      "Iteration 12, loss = 0.27374246\n",
      "Iteration 10, loss = 0.27409121\n",
      "Iteration 13, loss = 0.27341455\n",
      "Iteration 11, loss = 0.27305545\n",
      "Iteration 14, loss = 0.27328172\n",
      "Iteration 12, loss = 0.27331163\n",
      "Iteration 15, loss = 0.27259401\n",
      "Iteration 13, loss = 0.27297802\n",
      "Iteration 16, loss = 0.27228597\n",
      "Iteration 14, loss = 0.27267495\n",
      "Iteration 17, loss = 0.27205640\n",
      "Iteration 15, loss = 0.27264118\n",
      "Iteration 18, loss = 0.27247889\n",
      "Iteration 16, loss = 0.27284614\n",
      "Iteration 19, loss = 0.27143216\n",
      "Iteration 17, loss = 0.27251164\n",
      "Iteration 20, loss = 0.27069218\n",
      "Iteration 18, loss = 0.27293547\n",
      "Iteration 21, loss = 0.27212917\n",
      "Iteration 22, loss = 0.27175979\n",
      "Iteration 19, loss = 0.27196840\n",
      "Iteration 23, loss = 0.27055668\n",
      "Iteration 20, loss = 0.27143485\n",
      "Iteration 24, loss = 0.27116244\n",
      "Iteration 21, loss = 0.27232309\n",
      "Iteration 25, loss = 0.27132185\n",
      "Iteration 22, loss = 0.27348166\n",
      "Iteration 23, loss = 0.27115441Iteration 26, loss = 0.27079185\n",
      "\n",
      "Iteration 24, loss = 0.27213829\n",
      "Iteration 27, loss = 0.27084186\n",
      "Iteration 25, loss = 0.27192169\n",
      "Iteration 28, loss = 0.27009684\n",
      "Iteration 26, loss = 0.27175720\n",
      "Iteration 29, loss = 0.27049124\n",
      "Iteration 27, loss = 0.27170749\n",
      "Iteration 30, loss = 0.27006582\n",
      "Iteration 31, loss = 0.26960031\n",
      "Iteration 28, loss = 0.27157253\n",
      "Iteration 32, loss = 0.27055797\n",
      "Iteration 29, loss = 0.27125376\n",
      "Iteration 33, loss = 0.27158761\n",
      "Iteration 30, loss = 0.27173724\n",
      "Iteration 34, loss = 0.26945771\n",
      "Iteration 31, loss = 0.27087789\n",
      "Iteration 35, loss = 0.27079361\n",
      "Iteration 32, loss = 0.27164919\n",
      "Iteration 36, loss = 0.27026710\n",
      "Iteration 33, loss = 0.27271452\n",
      "Iteration 37, loss = 0.27102944\n",
      "Iteration 34, loss = 0.27171420\n",
      "Iteration 38, loss = 0.26974515\n",
      "Iteration 35, loss = 0.27201431\n",
      "Iteration 39, loss = 0.27025176\n",
      "Iteration 36, loss = 0.27164919\n",
      "Iteration 40, loss = 0.26993207\n",
      "Iteration 37, loss = 0.27203150\n",
      "Iteration 41, loss = 0.26948327\n",
      "Iteration 38, loss = 0.27113295\n",
      "Iteration 42, loss = 0.26992755\n",
      "Iteration 39, loss = 0.27115814\n",
      "Iteration 43, loss = 0.26977633\n",
      "Iteration 40, loss = 0.27118130\n",
      "Iteration 44, loss = 0.27082264\n",
      "Iteration 41, loss = 0.27199187\n",
      "Iteration 45, loss = 0.27044693\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 42, loss = 0.27121154\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29742431\n",
      "Iteration 1, loss = 0.29813135\n",
      "Iteration 2, loss = 0.28058529\n",
      "Iteration 2, loss = 0.28017784\n",
      "Iteration 3, loss = 0.27872100\n",
      "Iteration 3, loss = 0.27831003\n",
      "Iteration 4, loss = 0.27625250\n",
      "Iteration 4, loss = 0.27640804\n",
      "Iteration 5, loss = 0.27696418\n",
      "Iteration 5, loss = 0.27681457\n",
      "Iteration 6, loss = 0.27492821\n",
      "Iteration 6, loss = 0.27464329\n",
      "Iteration 7, loss = 0.27454965\n",
      "Iteration 7, loss = 0.27375567\n",
      "Iteration 8, loss = 0.27471192\n",
      "Iteration 8, loss = 0.27265117\n",
      "Iteration 9, loss = 0.27441445\n",
      "Iteration 9, loss = 0.27264181\n",
      "Iteration 10, loss = 0.27376441\n",
      "Iteration 10, loss = 0.27165588\n",
      "Iteration 11, loss = 0.27247387\n",
      "Iteration 11, loss = 0.27052552\n",
      "Iteration 12, loss = 0.27221770\n",
      "Iteration 12, loss = 0.27099527\n",
      "Iteration 13, loss = 0.27217986\n",
      "Iteration 13, loss = 0.27028056\n",
      "Iteration 14, loss = 0.27297322\n",
      "Iteration 14, loss = 0.27121498\n",
      "Iteration 15, loss = 0.27161737\n",
      "Iteration 15, loss = 0.27017238\n",
      "Iteration 16, loss = 0.27108283\n",
      "Iteration 16, loss = 0.27108844\n",
      "Iteration 17, loss = 0.27202806\n",
      "Iteration 17, loss = 0.26949254\n",
      "Iteration 18, loss = 0.27129944\n",
      "Iteration 18, loss = 0.26969083\n",
      "Iteration 19, loss = 0.27079990\n",
      "Iteration 19, loss = 0.26998827\n",
      "Iteration 20, loss = 0.27061553\n",
      "Iteration 20, loss = 0.27003171\n",
      "Iteration 21, loss = 0.27005946\n",
      "Iteration 21, loss = 0.26863641\n",
      "Iteration 22, loss = 0.27104371\n",
      "Iteration 22, loss = 0.26959444\n",
      "Iteration 23, loss = 0.26887689\n",
      "Iteration 23, loss = 0.27035016\n",
      "Iteration 24, loss = 0.26983647\n",
      "Iteration 24, loss = 0.27055012\n",
      "Iteration 25, loss = 0.26921426\n",
      "Iteration 25, loss = 0.26897014\n",
      "Iteration 26, loss = 0.26826022\n",
      "Iteration 26, loss = 0.26973996\n",
      "Iteration 27, loss = 0.26990521\n",
      "Iteration 27, loss = 0.26840817\n",
      "Iteration 28, loss = 0.27068638\n",
      "Iteration 28, loss = 0.26887682\n",
      "Iteration 29, loss = 0.26967926\n",
      "Iteration 29, loss = 0.26790605\n",
      "Iteration 30, loss = 0.26916551\n",
      "Iteration 30, loss = 0.26767854\n",
      "Iteration 31, loss = 0.26988730\n",
      "Iteration 31, loss = 0.26815782\n",
      "Iteration 32, loss = 0.26906722\n",
      "Iteration 32, loss = 0.26695730\n",
      "Iteration 33, loss = 0.26910415\n",
      "Iteration 33, loss = 0.26739497\n",
      "Iteration 34, loss = 0.26932328\n",
      "Iteration 34, loss = 0.26718844\n",
      "Iteration 35, loss = 0.26974541\n",
      "Iteration 35, loss = 0.26814645\n",
      "Iteration 36, loss = 0.26883769\n",
      "Iteration 36, loss = 0.26687451\n",
      "Iteration 37, loss = 0.26922925\n",
      "Iteration 37, loss = 0.26709068\n",
      "Iteration 38, loss = 0.26984404\n",
      "Iteration 38, loss = 0.26683752\n",
      "Iteration 39, loss = 0.26919411\n",
      "Iteration 39, loss = 0.26843930\n",
      "Iteration 40, loss = 0.26953656\n",
      "Iteration 40, loss = 0.26755600\n",
      "Iteration 41, loss = 0.26999985\n",
      "Iteration 41, loss = 0.26872762\n",
      "Iteration 42, loss = 0.26945229\n",
      "Iteration 42, loss = 0.26674206\n",
      "Iteration 43, loss = 0.27058506\n",
      "Iteration 43, loss = 0.26807945\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 44, loss = 0.27036323\n",
      "Iteration 45, loss = 0.26893395\n",
      "Iteration 46, loss = 0.26941011\n",
      "Iteration 47, loss = 0.26880340\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30660418\n",
      "Iteration 1, loss = 0.30894969\n",
      "Iteration 2, loss = 0.29114148\n",
      "Iteration 2, loss = 0.29304313\n",
      "Iteration 3, loss = 0.28915684\n",
      "Iteration 4, loss = 0.28799723\n",
      "Iteration 3, loss = 0.29089549\n",
      "Iteration 4, loss = 0.29032866\n",
      "Iteration 5, loss = 0.28676182\n",
      "Iteration 5, loss = 0.29032712\n",
      "Iteration 6, loss = 0.28570109\n",
      "Iteration 7, loss = 0.28415827\n",
      "Iteration 6, loss = 0.28818775\n",
      "Iteration 8, loss = 0.28506187\n",
      "Iteration 9, loss = 0.28338999\n",
      "Iteration 7, loss = 0.28654118\n",
      "Iteration 10, loss = 0.28383023\n",
      "Iteration 8, loss = 0.28761563\n",
      "Iteration 11, loss = 0.28401534\n",
      "Iteration 9, loss = 0.28617754\n",
      "Iteration 12, loss = 0.28410953\n",
      "Iteration 10, loss = 0.28555775\n",
      "Iteration 13, loss = 0.28226003\n",
      "Iteration 11, loss = 0.28575651\n",
      "Iteration 12, loss = 0.28544504\n",
      "Iteration 14, loss = 0.28362735\n",
      "Iteration 13, loss = 0.28495358\n",
      "Iteration 15, loss = 0.28276453\n",
      "Iteration 14, loss = 0.28503917\n",
      "Iteration 16, loss = 0.28311329\n",
      "Iteration 15, loss = 0.28537371\n",
      "Iteration 17, loss = 0.28244882\n",
      "Iteration 16, loss = 0.28458314\n",
      "Iteration 18, loss = 0.28345793\n",
      "Iteration 17, loss = 0.28496826\n",
      "Iteration 19, loss = 0.28244728\n",
      "Iteration 18, loss = 0.28527229\n",
      "Iteration 20, loss = 0.28209942\n",
      "Iteration 19, loss = 0.28389422\n",
      "Iteration 21, loss = 0.28278063\n",
      "Iteration 20, loss = 0.28477370\n",
      "Iteration 22, loss = 0.28250743\n",
      "Iteration 21, loss = 0.28546471\n",
      "Iteration 23, loss = 0.28193679\n",
      "Iteration 22, loss = 0.28428724\n",
      "Iteration 24, loss = 0.28250979\n",
      "Iteration 23, loss = 0.28367517\n",
      "Iteration 25, loss = 0.28188429\n",
      "Iteration 24, loss = 0.28485512\n",
      "Iteration 26, loss = 0.28289779\n",
      "Iteration 25, loss = 0.28348497\n",
      "Iteration 27, loss = 0.28222414\n",
      "Iteration 28, loss = 0.28227027\n",
      "Iteration 26, loss = 0.28450368\n",
      "Iteration 27, loss = 0.28631136\n",
      "Iteration 29, loss = 0.28135048\n",
      "Iteration 28, loss = 0.28466568\n",
      "Iteration 30, loss = 0.28150914\n",
      "Iteration 29, loss = 0.28376452\n",
      "Iteration 31, loss = 0.28173916\n",
      "Iteration 30, loss = 0.28405215\n",
      "Iteration 32, loss = 0.28124988\n",
      "Iteration 33, loss = 0.28197521\n",
      "Iteration 31, loss = 0.28385984\n",
      "Iteration 34, loss = 0.28214817\n",
      "Iteration 32, loss = 0.28326651\n",
      "Iteration 35, loss = 0.28247584\n",
      "Iteration 33, loss = 0.28493662\n",
      "Iteration 36, loss = 0.28352025\n",
      "Iteration 34, loss = 0.28398854\n",
      "Iteration 37, loss = 0.28171827\n",
      "Iteration 35, loss = 0.28350950\n",
      "Iteration 38, loss = 0.28136818\n",
      "Iteration 36, loss = 0.28628580\n",
      "Iteration 39, loss = 0.28179791\n",
      "Iteration 37, loss = 0.28343881\n",
      "Iteration 40, loss = 0.28256521\n",
      "Iteration 38, loss = 0.28346800\n",
      "Iteration 41, loss = 0.28119099\n",
      "Iteration 39, loss = 0.28353587\n",
      "Iteration 42, loss = 0.28170059\n",
      "Iteration 40, loss = 0.28440939\n",
      "Iteration 43, loss = 0.28191410\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 41, loss = 0.28329326\n",
      "Iteration 42, loss = 0.28313340\n",
      "Iteration 1, loss = 0.30779862\n",
      "Iteration 43, loss = 0.28401795\n",
      "Iteration 2, loss = 0.29058158\n",
      "Iteration 44, loss = 0.28364603\n",
      "Iteration 3, loss = 0.28786793\n",
      "Iteration 45, loss = 0.28383356\n",
      "Iteration 4, loss = 0.28686749\n",
      "Iteration 46, loss = 0.28463187\n",
      "Iteration 47, loss = 0.28448329\n",
      "Iteration 5, loss = 0.28645198\n",
      "Iteration 48, loss = 0.28484000\n",
      "Iteration 6, loss = 0.28473529\n",
      "Iteration 49, loss = 0.28409670\n",
      "Iteration 7, loss = 0.28438473\n",
      "Iteration 50, loss = 0.28277186\n",
      "Iteration 8, loss = 0.28361144\n",
      "Iteration 51, loss = 0.28293621\n",
      "Iteration 9, loss = 0.28258912\n",
      "Iteration 52, loss = 0.28277881\n",
      "Iteration 10, loss = 0.28289162\n",
      "Iteration 53, loss = 0.28389212\n",
      "Iteration 11, loss = 0.28286444\n",
      "Iteration 54, loss = 0.28309652\n",
      "Iteration 12, loss = 0.28291115\n",
      "Iteration 55, loss = 0.28344149\n",
      "Iteration 13, loss = 0.28171604\n",
      "Iteration 56, loss = 0.28369806\n",
      "Iteration 14, loss = 0.28302370\n",
      "Iteration 57, loss = 0.28371526\n",
      "Iteration 15, loss = 0.28263947\n",
      "Iteration 58, loss = 0.28294886\n",
      "Iteration 16, loss = 0.28278263\n",
      "Iteration 59, loss = 0.28436086\n",
      "Iteration 17, loss = 0.28233484\n",
      "Iteration 60, loss = 0.28262846\n",
      "Iteration 18, loss = 0.28314639\n",
      "Iteration 19, loss = 0.28243298\n",
      "Iteration 61, loss = 0.28365580\n",
      "Iteration 62, loss = 0.28338535\n",
      "Iteration 20, loss = 0.28140473\n",
      "Iteration 63, loss = 0.28277967\n",
      "Iteration 64, loss = 0.28304661\n",
      "Iteration 21, loss = 0.28210271\n",
      "Iteration 65, loss = 0.28434780\n",
      "Iteration 22, loss = 0.28170626\n",
      "Iteration 66, loss = 0.28313737\n",
      "Iteration 23, loss = 0.28101708\n",
      "Iteration 67, loss = 0.28324445\n",
      "Iteration 24, loss = 0.28274444\n",
      "Iteration 68, loss = 0.28319831\n",
      "Iteration 25, loss = 0.28179132\n",
      "Iteration 69, loss = 0.28275717\n",
      "Iteration 26, loss = 0.28258110\n",
      "Iteration 70, loss = 0.28291441\n",
      "Iteration 27, loss = 0.28272727\n",
      "Iteration 71, loss = 0.28246988\n",
      "Iteration 28, loss = 0.28189198\n",
      "Iteration 72, loss = 0.28390991\n",
      "Iteration 29, loss = 0.28039734\n",
      "Iteration 73, loss = 0.28274918\n",
      "Iteration 30, loss = 0.28127213\n",
      "Iteration 74, loss = 0.28286726\n",
      "Iteration 31, loss = 0.28118491\n",
      "Iteration 75, loss = 0.28395591\n",
      "Iteration 32, loss = 0.28095217\n",
      "Iteration 76, loss = 0.28402079\n",
      "Iteration 33, loss = 0.28150056\n",
      "Iteration 77, loss = 0.28285202\n",
      "Iteration 34, loss = 0.28319783\n",
      "Iteration 78, loss = 0.28314978\n",
      "Iteration 35, loss = 0.28158031\n",
      "Iteration 79, loss = 0.28334980\n",
      "Iteration 36, loss = 0.28269203\n",
      "Iteration 80, loss = 0.28298654\n",
      "Iteration 37, loss = 0.28218468\n",
      "Iteration 81, loss = 0.28283467\n",
      "Iteration 38, loss = 0.28101616\n",
      "Iteration 82, loss = 0.28377636\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 39, loss = 0.28071176\n",
      "Iteration 40, loss = 0.28120589\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30811770\n",
      "Iteration 1, loss = 0.30857047\n",
      "Iteration 2, loss = 0.29130611\n",
      "Iteration 2, loss = 0.29222807\n",
      "Iteration 3, loss = 0.28779321\n",
      "Iteration 3, loss = 0.28784828\n",
      "Iteration 4, loss = 0.28746076\n",
      "Iteration 4, loss = 0.28755794\n",
      "Iteration 5, loss = 0.28665180\n",
      "Iteration 5, loss = 0.28545555\n",
      "Iteration 6, loss = 0.28492605\n",
      "Iteration 6, loss = 0.28405992\n",
      "Iteration 7, loss = 0.28458955\n",
      "Iteration 7, loss = 0.28406827\n",
      "Iteration 8, loss = 0.28363784\n",
      "Iteration 8, loss = 0.28258406\n",
      "Iteration 9, loss = 0.28290798\n",
      "Iteration 9, loss = 0.28236080\n",
      "Iteration 10, loss = 0.28198243\n",
      "Iteration 10, loss = 0.28233732\n",
      "Iteration 11, loss = 0.28210202\n",
      "Iteration 11, loss = 0.28158403\n",
      "Iteration 12, loss = 0.28195941\n",
      "Iteration 12, loss = 0.28148798\n",
      "Iteration 13, loss = 0.28248896\n",
      "Iteration 13, loss = 0.28219915\n",
      "Iteration 14, loss = 0.28165579\n",
      "Iteration 14, loss = 0.28212652\n",
      "Iteration 15, loss = 0.28142976\n",
      "Iteration 15, loss = 0.28164913\n",
      "Iteration 16, loss = 0.28082692\n",
      "Iteration 16, loss = 0.28235502\n",
      "Iteration 17, loss = 0.28167589\n",
      "Iteration 17, loss = 0.28243275\n",
      "Iteration 18, loss = 0.28129917\n",
      "Iteration 18, loss = 0.28228231\n",
      "Iteration 19, loss = 0.28090163\n",
      "Iteration 19, loss = 0.28130156\n",
      "Iteration 20, loss = 0.28079723\n",
      "Iteration 20, loss = 0.28092261\n",
      "Iteration 21, loss = 0.28099473\n",
      "Iteration 21, loss = 0.28218739\n",
      "Iteration 22, loss = 0.28130880\n",
      "Iteration 22, loss = 0.28050422\n",
      "Iteration 23, loss = 0.28210989\n",
      "Iteration 23, loss = 0.28144139\n",
      "Iteration 24, loss = 0.28186880\n",
      "Iteration 24, loss = 0.28199451\n",
      "Iteration 25, loss = 0.27979403\n",
      "Iteration 25, loss = 0.28103431\n",
      "Iteration 26, loss = 0.28046999\n",
      "Iteration 26, loss = 0.28155892\n",
      "Iteration 27, loss = 0.28015351\n",
      "Iteration 27, loss = 0.28151906\n",
      "Iteration 28, loss = 0.27959215\n",
      "Iteration 28, loss = 0.28054670\n",
      "Iteration 29, loss = 0.27924796\n",
      "Iteration 29, loss = 0.28034222\n",
      "Iteration 30, loss = 0.28019899\n",
      "Iteration 30, loss = 0.28106363\n",
      "Iteration 31, loss = 0.27952636\n",
      "Iteration 31, loss = 0.28187241\n",
      "Iteration 32, loss = 0.27994531\n",
      "Iteration 32, loss = 0.28134591\n",
      "Iteration 33, loss = 0.27986478\n",
      "Iteration 33, loss = 0.28084622\n",
      "Iteration 34, loss = 0.27982658\n",
      "Iteration 34, loss = 0.28132574\n",
      "Iteration 35, loss = 0.27957309\n",
      "Iteration 35, loss = 0.28122913\n",
      "Iteration 36, loss = 0.27996063\n",
      "Iteration 36, loss = 0.28235258\n",
      "Iteration 37, loss = 0.28087002\n",
      "Iteration 37, loss = 0.28150099\n",
      "Iteration 38, loss = 0.28045988\n",
      "Iteration 38, loss = 0.28017596\n",
      "Iteration 39, loss = 0.27970531\n",
      "Iteration 39, loss = 0.28068771\n",
      "Iteration 40, loss = 0.27957611\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 40, loss = 0.28066513\n",
      "Iteration 41, loss = 0.28080037\n",
      "Iteration 1, loss = 0.30855007\n",
      "Iteration 42, loss = 0.28110082\n",
      "Iteration 2, loss = 0.29225745\n",
      "Iteration 43, loss = 0.28083607\n",
      "Iteration 3, loss = 0.28808128\n",
      "Iteration 44, loss = 0.28016900\n",
      "Iteration 4, loss = 0.28773619\n",
      "Iteration 45, loss = 0.28120198\n",
      "Iteration 5, loss = 0.28647357\n",
      "Iteration 46, loss = 0.28108064\n",
      "Iteration 6, loss = 0.28544044\n",
      "Iteration 47, loss = 0.28154043\n",
      "Iteration 7, loss = 0.28570709\n",
      "Iteration 48, loss = 0.28119860\n",
      "Iteration 8, loss = 0.28359064\n",
      "Iteration 9, loss = 0.28356421\n",
      "Iteration 49, loss = 0.27970967\n",
      "Iteration 10, loss = 0.28235668\n",
      "Iteration 11, loss = 0.28315667\n",
      "Iteration 50, loss = 0.28016698\n",
      "Iteration 12, loss = 0.28233996\n",
      "Iteration 51, loss = 0.28028167\n",
      "Iteration 52, loss = 0.28005033\n",
      "Iteration 13, loss = 0.28341750\n",
      "Iteration 53, loss = 0.28001016\n",
      "Iteration 14, loss = 0.28261160\n",
      "Iteration 54, loss = 0.28056644\n",
      "Iteration 15, loss = 0.28172372\n",
      "Iteration 55, loss = 0.28058709\n",
      "Iteration 16, loss = 0.28164201\n",
      "Iteration 56, loss = 0.28139859\n",
      "Iteration 17, loss = 0.28218964\n",
      "Iteration 57, loss = 0.28068548\n",
      "Iteration 18, loss = 0.28316294\n",
      "Iteration 19, loss = 0.28206596\n",
      "Iteration 58, loss = 0.28055808\n",
      "Iteration 20, loss = 0.28167653\n",
      "Iteration 59, loss = 0.28121022\n",
      "Iteration 21, loss = 0.28204180\n",
      "Iteration 60, loss = 0.27971798\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 22, loss = 0.28190582\n",
      "Iteration 23, loss = 0.28194890\n",
      "Iteration 1, loss = 0.30875645\n",
      "Iteration 24, loss = 0.28208041\n",
      "Iteration 2, loss = 0.29206024\n",
      "Iteration 25, loss = 0.28162434\n",
      "Iteration 3, loss = 0.28924346\n",
      "Iteration 26, loss = 0.28139283\n",
      "Iteration 4, loss = 0.28903540\n",
      "Iteration 27, loss = 0.28198958\n",
      "Iteration 5, loss = 0.28794333\n",
      "Iteration 28, loss = 0.28066058\n",
      "Iteration 6, loss = 0.28599708\n",
      "Iteration 29, loss = 0.28106935\n",
      "Iteration 7, loss = 0.28638602\n",
      "Iteration 30, loss = 0.28145763\n",
      "Iteration 8, loss = 0.28474238\n",
      "Iteration 31, loss = 0.28104986\n",
      "Iteration 9, loss = 0.28399347\n",
      "Iteration 32, loss = 0.28122287\n",
      "Iteration 10, loss = 0.28355515\n",
      "Iteration 11, loss = 0.28414960\n",
      "Iteration 33, loss = 0.28061812\n",
      "Iteration 12, loss = 0.28368572\n",
      "Iteration 34, loss = 0.28111328\n",
      "Iteration 35, loss = 0.28109030\n",
      "Iteration 13, loss = 0.28394570\n",
      "Iteration 36, loss = 0.28169389\n",
      "Iteration 14, loss = 0.28349989\n",
      "Iteration 37, loss = 0.28204024\n",
      "Iteration 15, loss = 0.28262492\n",
      "Iteration 38, loss = 0.28072074\n",
      "Iteration 39, loss = 0.28055871\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 16, loss = 0.28331518\n",
      "Iteration 17, loss = 0.28211011\n",
      "Iteration 1, loss = 0.30865602\n",
      "Iteration 18, loss = 0.28229311\n",
      "Iteration 2, loss = 0.29051701\n",
      "Iteration 19, loss = 0.28195049\n",
      "Iteration 20, loss = 0.28217875\n",
      "Iteration 3, loss = 0.28849330\n",
      "Iteration 21, loss = 0.28249795\n",
      "Iteration 4, loss = 0.28660367\n",
      "Iteration 22, loss = 0.28296104\n",
      "Iteration 5, loss = 0.28635795\n",
      "Iteration 23, loss = 0.28260040\n",
      "Iteration 6, loss = 0.28626245\n",
      "Iteration 24, loss = 0.28165789\n",
      "Iteration 7, loss = 0.28548671\n",
      "Iteration 25, loss = 0.28236878\n",
      "Iteration 8, loss = 0.28485073\n",
      "Iteration 26, loss = 0.28281282\n",
      "Iteration 9, loss = 0.28447915\n",
      "Iteration 27, loss = 0.28186843\n",
      "Iteration 10, loss = 0.28345233\n",
      "Iteration 28, loss = 0.28244250\n",
      "Iteration 11, loss = 0.28424749\n",
      "Iteration 29, loss = 0.28221011\n",
      "Iteration 12, loss = 0.28348571\n",
      "Iteration 30, loss = 0.28237812\n",
      "Iteration 13, loss = 0.28461252\n",
      "Iteration 31, loss = 0.28143103\n",
      "Iteration 14, loss = 0.28292150\n",
      "Iteration 32, loss = 0.28105831\n",
      "Iteration 15, loss = 0.28319519\n",
      "Iteration 33, loss = 0.28199372\n",
      "Iteration 16, loss = 0.28204680\n",
      "Iteration 34, loss = 0.28280995\n",
      "Iteration 17, loss = 0.28231146\n",
      "Iteration 35, loss = 0.28238181\n",
      "Iteration 36, loss = 0.28218279\n",
      "Iteration 18, loss = 0.28314118\n",
      "Iteration 37, loss = 0.28186131\n",
      "Iteration 19, loss = 0.28157704\n",
      "Iteration 38, loss = 0.28105739\n",
      "Iteration 20, loss = 0.28161914\n",
      "Iteration 39, loss = 0.28083583\n",
      "Iteration 21, loss = 0.28187701\n",
      "Iteration 40, loss = 0.28164212\n",
      "Iteration 22, loss = 0.28173221\n",
      "Iteration 41, loss = 0.28076873\n",
      "Iteration 23, loss = 0.28253391\n",
      "Iteration 42, loss = 0.28180414\n",
      "Iteration 24, loss = 0.28152711\n",
      "Iteration 43, loss = 0.28242454\n",
      "Iteration 25, loss = 0.28191246\n",
      "Iteration 44, loss = 0.28148262\n",
      "Iteration 26, loss = 0.28235827\n",
      "Iteration 45, loss = 0.28195413\n",
      "Iteration 27, loss = 0.28106103\n",
      "Iteration 46, loss = 0.28076267\n",
      "Iteration 28, loss = 0.28123282\n",
      "Iteration 47, loss = 0.28273818\n",
      "Iteration 29, loss = 0.28009656\n",
      "Iteration 48, loss = 0.28213819\n",
      "Iteration 30, loss = 0.28127864\n",
      "Iteration 49, loss = 0.28143445\n",
      "Iteration 31, loss = 0.28070248\n",
      "Iteration 50, loss = 0.28029683\n",
      "Iteration 32, loss = 0.28014308\n",
      "Iteration 51, loss = 0.28106425\n",
      "Iteration 33, loss = 0.28097423\n",
      "Iteration 52, loss = 0.28222266\n",
      "Iteration 34, loss = 0.28172741\n",
      "Iteration 53, loss = 0.28189987\n",
      "Iteration 35, loss = 0.28137644\n",
      "Iteration 54, loss = 0.28107937\n",
      "Iteration 36, loss = 0.28099114\n",
      "Iteration 55, loss = 0.28128121\n",
      "Iteration 37, loss = 0.28139690\n",
      "Iteration 56, loss = 0.28069577\n",
      "Iteration 38, loss = 0.28057008\n",
      "Iteration 57, loss = 0.28136349\n",
      "Iteration 39, loss = 0.28017791\n",
      "Iteration 58, loss = 0.28062995\n",
      "Iteration 40, loss = 0.28033282\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 59, loss = 0.28244376\n",
      "Iteration 1, loss = 0.31015780\n",
      "Iteration 60, loss = 0.28149034\n",
      "Iteration 2, loss = 0.29208247\n",
      "Iteration 61, loss = 0.28101272\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 3, loss = 0.29001257\n",
      "Iteration 4, loss = 0.28810390\n",
      "Iteration 5, loss = 0.28896123\n",
      "Iteration 1, loss = 0.30940797\n",
      "Iteration 6, loss = 0.28831162\n",
      "Iteration 2, loss = 0.29144137\n",
      "Iteration 7, loss = 0.28700081\n",
      "Iteration 3, loss = 0.29055925\n",
      "Iteration 8, loss = 0.28640244\n",
      "Iteration 4, loss = 0.28749028\n",
      "Iteration 9, loss = 0.28630077\n",
      "Iteration 5, loss = 0.28745199\n",
      "Iteration 10, loss = 0.28484992\n",
      "Iteration 6, loss = 0.28621105\n",
      "Iteration 11, loss = 0.28618070\n",
      "Iteration 7, loss = 0.28569893\n",
      "Iteration 8, loss = 0.28513092\n",
      "Iteration 12, loss = 0.28522293\n",
      "Iteration 9, loss = 0.28489002\n",
      "Iteration 13, loss = 0.28499123\n",
      "Iteration 10, loss = 0.28347185\n",
      "Iteration 14, loss = 0.28347509\n",
      "Iteration 11, loss = 0.28457307\n",
      "Iteration 15, loss = 0.28390311\n",
      "Iteration 12, loss = 0.28269857\n",
      "Iteration 16, loss = 0.28415455\n",
      "Iteration 13, loss = 0.28414259\n",
      "Iteration 17, loss = 0.28347378\n",
      "Iteration 14, loss = 0.28235576\n",
      "Iteration 15, loss = 0.28276003\n",
      "Iteration 18, loss = 0.28355883\n",
      "Iteration 16, loss = 0.28319614\n",
      "Iteration 19, loss = 0.28354005\n",
      "Iteration 17, loss = 0.28171100Iteration 20, loss = 0.28296587\n",
      "\n",
      "Iteration 21, loss = 0.28403606\n",
      "Iteration 18, loss = 0.28274777\n",
      "Iteration 22, loss = 0.28291541\n",
      "Iteration 19, loss = 0.28181609\n",
      "Iteration 23, loss = 0.28280327\n",
      "Iteration 20, loss = 0.28263051\n",
      "Iteration 24, loss = 0.28275102\n",
      "Iteration 21, loss = 0.28261202\n",
      "Iteration 25, loss = 0.28339811\n",
      "Iteration 22, loss = 0.28206454\n",
      "Iteration 26, loss = 0.28417419\n",
      "Iteration 23, loss = 0.28278360\n",
      "Iteration 27, loss = 0.28338729\n",
      "Iteration 24, loss = 0.28328138\n",
      "Iteration 28, loss = 0.28263541\n",
      "Iteration 25, loss = 0.28291691\n",
      "Iteration 29, loss = 0.28234943\n",
      "Iteration 26, loss = 0.28309925\n",
      "Iteration 30, loss = 0.28288098\n",
      "Iteration 27, loss = 0.28185995\n",
      "Iteration 31, loss = 0.28251670\n",
      "Iteration 28, loss = 0.28122617\n",
      "Iteration 32, loss = 0.28181465\n",
      "Iteration 29, loss = 0.28155415\n",
      "Iteration 33, loss = 0.28270434\n",
      "Iteration 30, loss = 0.28214837\n",
      "Iteration 31, loss = 0.28100150\n",
      "Iteration 34, loss = 0.28300080\n",
      "Iteration 32, loss = 0.28034353\n",
      "Iteration 35, loss = 0.28384988\n",
      "Iteration 33, loss = 0.28219992\n",
      "Iteration 36, loss = 0.28249690\n",
      "Iteration 34, loss = 0.28217950\n",
      "Iteration 37, loss = 0.28336354\n",
      "Iteration 35, loss = 0.28238513\n",
      "Iteration 38, loss = 0.28257090\n",
      "Iteration 36, loss = 0.28113365\n",
      "Iteration 39, loss = 0.28225976\n",
      "Iteration 37, loss = 0.28199074\n",
      "Iteration 40, loss = 0.28241563\n",
      "Iteration 38, loss = 0.28142673\n",
      "Iteration 41, loss = 0.28161306\n",
      "Iteration 39, loss = 0.28149555\n",
      "Iteration 42, loss = 0.28246715\n",
      "Iteration 40, loss = 0.28101576\n",
      "Iteration 43, loss = 0.28359392\n",
      "Iteration 41, loss = 0.28220001\n",
      "Iteration 44, loss = 0.28217838\n",
      "Iteration 42, loss = 0.28240024\n",
      "Iteration 45, loss = 0.28222073\n",
      "Iteration 43, loss = 0.28294108\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 46, loss = 0.28188608\n",
      "Iteration 47, loss = 0.28213031\n",
      "Iteration 48, loss = 0.28188077\n",
      "Iteration 49, loss = 0.28233691\n",
      "Iteration 50, loss = 0.28127494\n",
      "Iteration 51, loss = 0.28236053\n",
      "Iteration 52, loss = 0.28227619\n",
      "Iteration 53, loss = 0.28289082\n",
      "Iteration 54, loss = 0.28140486\n",
      "Iteration 55, loss = 0.28198730\n",
      "Iteration 56, loss = 0.28209894\n",
      "Iteration 57, loss = 0.28276757\n",
      "Iteration 58, loss = 0.28329979\n",
      "Iteration 59, loss = 0.28235298\n",
      "Iteration 60, loss = 0.28147571\n",
      "Iteration 61, loss = 0.28163527\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29580286\n",
      "Iteration 1, loss = 0.29559997\n",
      "Iteration 2, loss = 0.28414260\n",
      "Iteration 2, loss = 0.28226508\n",
      "Iteration 3, loss = 0.27929542\n",
      "Iteration 3, loss = 0.28251984\n",
      "Iteration 4, loss = 0.27998586\n",
      "Iteration 4, loss = 0.27772308\n",
      "Iteration 5, loss = 0.27891685\n",
      "Iteration 5, loss = 0.27759307\n",
      "Iteration 6, loss = 0.27552296\n",
      "Iteration 6, loss = 0.27750625\n",
      "Iteration 7, loss = 0.27564673\n",
      "Iteration 7, loss = 0.27772320\n",
      "Iteration 8, loss = 0.27631393\n",
      "Iteration 8, loss = 0.27464749\n",
      "Iteration 9, loss = 0.27622367\n",
      "Iteration 9, loss = 0.27413014\n",
      "Iteration 10, loss = 0.27639507\n",
      "Iteration 10, loss = 0.27410643\n",
      "Iteration 11, loss = 0.27589325\n",
      "Iteration 11, loss = 0.27326796\n",
      "Iteration 12, loss = 0.27567842\n",
      "Iteration 12, loss = 0.27364670\n",
      "Iteration 13, loss = 0.27657083\n",
      "Iteration 13, loss = 0.27300168\n",
      "Iteration 14, loss = 0.27547618\n",
      "Iteration 14, loss = 0.27286572\n",
      "Iteration 15, loss = 0.27531562\n",
      "Iteration 15, loss = 0.27283102\n",
      "Iteration 16, loss = 0.27661785\n",
      "Iteration 16, loss = 0.27376626\n",
      "Iteration 17, loss = 0.27464703\n",
      "Iteration 17, loss = 0.27235566Iteration 18, loss = 0.27561536\n",
      "\n",
      "Iteration 19, loss = 0.27517284\n",
      "Iteration 18, loss = 0.27279964\n",
      "Iteration 20, loss = 0.27502253\n",
      "Iteration 19, loss = 0.27234798\n",
      "Iteration 21, loss = 0.27465183\n",
      "Iteration 20, loss = 0.27196614\n",
      "Iteration 22, loss = 0.27542255\n",
      "Iteration 21, loss = 0.27184441\n",
      "Iteration 23, loss = 0.27508219\n",
      "Iteration 22, loss = 0.27194287\n",
      "Iteration 24, loss = 0.27456952\n",
      "Iteration 23, loss = 0.27194577\n",
      "Iteration 25, loss = 0.27525857\n",
      "Iteration 24, loss = 0.27101155\n",
      "Iteration 26, loss = 0.27445265\n",
      "Iteration 25, loss = 0.27239383\n",
      "Iteration 27, loss = 0.27388215\n",
      "Iteration 26, loss = 0.27206714\n",
      "Iteration 28, loss = 0.27481858\n",
      "Iteration 27, loss = 0.27128127\n",
      "Iteration 29, loss = 0.27421367\n",
      "Iteration 28, loss = 0.27175222\n",
      "Iteration 30, loss = 0.27452182\n",
      "Iteration 29, loss = 0.27124086\n",
      "Iteration 31, loss = 0.27581891\n",
      "Iteration 30, loss = 0.27125758\n",
      "Iteration 32, loss = 0.27452628\n",
      "Iteration 31, loss = 0.27232456\n",
      "Iteration 33, loss = 0.27496968\n",
      "Iteration 32, loss = 0.27175284\n",
      "Iteration 34, loss = 0.27471274\n",
      "Iteration 33, loss = 0.27181578\n",
      "Iteration 34, loss = 0.27216387\n",
      "Iteration 35, loss = 0.27394313\n",
      "Iteration 35, loss = 0.27082309\n",
      "Iteration 36, loss = 0.27463793\n",
      "Iteration 36, loss = 0.27137702\n",
      "Iteration 37, loss = 0.27361238\n",
      "Iteration 37, loss = 0.27109856\n",
      "Iteration 38, loss = 0.27454600\n",
      "Iteration 38, loss = 0.27222759\n",
      "Iteration 39, loss = 0.27445308\n",
      "Iteration 39, loss = 0.27228133\n",
      "Iteration 40, loss = 0.27465765\n",
      "Iteration 40, loss = 0.27142177\n",
      "Iteration 41, loss = 0.27441890\n",
      "Iteration 41, loss = 0.27221411\n",
      "Iteration 42, loss = 0.27465901\n",
      "Iteration 42, loss = 0.27123540\n",
      "Iteration 43, loss = 0.27379786\n",
      "Iteration 44, loss = 0.27514724\n",
      "Iteration 43, loss = 0.27108156\n",
      "Iteration 45, loss = 0.27451053\n",
      "Iteration 44, loss = 0.27220659\n",
      "Iteration 46, loss = 0.27490885\n",
      "Iteration 45, loss = 0.27156958\n",
      "Iteration 47, loss = 0.27441433\n",
      "Iteration 46, loss = 0.27185798\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 48, loss = 0.27463655\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29971605\n",
      "Iteration 1, loss = 0.29823288\n",
      "Iteration 2, loss = 0.28415588\n",
      "Iteration 2, loss = 0.28271727\n",
      "Iteration 3, loss = 0.28065081\n",
      "Iteration 3, loss = 0.28043305\n",
      "Iteration 4, loss = 0.27818714\n",
      "Iteration 4, loss = 0.27803227\n",
      "Iteration 5, loss = 0.27761548\n",
      "Iteration 5, loss = 0.27754066\n",
      "Iteration 6, loss = 0.27604150\n",
      "Iteration 6, loss = 0.27555898\n",
      "Iteration 7, loss = 0.27645800\n",
      "Iteration 7, loss = 0.27702819\n",
      "Iteration 8, loss = 0.27502323\n",
      "Iteration 8, loss = 0.27570131\n",
      "Iteration 9, loss = 0.27381684\n",
      "Iteration 9, loss = 0.27421562\n",
      "Iteration 10, loss = 0.27358770\n",
      "Iteration 10, loss = 0.27483147\n",
      "Iteration 11, loss = 0.27286217\n",
      "Iteration 11, loss = 0.27413859\n",
      "Iteration 12, loss = 0.27324926\n",
      "Iteration 12, loss = 0.27342195\n",
      "Iteration 13, loss = 0.27443200\n",
      "Iteration 13, loss = 0.27429342\n",
      "Iteration 14, loss = 0.27299982\n",
      "Iteration 14, loss = 0.27367461\n",
      "Iteration 15, loss = 0.27266748\n",
      "Iteration 15, loss = 0.27315663\n",
      "Iteration 16, loss = 0.27213813\n",
      "Iteration 16, loss = 0.27429678\n",
      "Iteration 17, loss = 0.27258982\n",
      "Iteration 17, loss = 0.27370413\n",
      "Iteration 18, loss = 0.27228976\n",
      "Iteration 18, loss = 0.27388155\n",
      "Iteration 19, loss = 0.27032739\n",
      "Iteration 19, loss = 0.27245615\n",
      "Iteration 20, loss = 0.27192988\n",
      "Iteration 20, loss = 0.27341273\n",
      "Iteration 21, loss = 0.27259099\n",
      "Iteration 21, loss = 0.27343719\n",
      "Iteration 22, loss = 0.27260281\n",
      "Iteration 22, loss = 0.27265945\n",
      "Iteration 23, loss = 0.27259788\n",
      "Iteration 23, loss = 0.27112866\n",
      "Iteration 24, loss = 0.27233766\n",
      "Iteration 24, loss = 0.27066956\n",
      "Iteration 25, loss = 0.27478768\n",
      "Iteration 25, loss = 0.27232108\n",
      "Iteration 26, loss = 0.27331513\n",
      "Iteration 26, loss = 0.27103430\n",
      "Iteration 27, loss = 0.27357353\n",
      "Iteration 27, loss = 0.27218813\n",
      "Iteration 28, loss = 0.27232608\n",
      "Iteration 28, loss = 0.27154048\n",
      "Iteration 29, loss = 0.27275817\n",
      "Iteration 29, loss = 0.27079190\n",
      "Iteration 30, loss = 0.27336270\n",
      "Iteration 30, loss = 0.27188590\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 31, loss = 0.27337290\n",
      "Iteration 1, loss = 0.29729786\n",
      "Iteration 32, loss = 0.27273163\n",
      "Iteration 2, loss = 0.28111399\n",
      "Iteration 33, loss = 0.27341746\n",
      "Iteration 3, loss = 0.27898514\n",
      "Iteration 34, loss = 0.27257531\n",
      "Iteration 4, loss = 0.27722363\n",
      "Iteration 35, loss = 0.27169494\n",
      "Iteration 36, loss = 0.27295197\n",
      "Iteration 5, loss = 0.27608980\n",
      "Iteration 37, loss = 0.27379486\n",
      "Iteration 6, loss = 0.27482471\n",
      "Iteration 38, loss = 0.27189316\n",
      "Iteration 7, loss = 0.27603604\n",
      "Iteration 39, loss = 0.27381807\n",
      "Iteration 8, loss = 0.27416295\n",
      "Iteration 40, loss = 0.27225692\n",
      "Iteration 9, loss = 0.27312163\n",
      "Iteration 41, loss = 0.27246611\n",
      "Iteration 10, loss = 0.27347370\n",
      "Iteration 42, loss = 0.27170747\n",
      "Iteration 43, loss = 0.27194231\n",
      "Iteration 11, loss = 0.27301923\n",
      "Iteration 12, loss = 0.27304865\n",
      "Iteration 44, loss = 0.27172172\n",
      "Iteration 45, loss = 0.27179421\n",
      "Iteration 13, loss = 0.27314930\n",
      "Iteration 46, loss = 0.27133590\n",
      "Iteration 14, loss = 0.27357029\n",
      "Iteration 47, loss = 0.27228244\n",
      "Iteration 15, loss = 0.27296768\n",
      "Iteration 48, loss = 0.27323412\n",
      "Iteration 16, loss = 0.27182833\n",
      "Iteration 49, loss = 0.27223747\n",
      "Iteration 17, loss = 0.27199695\n",
      "Iteration 50, loss = 0.27171929\n",
      "Iteration 18, loss = 0.27111052\n",
      "Iteration 51, loss = 0.27262968\n",
      "Iteration 19, loss = 0.27061923\n",
      "Iteration 52, loss = 0.27257003\n",
      "Iteration 20, loss = 0.27030349\n",
      "Iteration 53, loss = 0.27200063\n",
      "Iteration 21, loss = 0.27211457\n",
      "Iteration 54, loss = 0.27132084\n",
      "Iteration 55, loss = 0.27178949\n",
      "Iteration 22, loss = 0.27103524\n",
      "Iteration 23, loss = 0.27071348\n",
      "Iteration 56, loss = 0.27126037\n",
      "Iteration 24, loss = 0.27012314\n",
      "Iteration 57, loss = 0.27205762\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 25, loss = 0.27127725\n",
      "Iteration 26, loss = 0.27002350\n",
      "Iteration 1, loss = 0.29643027\n",
      "Iteration 27, loss = 0.27059868\n",
      "Iteration 2, loss = 0.28032554\n",
      "Iteration 28, loss = 0.27107813\n",
      "Iteration 3, loss = 0.27717836\n",
      "Iteration 29, loss = 0.27091760\n",
      "Iteration 4, loss = 0.27552512\n",
      "Iteration 30, loss = 0.27001141\n",
      "Iteration 5, loss = 0.27480725\n",
      "Iteration 31, loss = 0.26965391\n",
      "Iteration 6, loss = 0.27409268\n",
      "Iteration 32, loss = 0.26986804\n",
      "Iteration 7, loss = 0.27424693\n",
      "Iteration 33, loss = 0.27053694\n",
      "Iteration 8, loss = 0.27247543\n",
      "Iteration 34, loss = 0.26946351\n",
      "Iteration 9, loss = 0.27135353\n",
      "Iteration 10, loss = 0.27166642\n",
      "Iteration 35, loss = 0.26923870\n",
      "Iteration 36, loss = 0.26975289\n",
      "Iteration 11, loss = 0.27141051\n",
      "Iteration 12, loss = 0.27068399\n",
      "Iteration 37, loss = 0.26926482\n",
      "Iteration 13, loss = 0.27143679\n",
      "Iteration 38, loss = 0.26966987\n",
      "Iteration 14, loss = 0.27134811\n",
      "Iteration 39, loss = 0.27160329\n",
      "Iteration 15, loss = 0.27138696\n",
      "Iteration 40, loss = 0.26956996\n",
      "Iteration 16, loss = 0.27033754\n",
      "Iteration 41, loss = 0.26978656\n",
      "Iteration 17, loss = 0.27111940\n",
      "Iteration 42, loss = 0.27024101\n",
      "Iteration 18, loss = 0.27050655\n",
      "Iteration 43, loss = 0.26995904\n",
      "Iteration 19, loss = 0.26948310\n",
      "Iteration 44, loss = 0.26913276\n",
      "Iteration 20, loss = 0.26973410\n",
      "Iteration 45, loss = 0.26997085\n",
      "Iteration 21, loss = 0.27041570\n",
      "Iteration 46, loss = 0.26914642\n",
      "Iteration 22, loss = 0.27014126\n",
      "Iteration 47, loss = 0.27003100\n",
      "Iteration 23, loss = 0.27017018\n",
      "Iteration 48, loss = 0.27098690\n",
      "Iteration 24, loss = 0.27036012\n",
      "Iteration 49, loss = 0.26957951\n",
      "Iteration 25, loss = 0.27016809\n",
      "Iteration 50, loss = 0.26976739\n",
      "Iteration 26, loss = 0.27067879\n",
      "Iteration 27, loss = 0.26985952\n",
      "Iteration 51, loss = 0.26950430\n",
      "Iteration 28, loss = 0.27124598\n",
      "Iteration 52, loss = 0.26976219\n",
      "Iteration 29, loss = 0.27069959\n",
      "Iteration 53, loss = 0.26986862\n",
      "Iteration 30, loss = 0.26953300\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 54, loss = 0.27091677\n",
      "Iteration 55, loss = 0.27025006\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29755777\n",
      "Iteration 2, loss = 0.28176451\n",
      "Iteration 1, loss = 0.29649252\n",
      "Iteration 3, loss = 0.28027500\n",
      "Iteration 2, loss = 0.28252193\n",
      "Iteration 4, loss = 0.27857205\n",
      "Iteration 3, loss = 0.27876896\n",
      "Iteration 5, loss = 0.27763244\n",
      "Iteration 4, loss = 0.27725264\n",
      "Iteration 6, loss = 0.27615963\n",
      "Iteration 5, loss = 0.27606445\n",
      "Iteration 7, loss = 0.27677799\n",
      "Iteration 6, loss = 0.27558725\n",
      "Iteration 8, loss = 0.27464812\n",
      "Iteration 7, loss = 0.27577649\n",
      "Iteration 9, loss = 0.27350052\n",
      "Iteration 8, loss = 0.27492677\n",
      "Iteration 10, loss = 0.27462897\n",
      "Iteration 9, loss = 0.27402647\n",
      "Iteration 11, loss = 0.27482996\n",
      "Iteration 10, loss = 0.27390851\n",
      "Iteration 12, loss = 0.27330256\n",
      "Iteration 11, loss = 0.27465736\n",
      "Iteration 13, loss = 0.27378428\n",
      "Iteration 12, loss = 0.27289211\n",
      "Iteration 13, loss = 0.27272920\n",
      "Iteration 14, loss = 0.27272778\n",
      "Iteration 14, loss = 0.27422202\n",
      "Iteration 15, loss = 0.27232184\n",
      "Iteration 15, loss = 0.27243127\n",
      "Iteration 16, loss = 0.27131602\n",
      "Iteration 16, loss = 0.27267452\n",
      "Iteration 17, loss = 0.27289248\n",
      "Iteration 17, loss = 0.27317473\n",
      "Iteration 18, loss = 0.27340952\n",
      "Iteration 18, loss = 0.27453621\n",
      "Iteration 19, loss = 0.27087162\n",
      "Iteration 19, loss = 0.27228203\n",
      "Iteration 20, loss = 0.27188990\n",
      "Iteration 20, loss = 0.27219639\n",
      "Iteration 21, loss = 0.27170935\n",
      "Iteration 21, loss = 0.27220928\n",
      "Iteration 22, loss = 0.27225939\n",
      "Iteration 22, loss = 0.27205430\n",
      "Iteration 23, loss = 0.27114784\n",
      "Iteration 23, loss = 0.27206862\n",
      "Iteration 24, loss = 0.27205852\n",
      "Iteration 24, loss = 0.27277259\n",
      "Iteration 25, loss = 0.27188300\n",
      "Iteration 25, loss = 0.27312487\n",
      "Iteration 26, loss = 0.27139925\n",
      "Iteration 26, loss = 0.27199236\n",
      "Iteration 27, loss = 0.27122773\n",
      "Iteration 27, loss = 0.27120916\n",
      "Iteration 28, loss = 0.27246238\n",
      "Iteration 28, loss = 0.27183459\n",
      "Iteration 29, loss = 0.27202957\n",
      "Iteration 29, loss = 0.27201670\n",
      "Iteration 30, loss = 0.27039602\n",
      "Iteration 30, loss = 0.27110679\n",
      "Iteration 31, loss = 0.27172550\n",
      "Iteration 31, loss = 0.27172196\n",
      "Iteration 32, loss = 0.27116726\n",
      "Iteration 32, loss = 0.27241842\n",
      "Iteration 33, loss = 0.27083113\n",
      "Iteration 33, loss = 0.27145205\n",
      "Iteration 34, loss = 0.27022387\n",
      "Iteration 35, loss = 0.27144649\n",
      "Iteration 34, loss = 0.27095984\n",
      "Iteration 36, loss = 0.27117695\n",
      "Iteration 35, loss = 0.27142018\n",
      "Iteration 37, loss = 0.27035647\n",
      "Iteration 36, loss = 0.27168299\n",
      "Iteration 38, loss = 0.27049681\n",
      "Iteration 37, loss = 0.27083784\n",
      "Iteration 39, loss = 0.27241177\n",
      "Iteration 38, loss = 0.27107294\n",
      "Iteration 40, loss = 0.27128507\n",
      "Iteration 39, loss = 0.27245283\n",
      "Iteration 41, loss = 0.27135046\n",
      "Iteration 40, loss = 0.27155186\n",
      "Iteration 42, loss = 0.27217506\n",
      "Iteration 41, loss = 0.27075731\n",
      "Iteration 43, loss = 0.27178084\n",
      "Iteration 42, loss = 0.27227115\n",
      "Iteration 44, loss = 0.27056393\n",
      "Iteration 43, loss = 0.27120187\n",
      "Iteration 45, loss = 0.27135147\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 44, loss = 0.27115938\n",
      "Iteration 45, loss = 0.27093070\n",
      "Iteration 1, loss = 0.29585073\n",
      "Iteration 46, loss = 0.27081175\n",
      "Iteration 2, loss = 0.28216937\n",
      "Iteration 47, loss = 0.27113027\n",
      "Iteration 3, loss = 0.27971729\n",
      "Iteration 48, loss = 0.27230500\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 4, loss = 0.27791320\n",
      "Iteration 1, loss = 0.29532052\n",
      "Iteration 5, loss = 0.27649193\n",
      "Iteration 2, loss = 0.28039124\n",
      "Iteration 6, loss = 0.27641825\n",
      "Iteration 3, loss = 0.27900544\n",
      "Iteration 7, loss = 0.27699388\n",
      "Iteration 4, loss = 0.27662082\n",
      "Iteration 8, loss = 0.27571751\n",
      "Iteration 5, loss = 0.27593827\n",
      "Iteration 6, loss = 0.27537744\n",
      "Iteration 9, loss = 0.27470938\n",
      "Iteration 7, loss = 0.27380319\n",
      "Iteration 10, loss = 0.27465392\n",
      "Iteration 8, loss = 0.27403634\n",
      "Iteration 11, loss = 0.27421868\n",
      "Iteration 9, loss = 0.27210131\n",
      "Iteration 12, loss = 0.27315728\n",
      "Iteration 10, loss = 0.27263501\n",
      "Iteration 13, loss = 0.27273715\n",
      "Iteration 11, loss = 0.27308284\n",
      "Iteration 14, loss = 0.27368287\n",
      "Iteration 12, loss = 0.27257872\n",
      "Iteration 15, loss = 0.27262121\n",
      "Iteration 13, loss = 0.27107652\n",
      "Iteration 16, loss = 0.27208190\n",
      "Iteration 14, loss = 0.27214953\n",
      "Iteration 17, loss = 0.27267829\n",
      "Iteration 15, loss = 0.27140823\n",
      "Iteration 18, loss = 0.27473621\n",
      "Iteration 16, loss = 0.27106948\n",
      "Iteration 19, loss = 0.27208214\n",
      "Iteration 17, loss = 0.27085443\n",
      "Iteration 20, loss = 0.27172430\n",
      "Iteration 18, loss = 0.27247073\n",
      "Iteration 21, loss = 0.27257800\n",
      "Iteration 19, loss = 0.27148085\n",
      "Iteration 22, loss = 0.27265214\n",
      "Iteration 20, loss = 0.26983860\n",
      "Iteration 23, loss = 0.27243479\n",
      "Iteration 24, loss = 0.27112798\n",
      "Iteration 21, loss = 0.27059889\n",
      "Iteration 22, loss = 0.27027192Iteration 25, loss = 0.27110484\n",
      "\n",
      "Iteration 26, loss = 0.27255431\n",
      "Iteration 23, loss = 0.27001891\n",
      "Iteration 27, loss = 0.27155216\n",
      "Iteration 24, loss = 0.27038194\n",
      "Iteration 28, loss = 0.27254642\n",
      "Iteration 25, loss = 0.27016543\n",
      "Iteration 29, loss = 0.27129512\n",
      "Iteration 26, loss = 0.27077139\n",
      "Iteration 30, loss = 0.27092254\n",
      "Iteration 27, loss = 0.27084280\n",
      "Iteration 31, loss = 0.27218575\n",
      "Iteration 32, loss = 0.27217495\n",
      "Iteration 28, loss = 0.27002162\n",
      "Iteration 33, loss = 0.27100858\n",
      "Iteration 29, loss = 0.26995727\n",
      "Iteration 34, loss = 0.27097309\n",
      "Iteration 30, loss = 0.27006057\n",
      "Iteration 35, loss = 0.27184125\n",
      "Iteration 31, loss = 0.27055040\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 36, loss = 0.27208636\n",
      "Iteration 37, loss = 0.27041532\n",
      "Iteration 38, loss = 0.27104318\n",
      "Iteration 39, loss = 0.27204376\n",
      "Iteration 40, loss = 0.27164594\n",
      "Iteration 41, loss = 0.27088369\n",
      "Iteration 42, loss = 0.27260363\n",
      "Iteration 43, loss = 0.27120242\n",
      "Iteration 44, loss = 0.27086848\n",
      "Iteration 45, loss = 0.27037330\n",
      "Iteration 46, loss = 0.27052770\n",
      "Iteration 47, loss = 0.27157226\n",
      "Iteration 48, loss = 0.27256725\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29840203\n",
      "Iteration 1, loss = 0.29822835\n",
      "Iteration 2, loss = 0.28353575\n",
      "Iteration 2, loss = 0.28439192\n",
      "Iteration 3, loss = 0.28223055\n",
      "Iteration 3, loss = 0.28072146\n",
      "Iteration 4, loss = 0.28126133\n",
      "Iteration 4, loss = 0.28013389\n",
      "Iteration 5, loss = 0.28036808\n",
      "Iteration 5, loss = 0.28093926\n",
      "Iteration 6, loss = 0.27809924\n",
      "Iteration 6, loss = 0.27813991\n",
      "Iteration 7, loss = 0.27945962\n",
      "Iteration 7, loss = 0.27820402\n",
      "Iteration 8, loss = 0.27710298\n",
      "Iteration 8, loss = 0.27685755\n",
      "Iteration 9, loss = 0.27801687\n",
      "Iteration 9, loss = 0.27745186\n",
      "Iteration 10, loss = 0.27711288\n",
      "Iteration 10, loss = 0.27677795\n",
      "Iteration 11, loss = 0.27635995\n",
      "Iteration 11, loss = 0.27557713\n",
      "Iteration 12, loss = 0.27663837\n",
      "Iteration 12, loss = 0.27608423\n",
      "Iteration 13, loss = 0.27673806\n",
      "Iteration 13, loss = 0.27596526\n",
      "Iteration 14, loss = 0.27531704\n",
      "Iteration 14, loss = 0.27606900\n",
      "Iteration 15, loss = 0.27583064\n",
      "Iteration 15, loss = 0.27600391\n",
      "Iteration 16, loss = 0.27600389\n",
      "Iteration 16, loss = 0.27576873\n",
      "Iteration 17, loss = 0.27572668\n",
      "Iteration 17, loss = 0.27578551\n",
      "Iteration 18, loss = 0.27591633\n",
      "Iteration 18, loss = 0.27651726\n",
      "Iteration 19, loss = 0.27541407\n",
      "Iteration 19, loss = 0.27664603\n",
      "Iteration 20, loss = 0.27484409\n",
      "Iteration 20, loss = 0.27488689\n",
      "Iteration 21, loss = 0.27412403\n",
      "Iteration 21, loss = 0.27505959\n",
      "Iteration 22, loss = 0.27567695\n",
      "Iteration 22, loss = 0.27583514\n",
      "Iteration 23, loss = 0.27570519\n",
      "Iteration 23, loss = 0.27589646\n",
      "Iteration 24, loss = 0.27508243\n",
      "Iteration 24, loss = 0.27436479\n",
      "Iteration 25, loss = 0.27563686\n",
      "Iteration 25, loss = 0.27634830\n",
      "Iteration 26, loss = 0.27600665\n",
      "Iteration 26, loss = 0.27486889\n",
      "Iteration 27, loss = 0.27522381\n",
      "Iteration 27, loss = 0.27451289\n",
      "Iteration 28, loss = 0.27392547\n",
      "Iteration 28, loss = 0.27450058\n",
      "Iteration 29, loss = 0.27475574\n",
      "Iteration 29, loss = 0.27507121\n",
      "Iteration 30, loss = 0.27527013\n",
      "Iteration 30, loss = 0.27516741\n",
      "Iteration 31, loss = 0.27537697\n",
      "Iteration 31, loss = 0.27452626\n",
      "Iteration 32, loss = 0.27615918\n",
      "Iteration 32, loss = 0.27626221\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 33, loss = 0.27487992\n",
      "Iteration 34, loss = 0.27531868\n",
      "Iteration 1, loss = 0.29690285\n",
      "Iteration 35, loss = 0.27493073\n",
      "Iteration 2, loss = 0.28398519\n",
      "Iteration 36, loss = 0.27463317\n",
      "Iteration 37, loss = 0.27511508\n",
      "Iteration 3, loss = 0.28161399\n",
      "Iteration 38, loss = 0.27635591\n",
      "Iteration 4, loss = 0.28175743\n",
      "Iteration 39, loss = 0.27580537\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 5, loss = 0.28049684\n",
      "Iteration 6, loss = 0.28084462\n",
      "Iteration 1, loss = 0.29612327\n",
      "Iteration 7, loss = 0.27986623\n",
      "Iteration 2, loss = 0.28379614\n",
      "Iteration 8, loss = 0.27993736\n",
      "Iteration 3, loss = 0.28104234\n",
      "Iteration 9, loss = 0.27884398\n",
      "Iteration 4, loss = 0.28155495\n",
      "Iteration 10, loss = 0.27949142\n",
      "Iteration 5, loss = 0.28019234\n",
      "Iteration 11, loss = 0.27707911\n",
      "Iteration 6, loss = 0.27965374\n",
      "Iteration 12, loss = 0.27697687\n",
      "Iteration 7, loss = 0.27896571\n",
      "Iteration 13, loss = 0.27704194\n",
      "Iteration 8, loss = 0.27877541\n",
      "Iteration 14, loss = 0.27646118\n",
      "Iteration 9, loss = 0.27814752\n",
      "Iteration 15, loss = 0.27706318\n",
      "Iteration 10, loss = 0.27882128\n",
      "Iteration 16, loss = 0.27621748\n",
      "Iteration 11, loss = 0.27636578\n",
      "Iteration 17, loss = 0.27708606\n",
      "Iteration 12, loss = 0.27742218\n",
      "Iteration 18, loss = 0.27519398\n",
      "Iteration 19, loss = 0.27593699\n",
      "Iteration 13, loss = 0.27735943\n",
      "Iteration 20, loss = 0.27518510\n",
      "Iteration 14, loss = 0.27599184\n",
      "Iteration 21, loss = 0.27492628\n",
      "Iteration 15, loss = 0.27758205\n",
      "Iteration 22, loss = 0.27497948\n",
      "Iteration 16, loss = 0.27687243\n",
      "Iteration 23, loss = 0.27595437\n",
      "Iteration 17, loss = 0.27655210\n",
      "Iteration 18, loss = 0.27617447\n",
      "Iteration 24, loss = 0.27520819\n",
      "Iteration 19, loss = 0.27599397\n",
      "Iteration 25, loss = 0.27584165\n",
      "Iteration 20, loss = 0.27602930\n",
      "Iteration 26, loss = 0.27642180\n",
      "Iteration 21, loss = 0.27575667\n",
      "Iteration 27, loss = 0.27468857\n",
      "Iteration 22, loss = 0.27550410\n",
      "Iteration 28, loss = 0.27468877\n",
      "Iteration 23, loss = 0.27568199\n",
      "Iteration 29, loss = 0.27535380\n",
      "Iteration 24, loss = 0.27528520\n",
      "Iteration 30, loss = 0.27592602\n",
      "Iteration 25, loss = 0.27524127\n",
      "Iteration 31, loss = 0.27614715\n",
      "Iteration 26, loss = 0.27575539\n",
      "Iteration 32, loss = 0.27654337\n",
      "Iteration 27, loss = 0.27552593\n",
      "Iteration 33, loss = 0.27587213\n",
      "Iteration 28, loss = 0.27485936\n",
      "Iteration 34, loss = 0.27491206\n",
      "Iteration 29, loss = 0.27540878\n",
      "Iteration 35, loss = 0.27474103\n",
      "Iteration 30, loss = 0.27522696\n",
      "Iteration 36, loss = 0.27508381\n",
      "Iteration 31, loss = 0.27558142\n",
      "Iteration 37, loss = 0.27499089\n",
      "Iteration 32, loss = 0.27600492\n",
      "Iteration 38, loss = 0.27699474\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 33, loss = 0.27525238\n",
      "Iteration 34, loss = 0.27437184\n",
      "Iteration 1, loss = 0.29842547\n",
      "Iteration 35, loss = 0.27474392\n",
      "Iteration 2, loss = 0.28376929\n",
      "Iteration 36, loss = 0.27442984\n",
      "Iteration 3, loss = 0.28079306\n",
      "Iteration 37, loss = 0.27494202\n",
      "Iteration 4, loss = 0.28107336\n",
      "Iteration 38, loss = 0.27485970\n",
      "Iteration 5, loss = 0.28013558\n",
      "Iteration 6, loss = 0.27931762\n",
      "Iteration 39, loss = 0.27570846\n",
      "Iteration 7, loss = 0.27925691\n",
      "Iteration 40, loss = 0.27547166\n",
      "Iteration 41, loss = 0.27446668\n",
      "Iteration 8, loss = 0.27843635\n",
      "Iteration 42, loss = 0.27584025\n",
      "Iteration 9, loss = 0.27841572\n",
      "Iteration 43, loss = 0.27434008\n",
      "Iteration 10, loss = 0.27849935\n",
      "Iteration 44, loss = 0.27521580\n",
      "Iteration 11, loss = 0.27705029\n",
      "Iteration 45, loss = 0.27372668\n",
      "Iteration 12, loss = 0.27840886\n",
      "Iteration 46, loss = 0.27592975\n",
      "Iteration 13, loss = 0.27757874\n",
      "Iteration 47, loss = 0.27529681\n",
      "Iteration 14, loss = 0.27651803\n",
      "Iteration 48, loss = 0.27469683\n",
      "Iteration 15, loss = 0.27751163\n",
      "Iteration 49, loss = 0.27463066\n",
      "Iteration 16, loss = 0.27679161\n",
      "Iteration 50, loss = 0.27510226\n",
      "Iteration 17, loss = 0.27718901\n",
      "Iteration 51, loss = 0.27680425\n",
      "Iteration 18, loss = 0.27627676\n",
      "Iteration 52, loss = 0.27625804\n",
      "Iteration 19, loss = 0.27689192\n",
      "Iteration 20, loss = 0.27549758\n",
      "Iteration 53, loss = 0.27515853\n",
      "Iteration 21, loss = 0.27597247\n",
      "Iteration 54, loss = 0.27487592\n",
      "Iteration 22, loss = 0.27534696\n",
      "Iteration 55, loss = 0.27485392\n",
      "Iteration 23, loss = 0.27619506\n",
      "Iteration 56, loss = 0.27495686\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 24, loss = 0.27702260\n",
      "Iteration 25, loss = 0.27603645\n",
      "Iteration 1, loss = 0.29647825\n",
      "Iteration 26, loss = 0.27647036\n",
      "Iteration 2, loss = 0.28398630\n",
      "Iteration 27, loss = 0.27519146\n",
      "Iteration 3, loss = 0.28144536\n",
      "Iteration 28, loss = 0.27544001\n",
      "Iteration 4, loss = 0.27954398\n",
      "Iteration 29, loss = 0.27649410\n",
      "Iteration 5, loss = 0.27821717\n",
      "Iteration 6, loss = 0.27804040\n",
      "Iteration 30, loss = 0.27607856\n",
      "Iteration 7, loss = 0.27788145\n",
      "Iteration 31, loss = 0.27543725\n",
      "Iteration 32, loss = 0.27661855\n",
      "Iteration 8, loss = 0.27782236\n",
      "Iteration 9, loss = 0.27833610\n",
      "Iteration 33, loss = 0.27537911\n",
      "Iteration 10, loss = 0.27820575\n",
      "Iteration 34, loss = 0.27514357\n",
      "Iteration 11, loss = 0.27623751\n",
      "Iteration 35, loss = 0.27636703\n",
      "Iteration 12, loss = 0.27614416\n",
      "Iteration 36, loss = 0.27531634\n",
      "Iteration 13, loss = 0.27460708\n",
      "Iteration 37, loss = 0.27687862\n",
      "Iteration 14, loss = 0.27541914\n",
      "Iteration 38, loss = 0.27654536\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 15, loss = 0.27696296\n",
      "Iteration 16, loss = 0.27730878\n",
      "Iteration 1, loss = 0.29836900\n",
      "Iteration 17, loss = 0.27571947\n",
      "Iteration 2, loss = 0.28409731\n",
      "Iteration 18, loss = 0.27572763\n",
      "Iteration 3, loss = 0.28212635\n",
      "Iteration 19, loss = 0.27514891\n",
      "Iteration 4, loss = 0.27999423\n",
      "Iteration 20, loss = 0.27544237\n",
      "Iteration 5, loss = 0.27933205\n",
      "Iteration 21, loss = 0.27433223\n",
      "Iteration 6, loss = 0.27832609\n",
      "Iteration 22, loss = 0.27401155\n",
      "Iteration 7, loss = 0.27778477\n",
      "Iteration 23, loss = 0.27427001\n",
      "Iteration 8, loss = 0.27906929\n",
      "Iteration 24, loss = 0.27396774\n",
      "Iteration 9, loss = 0.27823378\n",
      "Iteration 25, loss = 0.27397030\n",
      "Iteration 10, loss = 0.27749172\n",
      "Iteration 26, loss = 0.27315602\n",
      "Iteration 11, loss = 0.27726492\n",
      "Iteration 27, loss = 0.27417105\n",
      "Iteration 12, loss = 0.27631425\n",
      "Iteration 28, loss = 0.27362073\n",
      "Iteration 13, loss = 0.27544224\n",
      "Iteration 29, loss = 0.27494524\n",
      "Iteration 30, loss = 0.27411517\n",
      "Iteration 14, loss = 0.27590145\n",
      "Iteration 31, loss = 0.27317998\n",
      "Iteration 15, loss = 0.27544884\n",
      "Iteration 32, loss = 0.27396193\n",
      "Iteration 16, loss = 0.27621342\n",
      "Iteration 33, loss = 0.27436475\n",
      "Iteration 17, loss = 0.27464216\n",
      "Iteration 34, loss = 0.27407989\n",
      "Iteration 18, loss = 0.27511692\n",
      "Iteration 35, loss = 0.27334149\n",
      "Iteration 19, loss = 0.27534667\n",
      "Iteration 20, loss = 0.27541373\n",
      "Iteration 36, loss = 0.27348079\n",
      "Iteration 21, loss = 0.27520971\n",
      "Iteration 37, loss = 0.27467434\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 22, loss = 0.27411521\n",
      "Iteration 23, loss = 0.27461026\n",
      "Iteration 1, loss = 0.30080147\n",
      "Iteration 24, loss = 0.27471375\n",
      "Iteration 2, loss = 0.28510138\n",
      "Iteration 25, loss = 0.27477231\n",
      "Iteration 3, loss = 0.28263843\n",
      "Iteration 26, loss = 0.27463172\n",
      "Iteration 4, loss = 0.28137976\n",
      "Iteration 27, loss = 0.27462871\n",
      "Iteration 5, loss = 0.28055013\n",
      "Iteration 6, loss = 0.28005629\n",
      "Iteration 28, loss = 0.27498076\n",
      "Iteration 7, loss = 0.27961066\n",
      "Iteration 29, loss = 0.27472367\n",
      "Iteration 8, loss = 0.28130170\n",
      "Iteration 30, loss = 0.27457978\n",
      "Iteration 9, loss = 0.27976884\n",
      "Iteration 31, loss = 0.27450384\n",
      "Iteration 10, loss = 0.27974191\n",
      "Iteration 32, loss = 0.27391305\n",
      "Iteration 11, loss = 0.27819196\n",
      "Iteration 33, loss = 0.27525646\n",
      "Iteration 12, loss = 0.27729076\n",
      "Iteration 34, loss = 0.27458071\n",
      "Iteration 13, loss = 0.27709187\n",
      "Iteration 35, loss = 0.27408125\n",
      "Iteration 14, loss = 0.27819607\n",
      "Iteration 36, loss = 0.27403568\n",
      "Iteration 15, loss = 0.27741401\n",
      "Iteration 37, loss = 0.27431365\n",
      "Iteration 16, loss = 0.27746736\n",
      "Iteration 38, loss = 0.27409958\n",
      "Iteration 17, loss = 0.27834115\n",
      "Iteration 39, loss = 0.27539816\n",
      "Iteration 18, loss = 0.27710908\n",
      "Iteration 40, loss = 0.27343207\n",
      "Iteration 19, loss = 0.27643201\n",
      "Iteration 41, loss = 0.27394496\n",
      "Iteration 20, loss = 0.27679211\n",
      "Iteration 42, loss = 0.27515184\n",
      "Iteration 21, loss = 0.27634349\n",
      "Iteration 43, loss = 0.27549416\n",
      "Iteration 22, loss = 0.27595732\n",
      "Iteration 44, loss = 0.27429577\n",
      "Iteration 23, loss = 0.27631982\n",
      "Iteration 45, loss = 0.27510475\n",
      "Iteration 24, loss = 0.27596691\n",
      "Iteration 46, loss = 0.27426696\n",
      "Iteration 25, loss = 0.27701111\n",
      "Iteration 47, loss = 0.27346961\n",
      "Iteration 26, loss = 0.27569133\n",
      "Iteration 48, loss = 0.27346273\n",
      "Iteration 27, loss = 0.27603562\n",
      "Iteration 49, loss = 0.27502974\n",
      "Iteration 28, loss = 0.27575900\n",
      "Iteration 50, loss = 0.27531600\n",
      "Iteration 29, loss = 0.27611684\n",
      "Iteration 51, loss = 0.27437907\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 30, loss = 0.27626388\n",
      "Iteration 31, loss = 0.27584750\n",
      "Iteration 1, loss = 0.30293184\n",
      "Iteration 32, loss = 0.27616745\n",
      "Iteration 2, loss = 0.28536182\n",
      "Iteration 33, loss = 0.27683083\n",
      "Iteration 3, loss = 0.28221132\n",
      "Iteration 34, loss = 0.27650881\n",
      "Iteration 4, loss = 0.28181478\n",
      "Iteration 35, loss = 0.27528224\n",
      "Iteration 5, loss = 0.28095776\n",
      "Iteration 36, loss = 0.27597570\n",
      "Iteration 6, loss = 0.27986301\n",
      "Iteration 37, loss = 0.27556633\n",
      "Iteration 7, loss = 0.28059076\n",
      "Iteration 38, loss = 0.27565737\n",
      "Iteration 8, loss = 0.28062880\n",
      "Iteration 39, loss = 0.27675953\n",
      "Iteration 9, loss = 0.27942396\n",
      "Iteration 40, loss = 0.27539032\n",
      "Iteration 41, loss = 0.27572800\n",
      "Iteration 10, loss = 0.27901077\n",
      "Iteration 42, loss = 0.27626993\n",
      "Iteration 11, loss = 0.27868570\n",
      "Iteration 43, loss = 0.27611094\n",
      "Iteration 12, loss = 0.27730011\n",
      "Iteration 44, loss = 0.27583205\n",
      "Iteration 13, loss = 0.27706773\n",
      "Iteration 45, loss = 0.27614543\n",
      "Iteration 14, loss = 0.27713781\n",
      "Iteration 46, loss = 0.27560820\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 15, loss = 0.27660223\n",
      "Iteration 16, loss = 0.27683651\n",
      "Iteration 1, loss = 0.30228913\n",
      "Iteration 17, loss = 0.27646670\n",
      "Iteration 2, loss = 0.28390052\n",
      "Iteration 18, loss = 0.27671845\n",
      "Iteration 3, loss = 0.28250286\n",
      "Iteration 4, loss = 0.28106950\n",
      "Iteration 19, loss = 0.27635274\n",
      "Iteration 5, loss = 0.27999856\n",
      "Iteration 20, loss = 0.27562511\n",
      "Iteration 6, loss = 0.27931750\n",
      "Iteration 21, loss = 0.27629234\n",
      "Iteration 7, loss = 0.27884956\n",
      "Iteration 22, loss = 0.27589583\n",
      "Iteration 8, loss = 0.28016458\n",
      "Iteration 23, loss = 0.27597164\n",
      "Iteration 24, loss = 0.27531599\n",
      "Iteration 9, loss = 0.27875134\n",
      "Iteration 25, loss = 0.27737803\n",
      "Iteration 10, loss = 0.27818790\n",
      "Iteration 26, loss = 0.27603335\n",
      "Iteration 11, loss = 0.27935724\n",
      "Iteration 27, loss = 0.27634192\n",
      "Iteration 12, loss = 0.27737157\n",
      "Iteration 28, loss = 0.27615981\n",
      "Iteration 13, loss = 0.27669285\n",
      "Iteration 29, loss = 0.27541114\n",
      "Iteration 14, loss = 0.27740870\n",
      "Iteration 30, loss = 0.27497720\n",
      "Iteration 15, loss = 0.27731201\n",
      "Iteration 31, loss = 0.27543487\n",
      "Iteration 16, loss = 0.27687860\n",
      "Iteration 32, loss = 0.27555428\n",
      "Iteration 17, loss = 0.27805160\n",
      "Iteration 33, loss = 0.27629498\n",
      "Iteration 18, loss = 0.27749913\n",
      "Iteration 19, loss = 0.27734179\n",
      "Iteration 34, loss = 0.27579409\n",
      "Iteration 20, loss = 0.27729097\n",
      "Iteration 35, loss = 0.27509949\n",
      "Iteration 21, loss = 0.27828921\n",
      "Iteration 36, loss = 0.27793314\n",
      "Iteration 22, loss = 0.27745042\n",
      "Iteration 37, loss = 0.27611929\n",
      "Iteration 23, loss = 0.27691875\n",
      "Iteration 38, loss = 0.27545757\n",
      "Iteration 24, loss = 0.27726062\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 39, loss = 0.27766747\n",
      "Iteration 40, loss = 0.27492685\n",
      "Iteration 41, loss = 0.27586995\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.29662406\n",
      "Iteration 2, loss = 0.28544687\n",
      "Iteration 3, loss = 0.28479092\n",
      "Iteration 4, loss = 0.28421761\n",
      "Iteration 5, loss = 0.28413693\n",
      "Iteration 6, loss = 0.28323873\n",
      "Iteration 7, loss = 0.28242807\n",
      "Iteration 8, loss = 0.28216804\n",
      "Iteration 9, loss = 0.28230253\n",
      "Iteration 10, loss = 0.28237353\n",
      "Iteration 11, loss = 0.28158368\n",
      "Iteration 12, loss = 0.28143126\n",
      "Iteration 13, loss = 0.28211775\n",
      "Iteration 14, loss = 0.28087663\n",
      "Iteration 15, loss = 0.28165337\n",
      "Iteration 16, loss = 0.28084316\n",
      "Iteration 17, loss = 0.28137640\n",
      "Iteration 18, loss = 0.28161503\n",
      "Iteration 19, loss = 0.28127819\n",
      "Iteration 20, loss = 0.28121952\n",
      "Iteration 21, loss = 0.28220278\n",
      "Iteration 22, loss = 0.28139688\n",
      "Iteration 23, loss = 0.28100048\n",
      "Iteration 24, loss = 0.28056317\n",
      "Iteration 25, loss = 0.28168354\n",
      "Iteration 26, loss = 0.28073799\n",
      "Iteration 27, loss = 0.28123725\n",
      "Iteration 28, loss = 0.28057682\n",
      "Iteration 29, loss = 0.28061367\n",
      "Iteration 30, loss = 0.28188682\n",
      "Iteration 31, loss = 0.28251592\n",
      "Iteration 32, loss = 0.28082568\n",
      "Iteration 33, loss = 0.28071425\n",
      "Iteration 34, loss = 0.28090170\n",
      "Iteration 35, loss = 0.28023258\n",
      "Iteration 36, loss = 0.28145947\n",
      "Iteration 37, loss = 0.28120128\n",
      "Iteration 38, loss = 0.28099692\n",
      "Iteration 39, loss = 0.28115316\n",
      "Iteration 40, loss = 0.28121236\n",
      "Iteration 41, loss = 0.28107299\n",
      "Iteration 42, loss = 0.28172944\n",
      "Iteration 43, loss = 0.28058908\n",
      "Iteration 44, loss = 0.28102390\n",
      "Iteration 45, loss = 0.28088044\n",
      "Iteration 46, loss = 0.28201024\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Model Evaluation Metrics Using Untouched Test Dataset\n",
      "*****************************************************\n",
      "Model Training Time (s):   2.61036\n",
      "Model Prediction Time (s): 0.00257\n",
      "\n",
      "F1 Score:  0.25\n",
      "Accuracy:  0.90     AUC:       0.57\n",
      "Precision: 0.72     Recall:    0.15\n",
      "*****************************************************\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bankX,bankY = import_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(bankX),np.array(bankY), test_size=0.20)\n",
    "hyperNN(X_train, y_train, X_test, y_test,title=\"Model Complexity Curve for NN (Banking Data)\\nHyperparameter : No. Hidden Units\")\n",
    "h_units, learn_rate = NNGridSearchCV(X_train, y_train)\n",
    "estimator_bank = MLPClassifier(hidden_layer_sizes=(h_units,), solver='adam', activation='logistic', \n",
    "                               learning_rate_init=learn_rate, random_state=100,verbose=True)\n",
    "train_samp_bank, NN_train_score_bank, NN_fit_time_bank, NN_pred_time_bank = plot_learning_curve(estimator_bank, X_train, y_train,title=\"Neural Net Banking Data\")\n",
    "final_classifier_evaluation(estimator_bank, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankX,bankY = import_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(bankX),np.array(bankY), test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
