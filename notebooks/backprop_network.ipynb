{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8c4c699e-db6d-404c-a41b-f24d4f9879cf",
      "metadata": {
        "id": "8c4c699e-db6d-404c-a41b-f24d4f9879cf"
      },
      "source": [
        "## Skorch backprop network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "5b4942c4-d06d-4155-a658-2d73d60dc581",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b4942c4-d06d-4155-a658-2d73d60dc581",
        "outputId": "4f645bf6-c14e-4c11-e645-aa1c4af8ac3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'pyperch' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import subprocess\n",
        "\n",
        "# Installation on Google Colab\n",
        "!git clone https://github.com/jlm429/pyperch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "initial_id",
      "metadata": {
        "id": "initial_id"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from torch import nn, optim\n",
        "from skorch import NeuralNetClassifier\n",
        "from pyperch.pyperch.neural.backprop_nn import BackpropModule\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split,GridSearchCV\n",
        "from skorch.callbacks import EpochScoring,EarlyStopping\n",
        "from skorch.callbacks import Callback\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, make_scorer\n",
        "import torch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bdb50a68",
      "metadata": {},
      "outputs": [],
      "source": [
        "X1 = pd.read_csv('bankmarketing_X.csv')\n",
        "X2 = pd.read_csv('breastcancer_X.csv')\n",
        "Y1 = pd.read_csv('bankmarketing_Y.csv')\n",
        "Y2 = pd.read_csv('breastcancer_Y.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f5af54a",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "88ba06e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "X1 = X1.map(lambda x: int(x) if isinstance(x, bool) else x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "23f68f58",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>duration</th>\n",
              "      <th>campaign</th>\n",
              "      <th>pdays</th>\n",
              "      <th>previous</th>\n",
              "      <th>emp.var.rate</th>\n",
              "      <th>cons.price.idx</th>\n",
              "      <th>cons.conf.idx</th>\n",
              "      <th>euribor3m</th>\n",
              "      <th>nr.employed</th>\n",
              "      <th>...</th>\n",
              "      <th>month_oct</th>\n",
              "      <th>month_sep</th>\n",
              "      <th>day_of_week_fri</th>\n",
              "      <th>day_of_week_mon</th>\n",
              "      <th>day_of_week_thu</th>\n",
              "      <th>day_of_week_tue</th>\n",
              "      <th>day_of_week_wed</th>\n",
              "      <th>poutcome_failure</th>\n",
              "      <th>poutcome_nonexistent</th>\n",
              "      <th>poutcome_success</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.533700</td>\n",
              "      <td>0.011014</td>\n",
              "      <td>-0.568570</td>\n",
              "      <td>0.19593</td>\n",
              "      <td>-0.349533</td>\n",
              "      <td>0.648214</td>\n",
              "      <td>0.722036</td>\n",
              "      <td>0.886970</td>\n",
              "      <td>0.712496</td>\n",
              "      <td>0.331783</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.629750</td>\n",
              "      <td>-0.421650</td>\n",
              "      <td>-0.568570</td>\n",
              "      <td>0.19593</td>\n",
              "      <td>-0.349533</td>\n",
              "      <td>0.648214</td>\n",
              "      <td>0.722036</td>\n",
              "      <td>0.886970</td>\n",
              "      <td>0.712496</td>\n",
              "      <td>0.331783</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.291242</td>\n",
              "      <td>-0.124194</td>\n",
              "      <td>-0.568570</td>\n",
              "      <td>0.19593</td>\n",
              "      <td>-0.349533</td>\n",
              "      <td>0.648214</td>\n",
              "      <td>0.722036</td>\n",
              "      <td>0.886970</td>\n",
              "      <td>0.712496</td>\n",
              "      <td>0.331783</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.003093</td>\n",
              "      <td>-0.413924</td>\n",
              "      <td>-0.568570</td>\n",
              "      <td>0.19593</td>\n",
              "      <td>-0.349533</td>\n",
              "      <td>0.648214</td>\n",
              "      <td>0.722036</td>\n",
              "      <td>0.886970</td>\n",
              "      <td>0.712496</td>\n",
              "      <td>0.331783</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.533700</td>\n",
              "      <td>0.188715</td>\n",
              "      <td>-0.568570</td>\n",
              "      <td>0.19593</td>\n",
              "      <td>-0.349533</td>\n",
              "      <td>0.648214</td>\n",
              "      <td>0.722036</td>\n",
              "      <td>0.886970</td>\n",
              "      <td>0.712496</td>\n",
              "      <td>0.331783</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45302</th>\n",
              "      <td>-0.963589</td>\n",
              "      <td>-0.792505</td>\n",
              "      <td>-0.568570</td>\n",
              "      <td>0.19593</td>\n",
              "      <td>-0.349533</td>\n",
              "      <td>0.839270</td>\n",
              "      <td>0.590747</td>\n",
              "      <td>-0.475177</td>\n",
              "      <td>0.770731</td>\n",
              "      <td>0.844356</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45303</th>\n",
              "      <td>-0.099143</td>\n",
              "      <td>-0.151235</td>\n",
              "      <td>-0.568570</td>\n",
              "      <td>0.19593</td>\n",
              "      <td>-0.349533</td>\n",
              "      <td>0.839270</td>\n",
              "      <td>0.590747</td>\n",
              "      <td>-0.475177</td>\n",
              "      <td>0.771308</td>\n",
              "      <td>0.844356</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45304</th>\n",
              "      <td>-1.251737</td>\n",
              "      <td>-0.750011</td>\n",
              "      <td>-0.205228</td>\n",
              "      <td>0.19593</td>\n",
              "      <td>1.653035</td>\n",
              "      <td>-1.198669</td>\n",
              "      <td>-1.179930</td>\n",
              "      <td>-1.231926</td>\n",
              "      <td>-1.307303</td>\n",
              "      <td>-0.937905</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45305</th>\n",
              "      <td>1.725799</td>\n",
              "      <td>1.042454</td>\n",
              "      <td>-0.568570</td>\n",
              "      <td>0.19593</td>\n",
              "      <td>-0.349533</td>\n",
              "      <td>0.839270</td>\n",
              "      <td>-0.228083</td>\n",
              "      <td>0.951835</td>\n",
              "      <td>0.775344</td>\n",
              "      <td>0.844356</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45306</th>\n",
              "      <td>-0.579390</td>\n",
              "      <td>-0.321210</td>\n",
              "      <td>-0.568570</td>\n",
              "      <td>0.19593</td>\n",
              "      <td>-0.349533</td>\n",
              "      <td>-0.116014</td>\n",
              "      <td>-0.649591</td>\n",
              "      <td>-0.323827</td>\n",
              "      <td>0.287548</td>\n",
              "      <td>0.398100</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>45307 rows Ã— 63 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            age  duration  campaign    pdays  previous  emp.var.rate  \\\n",
              "0      1.533700  0.011014 -0.568570  0.19593 -0.349533      0.648214   \n",
              "1      1.629750 -0.421650 -0.568570  0.19593 -0.349533      0.648214   \n",
              "2     -0.291242 -0.124194 -0.568570  0.19593 -0.349533      0.648214   \n",
              "3     -0.003093 -0.413924 -0.568570  0.19593 -0.349533      0.648214   \n",
              "4      1.533700  0.188715 -0.568570  0.19593 -0.349533      0.648214   \n",
              "...         ...       ...       ...      ...       ...           ...   \n",
              "45302 -0.963589 -0.792505 -0.568570  0.19593 -0.349533      0.839270   \n",
              "45303 -0.099143 -0.151235 -0.568570  0.19593 -0.349533      0.839270   \n",
              "45304 -1.251737 -0.750011 -0.205228  0.19593  1.653035     -1.198669   \n",
              "45305  1.725799  1.042454 -0.568570  0.19593 -0.349533      0.839270   \n",
              "45306 -0.579390 -0.321210 -0.568570  0.19593 -0.349533     -0.116014   \n",
              "\n",
              "       cons.price.idx  cons.conf.idx  euribor3m  nr.employed  ...  month_oct  \\\n",
              "0            0.722036       0.886970   0.712496     0.331783  ...          0   \n",
              "1            0.722036       0.886970   0.712496     0.331783  ...          0   \n",
              "2            0.722036       0.886970   0.712496     0.331783  ...          0   \n",
              "3            0.722036       0.886970   0.712496     0.331783  ...          0   \n",
              "4            0.722036       0.886970   0.712496     0.331783  ...          0   \n",
              "...               ...            ...        ...          ...  ...        ...   \n",
              "45302        0.590747      -0.475177   0.770731     0.844356  ...          0   \n",
              "45303        0.590747      -0.475177   0.771308     0.844356  ...          0   \n",
              "45304       -1.179930      -1.231926  -1.307303    -0.937905  ...          0   \n",
              "45305       -0.228083       0.951835   0.775344     0.844356  ...          0   \n",
              "45306       -0.649591      -0.323827   0.287548     0.398100  ...          0   \n",
              "\n",
              "       month_sep  day_of_week_fri  day_of_week_mon  day_of_week_thu  \\\n",
              "0              0                0                1                0   \n",
              "1              0                0                1                0   \n",
              "2              0                0                1                0   \n",
              "3              0                0                1                0   \n",
              "4              0                0                1                0   \n",
              "...          ...              ...              ...              ...   \n",
              "45302          0                0                0                1   \n",
              "45303          0                1                0                0   \n",
              "45304          0                0                1                0   \n",
              "45305          0                1                0                0   \n",
              "45306          0                0                0                0   \n",
              "\n",
              "       day_of_week_tue  day_of_week_wed  poutcome_failure  \\\n",
              "0                    0                0                 0   \n",
              "1                    0                0                 0   \n",
              "2                    0                0                 0   \n",
              "3                    0                0                 0   \n",
              "4                    0                0                 0   \n",
              "...                ...              ...               ...   \n",
              "45302                0                0                 0   \n",
              "45303                0                0                 0   \n",
              "45304                0                0                 1   \n",
              "45305                0                0                 0   \n",
              "45306                0                1                 0   \n",
              "\n",
              "       poutcome_nonexistent  poutcome_success  \n",
              "0                         1                 0  \n",
              "1                         1                 0  \n",
              "2                         1                 0  \n",
              "3                         1                 0  \n",
              "4                         1                 0  \n",
              "...                     ...               ...  \n",
              "45302                     1                 0  \n",
              "45303                     1                 0  \n",
              "45304                     0                 0  \n",
              "45305                     1                 0  \n",
              "45306                     1                 0  \n",
              "\n",
              "[45307 rows x 63 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0819944b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0        0\n",
              "1        0\n",
              "2        0\n",
              "3        0\n",
              "4        0\n",
              "        ..\n",
              "45302    0\n",
              "45303    0\n",
              "45304    0\n",
              "45305    0\n",
              "45306    0\n",
              "Name: y, Length: 45307, dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "Y1['y']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "85622cac",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(np.array(X1),np.array(Y1), test_size=0.20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "303425b5",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [0],\n",
              "       [0],\n",
              "       ...,\n",
              "       [0],\n",
              "       [0],\n",
              "       [1]])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8a2e43f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "classes = np.unique(Y1)\n",
        "classes\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y = Y1['y'])\n",
        "#class_weights = dict(zip(np.unique(Y1), class_weights))\n",
        "\n",
        "class_weights = torch.FloatTensor(class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0226ba56",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(np.array(X1),np.array(Y1), test_size=0.20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c4d778a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "def hyperNN(X_train, y_train, X_test, y_test, title):\n",
        "\n",
        "    f1_test = []\n",
        "    f1_train = []\n",
        "    hlist = np.linspace(1,60,30).astype('int')\n",
        "    for i in hlist:         \n",
        "            clf = MLPClassifier(hidden_layer_sizes=(i,), solver='adam', activation='logistic', \n",
        "                                learning_rate_init=0.05, random_state=100,verbose=True)\n",
        "            clf.fit(X_train, y_train)\n",
        "            y_pred_test = clf.predict(X_test)\n",
        "            y_pred_train = clf.predict(X_train)\n",
        "            f1_test.append(f1_score(y_test, y_pred_test))\n",
        "            f1_train.append(f1_score(y_train, y_pred_train))\n",
        "      \n",
        "    plt.plot(hlist, f1_test, 'o-', color='r', label='Test F1 Score')\n",
        "    plt.plot(hlist, f1_train, 'o-', color = 'b', label='Train F1 Score')\n",
        "    plt.ylabel('Model F1 Score')\n",
        "    plt.xlabel('No. Hidden Units')\n",
        "    \n",
        "    plt.title(title)\n",
        "    plt.legend(loc='best')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "def NNGridSearchCV(X_train, y_train):\n",
        "    #parameters to search:\n",
        "    #number of hidden units\n",
        "    #learning_rate\n",
        "    h_units = [5, 10, 20, 30, 40, 50, 75, 100]\n",
        "    learning_rates = [0.01, 0.05, .1]\n",
        "    param_grid = {'hidden_layer_sizes': h_units, 'learning_rate_init': learning_rates}\n",
        "\n",
        "    net = GridSearchCV(estimator = MLPClassifier(solver='adam',activation='logistic',random_state=100),\n",
        "                       param_grid=param_grid, cv=10)\n",
        "    net.fit(X_train, y_train)\n",
        "    print(\"Per Hyperparameter tuning, best parameters are:\")\n",
        "    print(net.best_params_)\n",
        "    return net.best_params_['hidden_layer_sizes'], net.best_params_['learning_rate_init']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "8d6e3ee1",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 1, 0, 0])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train.ravel().astype(np.int64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "71835a1b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.43741579\n",
            "Iteration 2, loss = 0.22325699\n",
            "Iteration 3, loss = 0.20678118\n",
            "Iteration 4, loss = 0.20396634\n",
            "Iteration 5, loss = 0.20167220\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 6, loss = 0.20079593\n",
            "Iteration 7, loss = 0.19961376\n",
            "Iteration 8, loss = 0.19989066\n",
            "Iteration 9, loss = 0.19915843\n",
            "Iteration 10, loss = 0.19978176\n",
            "Iteration 11, loss = 0.19815131\n",
            "Iteration 12, loss = 0.19865795\n",
            "Iteration 13, loss = 0.19824427\n",
            "Iteration 14, loss = 0.19693202\n",
            "Iteration 15, loss = 0.19621969\n",
            "Iteration 16, loss = 0.19723358\n",
            "Iteration 17, loss = 0.19650233\n",
            "Iteration 18, loss = 0.19683291\n",
            "Iteration 19, loss = 0.19611906\n",
            "Iteration 20, loss = 0.19543573\n",
            "Iteration 21, loss = 0.19650307\n",
            "Iteration 22, loss = 0.19586033\n",
            "Iteration 23, loss = 0.19639017\n",
            "Iteration 24, loss = 0.19681670\n",
            "Iteration 25, loss = 0.19669534\n",
            "Iteration 26, loss = 0.19737455\n",
            "Iteration 27, loss = 0.19627511\n",
            "Iteration 28, loss = 0.19642740\n",
            "Iteration 29, loss = 0.19581633\n",
            "Iteration 30, loss = 0.19663961\n",
            "Iteration 31, loss = 0.19645824\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.21920743\n",
            "Iteration 2, loss = 0.18846208\n",
            "Iteration 3, loss = 0.18571794\n",
            "Iteration 4, loss = 0.18521434\n",
            "Iteration 5, loss = 0.18359771\n",
            "Iteration 6, loss = 0.18237191\n",
            "Iteration 7, loss = 0.18122941\n",
            "Iteration 8, loss = 0.18088410\n",
            "Iteration 9, loss = 0.17959114\n",
            "Iteration 10, loss = 0.17985624\n",
            "Iteration 11, loss = 0.17875561\n",
            "Iteration 12, loss = 0.17824672\n",
            "Iteration 13, loss = 0.17858973\n",
            "Iteration 14, loss = 0.17793073\n",
            "Iteration 15, loss = 0.17857934\n",
            "Iteration 16, loss = 0.17843932\n",
            "Iteration 17, loss = 0.17750842\n",
            "Iteration 18, loss = 0.17720542\n",
            "Iteration 19, loss = 0.17763558\n",
            "Iteration 20, loss = 0.17662834\n",
            "Iteration 21, loss = 0.17655530\n",
            "Iteration 22, loss = 0.17678437\n",
            "Iteration 23, loss = 0.17628768\n",
            "Iteration 24, loss = 0.17579586\n",
            "Iteration 25, loss = 0.17568013\n",
            "Iteration 26, loss = 0.17583554\n",
            "Iteration 27, loss = 0.17602601\n",
            "Iteration 28, loss = 0.17575706\n",
            "Iteration 29, loss = 0.17572795\n",
            "Iteration 30, loss = 0.17601239\n",
            "Iteration 31, loss = 0.17539560\n",
            "Iteration 32, loss = 0.17574019\n",
            "Iteration 33, loss = 0.17610944\n",
            "Iteration 34, loss = 0.17551202\n",
            "Iteration 35, loss = 0.17536363\n",
            "Iteration 36, loss = 0.17613144\n",
            "Iteration 37, loss = 0.17505214\n",
            "Iteration 38, loss = 0.17530157\n",
            "Iteration 39, loss = 0.17461667\n",
            "Iteration 40, loss = 0.17514782\n",
            "Iteration 41, loss = 0.17472639\n",
            "Iteration 42, loss = 0.17456233\n",
            "Iteration 43, loss = 0.17456471\n",
            "Iteration 44, loss = 0.17457837\n",
            "Iteration 45, loss = 0.17447400\n",
            "Iteration 46, loss = 0.17418309\n",
            "Iteration 47, loss = 0.17479783\n",
            "Iteration 48, loss = 0.17476468\n",
            "Iteration 49, loss = 0.17515276\n",
            "Iteration 50, loss = 0.17473555\n",
            "Iteration 51, loss = 0.17345669\n",
            "Iteration 52, loss = 0.17508023\n",
            "Iteration 53, loss = 0.17501095\n",
            "Iteration 54, loss = 0.17417814\n",
            "Iteration 55, loss = 0.17416905\n",
            "Iteration 56, loss = 0.17562060\n",
            "Iteration 57, loss = 0.17450571\n",
            "Iteration 58, loss = 0.17494139\n",
            "Iteration 59, loss = 0.17373549\n",
            "Iteration 60, loss = 0.17435655\n",
            "Iteration 61, loss = 0.17445140\n",
            "Iteration 62, loss = 0.17411418\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.22976928\n",
            "Iteration 2, loss = 0.18764927\n",
            "Iteration 3, loss = 0.18491835\n",
            "Iteration 4, loss = 0.18286048\n",
            "Iteration 5, loss = 0.18070877\n",
            "Iteration 6, loss = 0.17926002\n",
            "Iteration 7, loss = 0.17816716\n",
            "Iteration 8, loss = 0.17796634\n",
            "Iteration 9, loss = 0.17689466\n",
            "Iteration 10, loss = 0.17639755\n",
            "Iteration 11, loss = 0.17439005\n",
            "Iteration 12, loss = 0.17426365\n",
            "Iteration 13, loss = 0.17471033\n",
            "Iteration 14, loss = 0.17338235\n",
            "Iteration 15, loss = 0.17325315\n",
            "Iteration 16, loss = 0.17141308\n",
            "Iteration 17, loss = 0.17169531\n",
            "Iteration 18, loss = 0.17211026\n",
            "Iteration 19, loss = 0.17117063\n",
            "Iteration 20, loss = 0.16992544\n",
            "Iteration 21, loss = 0.16951733\n",
            "Iteration 22, loss = 0.16939219\n",
            "Iteration 23, loss = 0.16911971\n",
            "Iteration 24, loss = 0.16877741\n",
            "Iteration 25, loss = 0.16871505\n",
            "Iteration 26, loss = 0.16860499\n",
            "Iteration 27, loss = 0.16759495\n",
            "Iteration 28, loss = 0.16846577\n",
            "Iteration 29, loss = 0.16774480\n",
            "Iteration 30, loss = 0.16761972\n",
            "Iteration 31, loss = 0.16723299\n",
            "Iteration 32, loss = 0.16785949\n",
            "Iteration 33, loss = 0.16669461\n",
            "Iteration 34, loss = 0.16681110\n",
            "Iteration 35, loss = 0.16683690\n",
            "Iteration 36, loss = 0.16638318\n",
            "Iteration 37, loss = 0.16702888\n",
            "Iteration 38, loss = 0.16691485\n",
            "Iteration 39, loss = 0.16649119\n",
            "Iteration 40, loss = 0.16671018\n",
            "Iteration 41, loss = 0.16615249\n",
            "Iteration 42, loss = 0.16581031\n",
            "Iteration 43, loss = 0.16660003\n",
            "Iteration 44, loss = 0.16567161\n",
            "Iteration 45, loss = 0.16564193\n",
            "Iteration 46, loss = 0.16486988\n",
            "Iteration 47, loss = 0.16658793\n",
            "Iteration 48, loss = 0.16514637\n",
            "Iteration 49, loss = 0.16583718\n",
            "Iteration 50, loss = 0.16661848\n",
            "Iteration 51, loss = 0.16478758\n",
            "Iteration 52, loss = 0.16446813\n",
            "Iteration 53, loss = 0.16546323\n",
            "Iteration 54, loss = 0.16535350\n",
            "Iteration 55, loss = 0.16493539\n",
            "Iteration 56, loss = 0.16545272\n",
            "Iteration 57, loss = 0.16513652\n",
            "Iteration 58, loss = 0.16450277\n",
            "Iteration 59, loss = 0.16448586\n",
            "Iteration 60, loss = 0.16481047\n",
            "Iteration 61, loss = 0.16459502\n",
            "Iteration 62, loss = 0.16401013\n",
            "Iteration 63, loss = 0.16501272\n",
            "Iteration 64, loss = 0.16398376\n",
            "Iteration 65, loss = 0.16521033\n",
            "Iteration 66, loss = 0.16488922\n",
            "Iteration 67, loss = 0.16511999\n",
            "Iteration 68, loss = 0.16451465\n",
            "Iteration 69, loss = 0.16409239\n",
            "Iteration 70, loss = 0.16401109\n",
            "Iteration 71, loss = 0.16406637\n",
            "Iteration 72, loss = 0.16427868\n",
            "Iteration 73, loss = 0.16460133\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.21305715\n",
            "Iteration 2, loss = 0.18756308\n",
            "Iteration 3, loss = 0.18343774\n",
            "Iteration 4, loss = 0.18042171\n",
            "Iteration 5, loss = 0.17881244\n",
            "Iteration 6, loss = 0.17662447\n",
            "Iteration 7, loss = 0.17530974\n",
            "Iteration 8, loss = 0.17373356\n",
            "Iteration 9, loss = 0.17300811\n",
            "Iteration 10, loss = 0.17155498\n",
            "Iteration 11, loss = 0.17024197\n",
            "Iteration 12, loss = 0.17119872\n",
            "Iteration 13, loss = 0.16952131\n",
            "Iteration 14, loss = 0.16880756\n",
            "Iteration 15, loss = 0.16747351\n",
            "Iteration 16, loss = 0.16636455\n",
            "Iteration 17, loss = 0.16698798\n",
            "Iteration 18, loss = 0.16559850\n",
            "Iteration 19, loss = 0.16507030\n",
            "Iteration 20, loss = 0.16531595\n",
            "Iteration 21, loss = 0.16515835\n",
            "Iteration 22, loss = 0.16381886\n",
            "Iteration 23, loss = 0.16470077\n",
            "Iteration 24, loss = 0.16488713\n",
            "Iteration 25, loss = 0.16375620\n",
            "Iteration 26, loss = 0.16295579\n",
            "Iteration 27, loss = 0.16262841\n",
            "Iteration 28, loss = 0.16208330\n",
            "Iteration 29, loss = 0.16200233\n",
            "Iteration 30, loss = 0.16194517\n",
            "Iteration 31, loss = 0.16148414\n",
            "Iteration 32, loss = 0.16133190\n",
            "Iteration 33, loss = 0.16098088\n",
            "Iteration 34, loss = 0.16135377\n",
            "Iteration 35, loss = 0.16116529\n",
            "Iteration 36, loss = 0.16048056\n",
            "Iteration 37, loss = 0.16077590\n",
            "Iteration 38, loss = 0.16125745\n",
            "Iteration 39, loss = 0.16082300\n",
            "Iteration 40, loss = 0.15995011\n",
            "Iteration 41, loss = 0.15961092\n",
            "Iteration 42, loss = 0.16058138\n",
            "Iteration 43, loss = 0.15960834\n",
            "Iteration 44, loss = 0.15853625\n",
            "Iteration 45, loss = 0.15951519\n",
            "Iteration 46, loss = 0.15913374\n",
            "Iteration 47, loss = 0.15951171\n",
            "Iteration 48, loss = 0.15956168\n",
            "Iteration 49, loss = 0.15913225\n",
            "Iteration 50, loss = 0.15963579\n",
            "Iteration 51, loss = 0.15870331\n",
            "Iteration 52, loss = 0.15936552\n",
            "Iteration 53, loss = 0.15897835\n",
            "Iteration 54, loss = 0.15791057\n",
            "Iteration 55, loss = 0.15920840\n",
            "Iteration 56, loss = 0.15856156\n",
            "Iteration 57, loss = 0.15930213\n",
            "Iteration 58, loss = 0.15885372\n",
            "Iteration 59, loss = 0.15860921\n",
            "Iteration 60, loss = 0.15937995\n",
            "Iteration 61, loss = 0.15908661\n",
            "Iteration 62, loss = 0.15897247\n",
            "Iteration 63, loss = 0.15879013\n",
            "Iteration 64, loss = 0.15874267\n",
            "Iteration 65, loss = 0.15893406\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.21181220\n",
            "Iteration 2, loss = 0.18792878\n",
            "Iteration 3, loss = 0.18501975\n",
            "Iteration 4, loss = 0.18106248\n",
            "Iteration 5, loss = 0.17953676\n",
            "Iteration 6, loss = 0.17712844\n",
            "Iteration 7, loss = 0.17577828\n",
            "Iteration 8, loss = 0.17409951\n",
            "Iteration 9, loss = 0.17166484\n",
            "Iteration 10, loss = 0.17000915\n",
            "Iteration 11, loss = 0.16839769\n",
            "Iteration 12, loss = 0.16705843\n",
            "Iteration 13, loss = 0.16732819\n",
            "Iteration 14, loss = 0.16502005\n",
            "Iteration 15, loss = 0.16540975\n",
            "Iteration 16, loss = 0.16463284\n",
            "Iteration 17, loss = 0.16314559\n",
            "Iteration 18, loss = 0.16285269\n",
            "Iteration 19, loss = 0.16088009\n",
            "Iteration 20, loss = 0.16163415\n",
            "Iteration 21, loss = 0.16040961\n",
            "Iteration 22, loss = 0.16093801\n",
            "Iteration 23, loss = 0.15973763\n",
            "Iteration 24, loss = 0.15915107\n",
            "Iteration 25, loss = 0.15810007\n",
            "Iteration 26, loss = 0.15827562\n",
            "Iteration 27, loss = 0.15709982\n",
            "Iteration 28, loss = 0.15697823\n",
            "Iteration 29, loss = 0.15776464\n",
            "Iteration 30, loss = 0.15636699\n",
            "Iteration 31, loss = 0.15662113\n",
            "Iteration 32, loss = 0.15610255\n",
            "Iteration 33, loss = 0.15500460\n",
            "Iteration 34, loss = 0.15580891\n",
            "Iteration 35, loss = 0.15632709\n",
            "Iteration 36, loss = 0.15508176\n",
            "Iteration 37, loss = 0.15498503\n",
            "Iteration 38, loss = 0.15584666\n",
            "Iteration 39, loss = 0.15456883\n",
            "Iteration 40, loss = 0.15392587\n",
            "Iteration 41, loss = 0.15365135\n",
            "Iteration 42, loss = 0.15347384\n",
            "Iteration 43, loss = 0.15376030\n",
            "Iteration 44, loss = 0.15321404\n",
            "Iteration 45, loss = 0.15430260\n",
            "Iteration 46, loss = 0.15278585\n",
            "Iteration 47, loss = 0.15467268\n",
            "Iteration 48, loss = 0.15322923\n",
            "Iteration 49, loss = 0.15255790\n",
            "Iteration 50, loss = 0.15343270\n",
            "Iteration 51, loss = 0.15282759\n",
            "Iteration 52, loss = 0.15212245\n",
            "Iteration 53, loss = 0.15324020\n",
            "Iteration 54, loss = 0.15246569\n",
            "Iteration 55, loss = 0.15251353\n",
            "Iteration 56, loss = 0.15229868\n",
            "Iteration 57, loss = 0.15277147\n",
            "Iteration 58, loss = 0.15302297\n",
            "Iteration 59, loss = 0.15184060\n",
            "Iteration 60, loss = 0.15329999\n",
            "Iteration 61, loss = 0.15264515\n",
            "Iteration 62, loss = 0.15209214\n",
            "Iteration 63, loss = 0.15240435\n",
            "Iteration 64, loss = 0.15172772\n",
            "Iteration 65, loss = 0.15240559\n",
            "Iteration 66, loss = 0.15257115\n",
            "Iteration 67, loss = 0.15186541\n",
            "Iteration 68, loss = 0.15243498\n",
            "Iteration 69, loss = 0.15265630\n",
            "Iteration 70, loss = 0.15136580\n",
            "Iteration 71, loss = 0.15180382\n",
            "Iteration 72, loss = 0.15146558\n",
            "Iteration 73, loss = 0.15158982\n",
            "Iteration 74, loss = 0.15127341\n",
            "Iteration 75, loss = 0.15158425\n",
            "Iteration 76, loss = 0.15180607\n",
            "Iteration 77, loss = 0.15176663\n",
            "Iteration 78, loss = 0.15087223\n",
            "Iteration 79, loss = 0.15145506\n",
            "Iteration 80, loss = 0.15158857\n",
            "Iteration 81, loss = 0.15133987\n",
            "Iteration 82, loss = 0.15180325\n",
            "Iteration 83, loss = 0.15170417\n",
            "Iteration 84, loss = 0.15087551\n",
            "Iteration 85, loss = 0.15178406\n",
            "Iteration 86, loss = 0.15143287\n",
            "Iteration 87, loss = 0.15172322\n",
            "Iteration 88, loss = 0.15174214\n",
            "Iteration 89, loss = 0.15079817\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.21219881\n",
            "Iteration 2, loss = 0.18768240\n",
            "Iteration 3, loss = 0.18371239\n",
            "Iteration 4, loss = 0.18090673\n",
            "Iteration 5, loss = 0.17820678\n",
            "Iteration 6, loss = 0.17606155\n",
            "Iteration 7, loss = 0.17541970\n",
            "Iteration 8, loss = 0.17333905\n",
            "Iteration 9, loss = 0.17157946\n",
            "Iteration 10, loss = 0.17018362\n",
            "Iteration 11, loss = 0.16845466\n",
            "Iteration 12, loss = 0.16815098\n",
            "Iteration 13, loss = 0.16901068\n",
            "Iteration 14, loss = 0.16585971\n",
            "Iteration 15, loss = 0.16513708\n",
            "Iteration 16, loss = 0.16391629\n",
            "Iteration 17, loss = 0.16314121\n",
            "Iteration 18, loss = 0.16247155\n",
            "Iteration 19, loss = 0.16109492\n",
            "Iteration 20, loss = 0.16040747\n",
            "Iteration 21, loss = 0.16036879\n",
            "Iteration 22, loss = 0.15924591\n",
            "Iteration 23, loss = 0.15796726\n",
            "Iteration 24, loss = 0.15668572\n",
            "Iteration 25, loss = 0.15626892\n",
            "Iteration 26, loss = 0.15544064\n",
            "Iteration 27, loss = 0.15532655\n",
            "Iteration 28, loss = 0.15433555\n",
            "Iteration 29, loss = 0.15325261\n",
            "Iteration 30, loss = 0.15219340\n",
            "Iteration 31, loss = 0.15330966\n",
            "Iteration 32, loss = 0.15275872\n",
            "Iteration 33, loss = 0.15238582\n",
            "Iteration 34, loss = 0.15142279\n",
            "Iteration 35, loss = 0.15182575\n",
            "Iteration 36, loss = 0.15112545\n",
            "Iteration 37, loss = 0.15028744\n",
            "Iteration 38, loss = 0.14953934\n",
            "Iteration 39, loss = 0.14980031\n",
            "Iteration 40, loss = 0.15089198\n",
            "Iteration 41, loss = 0.14964233\n",
            "Iteration 42, loss = 0.14971835\n",
            "Iteration 43, loss = 0.14944642\n",
            "Iteration 44, loss = 0.14958941\n",
            "Iteration 45, loss = 0.14907273\n",
            "Iteration 46, loss = 0.14853977\n",
            "Iteration 47, loss = 0.14845189\n",
            "Iteration 48, loss = 0.14756441\n",
            "Iteration 49, loss = 0.14825701\n",
            "Iteration 50, loss = 0.14938979\n",
            "Iteration 51, loss = 0.14837675\n",
            "Iteration 52, loss = 0.14789368\n",
            "Iteration 53, loss = 0.14759805\n",
            "Iteration 54, loss = 0.14842041\n",
            "Iteration 55, loss = 0.14773641\n",
            "Iteration 56, loss = 0.14613499\n",
            "Iteration 57, loss = 0.14816174\n",
            "Iteration 58, loss = 0.14680686\n",
            "Iteration 59, loss = 0.14733038\n",
            "Iteration 60, loss = 0.14727386\n",
            "Iteration 61, loss = 0.14715885\n",
            "Iteration 62, loss = 0.14662904\n",
            "Iteration 63, loss = 0.14797004\n",
            "Iteration 64, loss = 0.14715301\n",
            "Iteration 65, loss = 0.14735458\n",
            "Iteration 66, loss = 0.14691986\n",
            "Iteration 67, loss = 0.14613845\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.21479910\n",
            "Iteration 2, loss = 0.18853165\n",
            "Iteration 3, loss = 0.18491098\n",
            "Iteration 4, loss = 0.18154454\n",
            "Iteration 5, loss = 0.17969013\n",
            "Iteration 6, loss = 0.17790275\n",
            "Iteration 7, loss = 0.17604608\n",
            "Iteration 8, loss = 0.17454047\n",
            "Iteration 9, loss = 0.17115741\n",
            "Iteration 10, loss = 0.16995783\n",
            "Iteration 11, loss = 0.16951417\n",
            "Iteration 12, loss = 0.16745732\n",
            "Iteration 13, loss = 0.16563407\n",
            "Iteration 14, loss = 0.16501395\n",
            "Iteration 15, loss = 0.16446026\n",
            "Iteration 16, loss = 0.16247661\n",
            "Iteration 17, loss = 0.16051134\n",
            "Iteration 18, loss = 0.15955559\n",
            "Iteration 19, loss = 0.15891048\n",
            "Iteration 20, loss = 0.15782147\n",
            "Iteration 21, loss = 0.15620031\n",
            "Iteration 22, loss = 0.15524642\n",
            "Iteration 23, loss = 0.15586998\n",
            "Iteration 24, loss = 0.15437977\n",
            "Iteration 25, loss = 0.15412718\n",
            "Iteration 26, loss = 0.15238499\n",
            "Iteration 27, loss = 0.15391789\n",
            "Iteration 28, loss = 0.15292843\n",
            "Iteration 29, loss = 0.15040387\n",
            "Iteration 30, loss = 0.15002247\n",
            "Iteration 31, loss = 0.14891856\n",
            "Iteration 32, loss = 0.14869380\n",
            "Iteration 33, loss = 0.15027432\n",
            "Iteration 34, loss = 0.14851594\n",
            "Iteration 35, loss = 0.14818979\n",
            "Iteration 36, loss = 0.14710347\n",
            "Iteration 37, loss = 0.14726177\n",
            "Iteration 38, loss = 0.14703707\n",
            "Iteration 39, loss = 0.14766524\n",
            "Iteration 40, loss = 0.14676998\n",
            "Iteration 41, loss = 0.14690751\n",
            "Iteration 42, loss = 0.14639768\n",
            "Iteration 43, loss = 0.14623967\n",
            "Iteration 44, loss = 0.14660533\n",
            "Iteration 45, loss = 0.14517045\n",
            "Iteration 46, loss = 0.14582955\n",
            "Iteration 47, loss = 0.14516004\n",
            "Iteration 48, loss = 0.14472836\n",
            "Iteration 49, loss = 0.14469440\n",
            "Iteration 50, loss = 0.14417587\n",
            "Iteration 51, loss = 0.14507347\n",
            "Iteration 52, loss = 0.14327410\n",
            "Iteration 53, loss = 0.14586326\n",
            "Iteration 54, loss = 0.14396413\n",
            "Iteration 55, loss = 0.14293884\n",
            "Iteration 56, loss = 0.14325273\n",
            "Iteration 57, loss = 0.14312035\n",
            "Iteration 58, loss = 0.14388622\n",
            "Iteration 59, loss = 0.14402298\n",
            "Iteration 60, loss = 0.14339893\n",
            "Iteration 61, loss = 0.14377052\n",
            "Iteration 62, loss = 0.14300439\n",
            "Iteration 63, loss = 0.14219663\n",
            "Iteration 64, loss = 0.14419300\n",
            "Iteration 65, loss = 0.14309188\n",
            "Iteration 66, loss = 0.14239388\n",
            "Iteration 67, loss = 0.14193901\n",
            "Iteration 68, loss = 0.14202561\n",
            "Iteration 69, loss = 0.14313883\n",
            "Iteration 70, loss = 0.14295373\n",
            "Iteration 71, loss = 0.14391475\n",
            "Iteration 72, loss = 0.14214785\n",
            "Iteration 73, loss = 0.14303829\n",
            "Iteration 74, loss = 0.14453629\n",
            "Iteration 75, loss = 0.14304903\n",
            "Iteration 76, loss = 0.14263025\n",
            "Iteration 77, loss = 0.14158936\n",
            "Iteration 78, loss = 0.14314497\n",
            "Iteration 79, loss = 0.14340105\n",
            "Iteration 80, loss = 0.14169313\n",
            "Iteration 81, loss = 0.14271919\n",
            "Iteration 82, loss = 0.14164061\n",
            "Iteration 83, loss = 0.14106043\n",
            "Iteration 84, loss = 0.14283792\n",
            "Iteration 85, loss = 0.14363182\n",
            "Iteration 86, loss = 0.14158597\n",
            "Iteration 87, loss = 0.14103368\n",
            "Iteration 88, loss = 0.14185895\n",
            "Iteration 89, loss = 0.14163611\n",
            "Iteration 90, loss = 0.14087732\n",
            "Iteration 91, loss = 0.14145024\n",
            "Iteration 92, loss = 0.14183986\n",
            "Iteration 93, loss = 0.14251853\n",
            "Iteration 94, loss = 0.14076787\n",
            "Iteration 95, loss = 0.14136710\n",
            "Iteration 96, loss = 0.14117584\n",
            "Iteration 97, loss = 0.13990005\n",
            "Iteration 98, loss = 0.14015518\n",
            "Iteration 99, loss = 0.14186246\n",
            "Iteration 100, loss = 0.13948274\n",
            "Iteration 101, loss = 0.14182151\n",
            "Iteration 102, loss = 0.14057295\n",
            "Iteration 103, loss = 0.14011727\n",
            "Iteration 104, loss = 0.14101654\n",
            "Iteration 105, loss = 0.14126019\n",
            "Iteration 106, loss = 0.13961560\n",
            "Iteration 107, loss = 0.13945643\n",
            "Iteration 108, loss = 0.14076136\n",
            "Iteration 109, loss = 0.14104441\n",
            "Iteration 110, loss = 0.14062154\n",
            "Iteration 111, loss = 0.13996996\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.20745966\n",
            "Iteration 2, loss = 0.18737179\n",
            "Iteration 3, loss = 0.18307950\n",
            "Iteration 4, loss = 0.17917612\n",
            "Iteration 5, loss = 0.17757907\n",
            "Iteration 6, loss = 0.17515405\n",
            "Iteration 7, loss = 0.17335507\n",
            "Iteration 8, loss = 0.17149107\n",
            "Iteration 9, loss = 0.17027044\n",
            "Iteration 10, loss = 0.16983489\n",
            "Iteration 11, loss = 0.16758600\n",
            "Iteration 12, loss = 0.16662360\n",
            "Iteration 13, loss = 0.16463432\n",
            "Iteration 14, loss = 0.16378826\n",
            "Iteration 15, loss = 0.16229067\n",
            "Iteration 16, loss = 0.16188809\n",
            "Iteration 17, loss = 0.16066907\n",
            "Iteration 18, loss = 0.15825218\n",
            "Iteration 19, loss = 0.15750249\n",
            "Iteration 20, loss = 0.15782239\n",
            "Iteration 21, loss = 0.15674587\n",
            "Iteration 22, loss = 0.15480327\n",
            "Iteration 23, loss = 0.15419821\n",
            "Iteration 24, loss = 0.15345661\n",
            "Iteration 25, loss = 0.15370940\n",
            "Iteration 26, loss = 0.15242247\n",
            "Iteration 27, loss = 0.15207187\n",
            "Iteration 28, loss = 0.15155303\n",
            "Iteration 29, loss = 0.15038666\n",
            "Iteration 30, loss = 0.14975137\n",
            "Iteration 31, loss = 0.14830005\n",
            "Iteration 32, loss = 0.15009530\n",
            "Iteration 33, loss = 0.14701159\n",
            "Iteration 34, loss = 0.14818637\n",
            "Iteration 35, loss = 0.14829708\n",
            "Iteration 36, loss = 0.14787297\n",
            "Iteration 37, loss = 0.14502427\n",
            "Iteration 38, loss = 0.14690520\n",
            "Iteration 39, loss = 0.14540941\n",
            "Iteration 40, loss = 0.14579035\n",
            "Iteration 41, loss = 0.14378312\n",
            "Iteration 42, loss = 0.14308806\n",
            "Iteration 43, loss = 0.14472805\n",
            "Iteration 44, loss = 0.14338323\n",
            "Iteration 45, loss = 0.14236630\n",
            "Iteration 46, loss = 0.14132280\n",
            "Iteration 47, loss = 0.14394374\n",
            "Iteration 48, loss = 0.14208255\n",
            "Iteration 49, loss = 0.14076766\n",
            "Iteration 50, loss = 0.14089599\n",
            "Iteration 51, loss = 0.13965874\n",
            "Iteration 52, loss = 0.14081272\n",
            "Iteration 53, loss = 0.14070068\n",
            "Iteration 54, loss = 0.14123404\n",
            "Iteration 55, loss = 0.13836167\n",
            "Iteration 56, loss = 0.13872185\n",
            "Iteration 57, loss = 0.13855872\n",
            "Iteration 58, loss = 0.14036978\n",
            "Iteration 59, loss = 0.13905921\n",
            "Iteration 60, loss = 0.13908181\n",
            "Iteration 61, loss = 0.13863245\n",
            "Iteration 62, loss = 0.13749326\n",
            "Iteration 63, loss = 0.13888289\n",
            "Iteration 64, loss = 0.13809100\n",
            "Iteration 65, loss = 0.13826702\n",
            "Iteration 66, loss = 0.13804186\n",
            "Iteration 67, loss = 0.13802471\n",
            "Iteration 68, loss = 0.13725479\n",
            "Iteration 69, loss = 0.13475857\n",
            "Iteration 70, loss = 0.13668139\n",
            "Iteration 71, loss = 0.13714076\n",
            "Iteration 72, loss = 0.13570872\n",
            "Iteration 73, loss = 0.13631702\n",
            "Iteration 74, loss = 0.13607636\n",
            "Iteration 75, loss = 0.13508327\n",
            "Iteration 76, loss = 0.13480498\n",
            "Iteration 77, loss = 0.13347094\n",
            "Iteration 78, loss = 0.13621817\n",
            "Iteration 79, loss = 0.13411697\n",
            "Iteration 80, loss = 0.13544290\n",
            "Iteration 81, loss = 0.13497786\n",
            "Iteration 82, loss = 0.13400704\n",
            "Iteration 83, loss = 0.13476141\n",
            "Iteration 84, loss = 0.13427906\n",
            "Iteration 85, loss = 0.13403671\n",
            "Iteration 86, loss = 0.13309550\n",
            "Iteration 87, loss = 0.13484220\n",
            "Iteration 88, loss = 0.13374037\n",
            "Iteration 89, loss = 0.13579275\n",
            "Iteration 90, loss = 0.13458942\n",
            "Iteration 91, loss = 0.13313660\n",
            "Iteration 92, loss = 0.13408090\n",
            "Iteration 93, loss = 0.13378855\n",
            "Iteration 94, loss = 0.13386579\n",
            "Iteration 95, loss = 0.13290148\n",
            "Iteration 96, loss = 0.13195025\n",
            "Iteration 97, loss = 0.13198710\n",
            "Iteration 98, loss = 0.13449635\n",
            "Iteration 99, loss = 0.13279231\n",
            "Iteration 100, loss = 0.13283888\n",
            "Iteration 101, loss = 0.13313302\n",
            "Iteration 102, loss = 0.13404498\n",
            "Iteration 103, loss = 0.13471468\n",
            "Iteration 104, loss = 0.13159499\n",
            "Iteration 105, loss = 0.13257920\n",
            "Iteration 106, loss = 0.13405656\n",
            "Iteration 107, loss = 0.13318840\n",
            "Iteration 108, loss = 0.13117139\n",
            "Iteration 109, loss = 0.13409569\n",
            "Iteration 110, loss = 0.13281317\n",
            "Iteration 111, loss = 0.13210116\n",
            "Iteration 112, loss = 0.13075918\n",
            "Iteration 113, loss = 0.13253312\n",
            "Iteration 114, loss = 0.13347383\n",
            "Iteration 115, loss = 0.13233765\n",
            "Iteration 116, loss = 0.13142065\n",
            "Iteration 117, loss = 0.13232064\n",
            "Iteration 118, loss = 0.13170839\n",
            "Iteration 119, loss = 0.13216873\n",
            "Iteration 120, loss = 0.13232501\n",
            "Iteration 121, loss = 0.13278939\n",
            "Iteration 122, loss = 0.13157584\n",
            "Iteration 123, loss = 0.13377808\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.20972334\n",
            "Iteration 2, loss = 0.18957790\n",
            "Iteration 3, loss = 0.18395164\n",
            "Iteration 4, loss = 0.18216104\n",
            "Iteration 5, loss = 0.17814562\n",
            "Iteration 6, loss = 0.17635506\n",
            "Iteration 7, loss = 0.17562446\n",
            "Iteration 8, loss = 0.17343486\n",
            "Iteration 9, loss = 0.17192955\n",
            "Iteration 10, loss = 0.17165456\n",
            "Iteration 11, loss = 0.16793273\n",
            "Iteration 12, loss = 0.16721831\n",
            "Iteration 13, loss = 0.16396086\n",
            "Iteration 14, loss = 0.16393776\n",
            "Iteration 15, loss = 0.16147078\n",
            "Iteration 16, loss = 0.15937583\n",
            "Iteration 17, loss = 0.15869255\n",
            "Iteration 18, loss = 0.15694598\n",
            "Iteration 19, loss = 0.15671861\n",
            "Iteration 20, loss = 0.15376218\n",
            "Iteration 21, loss = 0.15280550\n",
            "Iteration 22, loss = 0.15225041\n",
            "Iteration 23, loss = 0.15252130\n",
            "Iteration 24, loss = 0.14933591\n",
            "Iteration 25, loss = 0.14994125\n",
            "Iteration 26, loss = 0.15019458\n",
            "Iteration 27, loss = 0.14945851\n",
            "Iteration 28, loss = 0.14729929\n",
            "Iteration 29, loss = 0.14604200\n",
            "Iteration 30, loss = 0.14677726\n",
            "Iteration 31, loss = 0.14518553\n",
            "Iteration 32, loss = 0.14528168\n",
            "Iteration 33, loss = 0.14563695\n",
            "Iteration 34, loss = 0.14375151\n",
            "Iteration 35, loss = 0.14354756\n",
            "Iteration 36, loss = 0.14369127\n",
            "Iteration 37, loss = 0.14235477\n",
            "Iteration 38, loss = 0.14180465\n",
            "Iteration 39, loss = 0.14139568\n",
            "Iteration 40, loss = 0.14266023\n",
            "Iteration 41, loss = 0.14047075\n",
            "Iteration 42, loss = 0.14044846\n",
            "Iteration 43, loss = 0.14026215\n",
            "Iteration 44, loss = 0.13895114\n",
            "Iteration 45, loss = 0.13926063\n",
            "Iteration 46, loss = 0.13783817\n",
            "Iteration 47, loss = 0.13850533\n",
            "Iteration 48, loss = 0.13756499\n",
            "Iteration 49, loss = 0.13766991\n",
            "Iteration 50, loss = 0.13812970\n",
            "Iteration 51, loss = 0.13760693\n",
            "Iteration 52, loss = 0.13754429\n",
            "Iteration 53, loss = 0.13742005\n",
            "Iteration 54, loss = 0.13599224\n",
            "Iteration 55, loss = 0.13521229\n",
            "Iteration 56, loss = 0.13746576\n",
            "Iteration 57, loss = 0.13723482\n",
            "Iteration 58, loss = 0.13707424\n",
            "Iteration 59, loss = 0.13592951\n",
            "Iteration 60, loss = 0.13735906\n",
            "Iteration 61, loss = 0.13478124\n",
            "Iteration 62, loss = 0.13455306\n",
            "Iteration 63, loss = 0.13521566\n",
            "Iteration 64, loss = 0.13593144\n",
            "Iteration 65, loss = 0.13442752\n",
            "Iteration 66, loss = 0.13457747\n",
            "Iteration 67, loss = 0.13501273\n",
            "Iteration 68, loss = 0.13497529\n",
            "Iteration 69, loss = 0.13459141\n",
            "Iteration 70, loss = 0.13440623\n",
            "Iteration 71, loss = 0.13362385\n",
            "Iteration 72, loss = 0.13362202\n",
            "Iteration 73, loss = 0.13329368\n",
            "Iteration 74, loss = 0.13380033\n",
            "Iteration 75, loss = 0.13282799\n",
            "Iteration 76, loss = 0.13208970\n",
            "Iteration 77, loss = 0.13198658\n",
            "Iteration 78, loss = 0.13336998\n",
            "Iteration 79, loss = 0.13384153\n",
            "Iteration 80, loss = 0.13165200\n",
            "Iteration 81, loss = 0.13243663\n",
            "Iteration 82, loss = 0.13189226\n",
            "Iteration 83, loss = 0.13156950\n",
            "Iteration 84, loss = 0.13155144\n",
            "Iteration 85, loss = 0.13101464\n",
            "Iteration 86, loss = 0.13041208\n",
            "Iteration 87, loss = 0.13204111\n",
            "Iteration 88, loss = 0.13270287\n",
            "Iteration 89, loss = 0.13084996\n",
            "Iteration 90, loss = 0.13045762\n",
            "Iteration 91, loss = 0.13144763\n",
            "Iteration 92, loss = 0.13034301\n",
            "Iteration 93, loss = 0.12967087\n",
            "Iteration 94, loss = 0.13023531\n",
            "Iteration 95, loss = 0.13041029\n",
            "Iteration 96, loss = 0.13100915\n",
            "Iteration 97, loss = 0.13244189\n",
            "Iteration 98, loss = 0.13151388\n",
            "Iteration 99, loss = 0.12960395\n",
            "Iteration 100, loss = 0.12782703\n",
            "Iteration 101, loss = 0.13095835\n",
            "Iteration 102, loss = 0.13027340\n",
            "Iteration 103, loss = 0.12957489\n",
            "Iteration 104, loss = 0.12951067\n",
            "Iteration 105, loss = 0.13228723\n",
            "Iteration 106, loss = 0.13047390\n",
            "Iteration 107, loss = 0.12899353\n",
            "Iteration 108, loss = 0.12830364\n",
            "Iteration 109, loss = 0.12990705\n",
            "Iteration 110, loss = 0.12988674\n",
            "Iteration 111, loss = 0.12975096\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.21231914\n",
            "Iteration 2, loss = 0.19009613\n",
            "Iteration 3, loss = 0.18567695\n",
            "Iteration 4, loss = 0.18081175\n",
            "Iteration 5, loss = 0.17821697\n",
            "Iteration 6, loss = 0.17606204\n",
            "Iteration 7, loss = 0.17598350\n",
            "Iteration 8, loss = 0.17215721\n",
            "Iteration 9, loss = 0.17121684\n",
            "Iteration 10, loss = 0.16922939\n",
            "Iteration 11, loss = 0.16834138\n",
            "Iteration 12, loss = 0.16615367\n",
            "Iteration 13, loss = 0.16316413\n",
            "Iteration 14, loss = 0.16236907\n",
            "Iteration 15, loss = 0.16083408\n",
            "Iteration 16, loss = 0.15936675\n",
            "Iteration 17, loss = 0.15632635\n",
            "Iteration 18, loss = 0.15682259\n",
            "Iteration 19, loss = 0.15378125\n",
            "Iteration 20, loss = 0.15464037\n",
            "Iteration 21, loss = 0.15164409\n",
            "Iteration 22, loss = 0.15191005\n",
            "Iteration 23, loss = 0.15010682\n",
            "Iteration 24, loss = 0.14962116\n",
            "Iteration 25, loss = 0.14976364\n",
            "Iteration 26, loss = 0.14876418\n",
            "Iteration 27, loss = 0.14627038\n",
            "Iteration 28, loss = 0.14823972\n",
            "Iteration 29, loss = 0.14532945\n",
            "Iteration 30, loss = 0.14626641\n",
            "Iteration 31, loss = 0.14451782\n",
            "Iteration 32, loss = 0.14356308\n",
            "Iteration 33, loss = 0.14148780\n",
            "Iteration 34, loss = 0.14224052\n",
            "Iteration 35, loss = 0.14054420\n",
            "Iteration 36, loss = 0.14249857\n",
            "Iteration 37, loss = 0.14035473\n",
            "Iteration 38, loss = 0.14080330\n",
            "Iteration 39, loss = 0.14074747\n",
            "Iteration 40, loss = 0.14089298\n",
            "Iteration 41, loss = 0.13774134\n",
            "Iteration 42, loss = 0.13721055\n",
            "Iteration 43, loss = 0.13903475\n",
            "Iteration 44, loss = 0.13678410\n",
            "Iteration 45, loss = 0.13811840\n",
            "Iteration 46, loss = 0.13613657\n",
            "Iteration 47, loss = 0.13631171\n",
            "Iteration 48, loss = 0.13491704\n",
            "Iteration 49, loss = 0.13682176\n",
            "Iteration 50, loss = 0.13652849\n",
            "Iteration 51, loss = 0.13224444\n",
            "Iteration 52, loss = 0.13329637\n",
            "Iteration 53, loss = 0.13297382\n",
            "Iteration 54, loss = 0.13521642\n",
            "Iteration 55, loss = 0.13254379\n",
            "Iteration 56, loss = 0.13276001\n",
            "Iteration 57, loss = 0.13140378\n",
            "Iteration 58, loss = 0.13296260\n",
            "Iteration 59, loss = 0.13115566\n",
            "Iteration 60, loss = 0.13147424\n",
            "Iteration 61, loss = 0.13024964\n",
            "Iteration 62, loss = 0.13087789\n",
            "Iteration 63, loss = 0.13227524\n",
            "Iteration 64, loss = 0.12973721\n",
            "Iteration 65, loss = 0.12962039\n",
            "Iteration 66, loss = 0.12942897\n",
            "Iteration 67, loss = 0.12899975\n",
            "Iteration 68, loss = 0.12913122\n",
            "Iteration 69, loss = 0.12850994\n",
            "Iteration 70, loss = 0.13019592\n",
            "Iteration 71, loss = 0.12919927\n",
            "Iteration 72, loss = 0.12890593\n",
            "Iteration 73, loss = 0.12642712\n",
            "Iteration 74, loss = 0.12995677\n",
            "Iteration 75, loss = 0.12839116\n",
            "Iteration 76, loss = 0.12768915\n",
            "Iteration 77, loss = 0.12709220\n",
            "Iteration 78, loss = 0.12664130\n",
            "Iteration 79, loss = 0.12739865\n",
            "Iteration 80, loss = 0.12918469\n",
            "Iteration 81, loss = 0.12731022\n",
            "Iteration 82, loss = 0.12739596\n",
            "Iteration 83, loss = 0.12810724\n",
            "Iteration 84, loss = 0.12661959\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.21360790\n",
            "Iteration 2, loss = 0.18900672\n",
            "Iteration 3, loss = 0.18459769\n",
            "Iteration 4, loss = 0.18016285\n",
            "Iteration 5, loss = 0.17843299\n",
            "Iteration 6, loss = 0.17670264\n",
            "Iteration 7, loss = 0.17411932\n",
            "Iteration 8, loss = 0.17319068\n",
            "Iteration 9, loss = 0.17136255\n",
            "Iteration 10, loss = 0.16832554\n",
            "Iteration 11, loss = 0.16659370\n",
            "Iteration 12, loss = 0.16656947\n",
            "Iteration 13, loss = 0.16314178\n",
            "Iteration 14, loss = 0.16256537\n",
            "Iteration 15, loss = 0.16035617\n",
            "Iteration 16, loss = 0.16010704\n",
            "Iteration 17, loss = 0.15863121\n",
            "Iteration 18, loss = 0.15587436\n",
            "Iteration 19, loss = 0.15500124\n",
            "Iteration 20, loss = 0.15434307\n",
            "Iteration 21, loss = 0.15260226\n",
            "Iteration 22, loss = 0.15057879\n",
            "Iteration 23, loss = 0.15058304\n",
            "Iteration 24, loss = 0.14858828\n",
            "Iteration 25, loss = 0.14801322\n",
            "Iteration 26, loss = 0.14852450\n",
            "Iteration 27, loss = 0.14562400\n",
            "Iteration 28, loss = 0.14622825\n",
            "Iteration 29, loss = 0.14515669\n",
            "Iteration 30, loss = 0.14338054\n",
            "Iteration 31, loss = 0.14312510\n",
            "Iteration 32, loss = 0.14001304\n",
            "Iteration 33, loss = 0.14154089\n",
            "Iteration 34, loss = 0.14052821\n",
            "Iteration 35, loss = 0.13789110\n",
            "Iteration 36, loss = 0.14034878\n",
            "Iteration 37, loss = 0.14022199\n",
            "Iteration 38, loss = 0.13674942\n",
            "Iteration 39, loss = 0.13702079\n",
            "Iteration 40, loss = 0.13572223\n",
            "Iteration 41, loss = 0.13666012\n",
            "Iteration 42, loss = 0.13378624\n",
            "Iteration 43, loss = 0.13429281\n",
            "Iteration 44, loss = 0.13218287\n",
            "Iteration 45, loss = 0.13451631\n",
            "Iteration 46, loss = 0.13202012\n",
            "Iteration 47, loss = 0.13190366\n",
            "Iteration 48, loss = 0.13110523\n",
            "Iteration 49, loss = 0.12998412\n",
            "Iteration 50, loss = 0.13249698\n",
            "Iteration 51, loss = 0.12997001\n",
            "Iteration 52, loss = 0.13125806\n",
            "Iteration 53, loss = 0.12936260\n",
            "Iteration 54, loss = 0.12848707\n",
            "Iteration 55, loss = 0.12862880\n",
            "Iteration 56, loss = 0.12850500\n",
            "Iteration 57, loss = 0.12743673\n",
            "Iteration 58, loss = 0.12873483\n",
            "Iteration 59, loss = 0.12797141\n",
            "Iteration 60, loss = 0.12695016\n",
            "Iteration 61, loss = 0.12627584\n",
            "Iteration 62, loss = 0.12713012\n",
            "Iteration 63, loss = 0.12651512\n",
            "Iteration 64, loss = 0.12540743\n",
            "Iteration 65, loss = 0.12732822\n",
            "Iteration 66, loss = 0.12647491\n",
            "Iteration 67, loss = 0.12771545\n",
            "Iteration 68, loss = 0.12647380\n",
            "Iteration 69, loss = 0.12571219\n",
            "Iteration 70, loss = 0.12538916\n",
            "Iteration 71, loss = 0.12495897\n",
            "Iteration 72, loss = 0.12735243\n",
            "Iteration 73, loss = 0.12584546\n",
            "Iteration 74, loss = 0.12464674\n",
            "Iteration 75, loss = 0.12391892\n",
            "Iteration 76, loss = 0.12524555\n",
            "Iteration 77, loss = 0.12520363\n",
            "Iteration 78, loss = 0.12475702\n",
            "Iteration 79, loss = 0.12234754\n",
            "Iteration 80, loss = 0.12106118\n",
            "Iteration 81, loss = 0.12333022\n",
            "Iteration 82, loss = 0.12397967\n",
            "Iteration 83, loss = 0.12360001\n",
            "Iteration 84, loss = 0.12363835\n",
            "Iteration 85, loss = 0.12481320\n",
            "Iteration 86, loss = 0.12224674\n",
            "Iteration 87, loss = 0.12108960\n",
            "Iteration 88, loss = 0.12433243\n",
            "Iteration 89, loss = 0.12218825\n",
            "Iteration 90, loss = 0.12305883\n",
            "Iteration 91, loss = 0.12416065\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.21187720\n",
            "Iteration 2, loss = 0.18827010\n",
            "Iteration 3, loss = 0.18355748\n",
            "Iteration 4, loss = 0.18073385\n",
            "Iteration 5, loss = 0.17747763\n",
            "Iteration 6, loss = 0.17709126\n",
            "Iteration 7, loss = 0.17528170\n",
            "Iteration 8, loss = 0.17139614\n",
            "Iteration 9, loss = 0.16936003\n",
            "Iteration 10, loss = 0.16842665\n",
            "Iteration 11, loss = 0.16848472\n",
            "Iteration 12, loss = 0.16584270\n",
            "Iteration 13, loss = 0.16315995\n",
            "Iteration 14, loss = 0.16246405\n",
            "Iteration 15, loss = 0.15968004\n",
            "Iteration 16, loss = 0.15918944\n",
            "Iteration 17, loss = 0.15689864\n",
            "Iteration 18, loss = 0.15662325\n",
            "Iteration 19, loss = 0.15604847\n",
            "Iteration 20, loss = 0.15439845\n",
            "Iteration 21, loss = 0.15220359\n",
            "Iteration 22, loss = 0.15144968\n",
            "Iteration 23, loss = 0.14867259\n",
            "Iteration 24, loss = 0.14817620\n",
            "Iteration 25, loss = 0.14770980\n",
            "Iteration 26, loss = 0.14755681\n",
            "Iteration 27, loss = 0.14628396\n",
            "Iteration 28, loss = 0.14478572\n",
            "Iteration 29, loss = 0.14416959\n",
            "Iteration 30, loss = 0.14259548\n",
            "Iteration 31, loss = 0.14238598\n",
            "Iteration 32, loss = 0.14174900\n",
            "Iteration 33, loss = 0.14116028\n",
            "Iteration 34, loss = 0.13823093\n",
            "Iteration 35, loss = 0.13790667\n",
            "Iteration 36, loss = 0.13766716\n",
            "Iteration 37, loss = 0.13739692\n",
            "Iteration 38, loss = 0.13820247\n",
            "Iteration 39, loss = 0.13473770\n",
            "Iteration 40, loss = 0.13625194\n",
            "Iteration 41, loss = 0.13445576\n",
            "Iteration 42, loss = 0.13287171\n",
            "Iteration 43, loss = 0.13205456\n",
            "Iteration 44, loss = 0.13354021\n",
            "Iteration 45, loss = 0.13316045\n",
            "Iteration 46, loss = 0.13133415\n",
            "Iteration 47, loss = 0.13075737\n",
            "Iteration 48, loss = 0.12999529\n",
            "Iteration 49, loss = 0.13015496\n",
            "Iteration 50, loss = 0.12989298\n",
            "Iteration 51, loss = 0.13095292\n",
            "Iteration 52, loss = 0.12808498\n",
            "Iteration 53, loss = 0.12860991\n",
            "Iteration 54, loss = 0.12694608\n",
            "Iteration 55, loss = 0.12705482\n",
            "Iteration 56, loss = 0.12729193\n",
            "Iteration 57, loss = 0.12582934\n",
            "Iteration 58, loss = 0.12624358\n",
            "Iteration 59, loss = 0.12693068\n",
            "Iteration 60, loss = 0.12560037\n",
            "Iteration 61, loss = 0.12470582\n",
            "Iteration 62, loss = 0.12676006\n",
            "Iteration 63, loss = 0.12476291\n",
            "Iteration 64, loss = 0.12833152\n",
            "Iteration 65, loss = 0.12437894\n",
            "Iteration 66, loss = 0.12641587\n",
            "Iteration 67, loss = 0.12302510\n",
            "Iteration 68, loss = 0.12287668\n",
            "Iteration 69, loss = 0.12289280\n",
            "Iteration 70, loss = 0.12503463\n",
            "Iteration 71, loss = 0.12458095\n",
            "Iteration 72, loss = 0.12234080\n",
            "Iteration 73, loss = 0.12358363\n",
            "Iteration 74, loss = 0.12417141\n",
            "Iteration 75, loss = 0.12353859\n",
            "Iteration 76, loss = 0.12193062\n",
            "Iteration 77, loss = 0.12294755\n",
            "Iteration 78, loss = 0.12105242\n",
            "Iteration 79, loss = 0.12386682\n",
            "Iteration 80, loss = 0.12333350\n",
            "Iteration 81, loss = 0.12230333\n",
            "Iteration 82, loss = 0.12049414\n",
            "Iteration 83, loss = 0.12200601\n",
            "Iteration 84, loss = 0.12209980\n",
            "Iteration 85, loss = 0.12010734\n",
            "Iteration 86, loss = 0.12017525\n",
            "Iteration 87, loss = 0.12331732\n",
            "Iteration 88, loss = 0.12084185\n",
            "Iteration 89, loss = 0.11941491\n",
            "Iteration 90, loss = 0.12029282\n",
            "Iteration 91, loss = 0.11871703\n",
            "Iteration 92, loss = 0.12263444\n",
            "Iteration 93, loss = 0.12331889\n",
            "Iteration 94, loss = 0.12051907\n",
            "Iteration 95, loss = 0.12115850\n",
            "Iteration 96, loss = 0.12059022\n",
            "Iteration 97, loss = 0.12157751\n",
            "Iteration 98, loss = 0.11988343\n",
            "Iteration 99, loss = 0.11817344\n",
            "Iteration 100, loss = 0.11794334\n",
            "Iteration 101, loss = 0.11976036\n",
            "Iteration 102, loss = 0.11822271\n",
            "Iteration 103, loss = 0.11909648\n",
            "Iteration 104, loss = 0.11845121\n",
            "Iteration 105, loss = 0.12017922\n",
            "Iteration 106, loss = 0.12252441\n",
            "Iteration 107, loss = 0.12086835\n",
            "Iteration 108, loss = 0.11849483\n",
            "Iteration 109, loss = 0.11839117\n",
            "Iteration 110, loss = 0.11754131\n",
            "Iteration 111, loss = 0.11885609\n",
            "Iteration 112, loss = 0.11836067\n",
            "Iteration 113, loss = 0.11853486\n",
            "Iteration 114, loss = 0.11764757\n",
            "Iteration 115, loss = 0.12000477\n",
            "Iteration 116, loss = 0.12008483\n",
            "Iteration 117, loss = 0.11916302\n",
            "Iteration 118, loss = 0.11918880\n",
            "Iteration 119, loss = 0.11811633\n",
            "Iteration 120, loss = 0.11724489\n",
            "Iteration 121, loss = 0.11821989\n",
            "Iteration 122, loss = 0.11816385\n",
            "Iteration 123, loss = 0.11930390\n",
            "Iteration 124, loss = 0.12014630\n",
            "Iteration 125, loss = 0.11852405\n",
            "Iteration 126, loss = 0.12128648\n",
            "Iteration 127, loss = 0.11756614\n",
            "Iteration 128, loss = 0.11897790\n",
            "Iteration 129, loss = 0.11717901\n",
            "Iteration 130, loss = 0.11589958\n",
            "Iteration 131, loss = 0.11650518\n",
            "Iteration 132, loss = 0.11674960\n",
            "Iteration 133, loss = 0.11610338\n",
            "Iteration 134, loss = 0.11592816\n",
            "Iteration 135, loss = 0.11893221\n",
            "Iteration 136, loss = 0.11912317\n",
            "Iteration 137, loss = 0.11622851\n",
            "Iteration 138, loss = 0.11758210\n",
            "Iteration 139, loss = 0.11624692\n",
            "Iteration 140, loss = 0.11609728\n",
            "Iteration 141, loss = 0.11754344\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.21925110\n",
            "Iteration 2, loss = 0.18790809\n",
            "Iteration 3, loss = 0.18389710\n",
            "Iteration 4, loss = 0.18154953\n",
            "Iteration 5, loss = 0.17776915\n",
            "Iteration 6, loss = 0.17618270\n",
            "Iteration 7, loss = 0.17467427\n",
            "Iteration 8, loss = 0.17263591\n",
            "Iteration 9, loss = 0.17024183\n",
            "Iteration 10, loss = 0.16963483\n",
            "Iteration 11, loss = 0.16724824\n",
            "Iteration 12, loss = 0.16519270\n",
            "Iteration 13, loss = 0.16424081\n",
            "Iteration 14, loss = 0.16131842\n",
            "Iteration 15, loss = 0.16117454\n",
            "Iteration 16, loss = 0.15914031\n",
            "Iteration 17, loss = 0.15681408\n",
            "Iteration 18, loss = 0.15472841\n",
            "Iteration 19, loss = 0.15227909\n",
            "Iteration 20, loss = 0.14976664\n",
            "Iteration 21, loss = 0.14943443\n",
            "Iteration 22, loss = 0.14808600\n",
            "Iteration 23, loss = 0.14731869\n",
            "Iteration 24, loss = 0.14461511\n",
            "Iteration 25, loss = 0.14554776\n",
            "Iteration 26, loss = 0.14388013\n",
            "Iteration 27, loss = 0.14249450\n",
            "Iteration 28, loss = 0.14126357\n",
            "Iteration 29, loss = 0.14057032\n",
            "Iteration 30, loss = 0.13858859\n",
            "Iteration 31, loss = 0.13878973\n",
            "Iteration 32, loss = 0.13621519\n",
            "Iteration 33, loss = 0.13734584\n",
            "Iteration 34, loss = 0.13620176\n",
            "Iteration 35, loss = 0.13440910\n",
            "Iteration 36, loss = 0.13423105\n",
            "Iteration 37, loss = 0.13442748\n",
            "Iteration 38, loss = 0.13304879\n",
            "Iteration 39, loss = 0.13192028\n",
            "Iteration 40, loss = 0.13156817\n",
            "Iteration 41, loss = 0.12995636\n",
            "Iteration 42, loss = 0.12910078\n",
            "Iteration 43, loss = 0.12971308\n",
            "Iteration 44, loss = 0.12955251\n",
            "Iteration 45, loss = 0.12940581\n",
            "Iteration 46, loss = 0.12806275\n",
            "Iteration 47, loss = 0.12720467\n",
            "Iteration 48, loss = 0.12726677\n",
            "Iteration 49, loss = 0.12584648\n",
            "Iteration 50, loss = 0.12638440\n",
            "Iteration 51, loss = 0.12373216\n",
            "Iteration 52, loss = 0.12826374\n",
            "Iteration 53, loss = 0.12500708\n",
            "Iteration 54, loss = 0.12356231\n",
            "Iteration 55, loss = 0.12380540\n",
            "Iteration 56, loss = 0.12485868\n",
            "Iteration 57, loss = 0.12308115\n",
            "Iteration 58, loss = 0.12321438\n",
            "Iteration 59, loss = 0.12234170\n",
            "Iteration 60, loss = 0.12303766\n",
            "Iteration 61, loss = 0.12174047\n",
            "Iteration 62, loss = 0.12081219\n",
            "Iteration 63, loss = 0.12176352\n",
            "Iteration 64, loss = 0.12194095\n",
            "Iteration 65, loss = 0.12104165\n",
            "Iteration 66, loss = 0.11959481\n",
            "Iteration 67, loss = 0.12222184\n",
            "Iteration 68, loss = 0.11930887\n",
            "Iteration 69, loss = 0.11859020\n",
            "Iteration 70, loss = 0.12013385\n",
            "Iteration 71, loss = 0.11872637\n",
            "Iteration 72, loss = 0.11926424\n",
            "Iteration 73, loss = 0.12076861\n",
            "Iteration 74, loss = 0.12137854\n",
            "Iteration 75, loss = 0.11761522\n",
            "Iteration 76, loss = 0.11831577\n",
            "Iteration 77, loss = 0.11811462\n",
            "Iteration 78, loss = 0.12030728\n",
            "Iteration 79, loss = 0.11898328\n",
            "Iteration 80, loss = 0.11508653\n",
            "Iteration 81, loss = 0.11751478\n",
            "Iteration 82, loss = 0.11958149\n",
            "Iteration 83, loss = 0.11628955\n",
            "Iteration 84, loss = 0.11412100\n",
            "Iteration 85, loss = 0.11550670\n",
            "Iteration 86, loss = 0.11700733\n",
            "Iteration 87, loss = 0.11656917\n",
            "Iteration 88, loss = 0.11693789\n",
            "Iteration 89, loss = 0.11907121\n",
            "Iteration 90, loss = 0.11759737\n",
            "Iteration 91, loss = 0.11681127\n",
            "Iteration 92, loss = 0.11471702\n",
            "Iteration 93, loss = 0.11662531\n",
            "Iteration 94, loss = 0.11615768\n",
            "Iteration 95, loss = 0.11824286\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.21364399\n",
            "Iteration 2, loss = 0.18768491\n",
            "Iteration 3, loss = 0.18434471\n",
            "Iteration 4, loss = 0.18129129\n",
            "Iteration 5, loss = 0.17833162\n",
            "Iteration 6, loss = 0.17598937\n",
            "Iteration 7, loss = 0.17332051\n",
            "Iteration 8, loss = 0.17234533\n",
            "Iteration 9, loss = 0.17081164\n",
            "Iteration 10, loss = 0.17004271\n",
            "Iteration 11, loss = 0.16972681\n",
            "Iteration 12, loss = 0.16650757\n",
            "Iteration 13, loss = 0.16293840\n",
            "Iteration 14, loss = 0.16210045\n",
            "Iteration 15, loss = 0.16025637\n",
            "Iteration 16, loss = 0.15908150\n",
            "Iteration 17, loss = 0.15678629\n",
            "Iteration 18, loss = 0.15712560\n",
            "Iteration 19, loss = 0.15346737\n",
            "Iteration 20, loss = 0.15358114\n",
            "Iteration 21, loss = 0.15378485\n",
            "Iteration 22, loss = 0.15126003\n",
            "Iteration 23, loss = 0.14978302\n",
            "Iteration 24, loss = 0.14813996\n",
            "Iteration 25, loss = 0.14757363\n",
            "Iteration 26, loss = 0.14680173\n",
            "Iteration 27, loss = 0.14542158\n",
            "Iteration 28, loss = 0.14533005\n",
            "Iteration 29, loss = 0.14423728\n",
            "Iteration 30, loss = 0.14067447\n",
            "Iteration 31, loss = 0.14160246\n",
            "Iteration 32, loss = 0.14035281\n",
            "Iteration 33, loss = 0.14031208\n",
            "Iteration 34, loss = 0.13647561\n",
            "Iteration 35, loss = 0.13717209\n",
            "Iteration 36, loss = 0.13640987\n",
            "Iteration 37, loss = 0.13275017\n",
            "Iteration 38, loss = 0.13351168\n",
            "Iteration 39, loss = 0.13322142\n",
            "Iteration 40, loss = 0.13225307\n",
            "Iteration 41, loss = 0.13222465\n",
            "Iteration 42, loss = 0.13052644\n",
            "Iteration 43, loss = 0.13137165\n",
            "Iteration 44, loss = 0.12954834\n",
            "Iteration 45, loss = 0.12661806\n",
            "Iteration 46, loss = 0.12947670\n",
            "Iteration 47, loss = 0.12727474\n",
            "Iteration 48, loss = 0.12824684\n",
            "Iteration 49, loss = 0.12676538\n",
            "Iteration 50, loss = 0.12618646\n",
            "Iteration 51, loss = 0.12662306\n",
            "Iteration 52, loss = 0.12512137\n",
            "Iteration 53, loss = 0.12541694\n",
            "Iteration 54, loss = 0.12428853\n",
            "Iteration 55, loss = 0.12276184\n",
            "Iteration 56, loss = 0.12315987\n",
            "Iteration 57, loss = 0.12647377\n",
            "Iteration 58, loss = 0.12230781\n",
            "Iteration 59, loss = 0.12193685\n",
            "Iteration 60, loss = 0.12048497\n",
            "Iteration 61, loss = 0.12093455\n",
            "Iteration 62, loss = 0.12324023\n",
            "Iteration 63, loss = 0.12449324\n",
            "Iteration 64, loss = 0.12074821\n",
            "Iteration 65, loss = 0.11983042\n",
            "Iteration 66, loss = 0.12056792\n",
            "Iteration 67, loss = 0.12168188\n",
            "Iteration 68, loss = 0.11849851\n",
            "Iteration 69, loss = 0.11964585\n",
            "Iteration 70, loss = 0.11934017\n",
            "Iteration 71, loss = 0.11741259\n",
            "Iteration 72, loss = 0.11770310\n",
            "Iteration 73, loss = 0.11812155\n",
            "Iteration 74, loss = 0.11846543\n",
            "Iteration 75, loss = 0.11714188\n",
            "Iteration 76, loss = 0.11739401\n",
            "Iteration 77, loss = 0.11658177\n",
            "Iteration 78, loss = 0.11899761\n",
            "Iteration 79, loss = 0.11691099\n",
            "Iteration 80, loss = 0.11562278\n",
            "Iteration 81, loss = 0.11693863\n",
            "Iteration 82, loss = 0.11587068\n",
            "Iteration 83, loss = 0.11608044\n",
            "Iteration 84, loss = 0.11767571\n",
            "Iteration 85, loss = 0.11499669\n",
            "Iteration 86, loss = 0.11667783\n",
            "Iteration 87, loss = 0.11537571\n",
            "Iteration 88, loss = 0.11438777\n",
            "Iteration 89, loss = 0.11221971\n",
            "Iteration 90, loss = 0.11671060\n",
            "Iteration 91, loss = 0.11443328\n",
            "Iteration 92, loss = 0.11397057\n",
            "Iteration 93, loss = 0.11610476\n",
            "Iteration 94, loss = 0.11568897\n",
            "Iteration 95, loss = 0.11242210\n",
            "Iteration 96, loss = 0.11592822\n",
            "Iteration 97, loss = 0.11286997\n",
            "Iteration 98, loss = 0.11342427\n",
            "Iteration 99, loss = 0.11412081\n",
            "Iteration 100, loss = 0.11393420\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.20661648\n",
            "Iteration 2, loss = 0.18766755\n",
            "Iteration 3, loss = 0.18356622\n",
            "Iteration 4, loss = 0.18100746\n",
            "Iteration 5, loss = 0.17923800\n",
            "Iteration 6, loss = 0.17624442\n",
            "Iteration 7, loss = 0.17471086\n",
            "Iteration 8, loss = 0.17260765\n",
            "Iteration 9, loss = 0.17112051\n",
            "Iteration 10, loss = 0.16982426\n",
            "Iteration 11, loss = 0.16647364\n",
            "Iteration 12, loss = 0.16461858\n",
            "Iteration 13, loss = 0.16273661\n",
            "Iteration 14, loss = 0.16179754\n",
            "Iteration 15, loss = 0.15881930\n",
            "Iteration 16, loss = 0.15732208\n",
            "Iteration 17, loss = 0.15509013\n",
            "Iteration 18, loss = 0.15340607\n",
            "Iteration 19, loss = 0.15335248\n",
            "Iteration 20, loss = 0.15254644\n",
            "Iteration 21, loss = 0.15171250\n",
            "Iteration 22, loss = 0.14895770\n",
            "Iteration 23, loss = 0.14946525\n",
            "Iteration 24, loss = 0.14641009\n",
            "Iteration 25, loss = 0.14668304\n",
            "Iteration 26, loss = 0.14387212\n",
            "Iteration 27, loss = 0.14309655\n",
            "Iteration 28, loss = 0.13935338\n",
            "Iteration 29, loss = 0.13803622\n",
            "Iteration 30, loss = 0.13720808\n",
            "Iteration 31, loss = 0.13691643\n",
            "Iteration 32, loss = 0.13780662\n",
            "Iteration 33, loss = 0.13909595\n",
            "Iteration 34, loss = 0.13744257\n",
            "Iteration 35, loss = 0.13376860\n",
            "Iteration 36, loss = 0.13181704\n",
            "Iteration 37, loss = 0.13033771\n",
            "Iteration 38, loss = 0.13315013\n",
            "Iteration 39, loss = 0.13203926\n",
            "Iteration 40, loss = 0.13329853\n",
            "Iteration 41, loss = 0.12912292\n",
            "Iteration 42, loss = 0.12729490\n",
            "Iteration 43, loss = 0.13058167\n",
            "Iteration 44, loss = 0.12691683\n",
            "Iteration 45, loss = 0.12724737\n",
            "Iteration 46, loss = 0.12798828\n",
            "Iteration 47, loss = 0.12545125\n",
            "Iteration 48, loss = 0.12391521\n",
            "Iteration 49, loss = 0.12351489\n",
            "Iteration 50, loss = 0.12182906\n",
            "Iteration 51, loss = 0.12405239\n",
            "Iteration 52, loss = 0.12095626\n",
            "Iteration 53, loss = 0.12434264\n",
            "Iteration 54, loss = 0.12393823\n",
            "Iteration 55, loss = 0.12197326\n",
            "Iteration 56, loss = 0.12273056\n",
            "Iteration 57, loss = 0.12036919\n",
            "Iteration 58, loss = 0.12099652\n",
            "Iteration 59, loss = 0.11990965\n",
            "Iteration 60, loss = 0.12115056\n",
            "Iteration 61, loss = 0.12018539\n",
            "Iteration 62, loss = 0.12135648\n",
            "Iteration 63, loss = 0.11991493\n",
            "Iteration 64, loss = 0.11759626\n",
            "Iteration 65, loss = 0.11792449\n",
            "Iteration 66, loss = 0.11795340\n",
            "Iteration 67, loss = 0.11904499\n",
            "Iteration 68, loss = 0.11826671\n",
            "Iteration 69, loss = 0.11665977\n",
            "Iteration 70, loss = 0.11686735\n",
            "Iteration 71, loss = 0.11827965\n",
            "Iteration 72, loss = 0.11868379\n",
            "Iteration 73, loss = 0.11696163\n",
            "Iteration 74, loss = 0.11522443\n",
            "Iteration 75, loss = 0.11899136\n",
            "Iteration 76, loss = 0.11805080\n",
            "Iteration 77, loss = 0.11688601\n",
            "Iteration 78, loss = 0.11431405\n",
            "Iteration 79, loss = 0.11496372\n",
            "Iteration 80, loss = 0.11655196\n",
            "Iteration 81, loss = 0.11485528\n",
            "Iteration 82, loss = 0.11732426\n",
            "Iteration 83, loss = 0.11295585\n",
            "Iteration 84, loss = 0.11546667\n",
            "Iteration 85, loss = 0.11564652\n",
            "Iteration 86, loss = 0.11791395\n",
            "Iteration 87, loss = 0.11537567\n",
            "Iteration 88, loss = 0.11625564\n",
            "Iteration 89, loss = 0.11407216\n",
            "Iteration 90, loss = 0.11423496\n",
            "Iteration 91, loss = 0.11456245\n",
            "Iteration 92, loss = 0.11344719\n",
            "Iteration 93, loss = 0.11524676\n",
            "Iteration 94, loss = 0.11270646\n",
            "Iteration 95, loss = 0.11493668\n",
            "Iteration 96, loss = 0.11634995\n",
            "Iteration 97, loss = 0.11237158\n",
            "Iteration 98, loss = 0.11310705\n",
            "Iteration 99, loss = 0.11705620\n",
            "Iteration 100, loss = 0.11312992\n",
            "Iteration 101, loss = 0.11314762\n",
            "Iteration 102, loss = 0.11603398\n",
            "Iteration 103, loss = 0.11363123\n",
            "Iteration 104, loss = 0.11335452\n",
            "Iteration 105, loss = 0.11356588\n",
            "Iteration 106, loss = 0.11392726\n",
            "Iteration 107, loss = 0.10896400\n",
            "Iteration 108, loss = 0.11414893\n",
            "Iteration 109, loss = 0.11199632\n",
            "Iteration 110, loss = 0.11218502\n",
            "Iteration 111, loss = 0.11213967\n",
            "Iteration 112, loss = 0.11472410\n",
            "Iteration 113, loss = 0.11554211\n",
            "Iteration 114, loss = 0.11184190\n",
            "Iteration 115, loss = 0.11237611\n",
            "Iteration 116, loss = 0.11130028\n",
            "Iteration 117, loss = 0.11203974\n",
            "Iteration 118, loss = 0.11337303\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.21128926\n",
            "Iteration 2, loss = 0.18829880\n",
            "Iteration 3, loss = 0.18470625\n",
            "Iteration 4, loss = 0.18141753\n",
            "Iteration 5, loss = 0.17844347\n",
            "Iteration 6, loss = 0.17647172\n",
            "Iteration 7, loss = 0.17464999\n",
            "Iteration 8, loss = 0.17238119\n",
            "Iteration 9, loss = 0.17085381\n",
            "Iteration 10, loss = 0.17016638\n",
            "Iteration 11, loss = 0.16755744\n",
            "Iteration 12, loss = 0.16668871\n",
            "Iteration 13, loss = 0.16296824\n",
            "Iteration 14, loss = 0.16198667\n",
            "Iteration 15, loss = 0.15938594\n",
            "Iteration 16, loss = 0.15996695\n",
            "Iteration 17, loss = 0.15785969\n",
            "Iteration 18, loss = 0.15600703\n",
            "Iteration 19, loss = 0.15395067\n",
            "Iteration 20, loss = 0.15156104\n",
            "Iteration 21, loss = 0.15147343\n",
            "Iteration 22, loss = 0.14921598\n",
            "Iteration 23, loss = 0.14780792\n",
            "Iteration 24, loss = 0.14681727\n",
            "Iteration 25, loss = 0.14604889\n",
            "Iteration 26, loss = 0.14434606\n",
            "Iteration 27, loss = 0.14607548\n",
            "Iteration 28, loss = 0.14224584\n",
            "Iteration 29, loss = 0.14305655\n",
            "Iteration 30, loss = 0.13964626\n",
            "Iteration 31, loss = 0.14115794\n",
            "Iteration 32, loss = 0.14158069\n",
            "Iteration 33, loss = 0.13692351\n",
            "Iteration 34, loss = 0.13707392\n",
            "Iteration 35, loss = 0.13611103\n",
            "Iteration 36, loss = 0.13597933\n",
            "Iteration 37, loss = 0.13448914\n",
            "Iteration 38, loss = 0.13289160\n",
            "Iteration 39, loss = 0.13453550\n",
            "Iteration 40, loss = 0.13254010\n",
            "Iteration 41, loss = 0.13038403\n",
            "Iteration 42, loss = 0.13035814\n",
            "Iteration 43, loss = 0.12932132\n",
            "Iteration 44, loss = 0.12898076\n",
            "Iteration 45, loss = 0.12829327\n",
            "Iteration 46, loss = 0.12930778\n",
            "Iteration 47, loss = 0.12850447\n",
            "Iteration 48, loss = 0.12664087\n",
            "Iteration 49, loss = 0.12328051\n",
            "Iteration 50, loss = 0.12719313\n",
            "Iteration 51, loss = 0.13047001\n",
            "Iteration 52, loss = 0.12771546\n",
            "Iteration 53, loss = 0.12309350\n",
            "Iteration 54, loss = 0.12454580\n",
            "Iteration 55, loss = 0.12228096\n",
            "Iteration 56, loss = 0.12735234\n",
            "Iteration 57, loss = 0.12390801\n",
            "Iteration 58, loss = 0.12257145\n",
            "Iteration 59, loss = 0.12152676\n",
            "Iteration 60, loss = 0.12146627\n",
            "Iteration 61, loss = 0.12138222\n",
            "Iteration 62, loss = 0.11951537\n",
            "Iteration 63, loss = 0.12010671\n",
            "Iteration 64, loss = 0.12285282\n",
            "Iteration 65, loss = 0.12123920\n",
            "Iteration 66, loss = 0.11947758\n",
            "Iteration 67, loss = 0.12050903\n",
            "Iteration 68, loss = 0.11884404\n",
            "Iteration 69, loss = 0.11814201\n",
            "Iteration 70, loss = 0.11806553\n",
            "Iteration 71, loss = 0.11727013\n",
            "Iteration 72, loss = 0.11787650\n",
            "Iteration 73, loss = 0.12149994\n",
            "Iteration 74, loss = 0.11885755\n",
            "Iteration 75, loss = 0.11796582\n",
            "Iteration 76, loss = 0.11649705\n",
            "Iteration 77, loss = 0.11411388\n",
            "Iteration 78, loss = 0.11475524\n",
            "Iteration 79, loss = 0.11447185\n",
            "Iteration 80, loss = 0.11477495\n",
            "Iteration 81, loss = 0.11366672\n",
            "Iteration 82, loss = 0.11635647\n",
            "Iteration 83, loss = 0.11306525\n",
            "Iteration 84, loss = 0.11470384\n",
            "Iteration 85, loss = 0.11561963\n",
            "Iteration 86, loss = 0.11513205\n",
            "Iteration 87, loss = 0.11455118\n",
            "Iteration 88, loss = 0.11498579\n",
            "Iteration 89, loss = 0.11188149\n",
            "Iteration 90, loss = 0.11304257\n",
            "Iteration 91, loss = 0.11325147\n",
            "Iteration 92, loss = 0.11227306\n",
            "Iteration 93, loss = 0.11181310\n",
            "Iteration 94, loss = 0.11264050\n",
            "Iteration 95, loss = 0.11589999\n",
            "Iteration 96, loss = 0.11206051\n",
            "Iteration 97, loss = 0.10873058\n",
            "Iteration 98, loss = 0.10958515\n",
            "Iteration 99, loss = 0.11033532\n",
            "Iteration 100, loss = 0.11053833\n",
            "Iteration 101, loss = 0.10926370\n",
            "Iteration 102, loss = 0.11060628\n",
            "Iteration 103, loss = 0.11074017\n",
            "Iteration 104, loss = 0.11057471\n",
            "Iteration 105, loss = 0.10984854\n",
            "Iteration 106, loss = 0.11011131\n",
            "Iteration 107, loss = 0.10981866\n",
            "Iteration 108, loss = 0.11185661\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.21476926\n",
            "Iteration 2, loss = 0.18837275\n",
            "Iteration 3, loss = 0.18509795\n",
            "Iteration 4, loss = 0.18132296\n",
            "Iteration 5, loss = 0.17942829\n",
            "Iteration 6, loss = 0.17660427\n",
            "Iteration 7, loss = 0.17536297\n",
            "Iteration 8, loss = 0.17401407\n",
            "Iteration 9, loss = 0.17102702\n",
            "Iteration 10, loss = 0.16893256\n",
            "Iteration 11, loss = 0.16676429\n",
            "Iteration 12, loss = 0.16427673\n",
            "Iteration 13, loss = 0.16090553\n",
            "Iteration 14, loss = 0.16290951\n",
            "Iteration 15, loss = 0.15932623\n",
            "Iteration 16, loss = 0.15650665\n",
            "Iteration 17, loss = 0.15653185\n",
            "Iteration 18, loss = 0.15468094\n",
            "Iteration 19, loss = 0.15174116\n",
            "Iteration 20, loss = 0.15109166\n",
            "Iteration 21, loss = 0.14912276\n",
            "Iteration 22, loss = 0.14645230\n",
            "Iteration 23, loss = 0.14504477\n",
            "Iteration 24, loss = 0.14435152\n",
            "Iteration 25, loss = 0.14220153\n",
            "Iteration 26, loss = 0.14139739\n",
            "Iteration 27, loss = 0.14211645\n",
            "Iteration 28, loss = 0.14098830\n",
            "Iteration 29, loss = 0.13871617\n",
            "Iteration 30, loss = 0.13656338\n",
            "Iteration 31, loss = 0.13623391\n",
            "Iteration 32, loss = 0.13599630\n",
            "Iteration 33, loss = 0.13608029\n",
            "Iteration 34, loss = 0.13369406\n",
            "Iteration 35, loss = 0.13254801\n",
            "Iteration 36, loss = 0.12939615\n",
            "Iteration 37, loss = 0.12943220\n",
            "Iteration 38, loss = 0.13012804\n",
            "Iteration 39, loss = 0.12990668\n",
            "Iteration 40, loss = 0.12670764\n",
            "Iteration 41, loss = 0.12724643\n",
            "Iteration 42, loss = 0.12748371\n",
            "Iteration 43, loss = 0.12715083\n",
            "Iteration 44, loss = 0.12640517\n",
            "Iteration 45, loss = 0.12452115\n",
            "Iteration 46, loss = 0.12616328\n",
            "Iteration 47, loss = 0.12580599\n",
            "Iteration 48, loss = 0.12506290\n",
            "Iteration 49, loss = 0.12333618\n",
            "Iteration 50, loss = 0.12249857\n",
            "Iteration 51, loss = 0.12209104\n",
            "Iteration 52, loss = 0.12091392\n",
            "Iteration 53, loss = 0.12152185\n",
            "Iteration 54, loss = 0.12176620\n",
            "Iteration 55, loss = 0.11948709\n",
            "Iteration 56, loss = 0.11880403\n",
            "Iteration 57, loss = 0.12029937\n",
            "Iteration 58, loss = 0.12109796\n",
            "Iteration 59, loss = 0.12264409\n",
            "Iteration 60, loss = 0.11959218\n",
            "Iteration 61, loss = 0.11988010\n",
            "Iteration 62, loss = 0.11946774\n",
            "Iteration 63, loss = 0.11978753\n",
            "Iteration 64, loss = 0.11734290\n",
            "Iteration 65, loss = 0.11759681\n",
            "Iteration 66, loss = 0.11712185\n",
            "Iteration 67, loss = 0.11590520\n",
            "Iteration 68, loss = 0.12055339\n",
            "Iteration 69, loss = 0.11778775\n",
            "Iteration 70, loss = 0.11654380\n",
            "Iteration 71, loss = 0.11808605\n",
            "Iteration 72, loss = 0.11666445\n",
            "Iteration 73, loss = 0.11592077\n",
            "Iteration 74, loss = 0.11669024\n",
            "Iteration 75, loss = 0.11333137\n",
            "Iteration 76, loss = 0.11398217\n",
            "Iteration 77, loss = 0.11253813\n",
            "Iteration 78, loss = 0.11215734\n",
            "Iteration 79, loss = 0.11476619\n",
            "Iteration 80, loss = 0.11167050\n",
            "Iteration 81, loss = 0.11353225\n",
            "Iteration 82, loss = 0.11292566\n",
            "Iteration 83, loss = 0.11274492\n",
            "Iteration 84, loss = 0.11356611\n",
            "Iteration 85, loss = 0.11418078\n",
            "Iteration 86, loss = 0.11242160\n",
            "Iteration 87, loss = 0.11222300\n",
            "Iteration 88, loss = 0.11267515\n",
            "Iteration 89, loss = 0.11217865\n",
            "Iteration 90, loss = 0.11176900\n",
            "Iteration 91, loss = 0.11424924\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.21605668\n",
            "Iteration 2, loss = 0.18946037\n",
            "Iteration 3, loss = 0.18491728\n",
            "Iteration 4, loss = 0.18145561\n",
            "Iteration 5, loss = 0.17889830\n",
            "Iteration 6, loss = 0.17655361\n",
            "Iteration 7, loss = 0.17491985\n",
            "Iteration 8, loss = 0.17350748\n",
            "Iteration 9, loss = 0.17140222\n",
            "Iteration 10, loss = 0.16880677\n",
            "Iteration 11, loss = 0.16650441\n",
            "Iteration 12, loss = 0.16540477\n",
            "Iteration 13, loss = 0.16307995\n",
            "Iteration 14, loss = 0.16207474\n",
            "Iteration 15, loss = 0.16031695\n",
            "Iteration 16, loss = 0.15785453\n",
            "Iteration 17, loss = 0.15521517\n",
            "Iteration 18, loss = 0.15404879\n",
            "Iteration 19, loss = 0.15382980\n",
            "Iteration 20, loss = 0.15177236\n",
            "Iteration 21, loss = 0.15259245\n",
            "Iteration 22, loss = 0.14868532\n",
            "Iteration 23, loss = 0.14713686\n",
            "Iteration 24, loss = 0.14785247\n",
            "Iteration 25, loss = 0.14464553\n",
            "Iteration 26, loss = 0.14598455\n",
            "Iteration 27, loss = 0.14330543\n",
            "Iteration 28, loss = 0.14193670\n",
            "Iteration 29, loss = 0.14111477\n",
            "Iteration 30, loss = 0.13984454\n",
            "Iteration 31, loss = 0.13719853\n",
            "Iteration 32, loss = 0.13728085\n",
            "Iteration 33, loss = 0.13492230\n",
            "Iteration 34, loss = 0.13479607\n",
            "Iteration 35, loss = 0.13294672\n",
            "Iteration 36, loss = 0.13529325\n",
            "Iteration 37, loss = 0.13266743\n",
            "Iteration 38, loss = 0.13302991\n",
            "Iteration 39, loss = 0.12932544\n",
            "Iteration 40, loss = 0.12885048\n",
            "Iteration 41, loss = 0.12861911\n",
            "Iteration 42, loss = 0.12826617\n",
            "Iteration 43, loss = 0.12795447\n",
            "Iteration 44, loss = 0.12827785\n",
            "Iteration 45, loss = 0.12528465\n",
            "Iteration 46, loss = 0.12636158\n",
            "Iteration 47, loss = 0.12508864\n",
            "Iteration 48, loss = 0.12533303\n",
            "Iteration 49, loss = 0.12477958\n",
            "Iteration 50, loss = 0.12395088\n",
            "Iteration 51, loss = 0.12526849\n",
            "Iteration 52, loss = 0.12306339\n",
            "Iteration 53, loss = 0.12303353\n",
            "Iteration 54, loss = 0.12365748\n",
            "Iteration 55, loss = 0.12271451\n",
            "Iteration 56, loss = 0.12141799\n",
            "Iteration 57, loss = 0.12198386\n",
            "Iteration 58, loss = 0.12178561\n",
            "Iteration 59, loss = 0.11928981\n",
            "Iteration 60, loss = 0.12006141\n",
            "Iteration 61, loss = 0.12194860\n",
            "Iteration 62, loss = 0.12269187\n",
            "Iteration 63, loss = 0.11925170\n",
            "Iteration 64, loss = 0.12170194\n",
            "Iteration 65, loss = 0.11835289\n",
            "Iteration 66, loss = 0.11566890\n",
            "Iteration 67, loss = 0.11713294\n",
            "Iteration 68, loss = 0.11716887\n",
            "Iteration 69, loss = 0.11667259\n",
            "Iteration 70, loss = 0.11745133\n",
            "Iteration 71, loss = 0.11512171\n",
            "Iteration 72, loss = 0.11667570\n",
            "Iteration 73, loss = 0.11583896\n",
            "Iteration 74, loss = 0.11553398\n",
            "Iteration 75, loss = 0.11429439\n",
            "Iteration 76, loss = 0.11323853\n",
            "Iteration 77, loss = 0.11182647\n",
            "Iteration 78, loss = 0.11380735\n",
            "Iteration 79, loss = 0.11425384\n",
            "Iteration 80, loss = 0.11101758\n",
            "Iteration 81, loss = 0.11199563\n",
            "Iteration 82, loss = 0.11422984\n",
            "Iteration 83, loss = 0.11362326\n",
            "Iteration 84, loss = 0.11257549\n",
            "Iteration 85, loss = 0.11121915\n",
            "Iteration 86, loss = 0.11083355\n",
            "Iteration 87, loss = 0.11082391\n",
            "Iteration 88, loss = 0.10988617\n",
            "Iteration 89, loss = 0.11380054\n",
            "Iteration 90, loss = 0.11062494\n",
            "Iteration 91, loss = 0.10916389\n",
            "Iteration 92, loss = 0.11095567\n",
            "Iteration 93, loss = 0.11144601\n",
            "Iteration 94, loss = 0.10882101\n",
            "Iteration 95, loss = 0.10830561\n",
            "Iteration 96, loss = 0.10866058\n",
            "Iteration 97, loss = 0.11094931\n",
            "Iteration 98, loss = 0.11096711\n",
            "Iteration 99, loss = 0.11008041\n",
            "Iteration 100, loss = 0.10823111\n",
            "Iteration 101, loss = 0.10640364\n",
            "Iteration 102, loss = 0.10755115\n",
            "Iteration 103, loss = 0.10862199\n",
            "Iteration 104, loss = 0.11025066\n",
            "Iteration 105, loss = 0.10800166\n",
            "Iteration 106, loss = 0.11013057\n",
            "Iteration 107, loss = 0.10893315\n",
            "Iteration 108, loss = 0.10655291\n",
            "Iteration 109, loss = 0.10963376\n",
            "Iteration 110, loss = 0.10488930\n",
            "Iteration 111, loss = 0.10183583\n",
            "Iteration 112, loss = 0.10388054\n",
            "Iteration 113, loss = 0.10692112\n",
            "Iteration 114, loss = 0.10438201\n",
            "Iteration 115, loss = 0.10421844\n",
            "Iteration 116, loss = 0.10465027\n",
            "Iteration 117, loss = 0.10598551\n",
            "Iteration 118, loss = 0.10707850\n",
            "Iteration 119, loss = 0.10521191\n",
            "Iteration 120, loss = 0.10381431\n",
            "Iteration 121, loss = 0.10454864\n",
            "Iteration 122, loss = 0.10316432\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.20968169\n",
            "Iteration 2, loss = 0.18877898\n",
            "Iteration 3, loss = 0.18499268\n",
            "Iteration 4, loss = 0.18185082\n",
            "Iteration 5, loss = 0.18023157\n",
            "Iteration 6, loss = 0.17571912\n",
            "Iteration 7, loss = 0.17440942\n",
            "Iteration 8, loss = 0.17225956\n",
            "Iteration 9, loss = 0.16913848\n",
            "Iteration 10, loss = 0.16776361\n",
            "Iteration 11, loss = 0.16511519\n",
            "Iteration 12, loss = 0.16399431\n",
            "Iteration 13, loss = 0.16293796\n",
            "Iteration 14, loss = 0.15957874\n",
            "Iteration 15, loss = 0.15806843\n",
            "Iteration 16, loss = 0.15665621\n",
            "Iteration 17, loss = 0.15588681\n",
            "Iteration 18, loss = 0.15671246\n",
            "Iteration 19, loss = 0.15395307\n",
            "Iteration 20, loss = 0.14922515\n",
            "Iteration 21, loss = 0.15045195\n",
            "Iteration 22, loss = 0.14875501\n",
            "Iteration 23, loss = 0.14658385\n",
            "Iteration 24, loss = 0.14417390\n",
            "Iteration 25, loss = 0.14565528\n",
            "Iteration 26, loss = 0.14303313\n",
            "Iteration 27, loss = 0.14165568\n",
            "Iteration 28, loss = 0.13781788\n",
            "Iteration 29, loss = 0.13861311\n",
            "Iteration 30, loss = 0.13846013\n",
            "Iteration 31, loss = 0.13550076\n",
            "Iteration 32, loss = 0.13376318\n",
            "Iteration 33, loss = 0.13289566\n",
            "Iteration 34, loss = 0.13482196\n",
            "Iteration 35, loss = 0.13198246\n",
            "Iteration 36, loss = 0.13153906\n",
            "Iteration 37, loss = 0.12813944\n",
            "Iteration 38, loss = 0.12943596\n",
            "Iteration 39, loss = 0.12903557\n",
            "Iteration 40, loss = 0.13062808\n",
            "Iteration 41, loss = 0.13001171\n",
            "Iteration 42, loss = 0.12515739\n",
            "Iteration 43, loss = 0.12400051\n",
            "Iteration 44, loss = 0.12536901\n",
            "Iteration 45, loss = 0.12686424\n",
            "Iteration 46, loss = 0.12533850\n",
            "Iteration 47, loss = 0.12373513\n",
            "Iteration 48, loss = 0.12369950\n",
            "Iteration 49, loss = 0.12065153\n",
            "Iteration 50, loss = 0.12013151\n",
            "Iteration 51, loss = 0.12104972\n",
            "Iteration 52, loss = 0.12107029\n",
            "Iteration 53, loss = 0.12085815\n",
            "Iteration 54, loss = 0.12116287\n",
            "Iteration 55, loss = 0.11481217\n",
            "Iteration 56, loss = 0.11574265\n",
            "Iteration 57, loss = 0.11640472\n",
            "Iteration 58, loss = 0.11479726\n",
            "Iteration 59, loss = 0.11430366\n",
            "Iteration 60, loss = 0.11454320\n",
            "Iteration 61, loss = 0.11946988\n",
            "Iteration 62, loss = 0.11473127\n",
            "Iteration 63, loss = 0.11369748\n",
            "Iteration 64, loss = 0.11382856\n",
            "Iteration 65, loss = 0.11432451\n",
            "Iteration 66, loss = 0.11400965\n",
            "Iteration 67, loss = 0.11423644\n",
            "Iteration 68, loss = 0.11598005\n",
            "Iteration 69, loss = 0.11472856\n",
            "Iteration 70, loss = 0.11045869\n",
            "Iteration 71, loss = 0.11143249\n",
            "Iteration 72, loss = 0.11185089\n",
            "Iteration 73, loss = 0.11193786\n",
            "Iteration 74, loss = 0.11434326\n",
            "Iteration 75, loss = 0.11324709\n",
            "Iteration 76, loss = 0.10899740\n",
            "Iteration 77, loss = 0.11111403\n",
            "Iteration 78, loss = 0.11268085\n",
            "Iteration 79, loss = 0.11258909\n",
            "Iteration 80, loss = 0.10792848\n",
            "Iteration 81, loss = 0.10827389\n",
            "Iteration 82, loss = 0.10835008\n",
            "Iteration 83, loss = 0.10921430\n",
            "Iteration 84, loss = 0.10737374\n",
            "Iteration 85, loss = 0.10994080\n",
            "Iteration 86, loss = 0.10872373\n",
            "Iteration 87, loss = 0.10994582\n",
            "Iteration 88, loss = 0.10864176\n",
            "Iteration 89, loss = 0.10970257\n",
            "Iteration 90, loss = 0.10735899\n",
            "Iteration 91, loss = 0.10671001\n",
            "Iteration 92, loss = 0.10578646\n",
            "Iteration 93, loss = 0.10572097\n",
            "Iteration 94, loss = 0.11007140\n",
            "Iteration 95, loss = 0.10912480\n",
            "Iteration 96, loss = 0.10969192\n",
            "Iteration 97, loss = 0.10860542\n",
            "Iteration 98, loss = 0.10427566\n",
            "Iteration 99, loss = 0.10578174\n",
            "Iteration 100, loss = 0.10835932\n",
            "Iteration 101, loss = 0.10549598\n",
            "Iteration 102, loss = 0.10567435\n",
            "Iteration 103, loss = 0.10672024\n",
            "Iteration 104, loss = 0.10923288\n",
            "Iteration 105, loss = 0.10696016\n",
            "Iteration 106, loss = 0.10591210\n",
            "Iteration 107, loss = 0.10549191\n",
            "Iteration 108, loss = 0.10368392\n",
            "Iteration 109, loss = 0.10439344\n",
            "Iteration 110, loss = 0.10629283\n",
            "Iteration 111, loss = 0.10622257\n",
            "Iteration 112, loss = 0.10386886\n",
            "Iteration 113, loss = 0.10516813\n",
            "Iteration 114, loss = 0.10319235\n",
            "Iteration 115, loss = 0.10505444\n",
            "Iteration 116, loss = 0.10442604\n",
            "Iteration 117, loss = 0.10573350\n",
            "Iteration 118, loss = 0.10375804\n",
            "Iteration 119, loss = 0.10420428\n",
            "Iteration 120, loss = 0.10520290\n",
            "Iteration 121, loss = 0.10438388\n",
            "Iteration 122, loss = 0.10555374\n",
            "Iteration 123, loss = 0.10421634\n",
            "Iteration 124, loss = 0.10211772\n",
            "Iteration 125, loss = 0.10208778\n",
            "Iteration 126, loss = 0.10525193\n",
            "Iteration 127, loss = 0.10408836\n",
            "Iteration 128, loss = 0.10260689\n",
            "Iteration 129, loss = 0.10348717\n",
            "Iteration 130, loss = 0.10150740\n",
            "Iteration 131, loss = 0.09964013\n",
            "Iteration 132, loss = 0.09842103\n",
            "Iteration 133, loss = 0.10126949\n",
            "Iteration 134, loss = 0.10346209\n",
            "Iteration 135, loss = 0.10237135\n",
            "Iteration 136, loss = 0.10554775\n",
            "Iteration 137, loss = 0.10476707\n",
            "Iteration 138, loss = 0.11415909\n",
            "Iteration 139, loss = 0.10775700\n",
            "Iteration 140, loss = 0.10106913\n",
            "Iteration 141, loss = 0.09909292\n",
            "Iteration 142, loss = 0.09687170\n",
            "Iteration 143, loss = 0.09803657\n",
            "Iteration 144, loss = 0.09879067\n",
            "Iteration 145, loss = 0.10169021\n",
            "Iteration 146, loss = 0.10487033\n",
            "Iteration 147, loss = 0.10349865\n",
            "Iteration 148, loss = 0.10338260\n",
            "Iteration 149, loss = 0.09979043\n",
            "Iteration 150, loss = 0.10529404\n",
            "Iteration 151, loss = 0.10159425\n",
            "Iteration 152, loss = 0.09891543\n",
            "Iteration 153, loss = 0.10100865\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.21485631\n",
            "Iteration 2, loss = 0.18817402\n",
            "Iteration 3, loss = 0.18485218\n",
            "Iteration 4, loss = 0.18176207\n",
            "Iteration 5, loss = 0.17822038\n",
            "Iteration 6, loss = 0.17731850\n",
            "Iteration 7, loss = 0.17642596\n",
            "Iteration 8, loss = 0.17468406\n",
            "Iteration 9, loss = 0.17135839\n",
            "Iteration 10, loss = 0.17032779\n",
            "Iteration 11, loss = 0.16785410\n",
            "Iteration 12, loss = 0.16584803\n",
            "Iteration 13, loss = 0.16391951\n",
            "Iteration 14, loss = 0.16076664\n",
            "Iteration 15, loss = 0.16149707\n",
            "Iteration 16, loss = 0.15935201\n",
            "Iteration 17, loss = 0.15593808\n",
            "Iteration 18, loss = 0.15507605\n",
            "Iteration 19, loss = 0.15394352\n",
            "Iteration 20, loss = 0.15267494\n",
            "Iteration 21, loss = 0.15036093\n",
            "Iteration 22, loss = 0.14992336\n",
            "Iteration 23, loss = 0.14741871\n",
            "Iteration 24, loss = 0.14798497\n",
            "Iteration 25, loss = 0.14559219\n",
            "Iteration 26, loss = 0.14519950\n",
            "Iteration 27, loss = 0.14518474\n",
            "Iteration 28, loss = 0.14240321\n",
            "Iteration 29, loss = 0.13977131\n",
            "Iteration 30, loss = 0.13922531\n",
            "Iteration 31, loss = 0.13890711\n",
            "Iteration 32, loss = 0.13683005\n",
            "Iteration 33, loss = 0.13698665\n",
            "Iteration 34, loss = 0.13595525\n",
            "Iteration 35, loss = 0.13608855\n",
            "Iteration 36, loss = 0.13222844\n",
            "Iteration 37, loss = 0.13351804\n",
            "Iteration 38, loss = 0.13187891\n",
            "Iteration 39, loss = 0.13202291\n",
            "Iteration 40, loss = 0.13186047\n",
            "Iteration 41, loss = 0.12877296\n",
            "Iteration 42, loss = 0.12796252\n",
            "Iteration 43, loss = 0.12608918\n",
            "Iteration 44, loss = 0.12751902\n",
            "Iteration 45, loss = 0.12624405\n",
            "Iteration 46, loss = 0.12504296\n",
            "Iteration 47, loss = 0.12648644\n",
            "Iteration 48, loss = 0.12394630\n",
            "Iteration 49, loss = 0.12257905\n",
            "Iteration 50, loss = 0.12187971\n",
            "Iteration 51, loss = 0.12274287\n",
            "Iteration 52, loss = 0.12181526\n",
            "Iteration 53, loss = 0.12250271\n",
            "Iteration 54, loss = 0.12143177\n",
            "Iteration 55, loss = 0.12073661\n",
            "Iteration 56, loss = 0.12017078\n",
            "Iteration 57, loss = 0.12043789\n",
            "Iteration 58, loss = 0.11681662\n",
            "Iteration 59, loss = 0.11784742\n",
            "Iteration 60, loss = 0.11860073\n",
            "Iteration 61, loss = 0.11648157\n",
            "Iteration 62, loss = 0.11930691\n",
            "Iteration 63, loss = 0.12052807\n",
            "Iteration 64, loss = 0.11720738\n",
            "Iteration 65, loss = 0.11580569\n",
            "Iteration 66, loss = 0.11477748\n",
            "Iteration 67, loss = 0.11707827\n",
            "Iteration 68, loss = 0.11344247\n",
            "Iteration 69, loss = 0.11388623\n",
            "Iteration 70, loss = 0.11854143\n",
            "Iteration 71, loss = 0.11485147\n",
            "Iteration 72, loss = 0.11361769\n",
            "Iteration 73, loss = 0.11485191\n",
            "Iteration 74, loss = 0.11243157\n",
            "Iteration 75, loss = 0.11517356\n",
            "Iteration 76, loss = 0.11243805\n",
            "Iteration 77, loss = 0.11062130\n",
            "Iteration 78, loss = 0.11567208\n",
            "Iteration 79, loss = 0.11077018\n",
            "Iteration 80, loss = 0.11438390\n",
            "Iteration 81, loss = 0.11208118\n",
            "Iteration 82, loss = 0.11188318\n",
            "Iteration 83, loss = 0.11073063\n",
            "Iteration 84, loss = 0.10996335\n",
            "Iteration 85, loss = 0.10882893\n",
            "Iteration 86, loss = 0.10943592\n",
            "Iteration 87, loss = 0.10957762\n",
            "Iteration 88, loss = 0.11130548\n",
            "Iteration 89, loss = 0.11157555\n",
            "Iteration 90, loss = 0.11225433\n",
            "Iteration 91, loss = 0.10769661\n",
            "Iteration 92, loss = 0.10766826\n",
            "Iteration 93, loss = 0.10923348\n",
            "Iteration 94, loss = 0.11001411\n",
            "Iteration 95, loss = 0.10943349\n",
            "Iteration 96, loss = 0.10852044\n",
            "Iteration 97, loss = 0.10995537\n",
            "Iteration 98, loss = 0.11179477\n",
            "Iteration 99, loss = 0.11014411\n",
            "Iteration 100, loss = 0.11151292\n",
            "Iteration 101, loss = 0.10751803\n",
            "Iteration 102, loss = 0.10960336\n",
            "Iteration 103, loss = 0.10830205\n",
            "Iteration 104, loss = 0.10771061\n",
            "Iteration 105, loss = 0.10764819\n",
            "Iteration 106, loss = 0.10702232\n",
            "Iteration 107, loss = 0.10951324\n",
            "Iteration 108, loss = 0.11006735\n",
            "Iteration 109, loss = 0.10590963\n",
            "Iteration 110, loss = 0.10562173\n",
            "Iteration 111, loss = 0.10629196\n",
            "Iteration 112, loss = 0.10931681\n",
            "Iteration 113, loss = 0.11059644\n",
            "Iteration 114, loss = 0.10687308\n",
            "Iteration 115, loss = 0.10519673\n",
            "Iteration 116, loss = 0.10564959\n",
            "Iteration 117, loss = 0.10815056\n",
            "Iteration 118, loss = 0.10446420\n",
            "Iteration 119, loss = 0.10590668\n",
            "Iteration 120, loss = 0.11249121\n",
            "Iteration 121, loss = 0.10686983\n",
            "Iteration 122, loss = 0.10784687\n",
            "Iteration 123, loss = 0.10978364\n",
            "Iteration 124, loss = 0.10668970\n",
            "Iteration 125, loss = 0.10593723\n",
            "Iteration 126, loss = 0.10585419\n",
            "Iteration 127, loss = 0.10578280\n",
            "Iteration 128, loss = 0.10662363\n",
            "Iteration 129, loss = 0.10648197\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.21410414\n",
            "Iteration 2, loss = 0.18869809\n",
            "Iteration 3, loss = 0.18536945\n",
            "Iteration 4, loss = 0.18315224\n",
            "Iteration 5, loss = 0.18081742\n",
            "Iteration 6, loss = 0.17842604\n",
            "Iteration 7, loss = 0.17665341\n",
            "Iteration 8, loss = 0.17242081\n",
            "Iteration 9, loss = 0.17243412\n",
            "Iteration 10, loss = 0.17028444\n",
            "Iteration 11, loss = 0.16762547\n",
            "Iteration 12, loss = 0.16584401\n",
            "Iteration 13, loss = 0.16605095\n",
            "Iteration 14, loss = 0.16266720\n",
            "Iteration 15, loss = 0.16172675\n",
            "Iteration 16, loss = 0.15926858\n",
            "Iteration 17, loss = 0.15770514\n",
            "Iteration 18, loss = 0.15693557\n",
            "Iteration 19, loss = 0.15392415\n",
            "Iteration 20, loss = 0.15186882\n",
            "Iteration 21, loss = 0.14967262\n",
            "Iteration 22, loss = 0.14891733\n",
            "Iteration 23, loss = 0.14911147\n",
            "Iteration 24, loss = 0.14649272\n",
            "Iteration 25, loss = 0.14686798\n",
            "Iteration 26, loss = 0.14372419\n",
            "Iteration 27, loss = 0.14259772\n",
            "Iteration 28, loss = 0.14010057\n",
            "Iteration 29, loss = 0.14151745\n",
            "Iteration 30, loss = 0.13835632\n",
            "Iteration 31, loss = 0.13809833\n",
            "Iteration 32, loss = 0.13683422\n",
            "Iteration 33, loss = 0.13740795\n",
            "Iteration 34, loss = 0.13384282\n",
            "Iteration 35, loss = 0.13127057\n",
            "Iteration 36, loss = 0.13163477\n",
            "Iteration 37, loss = 0.13188681\n",
            "Iteration 38, loss = 0.13062754\n",
            "Iteration 39, loss = 0.12842725\n",
            "Iteration 40, loss = 0.12817408\n",
            "Iteration 41, loss = 0.12724625\n",
            "Iteration 42, loss = 0.12801741\n",
            "Iteration 43, loss = 0.12457892\n",
            "Iteration 44, loss = 0.12648854\n",
            "Iteration 45, loss = 0.12496238\n",
            "Iteration 46, loss = 0.12280967\n",
            "Iteration 47, loss = 0.12382879\n",
            "Iteration 48, loss = 0.12302222\n",
            "Iteration 49, loss = 0.12200026\n",
            "Iteration 50, loss = 0.11918911\n",
            "Iteration 51, loss = 0.11721058\n",
            "Iteration 52, loss = 0.11649354\n",
            "Iteration 53, loss = 0.11860747\n",
            "Iteration 54, loss = 0.11639108\n",
            "Iteration 55, loss = 0.11840006\n",
            "Iteration 56, loss = 0.11868485\n",
            "Iteration 57, loss = 0.11647270\n",
            "Iteration 58, loss = 0.11863595\n",
            "Iteration 59, loss = 0.11401219\n",
            "Iteration 60, loss = 0.11289081\n",
            "Iteration 61, loss = 0.11182750\n",
            "Iteration 62, loss = 0.11195549\n",
            "Iteration 63, loss = 0.11314160\n",
            "Iteration 64, loss = 0.11246603\n",
            "Iteration 65, loss = 0.11238236\n",
            "Iteration 66, loss = 0.11257481\n",
            "Iteration 67, loss = 0.11396147\n",
            "Iteration 68, loss = 0.11312992\n",
            "Iteration 69, loss = 0.11190552\n",
            "Iteration 70, loss = 0.11271034\n",
            "Iteration 71, loss = 0.10852236\n",
            "Iteration 72, loss = 0.10658023\n",
            "Iteration 73, loss = 0.10759218\n",
            "Iteration 74, loss = 0.10987780\n",
            "Iteration 75, loss = 0.10959159\n",
            "Iteration 76, loss = 0.10992977\n",
            "Iteration 77, loss = 0.10895447\n",
            "Iteration 78, loss = 0.10998013\n",
            "Iteration 79, loss = 0.11284803\n",
            "Iteration 80, loss = 0.10663390\n",
            "Iteration 81, loss = 0.11005208\n",
            "Iteration 82, loss = 0.10641665\n",
            "Iteration 83, loss = 0.10761998\n",
            "Iteration 84, loss = 0.10711270\n",
            "Iteration 85, loss = 0.10304191\n",
            "Iteration 86, loss = 0.10772120\n",
            "Iteration 87, loss = 0.10628618\n",
            "Iteration 88, loss = 0.10693333\n",
            "Iteration 89, loss = 0.10632217\n",
            "Iteration 90, loss = 0.10858462\n",
            "Iteration 91, loss = 0.10624794\n",
            "Iteration 92, loss = 0.10651356\n",
            "Iteration 93, loss = 0.10444993\n",
            "Iteration 94, loss = 0.10452221\n",
            "Iteration 95, loss = 0.10744978\n",
            "Iteration 96, loss = 0.10507174\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.21461740\n",
            "Iteration 2, loss = 0.18873076\n",
            "Iteration 3, loss = 0.18414988\n",
            "Iteration 4, loss = 0.18184864\n",
            "Iteration 5, loss = 0.17784104\n",
            "Iteration 6, loss = 0.17613724\n",
            "Iteration 7, loss = 0.17383302\n",
            "Iteration 8, loss = 0.17148951\n",
            "Iteration 9, loss = 0.17107167\n",
            "Iteration 10, loss = 0.16930516\n",
            "Iteration 11, loss = 0.16561412\n",
            "Iteration 12, loss = 0.16442782\n",
            "Iteration 13, loss = 0.16113305\n",
            "Iteration 14, loss = 0.16087653\n",
            "Iteration 15, loss = 0.15752888\n",
            "Iteration 16, loss = 0.15656338\n",
            "Iteration 17, loss = 0.15484347\n",
            "Iteration 18, loss = 0.15196211\n",
            "Iteration 19, loss = 0.15143603\n",
            "Iteration 20, loss = 0.14884592\n",
            "Iteration 21, loss = 0.14925316\n",
            "Iteration 22, loss = 0.14856714\n",
            "Iteration 23, loss = 0.14773358\n",
            "Iteration 24, loss = 0.14554454\n",
            "Iteration 25, loss = 0.14405237\n",
            "Iteration 26, loss = 0.13917701\n",
            "Iteration 27, loss = 0.14239356\n",
            "Iteration 28, loss = 0.14081401\n",
            "Iteration 29, loss = 0.13986399\n",
            "Iteration 30, loss = 0.13668682\n",
            "Iteration 31, loss = 0.13407464\n",
            "Iteration 32, loss = 0.13301059\n",
            "Iteration 33, loss = 0.13296940\n",
            "Iteration 34, loss = 0.12977607\n",
            "Iteration 35, loss = 0.13146288\n",
            "Iteration 36, loss = 0.12886297\n",
            "Iteration 37, loss = 0.12862578\n",
            "Iteration 38, loss = 0.12442206\n",
            "Iteration 39, loss = 0.12394378\n",
            "Iteration 40, loss = 0.12792067\n",
            "Iteration 41, loss = 0.12542296\n",
            "Iteration 42, loss = 0.12129208\n",
            "Iteration 43, loss = 0.11901809\n",
            "Iteration 44, loss = 0.12061145\n",
            "Iteration 45, loss = 0.12090531\n",
            "Iteration 46, loss = 0.12054066\n",
            "Iteration 47, loss = 0.11874846\n",
            "Iteration 48, loss = 0.11732825\n",
            "Iteration 49, loss = 0.11745318\n",
            "Iteration 50, loss = 0.11600624\n",
            "Iteration 51, loss = 0.11578390\n",
            "Iteration 52, loss = 0.11437395\n",
            "Iteration 53, loss = 0.11616250\n",
            "Iteration 54, loss = 0.11559820\n",
            "Iteration 55, loss = 0.11363062\n",
            "Iteration 56, loss = 0.11157468\n",
            "Iteration 57, loss = 0.11198539\n",
            "Iteration 58, loss = 0.11402926\n",
            "Iteration 59, loss = 0.11077092\n",
            "Iteration 60, loss = 0.11431093\n",
            "Iteration 61, loss = 0.11129350\n",
            "Iteration 62, loss = 0.10949198\n",
            "Iteration 63, loss = 0.11118311\n",
            "Iteration 64, loss = 0.11134582\n",
            "Iteration 65, loss = 0.10668397\n",
            "Iteration 66, loss = 0.10750542\n",
            "Iteration 67, loss = 0.10858901\n",
            "Iteration 68, loss = 0.10762645\n",
            "Iteration 69, loss = 0.10720316\n",
            "Iteration 70, loss = 0.10706749\n",
            "Iteration 71, loss = 0.10431361\n",
            "Iteration 72, loss = 0.10681613\n",
            "Iteration 73, loss = 0.10773417\n",
            "Iteration 74, loss = 0.10753254\n",
            "Iteration 75, loss = 0.10581091\n",
            "Iteration 76, loss = 0.10509849\n",
            "Iteration 77, loss = 0.10805922\n",
            "Iteration 78, loss = 0.10732138\n",
            "Iteration 79, loss = 0.10489056\n",
            "Iteration 80, loss = 0.10513948\n",
            "Iteration 81, loss = 0.10508729\n",
            "Iteration 82, loss = 0.10453575\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.21237705\n",
            "Iteration 2, loss = 0.18959538\n",
            "Iteration 3, loss = 0.18579968\n",
            "Iteration 4, loss = 0.18299243\n",
            "Iteration 5, loss = 0.17981623\n",
            "Iteration 6, loss = 0.17864351\n",
            "Iteration 7, loss = 0.17802714\n",
            "Iteration 8, loss = 0.17414504\n",
            "Iteration 9, loss = 0.17206056\n",
            "Iteration 10, loss = 0.16966200\n",
            "Iteration 11, loss = 0.16969621\n",
            "Iteration 12, loss = 0.16949220\n",
            "Iteration 13, loss = 0.16432937\n",
            "Iteration 14, loss = 0.16380735\n",
            "Iteration 15, loss = 0.16285382\n",
            "Iteration 16, loss = 0.15919278\n",
            "Iteration 17, loss = 0.15663813\n",
            "Iteration 18, loss = 0.15487700\n",
            "Iteration 19, loss = 0.15368797\n",
            "Iteration 20, loss = 0.15389116\n",
            "Iteration 21, loss = 0.15011149\n",
            "Iteration 22, loss = 0.14768806\n",
            "Iteration 23, loss = 0.14638591\n",
            "Iteration 24, loss = 0.14513498\n",
            "Iteration 25, loss = 0.14365946\n",
            "Iteration 26, loss = 0.14061885\n",
            "Iteration 27, loss = 0.14115733\n",
            "Iteration 28, loss = 0.13750526\n",
            "Iteration 29, loss = 0.13654682\n",
            "Iteration 30, loss = 0.13475600\n",
            "Iteration 31, loss = 0.13591075\n",
            "Iteration 32, loss = 0.13183118\n",
            "Iteration 33, loss = 0.13191920\n",
            "Iteration 34, loss = 0.13009592\n",
            "Iteration 35, loss = 0.13139316\n",
            "Iteration 36, loss = 0.12933621\n",
            "Iteration 37, loss = 0.12681091\n",
            "Iteration 38, loss = 0.12313719\n",
            "Iteration 39, loss = 0.12367759\n",
            "Iteration 40, loss = 0.12433030\n",
            "Iteration 41, loss = 0.12422289\n",
            "Iteration 42, loss = 0.12329112\n",
            "Iteration 43, loss = 0.11874911\n",
            "Iteration 44, loss = 0.11768096\n",
            "Iteration 45, loss = 0.11780120\n",
            "Iteration 46, loss = 0.11823047\n",
            "Iteration 47, loss = 0.11570163\n",
            "Iteration 48, loss = 0.11901150\n",
            "Iteration 49, loss = 0.11639250\n",
            "Iteration 50, loss = 0.11402688\n",
            "Iteration 51, loss = 0.11540388\n",
            "Iteration 52, loss = 0.11575023\n",
            "Iteration 53, loss = 0.11280794\n",
            "Iteration 54, loss = 0.11287687\n",
            "Iteration 55, loss = 0.11299757\n",
            "Iteration 56, loss = 0.11138611\n",
            "Iteration 57, loss = 0.11158228\n",
            "Iteration 58, loss = 0.11093904\n",
            "Iteration 59, loss = 0.11139577\n",
            "Iteration 60, loss = 0.11060547\n",
            "Iteration 61, loss = 0.11065201\n",
            "Iteration 62, loss = 0.11057468\n",
            "Iteration 63, loss = 0.11004904\n",
            "Iteration 64, loss = 0.10749955\n",
            "Iteration 65, loss = 0.10347719\n",
            "Iteration 66, loss = 0.10925218\n",
            "Iteration 67, loss = 0.10880642\n",
            "Iteration 68, loss = 0.10825734\n",
            "Iteration 69, loss = 0.10880380\n",
            "Iteration 70, loss = 0.10585520\n",
            "Iteration 71, loss = 0.10237929\n",
            "Iteration 72, loss = 0.10581319\n",
            "Iteration 73, loss = 0.10552881\n",
            "Iteration 74, loss = 0.10706583\n",
            "Iteration 75, loss = 0.10418686\n",
            "Iteration 76, loss = 0.10715676\n",
            "Iteration 77, loss = 0.10338605\n",
            "Iteration 78, loss = 0.10076204\n",
            "Iteration 79, loss = 0.10404881\n",
            "Iteration 80, loss = 0.10405931\n",
            "Iteration 81, loss = 0.10412112\n",
            "Iteration 82, loss = 0.10376099\n",
            "Iteration 83, loss = 0.10210476\n",
            "Iteration 84, loss = 0.10115919\n",
            "Iteration 85, loss = 0.10131708\n",
            "Iteration 86, loss = 0.10019423\n",
            "Iteration 87, loss = 0.09975702\n",
            "Iteration 88, loss = 0.10194592\n",
            "Iteration 89, loss = 0.10221779\n",
            "Iteration 90, loss = 0.10068350\n",
            "Iteration 91, loss = 0.10528780\n",
            "Iteration 92, loss = 0.10136815\n",
            "Iteration 93, loss = 0.10182821\n",
            "Iteration 94, loss = 0.10153711\n",
            "Iteration 95, loss = 0.09688722\n",
            "Iteration 96, loss = 0.09799974\n",
            "Iteration 97, loss = 0.09996904\n",
            "Iteration 98, loss = 0.09841227\n",
            "Iteration 99, loss = 0.10169393\n",
            "Iteration 100, loss = 0.09931122\n",
            "Iteration 101, loss = 0.10164652\n",
            "Iteration 102, loss = 0.10175512\n",
            "Iteration 103, loss = 0.10017362\n",
            "Iteration 104, loss = 0.10096152\n",
            "Iteration 105, loss = 0.09717782\n",
            "Iteration 106, loss = 0.09545508\n",
            "Iteration 107, loss = 0.09718120\n",
            "Iteration 108, loss = 0.10093067\n",
            "Iteration 109, loss = 0.10004954\n",
            "Iteration 110, loss = 0.10287066\n",
            "Iteration 111, loss = 0.09518505\n",
            "Iteration 112, loss = 0.09923079\n",
            "Iteration 113, loss = 0.09663774\n",
            "Iteration 114, loss = 0.09500024\n",
            "Iteration 115, loss = 0.09447710\n",
            "Iteration 116, loss = 0.09902910\n",
            "Iteration 117, loss = 0.09779051\n",
            "Iteration 118, loss = 0.09916826\n",
            "Iteration 119, loss = 0.09685371\n",
            "Iteration 120, loss = 0.09550568\n",
            "Iteration 121, loss = 0.09885657\n",
            "Iteration 122, loss = 0.09847881\n",
            "Iteration 123, loss = 0.09634208\n",
            "Iteration 124, loss = 0.09530645\n",
            "Iteration 125, loss = 0.09632796\n",
            "Iteration 126, loss = 0.09383549\n",
            "Iteration 127, loss = 0.09427395\n",
            "Iteration 128, loss = 0.09837384\n",
            "Iteration 129, loss = 0.09547799\n",
            "Iteration 130, loss = 0.09875517\n",
            "Iteration 131, loss = 0.09415066\n",
            "Iteration 132, loss = 0.09567095\n",
            "Iteration 133, loss = 0.09634132\n",
            "Iteration 134, loss = 0.09514338\n",
            "Iteration 135, loss = 0.09792492\n",
            "Iteration 136, loss = 0.09409784\n",
            "Iteration 137, loss = 0.09622121\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.21335037\n",
            "Iteration 2, loss = 0.18896042\n",
            "Iteration 3, loss = 0.18449704\n",
            "Iteration 4, loss = 0.18144606\n",
            "Iteration 5, loss = 0.17886458\n",
            "Iteration 6, loss = 0.17624446\n",
            "Iteration 7, loss = 0.17510874\n",
            "Iteration 8, loss = 0.17170633\n",
            "Iteration 9, loss = 0.17088595\n",
            "Iteration 10, loss = 0.16847360\n",
            "Iteration 11, loss = 0.16675047\n",
            "Iteration 12, loss = 0.16582348\n",
            "Iteration 13, loss = 0.16175305\n",
            "Iteration 14, loss = 0.16036469\n",
            "Iteration 15, loss = 0.15931897\n",
            "Iteration 16, loss = 0.15871885\n",
            "Iteration 17, loss = 0.15675477\n",
            "Iteration 18, loss = 0.15483496\n",
            "Iteration 19, loss = 0.15315336\n",
            "Iteration 20, loss = 0.15220968\n",
            "Iteration 21, loss = 0.15268344\n",
            "Iteration 22, loss = 0.14999509\n",
            "Iteration 23, loss = 0.14787331\n",
            "Iteration 24, loss = 0.14613524\n",
            "Iteration 25, loss = 0.14363306\n",
            "Iteration 26, loss = 0.14280649\n",
            "Iteration 27, loss = 0.14303655\n",
            "Iteration 28, loss = 0.14008529\n",
            "Iteration 29, loss = 0.13890383\n",
            "Iteration 30, loss = 0.13567264\n",
            "Iteration 31, loss = 0.13652604\n",
            "Iteration 32, loss = 0.13516455\n",
            "Iteration 33, loss = 0.13225432\n",
            "Iteration 34, loss = 0.13156400\n",
            "Iteration 35, loss = 0.12982828\n",
            "Iteration 36, loss = 0.12860382\n",
            "Iteration 37, loss = 0.13094432\n",
            "Iteration 38, loss = 0.12702270\n",
            "Iteration 39, loss = 0.12579718\n",
            "Iteration 40, loss = 0.12671303\n",
            "Iteration 41, loss = 0.12648874\n",
            "Iteration 42, loss = 0.12347408\n",
            "Iteration 43, loss = 0.12670864\n",
            "Iteration 44, loss = 0.12630230\n",
            "Iteration 45, loss = 0.12252821\n",
            "Iteration 46, loss = 0.12240895\n",
            "Iteration 47, loss = 0.12104323\n",
            "Iteration 48, loss = 0.12087623\n",
            "Iteration 49, loss = 0.12029805\n",
            "Iteration 50, loss = 0.12300547\n",
            "Iteration 51, loss = 0.11899725\n",
            "Iteration 52, loss = 0.11768209\n",
            "Iteration 53, loss = 0.11808977\n",
            "Iteration 54, loss = 0.11962179\n",
            "Iteration 55, loss = 0.12004298\n",
            "Iteration 56, loss = 0.11884303\n",
            "Iteration 57, loss = 0.11595192\n",
            "Iteration 58, loss = 0.11533387\n",
            "Iteration 59, loss = 0.11797654\n",
            "Iteration 60, loss = 0.11754738\n",
            "Iteration 61, loss = 0.11526338\n",
            "Iteration 62, loss = 0.11245143\n",
            "Iteration 63, loss = 0.11447582\n",
            "Iteration 64, loss = 0.11440358\n",
            "Iteration 65, loss = 0.11364171\n",
            "Iteration 66, loss = 0.11585524\n",
            "Iteration 67, loss = 0.11343930\n",
            "Iteration 68, loss = 0.11099301\n",
            "Iteration 69, loss = 0.11119275\n",
            "Iteration 70, loss = 0.11186366\n",
            "Iteration 71, loss = 0.11356297\n",
            "Iteration 72, loss = 0.11582091\n",
            "Iteration 73, loss = 0.11290287\n",
            "Iteration 74, loss = 0.11046568\n",
            "Iteration 75, loss = 0.10736647\n",
            "Iteration 76, loss = 0.11443028\n",
            "Iteration 77, loss = 0.11201758\n",
            "Iteration 78, loss = 0.11059068\n",
            "Iteration 79, loss = 0.10895466\n",
            "Iteration 80, loss = 0.11348338\n",
            "Iteration 81, loss = 0.10918290\n",
            "Iteration 82, loss = 0.11146036\n",
            "Iteration 83, loss = 0.10981635\n",
            "Iteration 84, loss = 0.10722566\n",
            "Iteration 85, loss = 0.10896507\n",
            "Iteration 86, loss = 0.10877191\n",
            "Iteration 87, loss = 0.10774092\n",
            "Iteration 88, loss = 0.11061950\n",
            "Iteration 89, loss = 0.10796610\n",
            "Iteration 90, loss = 0.10832350\n",
            "Iteration 91, loss = 0.10863249\n",
            "Iteration 92, loss = 0.10602157\n",
            "Iteration 93, loss = 0.10515156\n",
            "Iteration 94, loss = 0.10680645\n",
            "Iteration 95, loss = 0.11094346\n",
            "Iteration 96, loss = 0.10635523\n",
            "Iteration 97, loss = 0.10415694\n",
            "Iteration 98, loss = 0.10154913\n",
            "Iteration 99, loss = 0.10490104\n",
            "Iteration 100, loss = 0.10991544\n",
            "Iteration 101, loss = 0.11009345\n",
            "Iteration 102, loss = 0.11006653\n",
            "Iteration 103, loss = 0.10772111\n",
            "Iteration 104, loss = 0.10712908\n",
            "Iteration 105, loss = 0.10313888\n",
            "Iteration 106, loss = 0.10230636\n",
            "Iteration 107, loss = 0.10043234\n",
            "Iteration 108, loss = 0.10499119\n",
            "Iteration 109, loss = 0.10703806\n",
            "Iteration 110, loss = 0.10672800\n",
            "Iteration 111, loss = 0.10844867\n",
            "Iteration 112, loss = 0.10702155\n",
            "Iteration 113, loss = 0.10629654\n",
            "Iteration 114, loss = 0.10195871\n",
            "Iteration 115, loss = 0.10251460\n",
            "Iteration 116, loss = 0.10514814\n",
            "Iteration 117, loss = 0.10099816\n",
            "Iteration 118, loss = 0.10297646\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.20871904\n",
            "Iteration 2, loss = 0.18812031\n",
            "Iteration 3, loss = 0.18495556\n",
            "Iteration 4, loss = 0.18216872\n",
            "Iteration 5, loss = 0.18058073\n",
            "Iteration 6, loss = 0.17619239\n",
            "Iteration 7, loss = 0.17374079\n",
            "Iteration 8, loss = 0.17161188\n",
            "Iteration 9, loss = 0.17017674\n",
            "Iteration 10, loss = 0.16705566\n",
            "Iteration 11, loss = 0.16558011\n",
            "Iteration 12, loss = 0.16247365\n",
            "Iteration 13, loss = 0.16343491\n",
            "Iteration 14, loss = 0.16173752\n",
            "Iteration 15, loss = 0.15958361\n",
            "Iteration 16, loss = 0.15765025\n",
            "Iteration 17, loss = 0.15593578\n",
            "Iteration 18, loss = 0.15468498\n",
            "Iteration 19, loss = 0.15404013\n",
            "Iteration 20, loss = 0.15163106\n",
            "Iteration 21, loss = 0.15067500\n",
            "Iteration 22, loss = 0.14976332\n",
            "Iteration 23, loss = 0.14906118\n",
            "Iteration 24, loss = 0.14692978\n",
            "Iteration 25, loss = 0.14575692\n",
            "Iteration 26, loss = 0.14501829\n",
            "Iteration 27, loss = 0.14378116\n",
            "Iteration 28, loss = 0.13996834\n",
            "Iteration 29, loss = 0.14202916\n",
            "Iteration 30, loss = 0.13786273\n",
            "Iteration 31, loss = 0.13751526\n",
            "Iteration 32, loss = 0.13614884\n",
            "Iteration 33, loss = 0.13453936\n",
            "Iteration 34, loss = 0.13440854\n",
            "Iteration 35, loss = 0.13362342\n",
            "Iteration 36, loss = 0.13020125\n",
            "Iteration 37, loss = 0.13183855\n",
            "Iteration 38, loss = 0.12991827\n",
            "Iteration 39, loss = 0.12878901\n",
            "Iteration 40, loss = 0.12958598\n",
            "Iteration 41, loss = 0.12867413\n",
            "Iteration 42, loss = 0.13062988\n",
            "Iteration 43, loss = 0.12634604\n",
            "Iteration 44, loss = 0.12555095\n",
            "Iteration 45, loss = 0.12439783\n",
            "Iteration 46, loss = 0.12464415\n",
            "Iteration 47, loss = 0.12235909\n",
            "Iteration 48, loss = 0.12182973\n",
            "Iteration 49, loss = 0.12083297\n",
            "Iteration 50, loss = 0.12147266\n",
            "Iteration 51, loss = 0.11781392\n",
            "Iteration 52, loss = 0.11839672\n",
            "Iteration 53, loss = 0.12061830\n",
            "Iteration 54, loss = 0.11952095\n",
            "Iteration 55, loss = 0.11867901\n",
            "Iteration 56, loss = 0.11964270\n",
            "Iteration 57, loss = 0.11731203\n",
            "Iteration 58, loss = 0.11771666\n",
            "Iteration 59, loss = 0.11733494\n",
            "Iteration 60, loss = 0.11679713\n",
            "Iteration 61, loss = 0.11546895\n",
            "Iteration 62, loss = 0.11612336\n",
            "Iteration 63, loss = 0.11319571\n",
            "Iteration 64, loss = 0.11488966\n",
            "Iteration 65, loss = 0.11767068\n",
            "Iteration 66, loss = 0.11264041\n",
            "Iteration 67, loss = 0.11283222\n",
            "Iteration 68, loss = 0.11026977\n",
            "Iteration 69, loss = 0.11353510\n",
            "Iteration 70, loss = 0.11314008\n",
            "Iteration 71, loss = 0.11140442\n",
            "Iteration 72, loss = 0.11114129\n",
            "Iteration 73, loss = 0.11067747\n",
            "Iteration 74, loss = 0.11550445\n",
            "Iteration 75, loss = 0.11552229\n",
            "Iteration 76, loss = 0.11607498\n",
            "Iteration 77, loss = 0.11243610\n",
            "Iteration 78, loss = 0.11072606\n",
            "Iteration 79, loss = 0.10802798\n",
            "Iteration 80, loss = 0.10723952\n",
            "Iteration 81, loss = 0.10917553\n",
            "Iteration 82, loss = 0.10861557\n",
            "Iteration 83, loss = 0.11203200\n",
            "Iteration 84, loss = 0.11169725\n",
            "Iteration 85, loss = 0.11022566\n",
            "Iteration 86, loss = 0.11127825\n",
            "Iteration 87, loss = 0.10703215\n",
            "Iteration 88, loss = 0.10713167\n",
            "Iteration 89, loss = 0.11302510\n",
            "Iteration 90, loss = 0.11132299\n",
            "Iteration 91, loss = 0.10800187\n",
            "Iteration 92, loss = 0.10655121\n",
            "Iteration 93, loss = 0.10841585\n",
            "Iteration 94, loss = 0.11088543\n",
            "Iteration 95, loss = 0.10837038\n",
            "Iteration 96, loss = 0.10770214\n",
            "Iteration 97, loss = 0.10795138\n",
            "Iteration 98, loss = 0.10889600\n",
            "Iteration 99, loss = 0.10976450\n",
            "Iteration 100, loss = 0.10892303\n",
            "Iteration 101, loss = 0.11107423\n",
            "Iteration 102, loss = 0.11347737\n",
            "Iteration 103, loss = 0.10789517\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.21304628\n",
            "Iteration 2, loss = 0.18827939\n",
            "Iteration 3, loss = 0.18497661\n",
            "Iteration 4, loss = 0.18174933\n",
            "Iteration 5, loss = 0.17858650\n",
            "Iteration 6, loss = 0.17527910\n",
            "Iteration 7, loss = 0.17336952\n",
            "Iteration 8, loss = 0.17266841\n",
            "Iteration 9, loss = 0.16966841\n",
            "Iteration 10, loss = 0.16775570\n",
            "Iteration 11, loss = 0.16592137\n",
            "Iteration 12, loss = 0.16571447\n",
            "Iteration 13, loss = 0.16197960\n",
            "Iteration 14, loss = 0.16123329\n",
            "Iteration 15, loss = 0.15846200\n",
            "Iteration 16, loss = 0.15589017\n",
            "Iteration 17, loss = 0.15600836\n",
            "Iteration 18, loss = 0.15493277\n",
            "Iteration 19, loss = 0.15147993\n",
            "Iteration 20, loss = 0.15031820\n",
            "Iteration 21, loss = 0.14939721\n",
            "Iteration 22, loss = 0.14679433\n",
            "Iteration 23, loss = 0.14540117\n",
            "Iteration 24, loss = 0.14135352\n",
            "Iteration 25, loss = 0.14193727\n",
            "Iteration 26, loss = 0.14238814\n",
            "Iteration 27, loss = 0.13980247\n",
            "Iteration 28, loss = 0.13905129\n",
            "Iteration 29, loss = 0.13636440\n",
            "Iteration 30, loss = 0.13603187\n",
            "Iteration 31, loss = 0.13352373\n",
            "Iteration 32, loss = 0.13218419\n",
            "Iteration 33, loss = 0.13177247\n",
            "Iteration 34, loss = 0.13029497\n",
            "Iteration 35, loss = 0.12998774\n",
            "Iteration 36, loss = 0.12853678\n",
            "Iteration 37, loss = 0.12761128\n",
            "Iteration 38, loss = 0.12649920\n",
            "Iteration 39, loss = 0.12552413\n",
            "Iteration 40, loss = 0.12245959\n",
            "Iteration 41, loss = 0.12322711\n",
            "Iteration 42, loss = 0.12198679\n",
            "Iteration 43, loss = 0.12119467\n",
            "Iteration 44, loss = 0.12128495\n",
            "Iteration 45, loss = 0.11858521\n",
            "Iteration 46, loss = 0.11976212\n",
            "Iteration 47, loss = 0.12043495\n",
            "Iteration 48, loss = 0.11885107\n",
            "Iteration 49, loss = 0.11871040\n",
            "Iteration 50, loss = 0.11695069\n",
            "Iteration 51, loss = 0.11577481\n",
            "Iteration 52, loss = 0.11651407\n",
            "Iteration 53, loss = 0.11703694\n",
            "Iteration 54, loss = 0.11723469\n",
            "Iteration 55, loss = 0.11661405\n",
            "Iteration 56, loss = 0.11694808\n",
            "Iteration 57, loss = 0.11253801\n",
            "Iteration 58, loss = 0.11188277\n",
            "Iteration 59, loss = 0.11110191\n",
            "Iteration 60, loss = 0.11209729\n",
            "Iteration 61, loss = 0.11061560\n",
            "Iteration 62, loss = 0.11217212\n",
            "Iteration 63, loss = 0.11459318\n",
            "Iteration 64, loss = 0.11040334\n",
            "Iteration 65, loss = 0.11061542\n",
            "Iteration 66, loss = 0.11182359\n",
            "Iteration 67, loss = 0.11122794\n",
            "Iteration 68, loss = 0.11241412\n",
            "Iteration 69, loss = 0.11346214\n",
            "Iteration 70, loss = 0.10801180\n",
            "Iteration 71, loss = 0.10849523\n",
            "Iteration 72, loss = 0.10966993\n",
            "Iteration 73, loss = 0.11020248\n",
            "Iteration 74, loss = 0.10783314\n",
            "Iteration 75, loss = 0.10563396\n",
            "Iteration 76, loss = 0.10714299\n",
            "Iteration 77, loss = 0.11071771\n",
            "Iteration 78, loss = 0.10404096\n",
            "Iteration 79, loss = 0.11069343\n",
            "Iteration 80, loss = 0.10996947\n",
            "Iteration 81, loss = 0.10214716\n",
            "Iteration 82, loss = 0.10633575\n",
            "Iteration 83, loss = 0.10462549\n",
            "Iteration 84, loss = 0.10894400\n",
            "Iteration 85, loss = 0.10667395\n",
            "Iteration 86, loss = 0.10630661\n",
            "Iteration 87, loss = 0.10702665\n",
            "Iteration 88, loss = 0.10425853\n",
            "Iteration 89, loss = 0.10254039\n",
            "Iteration 90, loss = 0.10366112\n",
            "Iteration 91, loss = 0.10428629\n",
            "Iteration 92, loss = 0.10604523\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.20975584\n",
            "Iteration 2, loss = 0.18795760\n",
            "Iteration 3, loss = 0.18555115\n",
            "Iteration 4, loss = 0.18103114\n",
            "Iteration 5, loss = 0.17818138\n",
            "Iteration 6, loss = 0.17675316\n",
            "Iteration 7, loss = 0.17548261\n",
            "Iteration 8, loss = 0.17192240\n",
            "Iteration 9, loss = 0.17181304\n",
            "Iteration 10, loss = 0.16784770\n",
            "Iteration 11, loss = 0.16543773\n",
            "Iteration 12, loss = 0.16309169\n",
            "Iteration 13, loss = 0.16240967\n",
            "Iteration 14, loss = 0.16056941\n",
            "Iteration 15, loss = 0.15749265\n",
            "Iteration 16, loss = 0.15673613\n",
            "Iteration 17, loss = 0.15378704\n",
            "Iteration 18, loss = 0.15211115\n",
            "Iteration 19, loss = 0.15077215\n",
            "Iteration 20, loss = 0.14934744\n",
            "Iteration 21, loss = 0.14795942\n",
            "Iteration 22, loss = 0.14740237\n",
            "Iteration 23, loss = 0.14750631\n",
            "Iteration 24, loss = 0.14342357\n",
            "Iteration 25, loss = 0.14262903\n",
            "Iteration 26, loss = 0.14062017\n",
            "Iteration 27, loss = 0.14189940\n",
            "Iteration 28, loss = 0.14024409\n",
            "Iteration 29, loss = 0.13713645\n",
            "Iteration 30, loss = 0.13801192\n",
            "Iteration 31, loss = 0.13658492\n",
            "Iteration 32, loss = 0.13396373\n",
            "Iteration 33, loss = 0.13294684\n",
            "Iteration 34, loss = 0.13234328\n",
            "Iteration 35, loss = 0.13057756\n",
            "Iteration 36, loss = 0.13205968\n",
            "Iteration 37, loss = 0.13148615\n",
            "Iteration 38, loss = 0.12859050\n",
            "Iteration 39, loss = 0.12722551\n",
            "Iteration 40, loss = 0.12713832\n",
            "Iteration 41, loss = 0.12686209\n",
            "Iteration 42, loss = 0.12858401\n",
            "Iteration 43, loss = 0.12496673\n",
            "Iteration 44, loss = 0.12636625\n",
            "Iteration 45, loss = 0.12499453\n",
            "Iteration 46, loss = 0.12375524\n",
            "Iteration 47, loss = 0.12410981\n",
            "Iteration 48, loss = 0.12227058\n",
            "Iteration 49, loss = 0.12063177\n",
            "Iteration 50, loss = 0.12025065\n",
            "Iteration 51, loss = 0.12351897\n",
            "Iteration 52, loss = 0.12173246\n",
            "Iteration 53, loss = 0.12119649\n",
            "Iteration 54, loss = 0.11963567\n",
            "Iteration 55, loss = 0.11828232\n",
            "Iteration 56, loss = 0.11689873\n",
            "Iteration 57, loss = 0.11625721\n",
            "Iteration 58, loss = 0.11585121\n",
            "Iteration 59, loss = 0.11875425\n",
            "Iteration 60, loss = 0.11706266\n",
            "Iteration 61, loss = 0.11504960\n",
            "Iteration 62, loss = 0.11460537\n",
            "Iteration 63, loss = 0.11570015\n",
            "Iteration 64, loss = 0.11361331\n",
            "Iteration 65, loss = 0.11166150\n",
            "Iteration 66, loss = 0.11563868\n",
            "Iteration 67, loss = 0.11304820\n",
            "Iteration 68, loss = 0.11294572\n",
            "Iteration 69, loss = 0.11069249\n",
            "Iteration 70, loss = 0.11235561\n",
            "Iteration 71, loss = 0.10943755\n",
            "Iteration 72, loss = 0.10993529\n",
            "Iteration 73, loss = 0.11008486\n",
            "Iteration 74, loss = 0.10911901\n",
            "Iteration 75, loss = 0.10913780\n",
            "Iteration 76, loss = 0.11084098\n",
            "Iteration 77, loss = 0.11124630\n",
            "Iteration 78, loss = 0.10991812\n",
            "Iteration 79, loss = 0.10694045\n",
            "Iteration 80, loss = 0.10763175\n",
            "Iteration 81, loss = 0.10714686\n",
            "Iteration 82, loss = 0.10735270\n",
            "Iteration 83, loss = 0.10714932\n",
            "Iteration 84, loss = 0.10907305\n",
            "Iteration 85, loss = 0.11251873\n",
            "Iteration 86, loss = 0.11027122\n",
            "Iteration 87, loss = 0.10599701\n",
            "Iteration 88, loss = 0.10376582\n",
            "Iteration 89, loss = 0.10186978\n",
            "Iteration 90, loss = 0.10127299\n",
            "Iteration 91, loss = 0.10333285\n",
            "Iteration 92, loss = 0.10520500\n",
            "Iteration 93, loss = 0.10756992\n",
            "Iteration 94, loss = 0.10819272\n",
            "Iteration 95, loss = 0.10699878\n",
            "Iteration 96, loss = 0.10217697\n",
            "Iteration 97, loss = 0.10349708\n",
            "Iteration 98, loss = 0.10268999\n",
            "Iteration 99, loss = 0.10298009\n",
            "Iteration 100, loss = 0.10674248\n",
            "Iteration 101, loss = 0.10630583\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.20932280\n",
            "Iteration 2, loss = 0.18798822\n",
            "Iteration 3, loss = 0.18639931\n",
            "Iteration 4, loss = 0.18404324\n",
            "Iteration 5, loss = 0.18107986\n",
            "Iteration 6, loss = 0.17749965\n",
            "Iteration 7, loss = 0.17492566\n",
            "Iteration 8, loss = 0.17190752\n",
            "Iteration 9, loss = 0.16952300\n",
            "Iteration 10, loss = 0.16959342\n",
            "Iteration 11, loss = 0.16696841\n",
            "Iteration 12, loss = 0.16500017\n",
            "Iteration 13, loss = 0.16284284\n",
            "Iteration 14, loss = 0.16159727\n",
            "Iteration 15, loss = 0.16084519\n",
            "Iteration 16, loss = 0.15723677\n",
            "Iteration 17, loss = 0.15651646\n",
            "Iteration 18, loss = 0.15598026\n",
            "Iteration 19, loss = 0.15346026\n",
            "Iteration 20, loss = 0.15336487\n",
            "Iteration 21, loss = 0.15064591\n",
            "Iteration 22, loss = 0.15045207\n",
            "Iteration 23, loss = 0.14810981\n",
            "Iteration 24, loss = 0.14629791\n",
            "Iteration 25, loss = 0.14565425\n",
            "Iteration 26, loss = 0.14528725\n",
            "Iteration 27, loss = 0.14419418\n",
            "Iteration 28, loss = 0.14205487\n",
            "Iteration 29, loss = 0.14010928\n",
            "Iteration 30, loss = 0.13732525\n",
            "Iteration 31, loss = 0.13829194\n",
            "Iteration 32, loss = 0.13639744\n",
            "Iteration 33, loss = 0.13617940\n",
            "Iteration 34, loss = 0.13374621\n",
            "Iteration 35, loss = 0.13416980\n",
            "Iteration 36, loss = 0.13309031\n",
            "Iteration 37, loss = 0.13355220\n",
            "Iteration 38, loss = 0.13369181\n",
            "Iteration 39, loss = 0.13024980\n",
            "Iteration 40, loss = 0.12909234\n",
            "Iteration 41, loss = 0.12883581\n",
            "Iteration 42, loss = 0.12740676\n",
            "Iteration 43, loss = 0.12653888\n",
            "Iteration 44, loss = 0.12685495\n",
            "Iteration 45, loss = 0.12736130\n",
            "Iteration 46, loss = 0.12592337\n",
            "Iteration 47, loss = 0.12554618\n",
            "Iteration 48, loss = 0.12664790\n",
            "Iteration 49, loss = 0.12168686\n",
            "Iteration 50, loss = 0.12123225\n",
            "Iteration 51, loss = 0.12271321\n",
            "Iteration 52, loss = 0.11949528\n",
            "Iteration 53, loss = 0.12151285\n",
            "Iteration 54, loss = 0.11931432\n",
            "Iteration 55, loss = 0.11901574\n",
            "Iteration 56, loss = 0.11828699\n",
            "Iteration 57, loss = 0.12246631\n",
            "Iteration 58, loss = 0.11846127\n",
            "Iteration 59, loss = 0.11809590\n",
            "Iteration 60, loss = 0.11775911\n",
            "Iteration 61, loss = 0.11639794\n",
            "Iteration 62, loss = 0.11439862\n",
            "Iteration 63, loss = 0.11746013\n",
            "Iteration 64, loss = 0.11357408\n",
            "Iteration 65, loss = 0.11447684\n",
            "Iteration 66, loss = 0.11568632\n",
            "Iteration 67, loss = 0.11682985\n",
            "Iteration 68, loss = 0.11825401\n",
            "Iteration 69, loss = 0.11359400\n",
            "Iteration 70, loss = 0.11409669\n",
            "Iteration 71, loss = 0.11253422\n",
            "Iteration 72, loss = 0.10855746\n",
            "Iteration 73, loss = 0.10922572\n",
            "Iteration 74, loss = 0.10864137\n",
            "Iteration 75, loss = 0.11205925\n",
            "Iteration 76, loss = 0.11085551\n",
            "Iteration 77, loss = 0.10860165\n",
            "Iteration 78, loss = 0.10815044\n",
            "Iteration 79, loss = 0.11474979\n",
            "Iteration 80, loss = 0.11230962\n",
            "Iteration 81, loss = 0.11164427\n",
            "Iteration 82, loss = 0.10884095\n",
            "Iteration 83, loss = 0.10686852\n",
            "Iteration 84, loss = 0.10434180\n",
            "Iteration 85, loss = 0.10767341\n",
            "Iteration 86, loss = 0.10681486\n",
            "Iteration 87, loss = 0.10895138\n",
            "Iteration 88, loss = 0.10935591\n",
            "Iteration 89, loss = 0.10449087\n",
            "Iteration 90, loss = 0.10715880\n",
            "Iteration 91, loss = 0.10684698\n",
            "Iteration 92, loss = 0.10780559\n",
            "Iteration 93, loss = 0.10419662\n",
            "Iteration 94, loss = 0.10598981\n",
            "Iteration 95, loss = 0.10720594\n",
            "Iteration 96, loss = 0.11286084\n",
            "Iteration 97, loss = 0.11124529\n",
            "Iteration 98, loss = 0.10774646\n",
            "Iteration 99, loss = 0.10429518\n",
            "Iteration 100, loss = 0.10413424\n",
            "Iteration 101, loss = 0.10859484\n",
            "Iteration 102, loss = 0.10736979\n",
            "Iteration 103, loss = 0.10279899\n",
            "Iteration 104, loss = 0.10123222\n",
            "Iteration 105, loss = 0.10357461\n",
            "Iteration 106, loss = 0.10539019\n",
            "Iteration 107, loss = 0.10545880\n",
            "Iteration 108, loss = 0.10281886\n",
            "Iteration 109, loss = 0.10320940\n",
            "Iteration 110, loss = 0.10251438\n",
            "Iteration 111, loss = 0.10344134\n",
            "Iteration 112, loss = 0.10352848\n",
            "Iteration 113, loss = 0.10456444\n",
            "Iteration 114, loss = 0.10164151\n",
            "Iteration 115, loss = 0.10083304\n",
            "Iteration 116, loss = 0.10063879\n",
            "Iteration 117, loss = 0.10445178\n",
            "Iteration 118, loss = 0.10108308\n",
            "Iteration 119, loss = 0.10309746\n",
            "Iteration 120, loss = 0.10443278\n",
            "Iteration 121, loss = 0.10260093\n",
            "Iteration 122, loss = 0.10498784\n",
            "Iteration 123, loss = 0.10390483\n",
            "Iteration 124, loss = 0.10459914\n",
            "Iteration 125, loss = 0.10106766\n",
            "Iteration 126, loss = 0.10307298\n",
            "Iteration 127, loss = 0.10459829\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.21386922\n",
            "Iteration 2, loss = 0.18768335\n",
            "Iteration 3, loss = 0.18522111\n",
            "Iteration 4, loss = 0.18109525\n",
            "Iteration 5, loss = 0.17903250\n",
            "Iteration 6, loss = 0.17654456\n",
            "Iteration 7, loss = 0.17497974\n",
            "Iteration 8, loss = 0.17120871\n",
            "Iteration 9, loss = 0.16998307\n",
            "Iteration 10, loss = 0.16850936\n",
            "Iteration 11, loss = 0.16665111\n",
            "Iteration 12, loss = 0.16375132\n",
            "Iteration 13, loss = 0.16206891\n",
            "Iteration 14, loss = 0.15989898\n",
            "Iteration 15, loss = 0.15839705\n",
            "Iteration 16, loss = 0.15777167\n",
            "Iteration 17, loss = 0.15675919\n",
            "Iteration 18, loss = 0.15309941\n",
            "Iteration 19, loss = 0.15017446\n",
            "Iteration 20, loss = 0.14948242\n",
            "Iteration 21, loss = 0.14862868\n",
            "Iteration 22, loss = 0.14689978\n",
            "Iteration 23, loss = 0.14714028\n",
            "Iteration 24, loss = 0.14241708\n",
            "Iteration 25, loss = 0.14244396\n",
            "Iteration 26, loss = 0.13976271\n",
            "Iteration 27, loss = 0.13800957\n",
            "Iteration 28, loss = 0.13905119\n",
            "Iteration 29, loss = 0.13457200\n",
            "Iteration 30, loss = 0.13601865\n",
            "Iteration 31, loss = 0.13585911\n",
            "Iteration 32, loss = 0.13336438\n",
            "Iteration 33, loss = 0.13298223\n",
            "Iteration 34, loss = 0.13073476\n",
            "Iteration 35, loss = 0.12891817\n",
            "Iteration 36, loss = 0.12721750\n",
            "Iteration 37, loss = 0.12817115\n",
            "Iteration 38, loss = 0.12537207\n",
            "Iteration 39, loss = 0.12666357\n",
            "Iteration 40, loss = 0.12278039\n",
            "Iteration 41, loss = 0.12402963\n",
            "Iteration 42, loss = 0.12623479\n",
            "Iteration 43, loss = 0.12709687\n",
            "Iteration 44, loss = 0.12423476\n",
            "Iteration 45, loss = 0.12282443\n",
            "Iteration 46, loss = 0.12018761\n",
            "Iteration 47, loss = 0.12148443\n",
            "Iteration 48, loss = 0.11921865\n",
            "Iteration 49, loss = 0.12000207\n",
            "Iteration 50, loss = 0.11744116\n",
            "Iteration 51, loss = 0.11635678\n",
            "Iteration 52, loss = 0.11644541\n",
            "Iteration 53, loss = 0.11899593\n",
            "Iteration 54, loss = 0.11399590\n",
            "Iteration 55, loss = 0.11555181\n",
            "Iteration 56, loss = 0.11348783\n",
            "Iteration 57, loss = 0.11343565\n",
            "Iteration 58, loss = 0.11146982\n",
            "Iteration 59, loss = 0.11382188\n",
            "Iteration 60, loss = 0.11442029\n",
            "Iteration 61, loss = 0.11177314\n",
            "Iteration 62, loss = 0.10998531\n",
            "Iteration 63, loss = 0.11384670\n",
            "Iteration 64, loss = 0.11708160\n",
            "Iteration 65, loss = 0.11578165\n",
            "Iteration 66, loss = 0.11294544\n",
            "Iteration 67, loss = 0.11038791\n",
            "Iteration 68, loss = 0.10928343\n",
            "Iteration 69, loss = 0.10628966\n",
            "Iteration 70, loss = 0.10746800\n",
            "Iteration 71, loss = 0.11094074\n",
            "Iteration 72, loss = 0.11013288\n",
            "Iteration 73, loss = 0.11019040\n",
            "Iteration 74, loss = 0.10773469\n",
            "Iteration 75, loss = 0.10655588\n",
            "Iteration 76, loss = 0.10612828\n",
            "Iteration 77, loss = 0.10617373\n",
            "Iteration 78, loss = 0.10365508\n",
            "Iteration 79, loss = 0.10121256\n",
            "Iteration 80, loss = 0.10531701\n",
            "Iteration 81, loss = 0.10533955\n",
            "Iteration 82, loss = 0.10809844\n",
            "Iteration 83, loss = 0.10644155\n",
            "Iteration 84, loss = 0.11071845\n",
            "Iteration 85, loss = 0.10800725\n",
            "Iteration 86, loss = 0.10866556\n",
            "Iteration 87, loss = 0.10095446\n",
            "Iteration 88, loss = 0.10422617\n",
            "Iteration 89, loss = 0.10172329\n",
            "Iteration 90, loss = 0.10385808\n",
            "Iteration 91, loss = 0.10059007\n",
            "Iteration 92, loss = 0.10111210\n",
            "Iteration 93, loss = 0.10409750\n",
            "Iteration 94, loss = 0.10389928\n",
            "Iteration 95, loss = 0.10364294\n",
            "Iteration 96, loss = 0.10173021\n",
            "Iteration 97, loss = 0.10025771\n",
            "Iteration 98, loss = 0.10181232\n",
            "Iteration 99, loss = 0.10342285\n",
            "Iteration 100, loss = 0.10167708\n",
            "Iteration 101, loss = 0.10440664\n",
            "Iteration 102, loss = 0.10309121\n",
            "Iteration 103, loss = 0.10297374\n",
            "Iteration 104, loss = 0.10204229\n",
            "Iteration 105, loss = 0.09817448\n",
            "Iteration 106, loss = 0.10142699\n",
            "Iteration 107, loss = 0.10042541\n",
            "Iteration 108, loss = 0.10071190\n",
            "Iteration 109, loss = 0.10241804\n",
            "Iteration 110, loss = 0.09752423\n",
            "Iteration 111, loss = 0.09762600\n",
            "Iteration 112, loss = 0.10119423\n",
            "Iteration 113, loss = 0.10142690\n",
            "Iteration 114, loss = 0.09772195\n",
            "Iteration 115, loss = 0.09728612\n",
            "Iteration 116, loss = 0.10114249\n",
            "Iteration 117, loss = 0.10152413\n",
            "Iteration 118, loss = 0.09847683\n",
            "Iteration 119, loss = 0.09827768\n",
            "Iteration 120, loss = 0.09723917\n",
            "Iteration 121, loss = 0.09948558\n",
            "Iteration 122, loss = 0.09817378\n",
            "Iteration 123, loss = 0.10059408\n",
            "Iteration 124, loss = 0.10129295\n",
            "Iteration 125, loss = 0.10215198\n",
            "Iteration 126, loss = 0.09867264\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.21928512\n",
            "Iteration 2, loss = 0.18812229\n",
            "Iteration 3, loss = 0.18391140\n",
            "Iteration 4, loss = 0.18069580\n",
            "Iteration 5, loss = 0.17945403\n",
            "Iteration 6, loss = 0.17742684\n",
            "Iteration 7, loss = 0.17499208\n",
            "Iteration 8, loss = 0.17497053\n",
            "Iteration 9, loss = 0.17091680\n",
            "Iteration 10, loss = 0.17006840\n",
            "Iteration 11, loss = 0.16699364\n",
            "Iteration 12, loss = 0.16496568\n",
            "Iteration 13, loss = 0.16542873\n",
            "Iteration 14, loss = 0.16262554\n",
            "Iteration 15, loss = 0.16321526\n",
            "Iteration 16, loss = 0.15846273\n",
            "Iteration 17, loss = 0.15839389\n",
            "Iteration 18, loss = 0.15646370\n",
            "Iteration 19, loss = 0.15641830\n",
            "Iteration 20, loss = 0.15327813\n",
            "Iteration 21, loss = 0.15005123\n",
            "Iteration 22, loss = 0.14681222\n",
            "Iteration 23, loss = 0.14710609\n",
            "Iteration 24, loss = 0.14650923\n",
            "Iteration 25, loss = 0.14535172\n",
            "Iteration 26, loss = 0.14244873\n",
            "Iteration 27, loss = 0.14292102\n",
            "Iteration 28, loss = 0.14080149\n",
            "Iteration 29, loss = 0.13878727\n",
            "Iteration 30, loss = 0.13835992\n",
            "Iteration 31, loss = 0.13655526\n",
            "Iteration 32, loss = 0.13521758\n",
            "Iteration 33, loss = 0.13573363\n",
            "Iteration 34, loss = 0.13269821\n",
            "Iteration 35, loss = 0.13042427\n",
            "Iteration 36, loss = 0.12927707\n",
            "Iteration 37, loss = 0.13264455\n",
            "Iteration 38, loss = 0.13082236\n",
            "Iteration 39, loss = 0.12997003\n",
            "Iteration 40, loss = 0.12793434\n",
            "Iteration 41, loss = 0.12585111\n",
            "Iteration 42, loss = 0.12601966\n",
            "Iteration 43, loss = 0.12256293\n",
            "Iteration 44, loss = 0.11990550\n",
            "Iteration 45, loss = 0.12061269\n",
            "Iteration 46, loss = 0.12085651\n",
            "Iteration 47, loss = 0.11974442\n",
            "Iteration 48, loss = 0.12049613\n",
            "Iteration 49, loss = 0.11907274\n",
            "Iteration 50, loss = 0.11710065\n",
            "Iteration 51, loss = 0.11709400\n",
            "Iteration 52, loss = 0.11582964\n",
            "Iteration 53, loss = 0.11135864\n",
            "Iteration 54, loss = 0.11264457\n",
            "Iteration 55, loss = 0.11155459\n",
            "Iteration 56, loss = 0.11251354\n",
            "Iteration 57, loss = 0.11268163\n",
            "Iteration 58, loss = 0.10793746\n",
            "Iteration 59, loss = 0.10847808\n",
            "Iteration 60, loss = 0.10966326\n",
            "Iteration 61, loss = 0.10851525\n",
            "Iteration 62, loss = 0.10852861\n",
            "Iteration 63, loss = 0.10705422\n",
            "Iteration 64, loss = 0.10878480\n",
            "Iteration 65, loss = 0.10731807\n",
            "Iteration 66, loss = 0.10924487\n",
            "Iteration 67, loss = 0.10812016\n",
            "Iteration 68, loss = 0.10540516\n",
            "Iteration 69, loss = 0.10657825\n",
            "Iteration 70, loss = 0.10712679\n",
            "Iteration 71, loss = 0.10589279\n",
            "Iteration 72, loss = 0.10447832\n",
            "Iteration 73, loss = 0.10249483\n",
            "Iteration 74, loss = 0.10360354\n",
            "Iteration 75, loss = 0.10186155\n",
            "Iteration 76, loss = 0.10171634\n",
            "Iteration 77, loss = 0.10215464\n",
            "Iteration 78, loss = 0.10444079\n",
            "Iteration 79, loss = 0.10488008\n",
            "Iteration 80, loss = 0.10580842\n",
            "Iteration 81, loss = 0.10359322\n",
            "Iteration 82, loss = 0.10070462\n",
            "Iteration 83, loss = 0.09972733\n",
            "Iteration 84, loss = 0.10180663\n",
            "Iteration 85, loss = 0.09946552\n",
            "Iteration 86, loss = 0.10009035\n",
            "Iteration 87, loss = 0.09780345\n",
            "Iteration 88, loss = 0.09786952\n",
            "Iteration 89, loss = 0.09743662\n",
            "Iteration 90, loss = 0.09915287\n",
            "Iteration 91, loss = 0.10202832\n",
            "Iteration 92, loss = 0.09980866\n",
            "Iteration 93, loss = 0.09915011\n",
            "Iteration 94, loss = 0.09981240\n",
            "Iteration 95, loss = 0.09851810\n",
            "Iteration 96, loss = 0.09630780\n",
            "Iteration 97, loss = 0.10264251\n",
            "Iteration 98, loss = 0.10084119\n",
            "Iteration 99, loss = 0.10257030\n",
            "Iteration 100, loss = 0.09639699\n",
            "Iteration 101, loss = 0.09759574\n",
            "Iteration 102, loss = 0.09453805\n",
            "Iteration 103, loss = 0.09833563\n",
            "Iteration 104, loss = 0.10043693\n",
            "Iteration 105, loss = 0.10131311\n",
            "Iteration 106, loss = 0.09693418\n",
            "Iteration 107, loss = 0.09488296\n",
            "Iteration 108, loss = 0.09474239\n",
            "Iteration 109, loss = 0.09620161\n",
            "Iteration 110, loss = 0.09586660\n",
            "Iteration 111, loss = 0.09493976\n",
            "Iteration 112, loss = 0.09493616\n",
            "Iteration 113, loss = 0.09897268\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(np.array(X_train),y_train.ravel().astype(np.int64), test_size=0.20)\n",
        "hyperNN(X_train, y_train, X_test, y_test,title=\"Model Complexity Curve for NN (Banking Data)\\nHyperparameter : No. Hidden Units\")\n",
        "h_units, learn_rate = NNGridSearchCV(X_train, y_train)\n",
        "# estimator_bank = MLPClassifier(hidden_layer_sizes=(h_units,), solver='adam', activation='logistic', \n",
        "#                                learning_rate_init=learn_rate, random_state=100,verbose=True)\n",
        "# train_samp_bank, NN_train_score_bank, NN_fit_time_bank, NN_pred_time_bank = plot_learning_curve(estimator_bank, X_train, y_train,title=\"Neural Net Banking Data\")\n",
        "# final_classifier_evaluation(estimator_bank, X_train, X_test, y_train, y_test)\n",
        "                               "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bd116f5",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0982225c-2a68-4c28-8555-e702ff89117d",
      "metadata": {
        "id": "0982225c-2a68-4c28-8555-e702ff89117d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  epoch    train_acc    train_loss    valid_acc    valid_loss     dur\n",
            "-------  -----------  ------------  -----------  ------------  ------\n",
            "      1       \u001b[36m0.8876\u001b[0m        \u001b[32m0.4706\u001b[0m       \u001b[35m0.8877\u001b[0m        \u001b[31m0.4473\u001b[0m  0.6787\n",
            "      2       0.8876        \u001b[32m0.4414\u001b[0m       0.8877        \u001b[31m0.4372\u001b[0m  0.5677\n",
            "      3       0.8876        \u001b[32m0.4351\u001b[0m       0.8877        \u001b[31m0.4334\u001b[0m  0.5485\n",
            "      4       0.8876        \u001b[32m0.4324\u001b[0m       0.8877        \u001b[31m0.4315\u001b[0m  0.5353\n",
            "      5       0.8876        \u001b[32m0.4309\u001b[0m       0.8877        \u001b[31m0.4303\u001b[0m  0.7135\n",
            "      6       0.8876        \u001b[32m0.4299\u001b[0m       0.8877        \u001b[31m0.4295\u001b[0m  0.5399\n",
            "      7       0.8876        \u001b[32m0.4292\u001b[0m       0.8877        \u001b[31m0.4289\u001b[0m  0.5743\n",
            "      8       0.8876        \u001b[32m0.4287\u001b[0m       0.8877        \u001b[31m0.4285\u001b[0m  0.5443\n",
            "      9       0.8876        \u001b[32m0.4283\u001b[0m       0.8877        \u001b[31m0.4281\u001b[0m  0.5624\n",
            "     10       0.8876        \u001b[32m0.4280\u001b[0m       0.8877        \u001b[31m0.4279\u001b[0m  0.5334\n",
            "     11       0.8876        \u001b[32m0.4278\u001b[0m       0.8877        \u001b[31m0.4276\u001b[0m  0.5372\n",
            "     12       0.8876        \u001b[32m0.4276\u001b[0m       0.8877        \u001b[31m0.4275\u001b[0m  0.5355\n",
            "     13       0.8876        \u001b[32m0.4274\u001b[0m       0.8877        \u001b[31m0.4273\u001b[0m  0.5684\n",
            "     14       0.8876        \u001b[32m0.4273\u001b[0m       0.8877        \u001b[31m0.4272\u001b[0m  0.5648\n",
            "     15       0.8876        \u001b[32m0.4272\u001b[0m       0.8877        \u001b[31m0.4271\u001b[0m  0.5704\n",
            "     16       0.8876        \u001b[32m0.4271\u001b[0m       0.8877        \u001b[31m0.4270\u001b[0m  0.7881\n",
            "     17       0.8876        \u001b[32m0.4270\u001b[0m       0.8877        \u001b[31m0.4269\u001b[0m  0.5614\n",
            "     18       0.8876        \u001b[32m0.4269\u001b[0m       0.8877        \u001b[31m0.4268\u001b[0m  0.5409\n",
            "     19       0.8876        \u001b[32m0.4268\u001b[0m       0.8877        \u001b[31m0.4267\u001b[0m  0.5415\n",
            "     20       0.8876        \u001b[32m0.4268\u001b[0m       0.8877        \u001b[31m0.4267\u001b[0m  0.5744\n",
            "     21       0.8876        \u001b[32m0.4267\u001b[0m       0.8877        \u001b[31m0.4266\u001b[0m  0.5430\n",
            "     22       0.8876        \u001b[32m0.4266\u001b[0m       0.8877        \u001b[31m0.4266\u001b[0m  0.5480\n",
            "     23       0.8876        \u001b[32m0.4266\u001b[0m       0.8877        \u001b[31m0.4265\u001b[0m  0.6875\n",
            "     24       0.8876        \u001b[32m0.4265\u001b[0m       0.8877        \u001b[31m0.4265\u001b[0m  0.6122\n",
            "     25       0.8876        \u001b[32m0.4265\u001b[0m       0.8877        \u001b[31m0.4265\u001b[0m  0.5598\n",
            "     26       0.8876        \u001b[32m0.4265\u001b[0m       0.8877        \u001b[31m0.4264\u001b[0m  0.5685\n",
            "     27       0.8876        \u001b[32m0.4264\u001b[0m       0.8877        \u001b[31m0.4264\u001b[0m  0.5732\n",
            "     28       0.8876        \u001b[32m0.4264\u001b[0m       0.8877        \u001b[31m0.4264\u001b[0m  0.5446\n",
            "     29       0.8876        \u001b[32m0.4264\u001b[0m       0.8877        \u001b[31m0.4263\u001b[0m  0.5352\n",
            "     30       0.8876        \u001b[32m0.4264\u001b[0m       0.8877        \u001b[31m0.4263\u001b[0m  0.5727\n",
            "     31       0.8876        \u001b[32m0.4263\u001b[0m       0.8877        \u001b[31m0.4263\u001b[0m  0.5345\n",
            "     32       0.8876        \u001b[32m0.4263\u001b[0m       0.8877        \u001b[31m0.4263\u001b[0m  0.5696\n",
            "     33       0.8876        \u001b[32m0.4263\u001b[0m       0.8877        \u001b[31m0.4262\u001b[0m  0.6265\n",
            "     34       0.8876        \u001b[32m0.4263\u001b[0m       0.8877        \u001b[31m0.4262\u001b[0m  0.5979\n",
            "     35       0.8876        \u001b[32m0.4262\u001b[0m       0.8877        \u001b[31m0.4262\u001b[0m  0.5464\n",
            "     36       0.8876        \u001b[32m0.4262\u001b[0m       0.8877        \u001b[31m0.4262\u001b[0m  0.5408\n",
            "     37       0.8876        \u001b[32m0.4262\u001b[0m       0.8877        \u001b[31m0.4262\u001b[0m  0.5538\n",
            "     38       0.8876        \u001b[32m0.4262\u001b[0m       0.8877        \u001b[31m0.4261\u001b[0m  0.5325\n",
            "     39       0.8876        \u001b[32m0.4262\u001b[0m       0.8877        \u001b[31m0.4261\u001b[0m  0.5406\n",
            "     40       0.8876        \u001b[32m0.4262\u001b[0m       0.8877        \u001b[31m0.4261\u001b[0m  0.6122\n",
            "     41       0.8876        \u001b[32m0.4261\u001b[0m       0.8877        \u001b[31m0.4261\u001b[0m  0.5621\n",
            "     42       0.8876        \u001b[32m0.4261\u001b[0m       0.8877        \u001b[31m0.4261\u001b[0m  0.6529\n",
            "     43       0.8876        \u001b[32m0.4261\u001b[0m       0.8877        \u001b[31m0.4261\u001b[0m  0.5798\n",
            "     44       0.8876        \u001b[32m0.4261\u001b[0m       0.8877        \u001b[31m0.4261\u001b[0m  0.5661\n",
            "     45       0.8876        \u001b[32m0.4261\u001b[0m       0.8877        \u001b[31m0.4261\u001b[0m  0.5380\n",
            "     46       0.8876        \u001b[32m0.4261\u001b[0m       0.8877        \u001b[31m0.4260\u001b[0m  0.5350\n",
            "     47       0.8876        \u001b[32m0.4261\u001b[0m       0.8877        \u001b[31m0.4260\u001b[0m  0.5360\n",
            "     48       0.8876        \u001b[32m0.4261\u001b[0m       0.8877        \u001b[31m0.4260\u001b[0m  0.5594\n",
            "     49       0.8876        \u001b[32m0.4261\u001b[0m       0.8877        \u001b[31m0.4260\u001b[0m  0.5411\n",
            "     50       0.8876        \u001b[32m0.4260\u001b[0m       0.8877        \u001b[31m0.4260\u001b[0m  0.5281\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_activation_fn(name):\n",
        "    if name == 'relu':\n",
        "        return nn.ReLU()\n",
        "    elif name == 'sigmoid':\n",
        "        return nn.Sigmoid()\n",
        "    elif name == 'tanh':\n",
        "        return nn.Tanh()\n",
        "    elif name == 'leaky_relu':\n",
        "        return nn.LeakyReLU()\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown activation function: {name}\")\n",
        "    \n",
        "#f1_scorer = make_scorer(f1_with_zero_division, greater_is_better=True)\n",
        "# precision_scorer = make_scorer(precision_with_zero_division, greater_is_better=True)\n",
        "\n",
        "# precision_callback = EpochScoring(\n",
        "#     scoring=precision_scorer,\n",
        "#     name='precision',\n",
        "#     lower_is_better=False\n",
        "# )\n",
        "\n",
        "# def recall_with_zero_division(y_true, y_pred):\n",
        "#     return recall_score(y_true, y_pred, zero_division=1)\n",
        "\n",
        "#def f1_with_zero_division(y_true, y_pred):\n",
        "#    return f1_score(y_true, y_pred)\n",
        "\n",
        "\n",
        "# precision_callback = EpochScoring(\n",
        "#     scoring=precision_with_zero_division,\n",
        "#     name='precision',\n",
        "#     lower_is_better=False\n",
        "# )\n",
        "\n",
        "# recall_callback = EpochScoring(\n",
        "#     scoring=recall_with_zero_division,\n",
        "#     name='recall',\n",
        "#     lower_is_better=False\n",
        "# )\n",
        "\n",
        "#f1_callback = EpochScoring(\n",
        "#    scoring=f1_with_zero_division,\n",
        "#    name='f1',\n",
        "#    #lower_is_better=False\n",
        "#)\n",
        "    \n",
        "activation_fn_name = 'sigmoid'\n",
        "activation_fn = get_activation_fn(activation_fn_name)\n",
        "auc = EpochScoring(scoring='roc_auc', lower_is_better=False,on_train=True)\n",
        "precision = EpochScoring(scoring='precision', lower_is_better=False, on_train=True)\n",
        "recall = EpochScoring(scoring='recall', lower_is_better=False, on_train=True)\n",
        "\n",
        "net = NeuralNetClassifier(\n",
        "    module=BackpropModule,\n",
        "    module__input_dim=X1.shape[1],\n",
        "    module__output_dim=2,\n",
        "    module__hidden_units=1,\n",
        "    module__hidden_layers=1,\n",
        "    module__activation=activation_fn,\n",
        "    module__dropout_percent=0.5,\n",
        "    max_epochs=50,\n",
        "    verbose=1,\n",
        "    callbacks=[#EpochScoring(scoring='precision', name='train_precision', on_train=True),\n",
        "               \n",
        "               EpochScoring(scoring='accuracy', name='train_acc', on_train=True),\n",
        "               EarlyStopping(monitor='valid_loss', patience=10),\n",
        "               #f1_callback,\n",
        "               \n",
        "               ],\n",
        "    criterion=nn.CrossEntropyLoss,\n",
        "    #criterion__weight=class_weights,\n",
        "    optimizer=optim.SGD,\n",
        "    lr=.05,\n",
        "    # Shuffle training data on each epoch\n",
        "    iterator_train__shuffle=True,\n",
        ")\n",
        "net.fit(X1.values.astype(np.float32), Y1.values.ravel().astype(np.int64))\n",
        "y_proba = net.predict_proba(X1.values.astype(np.float32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee1d2b18",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Dataset2\n",
        "\n",
        "def get_activation_fn(name):\n",
        "    if name == 'relu':\n",
        "        return nn.ReLU()\n",
        "    elif name == 'sigmoid':\n",
        "        return nn.Sigmoid()\n",
        "    elif name == 'tanh':\n",
        "        return nn.Tanh()\n",
        "    elif name == 'leaky_relu':\n",
        "        return nn.LeakyReLU()\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown activation function: {name}\")\n",
        "    \n",
        "#f1_scorer = make_scorer(f1_with_zero_division, greater_is_better=True)\n",
        "# precision_scorer = make_scorer(precision_with_zero_division, greater_is_better=True)\n",
        "\n",
        "# precision_callback = EpochScoring(\n",
        "#     scoring=precision_scorer,\n",
        "#     name='precision',\n",
        "#     lower_is_better=False\n",
        "# )\n",
        "\n",
        "# def recall_with_zero_division(y_true, y_pred):\n",
        "#     return recall_score(y_true, y_pred, zero_division=1)\n",
        "\n",
        "#def f1_with_zero_division(y_true, y_pred):\n",
        "#    return f1_score(y_true, y_pred)\n",
        "\n",
        "\n",
        "# precision_callback = EpochScoring(\n",
        "#     scoring=precision_with_zero_division,\n",
        "#     name='precision',\n",
        "#     lower_is_better=False\n",
        "# )\n",
        "\n",
        "# recall_callback = EpochScoring(\n",
        "#     scoring=recall_with_zero_division,\n",
        "#     name='recall',\n",
        "#     lower_is_better=False\n",
        "# )\n",
        "\n",
        "#f1_callback = EpochScoring(\n",
        "#    scoring=f1_with_zero_division,\n",
        "#    name='f1',\n",
        "#    #lower_is_better=False\n",
        "#)\n",
        "    \n",
        "activation_fn_name = 'sigmoid'\n",
        "activation_fn = get_activation_fn(activation_fn_name)\n",
        "auc = EpochScoring(scoring='roc_auc', lower_is_better=False,on_train=True)\n",
        "precision = EpochScoring(scoring='precision', lower_is_better=False, on_train=True)\n",
        "recall = EpochScoring(scoring='recall', lower_is_better=False, on_train=True)\n",
        "\n",
        "net = NeuralNetClassifier(\n",
        "    module=BackpropModule,\n",
        "    module__input_dim=X2.shape[1],\n",
        "    module__output_dim=2,\n",
        "    module__hidden_units=1,\n",
        "    module__hidden_layers=1,\n",
        "    module__activation=activation_fn,\n",
        "    module__dropout_percent=0,\n",
        "    max_epochs=200,\n",
        "    verbose=1,\n",
        "    callbacks=[#EpochScoring(scoring='precision', name='train_precision', on_train=True),\n",
        "               \n",
        "               EpochScoring(scoring='accuracy', name='train_acc', on_train=True),\n",
        "               EarlyStopping(monitor='valid_loss', patience=10),\n",
        "               #f1_callback,\n",
        "               \n",
        "               ],\n",
        "    criterion=nn.CrossEntropyLoss,\n",
        "    #criterion__weight=class_weights,\n",
        "    optimizer=optim.SGD,\n",
        "    lr=.05,\n",
        "    # Shuffle training data on each epoch\n",
        "    iterator_train__shuffle=True,\n",
        ")\n",
        "net.fit(X2.values.astype(np.float32), Y2.values.ravel().astype(np.int64))\n",
        "y_proba = net.predict_proba(X2.values.astype(np.float32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dda034a2",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "28af04f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "history = net.history\n",
        "\n",
        "# Convert the history to a DataFrame\n",
        "df_history = pd.DataFrame(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1b9d45a-652b-4279-9226-1c39461f91be",
      "metadata": {
        "id": "c1b9d45a-652b-4279-9226-1c39461f91be"
      },
      "source": [
        "## Plot learning curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "670816f1-cc05-4827-89ec-c7ea968e33af",
      "metadata": {
        "id": "670816f1-cc05-4827-89ec-c7ea968e33af",
        "outputId": "97422df4-283b-41ed-9b94-12728d06db02"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x730555779ff0>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#10 Iterations\n",
        "# plot the iterative learning curve (loss)\n",
        "plt.plot(net.history[:, 'train_loss'], label='Train Loss', color='navy')\n",
        "plt.plot(net.history[:, 'valid_loss'], label='Validation Loss', color='lightcoral')\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Iterative Learning Curve (Loss)\")\n",
        "plt.grid(visible=True)\n",
        "plt.legend(frameon=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3f8fd252-6841-4b8b-b7c6-b879a98d667f",
      "metadata": {
        "id": "3f8fd252-6841-4b8b-b7c6-b879a98d667f",
        "outputId": "dc774f44-1c28-461b-bd90-6b07f4064a4c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x730555605000>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plot the iterative learning curve (accuracy)\n",
        "plt.plot(net.history[:, 'train_acc'], label='Train Acc', color='cornflowerblue')\n",
        "plt.plot(net.history[:, 'valid_acc'], label='Validation Acc', color='chartreuse')\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Iterative Learning Curve (Accuracy)\")\n",
        "plt.grid(visible=True)\n",
        "plt.legend(frameon=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "634ba876",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "06a1a573-8543-4eab-a763-c45859561633",
      "metadata": {
        "id": "06a1a573-8543-4eab-a763-c45859561633"
      },
      "source": [
        "## Using sklearn learning curve with RO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "938e4dd3-565e-4ae5-b95e-449b87798379",
      "metadata": {
        "id": "938e4dd3-565e-4ae5-b95e-449b87798379",
        "outputId": "79bb2046-1e57-426c-aa1c-9d3dbb1cf720"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  epoch    train_acc    train_loss    valid_acc    valid_loss     dur\n",
            "-------  -----------  ------------  -----------  ------------  ------\n",
            "      1       \u001b[36m0.8833\u001b[0m        \u001b[32m0.6389\u001b[0m       \u001b[35m0.9000\u001b[0m        \u001b[31m0.6322\u001b[0m  0.0030\n",
            "      2       0.8833        \u001b[32m0.6335\u001b[0m       0.9000        \u001b[31m0.6291\u001b[0m  0.0029\n",
            "      3       0.8833        \u001b[32m0.6288\u001b[0m       0.9000        \u001b[31m0.6260\u001b[0m  0.0029\n",
            "      4       0.8833        0.6321       0.9000        \u001b[31m0.6229\u001b[0m  0.0030\n",
            "      5       0.8833        0.6292       0.9000        \u001b[31m0.6199\u001b[0m  0.0030\n",
            "      6       0.8833        \u001b[32m0.6226\u001b[0m       0.9000        \u001b[31m0.6170\u001b[0m  0.0027\n",
            "      7       0.8833        \u001b[32m0.6174\u001b[0m       0.9000        \u001b[31m0.6141\u001b[0m  0.0043\n",
            "      8       0.8833        0.6176       0.9000        \u001b[31m0.6112\u001b[0m  0.0028\n",
            "      9       0.8833        \u001b[32m0.6149\u001b[0m       0.9000        \u001b[31m0.6084\u001b[0m  0.0066\n",
            "     10       0.8833        \u001b[32m0.6139\u001b[0m       0.9000        \u001b[31m0.6057\u001b[0m  0.0026\n",
            "     11       0.8833        \u001b[32m0.6103\u001b[0m       0.9000        \u001b[31m0.6030\u001b[0m  0.0026\n",
            "     12       0.8833        0.6128       0.9000        \u001b[31m0.6004\u001b[0m  0.0031\n",
            "     13       0.8833        \u001b[32m0.6037\u001b[0m       0.9000        \u001b[31m0.5978\u001b[0m  0.0026\n",
            "     14       0.8833        \u001b[32m0.5992\u001b[0m       0.9000        \u001b[31m0.5953\u001b[0m  0.0025\n",
            "     15       0.8833        \u001b[32m0.5990\u001b[0m       0.9000        \u001b[31m0.5928\u001b[0m  0.0028\n",
            "     16       0.8833        \u001b[32m0.5950\u001b[0m       0.9000        \u001b[31m0.5904\u001b[0m  0.0036\n",
            "     17       0.8833        0.5972       0.9000        \u001b[31m0.5880\u001b[0m  0.0026\n",
            "     18       0.8833        \u001b[32m0.5896\u001b[0m       0.9000        \u001b[31m0.5857\u001b[0m  0.0028\n",
            "     19       0.8833        0.5962       0.9000        \u001b[31m0.5834\u001b[0m  0.0026\n",
            "     20       0.8833        \u001b[32m0.5869\u001b[0m       0.9000        \u001b[31m0.5811\u001b[0m  0.0026\n",
            "     21       0.8833        \u001b[32m0.5840\u001b[0m       0.9000        \u001b[31m0.5789\u001b[0m  0.0029\n",
            "     22       0.8833        \u001b[32m0.5788\u001b[0m       0.9000        \u001b[31m0.5768\u001b[0m  0.0026\n",
            "     23       0.8833        0.5865       0.9000        \u001b[31m0.5747\u001b[0m  0.0027\n",
            "     24       0.8833        0.5799       0.9000        \u001b[31m0.5726\u001b[0m  0.0027\n",
            "     25       0.8833        \u001b[32m0.5756\u001b[0m       0.9000        \u001b[31m0.5706\u001b[0m  0.0026\n",
            "     26       0.8833        0.5782       0.9000        \u001b[31m0.5685\u001b[0m  0.0027\n",
            "     27       0.8833        \u001b[32m0.5751\u001b[0m       0.9000        \u001b[31m0.5666\u001b[0m  0.0031\n",
            "     28       0.8833        \u001b[32m0.5729\u001b[0m       0.9000        \u001b[31m0.5647\u001b[0m  0.0025\n",
            "     29       0.8833        \u001b[32m0.5709\u001b[0m       0.9000        \u001b[31m0.5628\u001b[0m  0.0027\n",
            "     30       0.8833        \u001b[32m0.5704\u001b[0m       0.9000        \u001b[31m0.5609\u001b[0m  0.0029\n",
            "     31       0.8833        0.5706       0.9000        \u001b[31m0.5591\u001b[0m  0.0025\n",
            "     32       0.8833        \u001b[32m0.5656\u001b[0m       0.9000        \u001b[31m0.5573\u001b[0m  0.0027\n",
            "     33       0.8833        0.5667       0.9000        \u001b[31m0.5556\u001b[0m  0.0029\n",
            "     34       0.8833        \u001b[32m0.5613\u001b[0m       0.9000        \u001b[31m0.5539\u001b[0m  0.0025\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     35       0.8833        \u001b[32m0.5607\u001b[0m       0.9000        \u001b[31m0.5522\u001b[0m  0.0037\n",
            "     36       0.8833        \u001b[32m0.5563\u001b[0m       0.9000        \u001b[31m0.5505\u001b[0m  0.0062\n",
            "     37       0.8833        0.5592       0.9000        \u001b[31m0.5489\u001b[0m  0.0029\n",
            "     38       0.8833        \u001b[32m0.5557\u001b[0m       0.9000        \u001b[31m0.5473\u001b[0m  0.0032\n",
            "     39       0.8833        \u001b[32m0.5553\u001b[0m       0.9000        \u001b[31m0.5458\u001b[0m  0.0027\n",
            "     40       0.8833        \u001b[32m0.5532\u001b[0m       0.9000        \u001b[31m0.5442\u001b[0m  0.0029\n",
            "     41       0.8833        \u001b[32m0.5530\u001b[0m       0.9000        \u001b[31m0.5427\u001b[0m  0.0027\n",
            "     42       0.8833        \u001b[32m0.5487\u001b[0m       0.9000        \u001b[31m0.5413\u001b[0m  0.0027\n",
            "     43       0.8833        0.5505       0.9000        \u001b[31m0.5398\u001b[0m  0.0030\n",
            "     44       0.8833        0.5519       0.9000        \u001b[31m0.5384\u001b[0m  0.0024\n",
            "     45       0.8833        \u001b[32m0.5472\u001b[0m       0.9000        \u001b[31m0.5370\u001b[0m  0.0027\n",
            "     46       0.8833        \u001b[32m0.5465\u001b[0m       0.9000        \u001b[31m0.5356\u001b[0m  0.0030\n",
            "     47       0.8833        \u001b[32m0.5410\u001b[0m       0.9000        \u001b[31m0.5343\u001b[0m  0.0026\n",
            "     48       0.8833        0.5469       0.9000        \u001b[31m0.5330\u001b[0m  0.0028\n",
            "     49       0.8833        0.5435       0.9000        \u001b[31m0.5317\u001b[0m  0.0026\n",
            "     50       0.8833        0.5446       0.9000        \u001b[31m0.5304\u001b[0m  0.0029\n",
            "     51       0.8833        \u001b[32m0.5364\u001b[0m       0.9000        \u001b[31m0.5292\u001b[0m  0.0023\n",
            "     52       0.8833        \u001b[32m0.5336\u001b[0m       0.9000        \u001b[31m0.5280\u001b[0m  0.0026\n",
            "     53       0.8833        0.5347       0.9000        \u001b[31m0.5268\u001b[0m  0.0027\n",
            "     54       0.8833        0.5370       0.9000        \u001b[31m0.5256\u001b[0m  0.0028\n",
            "     55       0.8833        \u001b[32m0.5322\u001b[0m       0.9000        \u001b[31m0.5244\u001b[0m  0.0031\n",
            "     56       0.8833        0.5335       0.9000        \u001b[31m0.5233\u001b[0m  0.0026\n",
            "     57       0.8833        \u001b[32m0.5317\u001b[0m       0.9000        \u001b[31m0.5222\u001b[0m  0.0026\n",
            "     58       0.8833        0.5338       0.9000        \u001b[31m0.5211\u001b[0m  0.0026\n",
            "     59       0.8833        \u001b[32m0.5298\u001b[0m       0.9000        \u001b[31m0.5200\u001b[0m  0.0029\n",
            "     60       0.8833        \u001b[32m0.5282\u001b[0m       0.9000        \u001b[31m0.5189\u001b[0m  0.0068\n",
            "     61       0.8833        0.5319       0.9000        \u001b[31m0.5179\u001b[0m  0.0029\n",
            "     62       0.8833        \u001b[32m0.5253\u001b[0m       0.9000        \u001b[31m0.5168\u001b[0m  0.0029\n",
            "     63       0.8833        0.5292       0.9000        \u001b[31m0.5158\u001b[0m  0.0027\n",
            "     64       0.8833        \u001b[32m0.5250\u001b[0m       0.9000        \u001b[31m0.5148\u001b[0m  0.0030\n",
            "     65       0.8833        0.5256       0.9000        \u001b[31m0.5139\u001b[0m  0.0029\n",
            "     66       0.8833        \u001b[32m0.5245\u001b[0m       0.9000        \u001b[31m0.5129\u001b[0m  0.0028\n",
            "     67       0.8833        \u001b[32m0.5234\u001b[0m       0.9000        \u001b[31m0.5119\u001b[0m  0.0028\n",
            "     68       0.8833        0.5244       0.9000        \u001b[31m0.5110\u001b[0m  0.0026\n",
            "     69       0.8833        \u001b[32m0.5193\u001b[0m       0.9000        \u001b[31m0.5101\u001b[0m  0.0028\n",
            "     70       0.8833        0.5199       0.9000        \u001b[31m0.5092\u001b[0m  0.0027\n",
            "     71       0.8833        0.5232       0.9000        \u001b[31m0.5083\u001b[0m  0.0026\n",
            "     72       0.8833        0.5206       0.9000        \u001b[31m0.5074\u001b[0m  0.0028\n",
            "     73       0.8833        \u001b[32m0.5180\u001b[0m       0.9000        \u001b[31m0.5066\u001b[0m  0.0027\n",
            "     74       0.8833        \u001b[32m0.5153\u001b[0m       0.9000        \u001b[31m0.5058\u001b[0m  0.0025\n",
            "     75       0.8833        0.5164       0.9000        \u001b[31m0.5049\u001b[0m  0.0028\n",
            "     76       0.8833        0.5163       0.9000        \u001b[31m0.5041\u001b[0m  0.0027\n",
            "     77       0.8833        0.5170       0.9000        \u001b[31m0.5033\u001b[0m  0.0024\n",
            "     78       0.8833        \u001b[32m0.5116\u001b[0m       0.9000        \u001b[31m0.5025\u001b[0m  0.0033\n",
            "     79       0.8833        0.5131       0.9000        \u001b[31m0.5017\u001b[0m  0.0026\n",
            "     80       0.8833        \u001b[32m0.5109\u001b[0m       0.9000        \u001b[31m0.5010\u001b[0m  0.0027\n",
            "     81       0.8833        \u001b[32m0.5092\u001b[0m       0.9000        \u001b[31m0.5002\u001b[0m  0.0027\n",
            "     82       0.8833        \u001b[32m0.5087\u001b[0m       0.9000        \u001b[31m0.4995\u001b[0m  0.0027\n",
            "     83       0.8833        0.5099       0.9000        \u001b[31m0.4988\u001b[0m  0.0030\n",
            "     84       0.8833        \u001b[32m0.5069\u001b[0m       0.9000        \u001b[31m0.4981\u001b[0m  0.0033\n",
            "     85       0.8833        0.5078       0.9000        \u001b[31m0.4974\u001b[0m  0.0042\n",
            "     86       0.8833        0.5085       0.9000        \u001b[31m0.4967\u001b[0m  0.0038\n",
            "     87       0.8833        \u001b[32m0.5054\u001b[0m       0.9000        \u001b[31m0.4960\u001b[0m  0.0023\n",
            "     88       0.8833        \u001b[32m0.5049\u001b[0m       0.9000        \u001b[31m0.4954\u001b[0m  0.0025\n",
            "     89       0.8833        0.5076       0.9000        \u001b[31m0.4947\u001b[0m  0.0027\n",
            "     90       0.8833        0.5069       0.9000        \u001b[31m0.4940\u001b[0m  0.0028\n",
            "     91       0.8833        \u001b[32m0.5048\u001b[0m       0.9000        \u001b[31m0.4934\u001b[0m  0.0027\n",
            "     92       0.8833        \u001b[32m0.5032\u001b[0m       0.9000        \u001b[31m0.4928\u001b[0m  0.0027\n",
            "     93       0.8833        \u001b[32m0.5030\u001b[0m       0.9000        \u001b[31m0.4921\u001b[0m  0.0032\n",
            "     94       0.8833        \u001b[32m0.5014\u001b[0m       0.9000        \u001b[31m0.4915\u001b[0m  0.0025\n",
            "     95       0.8833        0.5037       0.9000        \u001b[31m0.4909\u001b[0m  0.0025\n",
            "     96       0.8833        0.5033       0.9000        \u001b[31m0.4903\u001b[0m  0.0030\n",
            "     97       0.8833        0.5023       0.9000        \u001b[31m0.4897\u001b[0m  0.0028\n",
            "     98       0.8833        \u001b[32m0.5005\u001b[0m       0.9000        \u001b[31m0.4891\u001b[0m  0.0044\n",
            "     99       0.8833        0.5007       0.9000        \u001b[31m0.4886\u001b[0m  0.0032\n",
            "    100       0.8833        \u001b[32m0.5001\u001b[0m       0.9000        \u001b[31m0.4880\u001b[0m  0.0031\n",
            "    101       0.8833        \u001b[32m0.4992\u001b[0m       0.9000        \u001b[31m0.4874\u001b[0m  0.0028\n",
            "    102       0.8833        \u001b[32m0.4967\u001b[0m       0.9000        \u001b[31m0.4869\u001b[0m  0.0040\n",
            "    103       0.8833        0.4993       0.9000        \u001b[31m0.4863\u001b[0m  0.0024\n",
            "    104       0.8833        0.4984       0.9000        \u001b[31m0.4858\u001b[0m  0.0027\n",
            "    105       0.8833        \u001b[32m0.4964\u001b[0m       0.9000        \u001b[31m0.4853\u001b[0m  0.0028\n",
            "    106       0.8833        \u001b[32m0.4958\u001b[0m       0.9000        \u001b[31m0.4847\u001b[0m  0.0027\n",
            "    107       0.8833        0.4988       0.9000        \u001b[31m0.4842\u001b[0m  0.0024\n",
            "    108       0.8833        \u001b[32m0.4954\u001b[0m       0.9000        \u001b[31m0.4837\u001b[0m  0.0029\n",
            "    109       0.8833        0.4971       0.9000        \u001b[31m0.4832\u001b[0m  0.0026\n",
            "    110       0.8833        0.4959       0.9000        \u001b[31m0.4827\u001b[0m  0.0029\n",
            "    111       0.8833        \u001b[32m0.4921\u001b[0m       0.9000        \u001b[31m0.4822\u001b[0m  0.0030\n",
            "    112       0.8833        0.4946       0.9000        \u001b[31m0.4817\u001b[0m  0.0027\n",
            "    113       0.8833        0.4937       0.9000        \u001b[31m0.4813\u001b[0m  0.0027\n",
            "    114       0.8833        0.4938       0.9000        \u001b[31m0.4808\u001b[0m  0.0031\n",
            "    115       0.8833        \u001b[32m0.4908\u001b[0m       0.9000        \u001b[31m0.4803\u001b[0m  0.0028\n",
            "    116       0.8833        0.4914       0.9000        \u001b[31m0.4799\u001b[0m  0.0028\n",
            "    117       0.8833        0.4921       0.9000        \u001b[31m0.4794\u001b[0m  0.0029\n",
            "    118       0.8833        0.4919       0.9000        \u001b[31m0.4790\u001b[0m  0.0026\n",
            "    119       0.8833        \u001b[32m0.4906\u001b[0m       0.9000        \u001b[31m0.4785\u001b[0m  0.0026\n",
            "    120       0.8833        \u001b[32m0.4879\u001b[0m       0.9000        \u001b[31m0.4781\u001b[0m  0.0030\n",
            "    121       0.8833        0.4929       0.9000        \u001b[31m0.4777\u001b[0m  0.0025\n",
            "    122       0.8833        0.4909       0.9000        \u001b[31m0.4772\u001b[0m  0.0028\n",
            "    123       0.8833        0.4887       0.9000        \u001b[31m0.4768\u001b[0m  0.0039\n",
            "    124       0.8833        \u001b[32m0.4868\u001b[0m       0.9000        \u001b[31m0.4764\u001b[0m  0.0028\n",
            "    125       0.8833        0.4886       0.9000        \u001b[31m0.4760\u001b[0m  0.0027\n",
            "    126       0.8833        0.4893       0.9000        \u001b[31m0.4756\u001b[0m  0.0031\n",
            "    127       0.8833        0.4870       0.9000        \u001b[31m0.4752\u001b[0m  0.0026\n",
            "    128       0.8833        \u001b[32m0.4852\u001b[0m       0.9000        \u001b[31m0.4748\u001b[0m  0.0027\n",
            "    129       0.8833        0.4865       0.9000        \u001b[31m0.4744\u001b[0m  0.0032\n",
            "    130       0.8833        0.4860       0.9000        \u001b[31m0.4740\u001b[0m  0.0038\n",
            "    131       0.8833        0.4872       0.9000        \u001b[31m0.4736\u001b[0m  0.0035\n",
            "    132       0.8833        0.4862       0.9000        \u001b[31m0.4733\u001b[0m  0.0036\n",
            "    133       0.8833        0.4886       0.9000        \u001b[31m0.4729\u001b[0m  0.0032\n",
            "    134       0.8833        \u001b[32m0.4834\u001b[0m       0.9000        \u001b[31m0.4725\u001b[0m  0.0034\n",
            "    135       0.8833        \u001b[32m0.4830\u001b[0m       0.9000        \u001b[31m0.4722\u001b[0m  0.0031\n",
            "    136       0.8833        0.4850       0.9000        \u001b[31m0.4718\u001b[0m  0.0027\n",
            "    137       0.8833        0.4838       0.9000        \u001b[31m0.4715\u001b[0m  0.0032\n",
            "    138       0.8833        0.4844       0.9000        \u001b[31m0.4711\u001b[0m  0.0028\n",
            "    139       0.8833        0.4846       0.9000        \u001b[31m0.4708\u001b[0m  0.0031\n",
            "    140       0.8833        0.4841       0.9000        \u001b[31m0.4704\u001b[0m  0.0027\n",
            "    141       0.8833        \u001b[32m0.4820\u001b[0m       0.9000        \u001b[31m0.4701\u001b[0m  0.0027\n",
            "    142       0.8833        0.4828       0.9000        \u001b[31m0.4697\u001b[0m  0.0026\n",
            "    143       0.8833        \u001b[32m0.4806\u001b[0m       0.9000        \u001b[31m0.4694\u001b[0m  0.0026\n",
            "    144       0.8833        0.4811       0.9000        \u001b[31m0.4691\u001b[0m  0.0024\n",
            "    145       0.8833        0.4810       0.9000        \u001b[31m0.4688\u001b[0m  0.0028\n",
            "    146       0.8833        0.4813       0.9000        \u001b[31m0.4684\u001b[0m  0.0024\n",
            "    147       0.8833        0.4820       0.9000        \u001b[31m0.4681\u001b[0m  0.0025\n",
            "    148       0.8833        0.4812       0.9000        \u001b[31m0.4678\u001b[0m  0.0028\n",
            "    149       0.8833        \u001b[32m0.4787\u001b[0m       0.9000        \u001b[31m0.4675\u001b[0m  0.0027\n",
            "    150       0.8833        0.4805       0.9000        \u001b[31m0.4672\u001b[0m  0.0025\n",
            "    151       0.8833        \u001b[32m0.4785\u001b[0m       0.9000        \u001b[31m0.4669\u001b[0m  0.0034\n",
            "    152       0.8833        \u001b[32m0.4783\u001b[0m       0.9000        \u001b[31m0.4666\u001b[0m  0.0040\n",
            "    153       0.8833        0.4806       0.9000        \u001b[31m0.4663\u001b[0m  0.0036\n",
            "    154       0.8833        0.4823       0.9000        \u001b[31m0.4660\u001b[0m  0.0035\n",
            "    155       0.8833        0.4811       0.9000        \u001b[31m0.4657\u001b[0m  0.0038\n",
            "    156       0.8833        0.4784       0.9000        \u001b[31m0.4654\u001b[0m  0.0040\n",
            "    157       0.8833        0.4805       0.9000        \u001b[31m0.4651\u001b[0m  0.0042\n",
            "    158       0.8833        0.4788       0.9000        \u001b[31m0.4648\u001b[0m  0.0028\n",
            "    159       0.8833        \u001b[32m0.4782\u001b[0m       0.9000        \u001b[31m0.4645\u001b[0m  0.0063\n",
            "    160       0.8833        0.4792       0.9000        \u001b[31m0.4643\u001b[0m  0.0045\n",
            "    161       0.8833        0.4797       0.9000        \u001b[31m0.4640\u001b[0m  0.0041\n",
            "    162       0.8833        \u001b[32m0.4770\u001b[0m       0.9000        \u001b[31m0.4637\u001b[0m  0.0028\n",
            "    163       0.8833        0.4779       0.9000        \u001b[31m0.4634\u001b[0m  0.0027\n",
            "    164       0.8833        0.4772       0.9000        \u001b[31m0.4632\u001b[0m  0.0029\n",
            "    165       0.8833        0.4786       0.9000        \u001b[31m0.4629\u001b[0m  0.0037\n",
            "    166       0.8833        0.4775       0.9000        \u001b[31m0.4626\u001b[0m  0.0026\n",
            "    167       0.8833        \u001b[32m0.4763\u001b[0m       0.9000        \u001b[31m0.4624\u001b[0m  0.0026\n",
            "    168       0.8833        \u001b[32m0.4745\u001b[0m       0.9000        \u001b[31m0.4621\u001b[0m  0.0033\n",
            "    169       0.8833        \u001b[32m0.4732\u001b[0m       0.9000        \u001b[31m0.4619\u001b[0m  0.0025\n",
            "    170       0.8833        0.4760       0.9000        \u001b[31m0.4616\u001b[0m  0.0028\n",
            "    171       0.8833        0.4764       0.9000        \u001b[31m0.4614\u001b[0m  0.0027\n",
            "    172       0.8833        0.4742       0.9000        \u001b[31m0.4611\u001b[0m  0.0026\n",
            "    173       0.8833        0.4745       0.9000        \u001b[31m0.4609\u001b[0m  0.0027\n",
            "    174       0.8833        \u001b[32m0.4730\u001b[0m       0.9000        \u001b[31m0.4606\u001b[0m  0.0032\n",
            "    175       0.8833        0.4751       0.9000        \u001b[31m0.4604\u001b[0m  0.0027\n",
            "    176       0.8833        0.4750       0.9000        \u001b[31m0.4602\u001b[0m  0.0027\n",
            "    177       0.8833        0.4749       0.9000        \u001b[31m0.4599\u001b[0m  0.0031\n",
            "    178       0.8833        \u001b[32m0.4722\u001b[0m       0.9000        \u001b[31m0.4597\u001b[0m  0.0026\n",
            "    179       0.8833        0.4725       0.9000        \u001b[31m0.4595\u001b[0m  0.0027\n",
            "    180       0.8833        0.4729       0.9000        \u001b[31m0.4592\u001b[0m  0.0029\n",
            "    181       0.8833        0.4732       0.9000        \u001b[31m0.4590\u001b[0m  0.0026\n",
            "    182       0.8833        \u001b[32m0.4711\u001b[0m       0.9000        \u001b[31m0.4588\u001b[0m  0.0025\n",
            "    183       0.8833        0.4731       0.9000        \u001b[31m0.4586\u001b[0m  0.0032\n",
            "    184       0.8833        0.4727       0.9000        \u001b[31m0.4583\u001b[0m  0.0025\n",
            "    185       0.8833        0.4736       0.9000        \u001b[31m0.4581\u001b[0m  0.0035\n",
            "    186       0.8833        \u001b[32m0.4704\u001b[0m       0.9000        \u001b[31m0.4579\u001b[0m  0.0037\n",
            "    187       0.8833        0.4724       0.9000        \u001b[31m0.4577\u001b[0m  0.0025\n",
            "    188       0.8833        0.4705       0.9000        \u001b[31m0.4575\u001b[0m  0.0028\n",
            "    189       0.8833        0.4720       0.9000        \u001b[31m0.4573\u001b[0m  0.0026\n",
            "    190       0.8833        0.4710       0.9000        \u001b[31m0.4571\u001b[0m  0.0028\n",
            "    191       0.8833        0.4710       0.9000        \u001b[31m0.4569\u001b[0m  0.0028\n",
            "    192       0.8833        0.4711       0.9000        \u001b[31m0.4567\u001b[0m  0.0026\n",
            "    193       0.8833        \u001b[32m0.4694\u001b[0m       0.9000        \u001b[31m0.4565\u001b[0m  0.0030\n",
            "    194       0.8833        0.4706       0.9000        \u001b[31m0.4563\u001b[0m  0.0027\n",
            "    195       0.8833        0.4698       0.9000        \u001b[31m0.4561\u001b[0m  0.0028\n",
            "    196       0.8833        0.4706       0.9000        \u001b[31m0.4559\u001b[0m  0.0027\n",
            "    197       0.8833        \u001b[32m0.4690\u001b[0m       0.9000        \u001b[31m0.4557\u001b[0m  0.0026\n",
            "    198       0.8833        0.4713       0.9000        \u001b[31m0.4555\u001b[0m  0.0030\n",
            "    199       0.8833        0.4714       0.9000        \u001b[31m0.4553\u001b[0m  0.0029\n",
            "    200       0.8833        0.4698       0.9000        \u001b[31m0.4551\u001b[0m  0.0024\n",
            "  epoch    train_acc    train_loss    valid_acc    valid_loss     dur\n",
            "-------  -----------  ------------  -----------  ------------  ------\n",
            "      1       \u001b[36m0.8849\u001b[0m        \u001b[32m0.6310\u001b[0m       \u001b[35m0.8878\u001b[0m        \u001b[31m0.6189\u001b[0m  0.0199\n",
            "      2       0.8849        \u001b[32m0.6166\u001b[0m       0.8878        \u001b[31m0.6085\u001b[0m  0.0109\n",
            "      3       0.8849        \u001b[32m0.6066\u001b[0m       0.8878        \u001b[31m0.5977\u001b[0m  0.0122\n",
            "      4       0.8849        \u001b[32m0.5954\u001b[0m       0.8878        \u001b[31m0.5890\u001b[0m  0.0121\n",
            "      5       0.8849        \u001b[32m0.5871\u001b[0m       0.8878        \u001b[31m0.5787\u001b[0m  0.0112\n",
            "      6       0.8849        \u001b[32m0.5764\u001b[0m       0.8878        \u001b[31m0.5693\u001b[0m  0.0102\n",
            "      7       0.8849        \u001b[32m0.5684\u001b[0m       0.8878        \u001b[31m0.5627\u001b[0m  0.0085\n",
            "      8       0.8849        \u001b[32m0.5614\u001b[0m       0.8878        \u001b[31m0.5539\u001b[0m  0.0078\n",
            "      9       0.8849        \u001b[32m0.5530\u001b[0m       0.8878        \u001b[31m0.5477\u001b[0m  0.0075\n",
            "     10       0.8849        \u001b[32m0.5478\u001b[0m       0.8878        \u001b[31m0.5411\u001b[0m  0.0073\n",
            "     11       0.8849        \u001b[32m0.5407\u001b[0m       0.8878        \u001b[31m0.5344\u001b[0m  0.0075\n",
            "     12       0.8849        \u001b[32m0.5356\u001b[0m       0.8878        \u001b[31m0.5290\u001b[0m  0.0074\n",
            "     13       0.8849        \u001b[32m0.5294\u001b[0m       0.8878        \u001b[31m0.5240\u001b[0m  0.0076\n",
            "     14       0.8849        \u001b[32m0.5244\u001b[0m       0.8878        \u001b[31m0.5195\u001b[0m  0.0070\n",
            "     15       0.8849        \u001b[32m0.5198\u001b[0m       0.8878        \u001b[31m0.5147\u001b[0m  0.0076\n",
            "     16       0.8849        \u001b[32m0.5155\u001b[0m       0.8878        \u001b[31m0.5114\u001b[0m  0.0076\n",
            "     17       0.8849        \u001b[32m0.5122\u001b[0m       0.8878        \u001b[31m0.5078\u001b[0m  0.0074\n",
            "     18       0.8849        \u001b[32m0.5095\u001b[0m       0.8878        \u001b[31m0.5040\u001b[0m  0.0073\n",
            "     19       0.8849        \u001b[32m0.5048\u001b[0m       0.8878        \u001b[31m0.5005\u001b[0m  0.0072\n",
            "     20       0.8849        \u001b[32m0.5024\u001b[0m       0.8878        \u001b[31m0.4973\u001b[0m  0.0068\n",
            "     21       0.8849        \u001b[32m0.4986\u001b[0m       0.8878        \u001b[31m0.4951\u001b[0m  0.0072\n",
            "     22       0.8849        \u001b[32m0.4967\u001b[0m       0.8878        \u001b[31m0.4926\u001b[0m  0.0107\n",
            "     23       0.8849        \u001b[32m0.4942\u001b[0m       0.8878        \u001b[31m0.4903\u001b[0m  0.0079\n",
            "     24       0.8849        \u001b[32m0.4921\u001b[0m       0.8878        \u001b[31m0.4885\u001b[0m  0.0082\n",
            "     25       0.8849        \u001b[32m0.4894\u001b[0m       0.8878        \u001b[31m0.4862\u001b[0m  0.0099\n",
            "     26       0.8849        \u001b[32m0.4885\u001b[0m       0.8878        \u001b[31m0.4840\u001b[0m  0.0094\n",
            "     27       0.8849        \u001b[32m0.4857\u001b[0m       0.8878        \u001b[31m0.4822\u001b[0m  0.0118\n",
            "     28       0.8849        \u001b[32m0.4845\u001b[0m       0.8878        \u001b[31m0.4802\u001b[0m  0.0111\n",
            "     29       0.8849        \u001b[32m0.4819\u001b[0m       0.8878        \u001b[31m0.4784\u001b[0m  0.0108\n",
            "     30       0.8849        \u001b[32m0.4807\u001b[0m       0.8878        \u001b[31m0.4767\u001b[0m  0.0099\n",
            "     31       0.8849        \u001b[32m0.4785\u001b[0m       0.8878        \u001b[31m0.4751\u001b[0m  0.0100\n",
            "     32       0.8849        \u001b[32m0.4765\u001b[0m       0.8878        \u001b[31m0.4735\u001b[0m  0.0121\n",
            "     33       0.8849        \u001b[32m0.4756\u001b[0m       0.8878        \u001b[31m0.4723\u001b[0m  0.0109\n",
            "     34       0.8849        \u001b[32m0.4744\u001b[0m       0.8878        \u001b[31m0.4710\u001b[0m  0.0106\n",
            "     35       0.8849        \u001b[32m0.4731\u001b[0m       0.8878        \u001b[31m0.4700\u001b[0m  0.0103\n",
            "     36       0.8849        \u001b[32m0.4717\u001b[0m       0.8878        \u001b[31m0.4688\u001b[0m  0.0074\n",
            "     37       0.8849        \u001b[32m0.4705\u001b[0m       0.8878        \u001b[31m0.4676\u001b[0m  0.0070\n",
            "     38       0.8849        \u001b[32m0.4692\u001b[0m       0.8878        \u001b[31m0.4667\u001b[0m  0.0078\n",
            "     39       0.8849        \u001b[32m0.4688\u001b[0m       0.8878        \u001b[31m0.4659\u001b[0m  0.0072\n",
            "     40       0.8849        \u001b[32m0.4680\u001b[0m       0.8878        \u001b[31m0.4649\u001b[0m  0.0074\n",
            "     41       0.8849        \u001b[32m0.4671\u001b[0m       0.8878        \u001b[31m0.4640\u001b[0m  0.0095\n",
            "     42       0.8849        \u001b[32m0.4660\u001b[0m       0.8878        \u001b[31m0.4631\u001b[0m  0.0074\n",
            "     43       0.8849        \u001b[32m0.4652\u001b[0m       0.8878        \u001b[31m0.4622\u001b[0m  0.0073\n",
            "     44       0.8849        \u001b[32m0.4641\u001b[0m       0.8878        \u001b[31m0.4614\u001b[0m  0.0068\n",
            "     45       0.8849        \u001b[32m0.4636\u001b[0m       0.8878        \u001b[31m0.4606\u001b[0m  0.0086\n",
            "     46       0.8849        \u001b[32m0.4627\u001b[0m       0.8878        \u001b[31m0.4599\u001b[0m  0.0070\n",
            "     47       0.8849        \u001b[32m0.4621\u001b[0m       0.8878        \u001b[31m0.4594\u001b[0m  0.0105\n",
            "     48       0.8849        \u001b[32m0.4621\u001b[0m       0.8878        \u001b[31m0.4587\u001b[0m  0.0171\n",
            "     49       0.8849        \u001b[32m0.4611\u001b[0m       0.8878        \u001b[31m0.4580\u001b[0m  0.0108\n",
            "     50       0.8849        \u001b[32m0.4599\u001b[0m       0.8878        \u001b[31m0.4574\u001b[0m  0.0172\n",
            "     51       0.8849        \u001b[32m0.4599\u001b[0m       0.8878        \u001b[31m0.4568\u001b[0m  0.0075\n",
            "     52       0.8849        \u001b[32m0.4591\u001b[0m       0.8878        \u001b[31m0.4563\u001b[0m  0.0108\n",
            "     53       0.8849        \u001b[32m0.4587\u001b[0m       0.8878        \u001b[31m0.4557\u001b[0m  0.0094\n",
            "     54       0.8849        \u001b[32m0.4574\u001b[0m       0.8878        \u001b[31m0.4551\u001b[0m  0.0077\n",
            "     55       0.8849        0.4578       0.8878        \u001b[31m0.4546\u001b[0m  0.0075\n",
            "     56       0.8849        0.4575       0.8878        \u001b[31m0.4542\u001b[0m  0.0073\n",
            "     57       0.8849        \u001b[32m0.4567\u001b[0m       0.8878        \u001b[31m0.4536\u001b[0m  0.0076\n",
            "     58       0.8849        \u001b[32m0.4558\u001b[0m       0.8878        \u001b[31m0.4531\u001b[0m  0.0073\n",
            "     59       0.8849        0.4563       0.8878        \u001b[31m0.4526\u001b[0m  0.0078\n",
            "     60       0.8849        \u001b[32m0.4551\u001b[0m       0.8878        \u001b[31m0.4522\u001b[0m  0.0068\n",
            "     61       0.8849        \u001b[32m0.4545\u001b[0m       0.8878        \u001b[31m0.4518\u001b[0m  0.0094\n",
            "     62       0.8849        \u001b[32m0.4541\u001b[0m       0.8878        \u001b[31m0.4514\u001b[0m  0.0072\n",
            "     63       0.8849        \u001b[32m0.4536\u001b[0m       0.8878        \u001b[31m0.4510\u001b[0m  0.0073\n",
            "     64       0.8849        \u001b[32m0.4536\u001b[0m       0.8878        \u001b[31m0.4506\u001b[0m  0.0072\n",
            "     65       0.8849        \u001b[32m0.4533\u001b[0m       0.8878        \u001b[31m0.4501\u001b[0m  0.0086\n",
            "     66       0.8849        \u001b[32m0.4525\u001b[0m       0.8878        \u001b[31m0.4497\u001b[0m  0.0077\n",
            "     67       0.8849        \u001b[32m0.4522\u001b[0m       0.8878        \u001b[31m0.4493\u001b[0m  0.0117\n",
            "     68       0.8849        \u001b[32m0.4517\u001b[0m       0.8878        \u001b[31m0.4490\u001b[0m  0.0113\n",
            "     69       0.8849        \u001b[32m0.4515\u001b[0m       0.8878        \u001b[31m0.4486\u001b[0m  0.0114\n",
            "     70       0.8849        \u001b[32m0.4514\u001b[0m       0.8878        \u001b[31m0.4483\u001b[0m  0.0077\n",
            "     71       0.8849        \u001b[32m0.4510\u001b[0m       0.8878        \u001b[31m0.4480\u001b[0m  0.0074\n",
            "     72       0.8849        \u001b[32m0.4504\u001b[0m       0.8878        \u001b[31m0.4478\u001b[0m  0.0077\n",
            "     73       0.8849        0.4505       0.8878        \u001b[31m0.4475\u001b[0m  0.0083\n",
            "     74       0.8849        \u001b[32m0.4499\u001b[0m       0.8878        \u001b[31m0.4471\u001b[0m  0.0074\n",
            "     75       0.8849        \u001b[32m0.4495\u001b[0m       0.8878        \u001b[31m0.4469\u001b[0m  0.0086\n",
            "     76       0.8849        0.4497       0.8878        \u001b[31m0.4466\u001b[0m  0.0074\n",
            "     77       0.8849        \u001b[32m0.4492\u001b[0m       0.8878        \u001b[31m0.4464\u001b[0m  0.0079\n",
            "     78       0.8849        \u001b[32m0.4489\u001b[0m       0.8878        \u001b[31m0.4462\u001b[0m  0.0073\n",
            "     79       0.8849        \u001b[32m0.4489\u001b[0m       0.8878        \u001b[31m0.4459\u001b[0m  0.0074\n",
            "     80       0.8849        \u001b[32m0.4483\u001b[0m       0.8878        \u001b[31m0.4458\u001b[0m  0.0070\n",
            "     81       0.8849        0.4484       0.8878        \u001b[31m0.4455\u001b[0m  0.0074\n",
            "     82       0.8849        0.4485       0.8878        \u001b[31m0.4452\u001b[0m  0.0070\n",
            "     83       0.8849        \u001b[32m0.4478\u001b[0m       0.8878        \u001b[31m0.4450\u001b[0m  0.0073\n",
            "     84       0.8849        \u001b[32m0.4476\u001b[0m       0.8878        \u001b[31m0.4448\u001b[0m  0.0097\n",
            "     85       0.8849        \u001b[32m0.4475\u001b[0m       0.8878        \u001b[31m0.4445\u001b[0m  0.0122\n",
            "     86       0.8849        \u001b[32m0.4471\u001b[0m       0.8878        \u001b[31m0.4444\u001b[0m  0.0136\n",
            "     87       0.8849        0.4474       0.8878        \u001b[31m0.4441\u001b[0m  0.0154\n",
            "     88       0.8849        \u001b[32m0.4465\u001b[0m       0.8878        \u001b[31m0.4439\u001b[0m  0.0150\n",
            "     89       0.8849        \u001b[32m0.4460\u001b[0m       0.8878        \u001b[31m0.4438\u001b[0m  0.0075\n",
            "     90       0.8849        0.4463       0.8878        \u001b[31m0.4436\u001b[0m  0.0083\n",
            "     91       0.8849        0.4464       0.8878        \u001b[31m0.4434\u001b[0m  0.0075\n",
            "     92       0.8849        0.4461       0.8878        \u001b[31m0.4432\u001b[0m  0.0087\n",
            "     93       0.8849        \u001b[32m0.4457\u001b[0m       0.8878        \u001b[31m0.4430\u001b[0m  0.0080\n",
            "     94       0.8849        \u001b[32m0.4456\u001b[0m       0.8878        \u001b[31m0.4428\u001b[0m  0.0074\n",
            "     95       0.8849        0.4458       0.8878        \u001b[31m0.4427\u001b[0m  0.0072\n",
            "     96       0.8849        \u001b[32m0.4453\u001b[0m       0.8878        \u001b[31m0.4425\u001b[0m  0.0076\n",
            "     97       0.8849        0.4454       0.8878        \u001b[31m0.4423\u001b[0m  0.0072\n",
            "     98       0.8849        \u001b[32m0.4446\u001b[0m       0.8878        \u001b[31m0.4422\u001b[0m  0.0074\n",
            "     99       0.8849        0.4450       0.8878        \u001b[31m0.4420\u001b[0m  0.0076\n",
            "    100       0.8849        \u001b[32m0.4445\u001b[0m       0.8878        \u001b[31m0.4419\u001b[0m  0.0093\n",
            "    101       0.8849        0.4447       0.8878        \u001b[31m0.4417\u001b[0m  0.0091\n",
            "    102       0.8849        \u001b[32m0.4442\u001b[0m       0.8878        \u001b[31m0.4416\u001b[0m  0.0070\n",
            "    103       0.8849        0.4443       0.8878        \u001b[31m0.4414\u001b[0m  0.0071\n",
            "    104       0.8849        \u001b[32m0.4440\u001b[0m       0.8878        \u001b[31m0.4412\u001b[0m  0.0071\n",
            "    105       0.8849        0.4441       0.8878        \u001b[31m0.4411\u001b[0m  0.0073\n",
            "    106       0.8849        \u001b[32m0.4438\u001b[0m       0.8878        \u001b[31m0.4409\u001b[0m  0.0079\n",
            "    107       0.8849        \u001b[32m0.4434\u001b[0m       0.8878        \u001b[31m0.4407\u001b[0m  0.0075\n",
            "    108       0.8849        \u001b[32m0.4430\u001b[0m       0.8878        \u001b[31m0.4406\u001b[0m  0.0073\n",
            "    109       0.8849        0.4431       0.8878        \u001b[31m0.4405\u001b[0m  0.0070\n",
            "    110       0.8849        0.4432       0.8878        \u001b[31m0.4404\u001b[0m  0.0078\n",
            "    111       0.8849        0.4434       0.8878        \u001b[31m0.4402\u001b[0m  0.0074\n",
            "    112       0.8849        \u001b[32m0.4427\u001b[0m       0.8878        \u001b[31m0.4401\u001b[0m  0.0073\n",
            "    113       0.8849        \u001b[32m0.4425\u001b[0m       0.8878        \u001b[31m0.4400\u001b[0m  0.0072\n",
            "    114       0.8849        0.4426       0.8878        \u001b[31m0.4399\u001b[0m  0.0075\n",
            "    115       0.8849        0.4426       0.8878        \u001b[31m0.4398\u001b[0m  0.0083\n",
            "    116       0.8849        0.4426       0.8878        \u001b[31m0.4396\u001b[0m  0.0071\n",
            "    117       0.8849        \u001b[32m0.4422\u001b[0m       0.8878        \u001b[31m0.4395\u001b[0m  0.0076\n",
            "    118       0.8849        \u001b[32m0.4421\u001b[0m       0.8878        \u001b[31m0.4394\u001b[0m  0.0091\n",
            "    119       0.8849        0.4422       0.8878        \u001b[31m0.4393\u001b[0m  0.0079\n",
            "    120       0.8849        0.4422       0.8878        \u001b[31m0.4392\u001b[0m  0.0087\n",
            "    121       0.8849        \u001b[32m0.4419\u001b[0m       0.8878        \u001b[31m0.4391\u001b[0m  0.0073\n",
            "    122       0.8849        \u001b[32m0.4418\u001b[0m       0.8878        \u001b[31m0.4390\u001b[0m  0.0076\n",
            "    123       0.8849        \u001b[32m0.4418\u001b[0m       0.8878        \u001b[31m0.4389\u001b[0m  0.0077\n",
            "    124       0.8849        \u001b[32m0.4417\u001b[0m       0.8878        \u001b[31m0.4388\u001b[0m  0.0074\n",
            "    125       0.8849        \u001b[32m0.4413\u001b[0m       0.8878        \u001b[31m0.4386\u001b[0m  0.0075\n",
            "    126       0.8849        0.4413       0.8878        \u001b[31m0.4385\u001b[0m  0.0070\n",
            "    127       0.8849        \u001b[32m0.4411\u001b[0m       0.8878        \u001b[31m0.4384\u001b[0m  0.0074\n",
            "    128       0.8849        \u001b[32m0.4410\u001b[0m       0.8878        \u001b[31m0.4383\u001b[0m  0.0078\n",
            "    129       0.8849        0.4410       0.8878        \u001b[31m0.4382\u001b[0m  0.0073\n",
            "    130       0.8849        \u001b[32m0.4409\u001b[0m       0.8878        \u001b[31m0.4381\u001b[0m  0.0069\n",
            "    131       0.8849        0.4410       0.8878        \u001b[31m0.4380\u001b[0m  0.0073\n",
            "    132       0.8849        \u001b[32m0.4408\u001b[0m       0.8878        \u001b[31m0.4379\u001b[0m  0.0071\n",
            "    133       0.8849        \u001b[32m0.4408\u001b[0m       0.8878        \u001b[31m0.4378\u001b[0m  0.0075\n",
            "    134       0.8849        \u001b[32m0.4405\u001b[0m       0.8878        \u001b[31m0.4378\u001b[0m  0.0085\n",
            "    135       0.8849        0.4406       0.8878        \u001b[31m0.4377\u001b[0m  0.0074\n",
            "    136       0.8849        \u001b[32m0.4403\u001b[0m       0.8878        \u001b[31m0.4376\u001b[0m  0.0073\n",
            "    137       0.8849        \u001b[32m0.4402\u001b[0m       0.8878        \u001b[31m0.4375\u001b[0m  0.0084\n",
            "    138       0.8849        0.4404       0.8878        \u001b[31m0.4374\u001b[0m  0.0108\n",
            "    139       0.8849        \u001b[32m0.4402\u001b[0m       0.8878        \u001b[31m0.4374\u001b[0m  0.0095\n",
            "    140       0.8849        \u001b[32m0.4401\u001b[0m       0.8878        \u001b[31m0.4373\u001b[0m  0.0080\n",
            "    141       0.8849        \u001b[32m0.4400\u001b[0m       0.8878        \u001b[31m0.4372\u001b[0m  0.0074\n",
            "    142       0.8849        0.4400       0.8878        \u001b[31m0.4371\u001b[0m  0.0075\n",
            "    143       0.8849        0.4401       0.8878        \u001b[31m0.4370\u001b[0m  0.0079\n",
            "    144       0.8849        \u001b[32m0.4399\u001b[0m       0.8878        \u001b[31m0.4370\u001b[0m  0.0071\n",
            "    145       0.8849        \u001b[32m0.4393\u001b[0m       0.8878        \u001b[31m0.4369\u001b[0m  0.0072\n",
            "    146       0.8849        0.4397       0.8878        \u001b[31m0.4368\u001b[0m  0.0072\n",
            "    147       0.8849        0.4395       0.8878        \u001b[31m0.4367\u001b[0m  0.0103\n",
            "    148       0.8849        0.4393       0.8878        \u001b[31m0.4367\u001b[0m  0.0076\n",
            "    149       0.8849        0.4397       0.8878        \u001b[31m0.4366\u001b[0m  0.0070\n",
            "    150       0.8849        \u001b[32m0.4390\u001b[0m       0.8878        \u001b[31m0.4365\u001b[0m  0.0096\n",
            "    151       0.8849        0.4394       0.8878        \u001b[31m0.4365\u001b[0m  0.0072\n",
            "    152       0.8849        0.4392       0.8878        \u001b[31m0.4364\u001b[0m  0.0076\n",
            "    153       0.8849        0.4392       0.8878        \u001b[31m0.4363\u001b[0m  0.0071\n",
            "    154       0.8849        0.4392       0.8878        \u001b[31m0.4363\u001b[0m  0.0077\n",
            "    155       0.8849        0.4392       0.8878        \u001b[31m0.4362\u001b[0m  0.0071\n",
            "    156       0.8849        \u001b[32m0.4390\u001b[0m       0.8878        \u001b[31m0.4361\u001b[0m  0.0077\n",
            "    157       0.8849        \u001b[32m0.4389\u001b[0m       0.8878        \u001b[31m0.4361\u001b[0m  0.0080\n",
            "    158       0.8849        0.4390       0.8878        \u001b[31m0.4360\u001b[0m  0.0074\n",
            "    159       0.8849        \u001b[32m0.4386\u001b[0m       0.8878        \u001b[31m0.4359\u001b[0m  0.0075\n",
            "    160       0.8849        0.4386       0.8878        \u001b[31m0.4359\u001b[0m  0.0074\n",
            "    161       0.8849        0.4389       0.8878        \u001b[31m0.4358\u001b[0m  0.0072\n",
            "    162       0.8849        \u001b[32m0.4386\u001b[0m       0.8878        \u001b[31m0.4357\u001b[0m  0.0074\n",
            "    163       0.8849        \u001b[32m0.4385\u001b[0m       0.8878        \u001b[31m0.4356\u001b[0m  0.0070\n",
            "    164       0.8849        0.4387       0.8878        \u001b[31m0.4356\u001b[0m  0.0084\n",
            "    165       0.8849        \u001b[32m0.4383\u001b[0m       0.8878        \u001b[31m0.4355\u001b[0m  0.0071\n",
            "    166       0.8849        \u001b[32m0.4380\u001b[0m       0.8878        \u001b[31m0.4355\u001b[0m  0.0074\n",
            "    167       0.8849        0.4381       0.8878        \u001b[31m0.4354\u001b[0m  0.0074\n",
            "    168       0.8849        0.4381       0.8878        \u001b[31m0.4354\u001b[0m  0.0092\n",
            "    169       0.8849        0.4383       0.8878        \u001b[31m0.4353\u001b[0m  0.0070\n",
            "    170       0.8849        0.4382       0.8878        \u001b[31m0.4352\u001b[0m  0.0074\n",
            "    171       0.8849        0.4381       0.8878        \u001b[31m0.4352\u001b[0m  0.0126\n",
            "    172       0.8849        \u001b[32m0.4378\u001b[0m       0.8878        \u001b[31m0.4351\u001b[0m  0.0068\n",
            "    173       0.8849        0.4380       0.8878        \u001b[31m0.4351\u001b[0m  0.0073\n",
            "    174       0.8849        0.4381       0.8878        \u001b[31m0.4350\u001b[0m  0.0072\n",
            "    175       0.8849        \u001b[32m0.4377\u001b[0m       0.8878        \u001b[31m0.4349\u001b[0m  0.0077\n",
            "    176       0.8849        \u001b[32m0.4377\u001b[0m       0.8878        \u001b[31m0.4349\u001b[0m  0.0079\n",
            "    177       0.8849        \u001b[32m0.4376\u001b[0m       0.8878        \u001b[31m0.4348\u001b[0m  0.0075\n",
            "    178       0.8849        \u001b[32m0.4374\u001b[0m       0.8878        \u001b[31m0.4348\u001b[0m  0.0074\n",
            "    179       0.8849        \u001b[32m0.4373\u001b[0m       0.8878        \u001b[31m0.4347\u001b[0m  0.0083\n",
            "    180       0.8849        0.4377       0.8878        \u001b[31m0.4347\u001b[0m  0.0068\n",
            "    181       0.8849        0.4374       0.8878        \u001b[31m0.4346\u001b[0m  0.0075\n",
            "    182       0.8849        0.4375       0.8878        \u001b[31m0.4346\u001b[0m  0.0088\n",
            "    183       0.8849        \u001b[32m0.4372\u001b[0m       0.8878        \u001b[31m0.4345\u001b[0m  0.0080\n",
            "    184       0.8849        0.4373       0.8878        \u001b[31m0.4345\u001b[0m  0.0078\n",
            "    185       0.8849        \u001b[32m0.4370\u001b[0m       0.8878        \u001b[31m0.4344\u001b[0m  0.0073\n",
            "    186       0.8849        0.4373       0.8878        \u001b[31m0.4344\u001b[0m  0.0102\n",
            "    187       0.8849        0.4370       0.8878        \u001b[31m0.4343\u001b[0m  0.0113\n",
            "    188       0.8849        0.4371       0.8878        \u001b[31m0.4343\u001b[0m  0.0108\n",
            "    189       0.8849        0.4372       0.8878        \u001b[31m0.4343\u001b[0m  0.0145\n",
            "    190       0.8849        \u001b[32m0.4369\u001b[0m       0.8878        \u001b[31m0.4342\u001b[0m  0.0102\n",
            "    191       0.8849        \u001b[32m0.4368\u001b[0m       0.8878        \u001b[31m0.4341\u001b[0m  0.0110\n",
            "    192       0.8849        0.4369       0.8878        \u001b[31m0.4341\u001b[0m  0.0077\n",
            "    193       0.8849        \u001b[32m0.4368\u001b[0m       0.8878        \u001b[31m0.4340\u001b[0m  0.0076\n",
            "    194       0.8849        0.4368       0.8878        \u001b[31m0.4340\u001b[0m  0.0074\n",
            "    195       0.8849        \u001b[32m0.4365\u001b[0m       0.8878        \u001b[31m0.4340\u001b[0m  0.0070\n",
            "    196       0.8849        0.4366       0.8878        \u001b[31m0.4339\u001b[0m  0.0074\n",
            "    197       0.8849        0.4367       0.8878        \u001b[31m0.4339\u001b[0m  0.0086\n",
            "    198       0.8849        0.4366       0.8878        \u001b[31m0.4338\u001b[0m  0.0077\n",
            "    199       0.8849        \u001b[32m0.4365\u001b[0m       0.8878        \u001b[31m0.4338\u001b[0m  0.0075\n",
            "    200       0.8849        \u001b[32m0.4365\u001b[0m       0.8878        \u001b[31m0.4337\u001b[0m  0.0070\n",
            "  epoch    train_acc    train_loss    valid_acc    valid_loss     dur\n",
            "-------  -----------  ------------  -----------  ------------  ------\n",
            "      1       \u001b[36m0.8867\u001b[0m        \u001b[32m0.5153\u001b[0m       \u001b[35m0.8855\u001b[0m        \u001b[31m0.5119\u001b[0m  0.0138\n",
            "      2       0.8867        \u001b[32m0.5093\u001b[0m       0.8855        \u001b[31m0.5060\u001b[0m  0.0175\n",
            "      3       0.8867        \u001b[32m0.5034\u001b[0m       0.8855        \u001b[31m0.5008\u001b[0m  0.0172\n",
            "      4       0.8867        \u001b[32m0.4987\u001b[0m       0.8855        \u001b[31m0.4960\u001b[0m  0.0173\n",
            "      5       0.8867        \u001b[32m0.4942\u001b[0m       0.8855        \u001b[31m0.4919\u001b[0m  0.0178\n",
            "      6       0.8867        \u001b[32m0.4902\u001b[0m       0.8855        \u001b[31m0.4881\u001b[0m  0.0181\n",
            "      7       0.8867        \u001b[32m0.4864\u001b[0m       0.8855        \u001b[31m0.4847\u001b[0m  0.0175\n",
            "      8       0.8867        \u001b[32m0.4832\u001b[0m       0.8855        \u001b[31m0.4820\u001b[0m  0.0166\n",
            "      9       0.8867        \u001b[32m0.4804\u001b[0m       0.8855        \u001b[31m0.4793\u001b[0m  0.0166\n",
            "     10       0.8867        \u001b[32m0.4778\u001b[0m       0.8855        \u001b[31m0.4768\u001b[0m  0.0147\n",
            "     11       0.8867        \u001b[32m0.4756\u001b[0m       0.8855        \u001b[31m0.4745\u001b[0m  0.0170\n",
            "     12       0.8867        \u001b[32m0.4730\u001b[0m       0.8855        \u001b[31m0.4726\u001b[0m  0.0182\n",
            "     13       0.8867        \u001b[32m0.4710\u001b[0m       0.8855        \u001b[31m0.4708\u001b[0m  0.0153\n",
            "     14       0.8867        \u001b[32m0.4695\u001b[0m       0.8855        \u001b[31m0.4690\u001b[0m  0.0164\n",
            "     15       0.8867        \u001b[32m0.4678\u001b[0m       0.8855        \u001b[31m0.4674\u001b[0m  0.0115\n",
            "     16       0.8867        \u001b[32m0.4659\u001b[0m       0.8855        \u001b[31m0.4659\u001b[0m  0.0153\n",
            "     17       0.8867        \u001b[32m0.4649\u001b[0m       0.8855        \u001b[31m0.4645\u001b[0m  0.0130\n",
            "     18       0.8867        \u001b[32m0.4635\u001b[0m       0.8855        \u001b[31m0.4632\u001b[0m  0.0120\n",
            "     19       0.8867        \u001b[32m0.4620\u001b[0m       0.8855        \u001b[31m0.4621\u001b[0m  0.0120\n",
            "     20       0.8867        \u001b[32m0.4610\u001b[0m       0.8855        \u001b[31m0.4610\u001b[0m  0.0112\n",
            "     21       0.8867        \u001b[32m0.4597\u001b[0m       0.8855        \u001b[31m0.4599\u001b[0m  0.0114\n",
            "     22       0.8867        \u001b[32m0.4587\u001b[0m       0.8855        \u001b[31m0.4589\u001b[0m  0.0144\n",
            "     23       0.8867        \u001b[32m0.4576\u001b[0m       0.8855        \u001b[31m0.4579\u001b[0m  0.0113\n",
            "     24       0.8867        \u001b[32m0.4569\u001b[0m       0.8855        \u001b[31m0.4572\u001b[0m  0.0191\n",
            "     25       0.8867        \u001b[32m0.4562\u001b[0m       0.8855        \u001b[31m0.4563\u001b[0m  0.0167\n",
            "     26       0.8867        \u001b[32m0.4554\u001b[0m       0.8855        \u001b[31m0.4556\u001b[0m  0.0116\n",
            "     27       0.8867        \u001b[32m0.4545\u001b[0m       0.8855        \u001b[31m0.4549\u001b[0m  0.0156\n",
            "     28       0.8867        \u001b[32m0.4540\u001b[0m       0.8855        \u001b[31m0.4542\u001b[0m  0.0114\n",
            "     29       0.8867        \u001b[32m0.4532\u001b[0m       0.8855        \u001b[31m0.4535\u001b[0m  0.0131\n",
            "     30       0.8867        \u001b[32m0.4524\u001b[0m       0.8855        \u001b[31m0.4529\u001b[0m  0.0113\n",
            "     31       0.8867        \u001b[32m0.4518\u001b[0m       0.8855        \u001b[31m0.4523\u001b[0m  0.0134\n",
            "     32       0.8867        \u001b[32m0.4511\u001b[0m       0.8855        \u001b[31m0.4517\u001b[0m  0.0117\n",
            "     33       0.8867        \u001b[32m0.4507\u001b[0m       0.8855        \u001b[31m0.4512\u001b[0m  0.0112\n",
            "     34       0.8867        \u001b[32m0.4500\u001b[0m       0.8855        \u001b[31m0.4506\u001b[0m  0.0114\n",
            "     35       0.8867        \u001b[32m0.4494\u001b[0m       0.8855        \u001b[31m0.4501\u001b[0m  0.0143\n",
            "     36       0.8867        \u001b[32m0.4491\u001b[0m       0.8855        \u001b[31m0.4496\u001b[0m  0.0110\n",
            "     37       0.8867        \u001b[32m0.4485\u001b[0m       0.8855        \u001b[31m0.4492\u001b[0m  0.0108\n",
            "     38       0.8867        \u001b[32m0.4482\u001b[0m       0.8855        \u001b[31m0.4487\u001b[0m  0.0119\n",
            "     39       0.8867        \u001b[32m0.4476\u001b[0m       0.8855        \u001b[31m0.4483\u001b[0m  0.0109\n",
            "     40       0.8867        \u001b[32m0.4472\u001b[0m       0.8855        \u001b[31m0.4479\u001b[0m  0.0157\n",
            "     41       0.8867        \u001b[32m0.4468\u001b[0m       0.8855        \u001b[31m0.4475\u001b[0m  0.0138\n",
            "     42       0.8867        \u001b[32m0.4463\u001b[0m       0.8855        \u001b[31m0.4471\u001b[0m  0.0123\n",
            "     43       0.8867        \u001b[32m0.4463\u001b[0m       0.8855        \u001b[31m0.4467\u001b[0m  0.0113\n",
            "     44       0.8867        \u001b[32m0.4456\u001b[0m       0.8855        \u001b[31m0.4464\u001b[0m  0.0117\n",
            "     45       0.8867        \u001b[32m0.4452\u001b[0m       0.8855        \u001b[31m0.4460\u001b[0m  0.0110\n",
            "     46       0.8867        \u001b[32m0.4451\u001b[0m       0.8855        \u001b[31m0.4457\u001b[0m  0.0110\n",
            "     47       0.8867        \u001b[32m0.4447\u001b[0m       0.8855        \u001b[31m0.4454\u001b[0m  0.0119\n",
            "     48       0.8867        \u001b[32m0.4443\u001b[0m       0.8855        \u001b[31m0.4451\u001b[0m  0.0111\n",
            "     49       0.8867        \u001b[32m0.4441\u001b[0m       0.8855        \u001b[31m0.4448\u001b[0m  0.0114\n",
            "     50       0.8867        \u001b[32m0.4438\u001b[0m       0.8855        \u001b[31m0.4445\u001b[0m  0.0115\n",
            "     51       0.8867        \u001b[32m0.4435\u001b[0m       0.8855        \u001b[31m0.4442\u001b[0m  0.0124\n",
            "     52       0.8867        \u001b[32m0.4431\u001b[0m       0.8855        \u001b[31m0.4440\u001b[0m  0.0112\n",
            "     53       0.8867        \u001b[32m0.4430\u001b[0m       0.8855        \u001b[31m0.4437\u001b[0m  0.0121\n",
            "     54       0.8867        \u001b[32m0.4426\u001b[0m       0.8855        \u001b[31m0.4434\u001b[0m  0.0125\n",
            "     55       0.8867        0.4426       0.8855        \u001b[31m0.4432\u001b[0m  0.0120\n",
            "     56       0.8867        \u001b[32m0.4422\u001b[0m       0.8855        \u001b[31m0.4430\u001b[0m  0.0128\n",
            "     57       0.8867        \u001b[32m0.4421\u001b[0m       0.8855        \u001b[31m0.4427\u001b[0m  0.0121\n",
            "     58       0.8867        \u001b[32m0.4418\u001b[0m       0.8855        \u001b[31m0.4425\u001b[0m  0.0119\n",
            "     59       0.8867        \u001b[32m0.4417\u001b[0m       0.8855        \u001b[31m0.4423\u001b[0m  0.0114\n",
            "     60       0.8867        \u001b[32m0.4413\u001b[0m       0.8855        \u001b[31m0.4421\u001b[0m  0.0114\n",
            "     61       0.8867        \u001b[32m0.4410\u001b[0m       0.8855        \u001b[31m0.4419\u001b[0m  0.0124\n",
            "     62       0.8867        \u001b[32m0.4408\u001b[0m       0.8855        \u001b[31m0.4417\u001b[0m  0.0128\n",
            "     63       0.8867        \u001b[32m0.4405\u001b[0m       0.8855        \u001b[31m0.4415\u001b[0m  0.0109\n",
            "     64       0.8867        0.4405       0.8855        \u001b[31m0.4413\u001b[0m  0.0124\n",
            "     65       0.8867        \u001b[32m0.4403\u001b[0m       0.8855        \u001b[31m0.4411\u001b[0m  0.0111\n",
            "     66       0.8867        \u001b[32m0.4402\u001b[0m       0.8855        \u001b[31m0.4409\u001b[0m  0.0122\n",
            "     67       0.8867        \u001b[32m0.4398\u001b[0m       0.8855        \u001b[31m0.4407\u001b[0m  0.0116\n",
            "     68       0.8867        \u001b[32m0.4397\u001b[0m       0.8855        \u001b[31m0.4406\u001b[0m  0.0129\n",
            "     69       0.8867        0.4397       0.8855        \u001b[31m0.4404\u001b[0m  0.0113\n",
            "     70       0.8867        \u001b[32m0.4395\u001b[0m       0.8855        \u001b[31m0.4402\u001b[0m  0.0108\n",
            "     71       0.8867        \u001b[32m0.4391\u001b[0m       0.8855        \u001b[31m0.4401\u001b[0m  0.0132\n",
            "     72       0.8867        0.4392       0.8855        \u001b[31m0.4399\u001b[0m  0.0119\n",
            "     73       0.8867        \u001b[32m0.4390\u001b[0m       0.8855        \u001b[31m0.4398\u001b[0m  0.0115\n",
            "     74       0.8867        \u001b[32m0.4388\u001b[0m       0.8855        \u001b[31m0.4397\u001b[0m  0.0110\n",
            "     75       0.8867        \u001b[32m0.4386\u001b[0m       0.8855        \u001b[31m0.4395\u001b[0m  0.0111\n",
            "     76       0.8867        \u001b[32m0.4383\u001b[0m       0.8855        \u001b[31m0.4394\u001b[0m  0.0110\n",
            "     77       0.8867        \u001b[32m0.4383\u001b[0m       0.8855        \u001b[31m0.4392\u001b[0m  0.0114\n",
            "     78       0.8867        0.4383       0.8855        \u001b[31m0.4391\u001b[0m  0.0110\n",
            "     79       0.8867        \u001b[32m0.4381\u001b[0m       0.8855        \u001b[31m0.4390\u001b[0m  0.0125\n",
            "     80       0.8867        \u001b[32m0.4379\u001b[0m       0.8855        \u001b[31m0.4388\u001b[0m  0.0125\n",
            "     81       0.8867        \u001b[32m0.4378\u001b[0m       0.8855        \u001b[31m0.4387\u001b[0m  0.0125\n",
            "     82       0.8867        \u001b[32m0.4377\u001b[0m       0.8855        \u001b[31m0.4386\u001b[0m  0.0182\n",
            "     83       0.8867        \u001b[32m0.4376\u001b[0m       0.8855        \u001b[31m0.4385\u001b[0m  0.0116\n",
            "     84       0.8867        \u001b[32m0.4375\u001b[0m       0.8855        \u001b[31m0.4383\u001b[0m  0.0130\n",
            "     85       0.8867        0.4375       0.8855        \u001b[31m0.4382\u001b[0m  0.0122\n",
            "     86       0.8867        \u001b[32m0.4370\u001b[0m       0.8855        \u001b[31m0.4381\u001b[0m  0.0127\n",
            "     87       0.8867        0.4371       0.8855        \u001b[31m0.4380\u001b[0m  0.0110\n",
            "     88       0.8867        0.4371       0.8855        \u001b[31m0.4379\u001b[0m  0.0109\n",
            "     89       0.8867        \u001b[32m0.4370\u001b[0m       0.8855        \u001b[31m0.4378\u001b[0m  0.0118\n",
            "     90       0.8867        \u001b[32m0.4369\u001b[0m       0.8855        \u001b[31m0.4377\u001b[0m  0.0112\n",
            "     91       0.8867        \u001b[32m0.4368\u001b[0m       0.8855        \u001b[31m0.4376\u001b[0m  0.0115\n",
            "     92       0.8867        \u001b[32m0.4365\u001b[0m       0.8855        \u001b[31m0.4375\u001b[0m  0.0121\n",
            "     93       0.8867        \u001b[32m0.4364\u001b[0m       0.8855        \u001b[31m0.4374\u001b[0m  0.0118\n",
            "     94       0.8867        \u001b[32m0.4364\u001b[0m       0.8855        \u001b[31m0.4373\u001b[0m  0.0110\n",
            "     95       0.8867        \u001b[32m0.4363\u001b[0m       0.8855        \u001b[31m0.4372\u001b[0m  0.0124\n",
            "     96       0.8867        \u001b[32m0.4363\u001b[0m       0.8855        \u001b[31m0.4372\u001b[0m  0.0122\n",
            "     97       0.8867        \u001b[32m0.4362\u001b[0m       0.8855        \u001b[31m0.4371\u001b[0m  0.0111\n",
            "     98       0.8867        \u001b[32m0.4359\u001b[0m       0.8855        \u001b[31m0.4370\u001b[0m  0.0116\n",
            "     99       0.8867        0.4361       0.8855        \u001b[31m0.4369\u001b[0m  0.0124\n",
            "    100       0.8867        \u001b[32m0.4359\u001b[0m       0.8855        \u001b[31m0.4368\u001b[0m  0.0113\n",
            "    101       0.8867        \u001b[32m0.4357\u001b[0m       0.8855        \u001b[31m0.4367\u001b[0m  0.0121\n",
            "    102       0.8867        0.4357       0.8855        \u001b[31m0.4366\u001b[0m  0.0122\n",
            "    103       0.8867        \u001b[32m0.4355\u001b[0m       0.8855        \u001b[31m0.4366\u001b[0m  0.0107\n",
            "    104       0.8867        0.4356       0.8855        \u001b[31m0.4365\u001b[0m  0.0123\n",
            "    105       0.8867        \u001b[32m0.4353\u001b[0m       0.8855        \u001b[31m0.4364\u001b[0m  0.0175\n",
            "    106       0.8867        \u001b[32m0.4352\u001b[0m       0.8855        \u001b[31m0.4363\u001b[0m  0.0170\n",
            "    107       0.8867        0.4354       0.8855        \u001b[31m0.4363\u001b[0m  0.0199\n",
            "    108       0.8867        \u001b[32m0.4352\u001b[0m       0.8855        \u001b[31m0.4362\u001b[0m  0.0185\n",
            "    109       0.8867        \u001b[32m0.4352\u001b[0m       0.8855        \u001b[31m0.4361\u001b[0m  0.0174\n",
            "    110       0.8867        \u001b[32m0.4350\u001b[0m       0.8855        \u001b[31m0.4360\u001b[0m  0.0210\n",
            "    111       0.8867        \u001b[32m0.4349\u001b[0m       0.8855        \u001b[31m0.4360\u001b[0m  0.0243\n",
            "    112       0.8867        0.4349       0.8855        \u001b[31m0.4359\u001b[0m  0.0122\n",
            "    113       0.8867        \u001b[32m0.4348\u001b[0m       0.8855        \u001b[31m0.4358\u001b[0m  0.0151\n",
            "    114       0.8867        \u001b[32m0.4347\u001b[0m       0.8855        \u001b[31m0.4358\u001b[0m  0.0111\n",
            "    115       0.8867        0.4348       0.8855        \u001b[31m0.4357\u001b[0m  0.0168\n",
            "    116       0.8867        \u001b[32m0.4347\u001b[0m       0.8855        \u001b[31m0.4356\u001b[0m  0.0169\n",
            "    117       0.8867        \u001b[32m0.4347\u001b[0m       0.8855        \u001b[31m0.4356\u001b[0m  0.0192\n",
            "    118       0.8867        \u001b[32m0.4346\u001b[0m       0.8855        \u001b[31m0.4355\u001b[0m  0.0164\n",
            "    119       0.8867        0.4347       0.8855        \u001b[31m0.4355\u001b[0m  0.0156\n",
            "    120       0.8867        \u001b[32m0.4345\u001b[0m       0.8855        \u001b[31m0.4354\u001b[0m  0.0122\n",
            "    121       0.8867        \u001b[32m0.4343\u001b[0m       0.8855        \u001b[31m0.4353\u001b[0m  0.0111\n",
            "    122       0.8867        0.4344       0.8855        \u001b[31m0.4353\u001b[0m  0.0124\n",
            "    123       0.8867        \u001b[32m0.4341\u001b[0m       0.8855        \u001b[31m0.4352\u001b[0m  0.0109\n",
            "    124       0.8867        0.4342       0.8855        \u001b[31m0.4352\u001b[0m  0.0111\n",
            "    125       0.8867        \u001b[32m0.4340\u001b[0m       0.8855        \u001b[31m0.4351\u001b[0m  0.0119\n",
            "    126       0.8867        0.4341       0.8855        \u001b[31m0.4350\u001b[0m  0.0123\n",
            "    127       0.8867        0.4341       0.8855        \u001b[31m0.4350\u001b[0m  0.0113\n",
            "    128       0.8867        0.4340       0.8855        \u001b[31m0.4349\u001b[0m  0.0124\n",
            "    129       0.8867        \u001b[32m0.4340\u001b[0m       0.8855        \u001b[31m0.4349\u001b[0m  0.0117\n",
            "    130       0.8867        \u001b[32m0.4339\u001b[0m       0.8855        \u001b[31m0.4348\u001b[0m  0.0114\n",
            "    131       0.8867        \u001b[32m0.4339\u001b[0m       0.8855        \u001b[31m0.4348\u001b[0m  0.0123\n",
            "    132       0.8867        \u001b[32m0.4337\u001b[0m       0.8855        \u001b[31m0.4347\u001b[0m  0.0113\n",
            "    133       0.8867        \u001b[32m0.4337\u001b[0m       0.8855        \u001b[31m0.4347\u001b[0m  0.0141\n",
            "    134       0.8867        \u001b[32m0.4336\u001b[0m       0.8855        \u001b[31m0.4346\u001b[0m  0.0120\n",
            "    135       0.8867        \u001b[32m0.4336\u001b[0m       0.8855        \u001b[31m0.4346\u001b[0m  0.0114\n",
            "    136       0.8867        \u001b[32m0.4335\u001b[0m       0.8855        \u001b[31m0.4345\u001b[0m  0.0115\n",
            "    137       0.8867        \u001b[32m0.4334\u001b[0m       0.8855        \u001b[31m0.4345\u001b[0m  0.0110\n",
            "    138       0.8867        \u001b[32m0.4334\u001b[0m       0.8855        \u001b[31m0.4344\u001b[0m  0.0116\n",
            "    139       0.8867        \u001b[32m0.4334\u001b[0m       0.8855        \u001b[31m0.4344\u001b[0m  0.0115\n",
            "    140       0.8867        0.4334       0.8855        \u001b[31m0.4343\u001b[0m  0.0133\n",
            "    141       0.8867        \u001b[32m0.4333\u001b[0m       0.8855        \u001b[31m0.4343\u001b[0m  0.0119\n",
            "    142       0.8867        \u001b[32m0.4332\u001b[0m       0.8855        \u001b[31m0.4342\u001b[0m  0.0132\n",
            "    143       0.8867        \u001b[32m0.4332\u001b[0m       0.8855        \u001b[31m0.4342\u001b[0m  0.0167\n",
            "    144       0.8867        \u001b[32m0.4332\u001b[0m       0.8855        \u001b[31m0.4342\u001b[0m  0.0158\n",
            "    145       0.8867        \u001b[32m0.4330\u001b[0m       0.8855        \u001b[31m0.4341\u001b[0m  0.0307\n",
            "    146       0.8867        0.4331       0.8855        \u001b[31m0.4341\u001b[0m  0.0174\n",
            "    147       0.8867        0.4331       0.8855        \u001b[31m0.4340\u001b[0m  0.0172\n",
            "    148       0.8867        0.4331       0.8855        \u001b[31m0.4340\u001b[0m  0.0190\n",
            "    149       0.8867        \u001b[32m0.4329\u001b[0m       0.8855        \u001b[31m0.4340\u001b[0m  0.0184\n",
            "    150       0.8867        \u001b[32m0.4329\u001b[0m       0.8855        \u001b[31m0.4339\u001b[0m  0.0179\n",
            "    151       0.8867        \u001b[32m0.4329\u001b[0m       0.8855        \u001b[31m0.4339\u001b[0m  0.0114\n",
            "    152       0.8867        0.4329       0.8855        \u001b[31m0.4338\u001b[0m  0.0117\n",
            "    153       0.8867        \u001b[32m0.4327\u001b[0m       0.8855        \u001b[31m0.4338\u001b[0m  0.0120\n",
            "    154       0.8867        0.4328       0.8855        \u001b[31m0.4338\u001b[0m  0.0123\n",
            "    155       0.8867        0.4328       0.8855        \u001b[31m0.4337\u001b[0m  0.0107\n",
            "    156       0.8867        0.4328       0.8855        \u001b[31m0.4337\u001b[0m  0.0113\n",
            "    157       0.8867        \u001b[32m0.4327\u001b[0m       0.8855        \u001b[31m0.4337\u001b[0m  0.0119\n",
            "    158       0.8867        \u001b[32m0.4327\u001b[0m       0.8855        \u001b[31m0.4336\u001b[0m  0.0113\n",
            "    159       0.8867        \u001b[32m0.4325\u001b[0m       0.8855        \u001b[31m0.4336\u001b[0m  0.0201\n",
            "    160       0.8867        0.4326       0.8855        \u001b[31m0.4335\u001b[0m  0.0112\n",
            "    161       0.8867        \u001b[32m0.4325\u001b[0m       0.8855        \u001b[31m0.4335\u001b[0m  0.0120\n",
            "    162       0.8867        \u001b[32m0.4325\u001b[0m       0.8855        \u001b[31m0.4335\u001b[0m  0.0119\n",
            "    163       0.8867        \u001b[32m0.4324\u001b[0m       0.8855        \u001b[31m0.4334\u001b[0m  0.0113\n",
            "    164       0.8867        \u001b[32m0.4323\u001b[0m       0.8855        \u001b[31m0.4334\u001b[0m  0.0111\n",
            "    165       0.8867        0.4324       0.8855        \u001b[31m0.4334\u001b[0m  0.0118\n",
            "    166       0.8867        0.4323       0.8855        \u001b[31m0.4333\u001b[0m  0.0118\n",
            "    167       0.8867        \u001b[32m0.4323\u001b[0m       0.8855        \u001b[31m0.4333\u001b[0m  0.0129\n",
            "    168       0.8867        \u001b[32m0.4322\u001b[0m       0.8855        \u001b[31m0.4333\u001b[0m  0.0115\n",
            "    169       0.8867        0.4323       0.8855        \u001b[31m0.4332\u001b[0m  0.0114\n",
            "    170       0.8867        0.4323       0.8855        \u001b[31m0.4332\u001b[0m  0.0114\n",
            "    171       0.8867        \u001b[32m0.4321\u001b[0m       0.8855        \u001b[31m0.4332\u001b[0m  0.0111\n",
            "    172       0.8867        0.4321       0.8855        \u001b[31m0.4331\u001b[0m  0.0111\n",
            "    173       0.8867        \u001b[32m0.4320\u001b[0m       0.8855        \u001b[31m0.4331\u001b[0m  0.0112\n",
            "    174       0.8867        \u001b[32m0.4320\u001b[0m       0.8855        \u001b[31m0.4331\u001b[0m  0.0124\n",
            "    175       0.8867        0.4320       0.8855        \u001b[31m0.4331\u001b[0m  0.0117\n",
            "    176       0.8867        \u001b[32m0.4320\u001b[0m       0.8855        \u001b[31m0.4330\u001b[0m  0.0113\n",
            "    177       0.8867        \u001b[32m0.4320\u001b[0m       0.8855        \u001b[31m0.4330\u001b[0m  0.0115\n",
            "    178       0.8867        \u001b[32m0.4319\u001b[0m       0.8855        \u001b[31m0.4330\u001b[0m  0.0138\n",
            "    179       0.8867        0.4319       0.8855        \u001b[31m0.4330\u001b[0m  0.0112\n",
            "    180       0.8867        0.4319       0.8855        \u001b[31m0.4329\u001b[0m  0.0122\n",
            "    181       0.8867        0.4319       0.8855        \u001b[31m0.4329\u001b[0m  0.0114\n",
            "    182       0.8867        \u001b[32m0.4318\u001b[0m       0.8855        \u001b[31m0.4329\u001b[0m  0.0109\n",
            "    183       0.8867        \u001b[32m0.4317\u001b[0m       0.8855        \u001b[31m0.4328\u001b[0m  0.0108\n",
            "    184       0.8867        \u001b[32m0.4317\u001b[0m       0.8855        \u001b[31m0.4328\u001b[0m  0.0129\n",
            "    185       0.8867        0.4318       0.8855        \u001b[31m0.4328\u001b[0m  0.0112\n",
            "    186       0.8867        \u001b[32m0.4317\u001b[0m       0.8855        \u001b[31m0.4328\u001b[0m  0.0132\n",
            "    187       0.8867        0.4318       0.8855        \u001b[31m0.4327\u001b[0m  0.0126\n",
            "    188       0.8867        \u001b[32m0.4317\u001b[0m       0.8855        \u001b[31m0.4327\u001b[0m  0.0110\n",
            "    189       0.8867        0.4317       0.8855        \u001b[31m0.4327\u001b[0m  0.0116\n",
            "    190       0.8867        \u001b[32m0.4315\u001b[0m       0.8855        \u001b[31m0.4327\u001b[0m  0.0122\n",
            "    191       0.8867        \u001b[32m0.4315\u001b[0m       0.8855        \u001b[31m0.4326\u001b[0m  0.0113\n",
            "    192       0.8867        0.4316       0.8855        \u001b[31m0.4326\u001b[0m  0.0112\n",
            "    193       0.8867        \u001b[32m0.4315\u001b[0m       0.8855        \u001b[31m0.4326\u001b[0m  0.0110\n",
            "    194       0.8867        0.4316       0.8855        \u001b[31m0.4325\u001b[0m  0.0114\n",
            "    195       0.8867        \u001b[32m0.4315\u001b[0m       0.8855        \u001b[31m0.4325\u001b[0m  0.0111\n",
            "    196       0.8867        \u001b[32m0.4314\u001b[0m       0.8855        \u001b[31m0.4325\u001b[0m  0.0113\n",
            "    197       0.8867        0.4315       0.8855        \u001b[31m0.4325\u001b[0m  0.0110\n",
            "    198       0.8867        \u001b[32m0.4313\u001b[0m       0.8855        \u001b[31m0.4325\u001b[0m  0.0114\n",
            "    199       0.8867        0.4315       0.8855        \u001b[31m0.4324\u001b[0m  0.0124\n",
            "    200       0.8867        0.4314       0.8855        \u001b[31m0.4324\u001b[0m  0.0111\n",
            "  epoch    train_acc    train_loss    valid_acc    valid_loss     dur\n",
            "-------  -----------  ------------  -----------  ------------  ------\n",
            "      1       \u001b[36m0.8821\u001b[0m        \u001b[32m0.6061\u001b[0m       \u001b[35m0.8803\u001b[0m        \u001b[31m0.5943\u001b[0m  0.0155\n",
            "      2       0.8821        \u001b[32m0.5869\u001b[0m       0.8803        \u001b[31m0.5773\u001b[0m  0.0172\n",
            "      3       0.8821        \u001b[32m0.5707\u001b[0m       0.8803        \u001b[31m0.5627\u001b[0m  0.0157\n",
            "      4       0.8821        \u001b[32m0.5567\u001b[0m       0.8803        \u001b[31m0.5501\u001b[0m  0.0154\n",
            "      5       0.8821        \u001b[32m0.5452\u001b[0m       0.8803        \u001b[31m0.5392\u001b[0m  0.0201\n",
            "      6       0.8821        \u001b[32m0.5346\u001b[0m       0.8803        \u001b[31m0.5302\u001b[0m  0.0154\n",
            "      7       0.8821        \u001b[32m0.5262\u001b[0m       0.8803        \u001b[31m0.5225\u001b[0m  0.0155\n",
            "      8       0.8821        \u001b[32m0.5190\u001b[0m       0.8803        \u001b[31m0.5157\u001b[0m  0.0151\n",
            "      9       0.8821        \u001b[32m0.5124\u001b[0m       0.8803        \u001b[31m0.5097\u001b[0m  0.0165\n",
            "     10       0.8821        \u001b[32m0.5067\u001b[0m       0.8803        \u001b[31m0.5045\u001b[0m  0.0151\n",
            "     11       0.8821        \u001b[32m0.5016\u001b[0m       0.8803        \u001b[31m0.5001\u001b[0m  0.0157\n",
            "     12       0.8821        \u001b[32m0.4976\u001b[0m       0.8803        \u001b[31m0.4961\u001b[0m  0.0157\n",
            "     13       0.8821        \u001b[32m0.4934\u001b[0m       0.8803        \u001b[31m0.4925\u001b[0m  0.0155\n",
            "     14       0.8821        \u001b[32m0.4901\u001b[0m       0.8803        \u001b[31m0.4892\u001b[0m  0.0159\n",
            "     15       0.8821        \u001b[32m0.4870\u001b[0m       0.8803        \u001b[31m0.4863\u001b[0m  0.0156\n",
            "     16       0.8821        \u001b[32m0.4842\u001b[0m       0.8803        \u001b[31m0.4837\u001b[0m  0.0178\n",
            "     17       0.8821        \u001b[32m0.4816\u001b[0m       0.8803        \u001b[31m0.4813\u001b[0m  0.0152\n",
            "     18       0.8821        \u001b[32m0.4796\u001b[0m       0.8803        \u001b[31m0.4792\u001b[0m  0.0151\n",
            "     19       0.8821        \u001b[32m0.4774\u001b[0m       0.8803        \u001b[31m0.4771\u001b[0m  0.0168\n",
            "     20       0.8821        \u001b[32m0.4751\u001b[0m       0.8803        \u001b[31m0.4752\u001b[0m  0.0151\n",
            "     21       0.8821        \u001b[32m0.4735\u001b[0m       0.8803        \u001b[31m0.4735\u001b[0m  0.0168\n",
            "     22       0.8821        \u001b[32m0.4718\u001b[0m       0.8803        \u001b[31m0.4719\u001b[0m  0.0157\n",
            "     23       0.8821        \u001b[32m0.4703\u001b[0m       0.8803        \u001b[31m0.4705\u001b[0m  0.0157\n",
            "     24       0.8821        \u001b[32m0.4688\u001b[0m       0.8803        \u001b[31m0.4690\u001b[0m  0.0152\n",
            "     25       0.8821        \u001b[32m0.4672\u001b[0m       0.8803        \u001b[31m0.4678\u001b[0m  0.0152\n",
            "     26       0.8821        \u001b[32m0.4660\u001b[0m       0.8803        \u001b[31m0.4666\u001b[0m  0.0201\n",
            "     27       0.8821        \u001b[32m0.4646\u001b[0m       0.8803        \u001b[31m0.4654\u001b[0m  0.0256\n",
            "     28       0.8821        \u001b[32m0.4640\u001b[0m       0.8803        \u001b[31m0.4644\u001b[0m  0.0161\n",
            "     29       0.8821        \u001b[32m0.4626\u001b[0m       0.8803        \u001b[31m0.4634\u001b[0m  0.0195\n",
            "     30       0.8821        \u001b[32m0.4615\u001b[0m       0.8803        \u001b[31m0.4625\u001b[0m  0.0164\n",
            "     31       0.8821        \u001b[32m0.4610\u001b[0m       0.8803        \u001b[31m0.4616\u001b[0m  0.0161\n",
            "     32       0.8821        \u001b[32m0.4600\u001b[0m       0.8803        \u001b[31m0.4608\u001b[0m  0.0154\n",
            "     33       0.8821        \u001b[32m0.4593\u001b[0m       0.8803        \u001b[31m0.4601\u001b[0m  0.0154\n",
            "     34       0.8821        \u001b[32m0.4587\u001b[0m       0.8803        \u001b[31m0.4593\u001b[0m  0.0154\n",
            "     35       0.8821        \u001b[32m0.4578\u001b[0m       0.8803        \u001b[31m0.4586\u001b[0m  0.0160\n",
            "     36       0.8821        \u001b[32m0.4569\u001b[0m       0.8803        \u001b[31m0.4580\u001b[0m  0.0175\n",
            "     37       0.8821        \u001b[32m0.4563\u001b[0m       0.8803        \u001b[31m0.4573\u001b[0m  0.0492\n",
            "     38       0.8821        \u001b[32m0.4556\u001b[0m       0.8803        \u001b[31m0.4567\u001b[0m  0.0248\n",
            "     39       0.8821        \u001b[32m0.4552\u001b[0m       0.8803        \u001b[31m0.4562\u001b[0m  0.0240\n",
            "     40       0.8821        \u001b[32m0.4544\u001b[0m       0.8803        \u001b[31m0.4556\u001b[0m  0.0173\n",
            "     41       0.8821        \u001b[32m0.4542\u001b[0m       0.8803        \u001b[31m0.4551\u001b[0m  0.0157\n",
            "     42       0.8821        \u001b[32m0.4534\u001b[0m       0.8803        \u001b[31m0.4546\u001b[0m  0.0154\n",
            "     43       0.8821        \u001b[32m0.4531\u001b[0m       0.8803        \u001b[31m0.4542\u001b[0m  0.0157\n",
            "     44       0.8821        \u001b[32m0.4526\u001b[0m       0.8803        \u001b[31m0.4537\u001b[0m  0.0215\n",
            "     45       0.8821        \u001b[32m0.4524\u001b[0m       0.8803        \u001b[31m0.4533\u001b[0m  0.0237\n",
            "     46       0.8821        \u001b[32m0.4517\u001b[0m       0.8803        \u001b[31m0.4529\u001b[0m  0.0223\n",
            "     47       0.8821        \u001b[32m0.4514\u001b[0m       0.8803        \u001b[31m0.4525\u001b[0m  0.0248\n",
            "     48       0.8821        \u001b[32m0.4509\u001b[0m       0.8803        \u001b[31m0.4521\u001b[0m  0.0221\n",
            "     49       0.8821        \u001b[32m0.4507\u001b[0m       0.8803        \u001b[31m0.4517\u001b[0m  0.0221\n",
            "     50       0.8821        \u001b[32m0.4502\u001b[0m       0.8803        \u001b[31m0.4514\u001b[0m  0.0254\n",
            "     51       0.8821        \u001b[32m0.4500\u001b[0m       0.8803        \u001b[31m0.4510\u001b[0m  0.0269\n",
            "     52       0.8821        \u001b[32m0.4495\u001b[0m       0.8803        \u001b[31m0.4507\u001b[0m  0.0237\n",
            "     53       0.8821        \u001b[32m0.4492\u001b[0m       0.8803        \u001b[31m0.4504\u001b[0m  0.0234\n",
            "     54       0.8821        \u001b[32m0.4487\u001b[0m       0.8803        \u001b[31m0.4501\u001b[0m  0.0236\n",
            "     55       0.8821        \u001b[32m0.4485\u001b[0m       0.8803        \u001b[31m0.4498\u001b[0m  0.0268\n",
            "     56       0.8821        \u001b[32m0.4483\u001b[0m       0.8803        \u001b[31m0.4495\u001b[0m  0.0310\n",
            "     57       0.8821        \u001b[32m0.4479\u001b[0m       0.8803        \u001b[31m0.4492\u001b[0m  0.0210\n",
            "     58       0.8821        \u001b[32m0.4478\u001b[0m       0.8803        \u001b[31m0.4489\u001b[0m  0.0206\n",
            "     59       0.8821        \u001b[32m0.4475\u001b[0m       0.8803        \u001b[31m0.4487\u001b[0m  0.0150\n",
            "     60       0.8821        \u001b[32m0.4470\u001b[0m       0.8803        \u001b[31m0.4484\u001b[0m  0.0162\n",
            "     61       0.8821        \u001b[32m0.4468\u001b[0m       0.8803        \u001b[31m0.4481\u001b[0m  0.0149\n",
            "     62       0.8821        \u001b[32m0.4467\u001b[0m       0.8803        \u001b[31m0.4479\u001b[0m  0.0153\n",
            "     63       0.8821        \u001b[32m0.4464\u001b[0m       0.8803        \u001b[31m0.4477\u001b[0m  0.0151\n",
            "     64       0.8821        \u001b[32m0.4463\u001b[0m       0.8803        \u001b[31m0.4475\u001b[0m  0.0148\n",
            "     65       0.8821        \u001b[32m0.4460\u001b[0m       0.8803        \u001b[31m0.4472\u001b[0m  0.0149\n",
            "     66       0.8821        \u001b[32m0.4457\u001b[0m       0.8803        \u001b[31m0.4470\u001b[0m  0.0157\n",
            "     67       0.8821        \u001b[32m0.4455\u001b[0m       0.8803        \u001b[31m0.4468\u001b[0m  0.0152\n",
            "     68       0.8821        \u001b[32m0.4453\u001b[0m       0.8803        \u001b[31m0.4466\u001b[0m  0.0156\n",
            "     69       0.8821        \u001b[32m0.4451\u001b[0m       0.8803        \u001b[31m0.4464\u001b[0m  0.0153\n",
            "     70       0.8821        \u001b[32m0.4448\u001b[0m       0.8803        \u001b[31m0.4462\u001b[0m  0.0161\n",
            "     71       0.8821        \u001b[32m0.4447\u001b[0m       0.8803        \u001b[31m0.4461\u001b[0m  0.0147\n",
            "     72       0.8821        \u001b[32m0.4446\u001b[0m       0.8803        \u001b[31m0.4459\u001b[0m  0.0151\n",
            "     73       0.8821        \u001b[32m0.4443\u001b[0m       0.8803        \u001b[31m0.4457\u001b[0m  0.0180\n",
            "     74       0.8821        \u001b[32m0.4439\u001b[0m       0.8803        \u001b[31m0.4455\u001b[0m  0.0155\n",
            "     75       0.8821        0.4441       0.8803        \u001b[31m0.4454\u001b[0m  0.0151\n",
            "     76       0.8821        \u001b[32m0.4439\u001b[0m       0.8803        \u001b[31m0.4452\u001b[0m  0.0159\n",
            "     77       0.8821        \u001b[32m0.4435\u001b[0m       0.8803        \u001b[31m0.4451\u001b[0m  0.0166\n",
            "     78       0.8821        0.4435       0.8803        \u001b[31m0.4449\u001b[0m  0.0152\n",
            "     79       0.8821        \u001b[32m0.4432\u001b[0m       0.8803        \u001b[31m0.4448\u001b[0m  0.0147\n",
            "     80       0.8821        \u001b[32m0.4431\u001b[0m       0.8803        \u001b[31m0.4446\u001b[0m  0.0161\n",
            "     81       0.8821        \u001b[32m0.4431\u001b[0m       0.8803        \u001b[31m0.4445\u001b[0m  0.0172\n",
            "     82       0.8821        \u001b[32m0.4428\u001b[0m       0.8803        \u001b[31m0.4443\u001b[0m  0.0149\n",
            "     83       0.8821        \u001b[32m0.4427\u001b[0m       0.8803        \u001b[31m0.4442\u001b[0m  0.0160\n",
            "     84       0.8821        \u001b[32m0.4426\u001b[0m       0.8803        \u001b[31m0.4441\u001b[0m  0.0165\n",
            "     85       0.8821        \u001b[32m0.4425\u001b[0m       0.8803        \u001b[31m0.4439\u001b[0m  0.0165\n",
            "     86       0.8821        \u001b[32m0.4425\u001b[0m       0.8803        \u001b[31m0.4438\u001b[0m  0.0155\n",
            "     87       0.8821        \u001b[32m0.4424\u001b[0m       0.8803        \u001b[31m0.4437\u001b[0m  0.0208\n",
            "     88       0.8821        \u001b[32m0.4421\u001b[0m       0.8803        \u001b[31m0.4436\u001b[0m  0.0234\n",
            "     89       0.8821        \u001b[32m0.4419\u001b[0m       0.8803        \u001b[31m0.4434\u001b[0m  0.0334\n",
            "     90       0.8821        0.4420       0.8803        \u001b[31m0.4433\u001b[0m  0.0244\n",
            "     91       0.8821        \u001b[32m0.4417\u001b[0m       0.8803        \u001b[31m0.4432\u001b[0m  0.0243\n",
            "     92       0.8821        \u001b[32m0.4416\u001b[0m       0.8803        \u001b[31m0.4431\u001b[0m  0.0151\n",
            "     93       0.8821        0.4416       0.8803        \u001b[31m0.4430\u001b[0m  0.0158\n",
            "     94       0.8821        \u001b[32m0.4414\u001b[0m       0.8803        \u001b[31m0.4429\u001b[0m  0.0152\n",
            "     95       0.8821        \u001b[32m0.4413\u001b[0m       0.8803        \u001b[31m0.4428\u001b[0m  0.0164\n",
            "     96       0.8821        \u001b[32m0.4412\u001b[0m       0.8803        \u001b[31m0.4427\u001b[0m  0.0152\n",
            "     97       0.8821        \u001b[32m0.4411\u001b[0m       0.8803        \u001b[31m0.4426\u001b[0m  0.0154\n",
            "     98       0.8821        \u001b[32m0.4410\u001b[0m       0.8803        \u001b[31m0.4425\u001b[0m  0.0164\n",
            "     99       0.8821        \u001b[32m0.4410\u001b[0m       0.8803        \u001b[31m0.4424\u001b[0m  0.0218\n",
            "    100       0.8821        \u001b[32m0.4408\u001b[0m       0.8803        \u001b[31m0.4423\u001b[0m  0.0190\n",
            "    101       0.8821        \u001b[32m0.4407\u001b[0m       0.8803        \u001b[31m0.4422\u001b[0m  0.0212\n",
            "    102       0.8821        \u001b[32m0.4407\u001b[0m       0.8803        \u001b[31m0.4421\u001b[0m  0.0157\n",
            "    103       0.8821        0.4407       0.8803        \u001b[31m0.4420\u001b[0m  0.0164\n",
            "    104       0.8821        \u001b[32m0.4403\u001b[0m       0.8803        \u001b[31m0.4420\u001b[0m  0.0159\n",
            "    105       0.8821        0.4404       0.8803        \u001b[31m0.4419\u001b[0m  0.0151\n",
            "    106       0.8821        0.4404       0.8803        \u001b[31m0.4418\u001b[0m  0.0150\n",
            "    107       0.8821        \u001b[32m0.4401\u001b[0m       0.8803        \u001b[31m0.4417\u001b[0m  0.0166\n",
            "    108       0.8821        0.4401       0.8803        \u001b[31m0.4416\u001b[0m  0.0151\n",
            "    109       0.8821        0.4401       0.8803        \u001b[31m0.4415\u001b[0m  0.0171\n",
            "    110       0.8821        \u001b[32m0.4401\u001b[0m       0.8803        \u001b[31m0.4415\u001b[0m  0.0160\n",
            "    111       0.8821        \u001b[32m0.4397\u001b[0m       0.8803        \u001b[31m0.4414\u001b[0m  0.0163\n",
            "    112       0.8821        0.4398       0.8803        \u001b[31m0.4413\u001b[0m  0.0156\n",
            "    113       0.8821        0.4399       0.8803        \u001b[31m0.4412\u001b[0m  0.0154\n",
            "    114       0.8821        \u001b[32m0.4396\u001b[0m       0.8803        \u001b[31m0.4411\u001b[0m  0.0161\n",
            "    115       0.8821        \u001b[32m0.4395\u001b[0m       0.8803        \u001b[31m0.4411\u001b[0m  0.0160\n",
            "    116       0.8821        0.4396       0.8803        \u001b[31m0.4410\u001b[0m  0.0152\n",
            "    117       0.8821        \u001b[32m0.4395\u001b[0m       0.8803        \u001b[31m0.4409\u001b[0m  0.0159\n",
            "    118       0.8821        \u001b[32m0.4392\u001b[0m       0.8803        \u001b[31m0.4409\u001b[0m  0.0158\n",
            "    119       0.8821        0.4392       0.8803        \u001b[31m0.4408\u001b[0m  0.0150\n",
            "    120       0.8821        \u001b[32m0.4392\u001b[0m       0.8803        \u001b[31m0.4407\u001b[0m  0.0147\n",
            "    121       0.8821        0.4392       0.8803        \u001b[31m0.4407\u001b[0m  0.0151\n",
            "    122       0.8821        \u001b[32m0.4391\u001b[0m       0.8803        \u001b[31m0.4406\u001b[0m  0.0156\n",
            "    123       0.8821        \u001b[32m0.4390\u001b[0m       0.8803        \u001b[31m0.4405\u001b[0m  0.0162\n",
            "    124       0.8821        \u001b[32m0.4389\u001b[0m       0.8803        \u001b[31m0.4405\u001b[0m  0.0152\n",
            "    125       0.8821        \u001b[32m0.4389\u001b[0m       0.8803        \u001b[31m0.4404\u001b[0m  0.0149\n",
            "    126       0.8821        0.4389       0.8803        \u001b[31m0.4404\u001b[0m  0.0157\n",
            "    127       0.8821        \u001b[32m0.4388\u001b[0m       0.8803        \u001b[31m0.4403\u001b[0m  0.0154\n",
            "    128       0.8821        0.4388       0.8803        \u001b[31m0.4402\u001b[0m  0.0151\n",
            "    129       0.8821        \u001b[32m0.4385\u001b[0m       0.8803        \u001b[31m0.4402\u001b[0m  0.0166\n",
            "    130       0.8821        0.4386       0.8803        \u001b[31m0.4401\u001b[0m  0.0158\n",
            "    131       0.8821        \u001b[32m0.4385\u001b[0m       0.8803        \u001b[31m0.4401\u001b[0m  0.0153\n",
            "    132       0.8821        0.4385       0.8803        \u001b[31m0.4400\u001b[0m  0.0156\n",
            "    133       0.8821        \u001b[32m0.4384\u001b[0m       0.8803        \u001b[31m0.4400\u001b[0m  0.0174\n",
            "    134       0.8821        0.4385       0.8803        \u001b[31m0.4399\u001b[0m  0.0220\n",
            "    135       0.8821        \u001b[32m0.4382\u001b[0m       0.8803        \u001b[31m0.4399\u001b[0m  0.0288\n",
            "    136       0.8821        \u001b[32m0.4382\u001b[0m       0.8803        \u001b[31m0.4398\u001b[0m  0.0223\n",
            "    137       0.8821        0.4382       0.8803        \u001b[31m0.4398\u001b[0m  0.0153\n",
            "    138       0.8821        \u001b[32m0.4381\u001b[0m       0.8803        \u001b[31m0.4397\u001b[0m  0.0157\n",
            "    139       0.8821        0.4382       0.8803        \u001b[31m0.4397\u001b[0m  0.0154\n",
            "    140       0.8821        \u001b[32m0.4381\u001b[0m       0.8803        \u001b[31m0.4396\u001b[0m  0.0153\n",
            "    141       0.8821        \u001b[32m0.4380\u001b[0m       0.8803        \u001b[31m0.4396\u001b[0m  0.0164\n",
            "    142       0.8821        \u001b[32m0.4380\u001b[0m       0.8803        \u001b[31m0.4395\u001b[0m  0.0158\n",
            "    143       0.8821        \u001b[32m0.4379\u001b[0m       0.8803        \u001b[31m0.4395\u001b[0m  0.0158\n",
            "    144       0.8821        \u001b[32m0.4379\u001b[0m       0.8803        \u001b[31m0.4394\u001b[0m  0.0155\n",
            "    145       0.8821        0.4379       0.8803        \u001b[31m0.4394\u001b[0m  0.0161\n",
            "    146       0.8821        \u001b[32m0.4378\u001b[0m       0.8803        \u001b[31m0.4393\u001b[0m  0.0156\n",
            "    147       0.8821        \u001b[32m0.4377\u001b[0m       0.8803        \u001b[31m0.4393\u001b[0m  0.0173\n",
            "    148       0.8821        0.4378       0.8803        \u001b[31m0.4392\u001b[0m  0.0153\n",
            "    149       0.8821        \u001b[32m0.4377\u001b[0m       0.8803        \u001b[31m0.4392\u001b[0m  0.0154\n",
            "    150       0.8821        \u001b[32m0.4375\u001b[0m       0.8803        \u001b[31m0.4392\u001b[0m  0.0196\n",
            "    151       0.8821        0.4376       0.8803        \u001b[31m0.4391\u001b[0m  0.0182\n",
            "    152       0.8821        \u001b[32m0.4375\u001b[0m       0.8803        \u001b[31m0.4391\u001b[0m  0.0169\n",
            "    153       0.8821        \u001b[32m0.4375\u001b[0m       0.8803        \u001b[31m0.4390\u001b[0m  0.0166\n",
            "    154       0.8821        \u001b[32m0.4374\u001b[0m       0.8803        \u001b[31m0.4390\u001b[0m  0.0164\n",
            "    155       0.8821        \u001b[32m0.4374\u001b[0m       0.8803        \u001b[31m0.4389\u001b[0m  0.0160\n",
            "    156       0.8821        \u001b[32m0.4373\u001b[0m       0.8803        \u001b[31m0.4389\u001b[0m  0.0150\n",
            "    157       0.8821        \u001b[32m0.4373\u001b[0m       0.8803        \u001b[31m0.4389\u001b[0m  0.0150\n",
            "    158       0.8821        0.4373       0.8803        \u001b[31m0.4388\u001b[0m  0.0160\n",
            "    159       0.8821        \u001b[32m0.4372\u001b[0m       0.8803        \u001b[31m0.4388\u001b[0m  0.0157\n",
            "    160       0.8821        \u001b[32m0.4371\u001b[0m       0.8803        \u001b[31m0.4388\u001b[0m  0.0165\n",
            "    161       0.8821        0.4371       0.8803        \u001b[31m0.4387\u001b[0m  0.0178\n",
            "    162       0.8821        0.4372       0.8803        \u001b[31m0.4387\u001b[0m  0.0221\n",
            "    163       0.8821        0.4372       0.8803        \u001b[31m0.4386\u001b[0m  0.0546\n",
            "    164       0.8821        \u001b[32m0.4371\u001b[0m       0.8803        \u001b[31m0.4386\u001b[0m  0.0357\n",
            "    165       0.8821        \u001b[32m0.4370\u001b[0m       0.8803        \u001b[31m0.4386\u001b[0m  0.0420\n",
            "    166       0.8821        \u001b[32m0.4369\u001b[0m       0.8803        \u001b[31m0.4385\u001b[0m  0.0289\n",
            "    167       0.8821        0.4370       0.8803        \u001b[31m0.4385\u001b[0m  0.0287\n",
            "    168       0.8821        \u001b[32m0.4369\u001b[0m       0.8803        \u001b[31m0.4385\u001b[0m  0.0335\n",
            "    169       0.8821        \u001b[32m0.4368\u001b[0m       0.8803        \u001b[31m0.4384\u001b[0m  0.0251\n",
            "    170       0.8821        0.4369       0.8803        \u001b[31m0.4384\u001b[0m  0.0329\n",
            "    171       0.8821        0.4368       0.8803        \u001b[31m0.4384\u001b[0m  0.0262\n",
            "    172       0.8821        \u001b[32m0.4367\u001b[0m       0.8803        \u001b[31m0.4383\u001b[0m  0.0418\n",
            "    173       0.8821        0.4367       0.8803        \u001b[31m0.4383\u001b[0m  0.0159\n",
            "    174       0.8821        \u001b[32m0.4366\u001b[0m       0.8803        \u001b[31m0.4383\u001b[0m  0.0155\n",
            "    175       0.8821        0.4367       0.8803        \u001b[31m0.4382\u001b[0m  0.0150\n",
            "    176       0.8821        \u001b[32m0.4366\u001b[0m       0.8803        \u001b[31m0.4382\u001b[0m  0.0152\n",
            "    177       0.8821        0.4366       0.8803        \u001b[31m0.4382\u001b[0m  0.0153\n",
            "    178       0.8821        0.4367       0.8803        \u001b[31m0.4381\u001b[0m  0.0239\n",
            "    179       0.8821        \u001b[32m0.4365\u001b[0m       0.8803        \u001b[31m0.4381\u001b[0m  0.0203\n",
            "    180       0.8821        0.4366       0.8803        \u001b[31m0.4381\u001b[0m  0.0159\n",
            "    181       0.8821        \u001b[32m0.4364\u001b[0m       0.8803        \u001b[31m0.4381\u001b[0m  0.0185\n",
            "    182       0.8821        0.4364       0.8803        \u001b[31m0.4380\u001b[0m  0.0266\n",
            "    183       0.8821        0.4365       0.8803        \u001b[31m0.4380\u001b[0m  0.0164\n",
            "    184       0.8821        0.4365       0.8803        \u001b[31m0.4380\u001b[0m  0.0236\n",
            "    185       0.8821        \u001b[32m0.4363\u001b[0m       0.8803        \u001b[31m0.4379\u001b[0m  0.0172\n",
            "    186       0.8821        \u001b[32m0.4363\u001b[0m       0.8803        \u001b[31m0.4379\u001b[0m  0.0170\n",
            "    187       0.8821        \u001b[32m0.4363\u001b[0m       0.8803        \u001b[31m0.4379\u001b[0m  0.0165\n",
            "    188       0.8821        0.4363       0.8803        \u001b[31m0.4379\u001b[0m  0.0203\n",
            "    189       0.8821        \u001b[32m0.4362\u001b[0m       0.8803        \u001b[31m0.4378\u001b[0m  0.0201\n",
            "    190       0.8821        \u001b[32m0.4361\u001b[0m       0.8803        \u001b[31m0.4378\u001b[0m  0.0177\n",
            "    191       0.8821        0.4361       0.8803        \u001b[31m0.4378\u001b[0m  0.0294\n",
            "    192       0.8821        0.4362       0.8803        \u001b[31m0.4378\u001b[0m  0.0237\n",
            "    193       0.8821        0.4361       0.8803        \u001b[31m0.4377\u001b[0m  0.0157\n",
            "    194       0.8821        0.4361       0.8803        \u001b[31m0.4377\u001b[0m  0.0154\n",
            "    195       0.8821        \u001b[32m0.4361\u001b[0m       0.8803        \u001b[31m0.4377\u001b[0m  0.0152\n",
            "    196       0.8821        0.4361       0.8803        \u001b[31m0.4377\u001b[0m  0.0150\n",
            "    197       0.8821        0.4361       0.8803        \u001b[31m0.4376\u001b[0m  0.0226\n",
            "    198       0.8821        0.4361       0.8803        \u001b[31m0.4376\u001b[0m  0.0151\n",
            "    199       0.8821        \u001b[32m0.4360\u001b[0m       0.8803        \u001b[31m0.4376\u001b[0m  0.0158\n",
            "    200       0.8821        0.4360       0.8803        \u001b[31m0.4376\u001b[0m  0.0156\n",
            "  epoch    train_acc    train_loss    valid_acc    valid_loss     dur\n",
            "-------  -----------  ------------  -----------  ------------  ------\n",
            "      1       \u001b[36m0.8830\u001b[0m        \u001b[32m0.4758\u001b[0m       \u001b[35m0.8808\u001b[0m        \u001b[31m0.4760\u001b[0m  0.0187\n",
            "      2       0.8830        \u001b[32m0.4734\u001b[0m       0.8808        \u001b[31m0.4737\u001b[0m  0.0201\n",
            "      3       0.8830        \u001b[32m0.4712\u001b[0m       0.8808        \u001b[31m0.4718\u001b[0m  0.0194\n",
            "      4       0.8830        \u001b[32m0.4693\u001b[0m       0.8808        \u001b[31m0.4700\u001b[0m  0.0251\n",
            "      5       0.8830        \u001b[32m0.4676\u001b[0m       0.8808        \u001b[31m0.4683\u001b[0m  0.0312\n",
            "      6       0.8830        \u001b[32m0.4659\u001b[0m       0.8808        \u001b[31m0.4668\u001b[0m  0.0212\n",
            "      7       0.8830        \u001b[32m0.4644\u001b[0m       0.8808        \u001b[31m0.4654\u001b[0m  0.0191\n",
            "      8       0.8830        \u001b[32m0.4630\u001b[0m       0.8808        \u001b[31m0.4640\u001b[0m  0.0218\n",
            "      9       0.8830        \u001b[32m0.4617\u001b[0m       0.8808        \u001b[31m0.4628\u001b[0m  0.0193\n",
            "     10       0.8830        \u001b[32m0.4606\u001b[0m       0.8808        \u001b[31m0.4617\u001b[0m  0.0192\n",
            "     11       0.8830        \u001b[32m0.4595\u001b[0m       0.8808        \u001b[31m0.4607\u001b[0m  0.0190\n",
            "     12       0.8830        \u001b[32m0.4584\u001b[0m       0.8808        \u001b[31m0.4597\u001b[0m  0.0207\n",
            "     13       0.8830        \u001b[32m0.4575\u001b[0m       0.8808        \u001b[31m0.4588\u001b[0m  0.0201\n",
            "     14       0.8830        \u001b[32m0.4566\u001b[0m       0.8808        \u001b[31m0.4580\u001b[0m  0.0219\n",
            "     15       0.8830        \u001b[32m0.4557\u001b[0m       0.8808        \u001b[31m0.4572\u001b[0m  0.0198\n",
            "     16       0.8830        \u001b[32m0.4550\u001b[0m       0.8808        \u001b[31m0.4564\u001b[0m  0.0206\n",
            "     17       0.8830        \u001b[32m0.4543\u001b[0m       0.8808        \u001b[31m0.4557\u001b[0m  0.0213\n",
            "     18       0.8830        \u001b[32m0.4536\u001b[0m       0.8808        \u001b[31m0.4551\u001b[0m  0.0221\n",
            "     19       0.8830        \u001b[32m0.4529\u001b[0m       0.8808        \u001b[31m0.4544\u001b[0m  0.0302\n",
            "     20       0.8830        \u001b[32m0.4523\u001b[0m       0.8808        \u001b[31m0.4538\u001b[0m  0.0301\n",
            "     21       0.8830        \u001b[32m0.4517\u001b[0m       0.8808        \u001b[31m0.4533\u001b[0m  0.0283\n",
            "     22       0.8830        \u001b[32m0.4511\u001b[0m       0.8808        \u001b[31m0.4527\u001b[0m  0.0200\n",
            "     23       0.8830        \u001b[32m0.4506\u001b[0m       0.8808        \u001b[31m0.4522\u001b[0m  0.0195\n",
            "     24       0.8830        \u001b[32m0.4501\u001b[0m       0.8808        \u001b[31m0.4517\u001b[0m  0.0195\n",
            "     25       0.8830        \u001b[32m0.4496\u001b[0m       0.8808        \u001b[31m0.4513\u001b[0m  0.0192\n",
            "     26       0.8830        \u001b[32m0.4492\u001b[0m       0.8808        \u001b[31m0.4508\u001b[0m  0.0193\n",
            "     27       0.8830        \u001b[32m0.4487\u001b[0m       0.8808        \u001b[31m0.4504\u001b[0m  0.0193\n",
            "     28       0.8830        \u001b[32m0.4483\u001b[0m       0.8808        \u001b[31m0.4500\u001b[0m  0.0208\n",
            "     29       0.8830        \u001b[32m0.4479\u001b[0m       0.8808        \u001b[31m0.4496\u001b[0m  0.0244\n",
            "     30       0.8830        \u001b[32m0.4475\u001b[0m       0.8808        \u001b[31m0.4493\u001b[0m  0.0255\n",
            "     31       0.8830        \u001b[32m0.4471\u001b[0m       0.8808        \u001b[31m0.4489\u001b[0m  0.0194\n",
            "     32       0.8830        \u001b[32m0.4468\u001b[0m       0.8808        \u001b[31m0.4486\u001b[0m  0.0198\n",
            "     33       0.8830        \u001b[32m0.4464\u001b[0m       0.8808        \u001b[31m0.4482\u001b[0m  0.0193\n",
            "     34       0.8830        \u001b[32m0.4461\u001b[0m       0.8808        \u001b[31m0.4479\u001b[0m  0.0190\n",
            "     35       0.8830        \u001b[32m0.4458\u001b[0m       0.8808        \u001b[31m0.4476\u001b[0m  0.0193\n",
            "     36       0.8830        \u001b[32m0.4455\u001b[0m       0.8808        \u001b[31m0.4473\u001b[0m  0.0194\n",
            "     37       0.8830        \u001b[32m0.4452\u001b[0m       0.8808        \u001b[31m0.4471\u001b[0m  0.0192\n",
            "     38       0.8830        \u001b[32m0.4450\u001b[0m       0.8808        \u001b[31m0.4468\u001b[0m  0.0202\n",
            "     39       0.8830        \u001b[32m0.4447\u001b[0m       0.8808        \u001b[31m0.4465\u001b[0m  0.0193\n",
            "     40       0.8830        \u001b[32m0.4444\u001b[0m       0.8808        \u001b[31m0.4463\u001b[0m  0.0215\n",
            "     41       0.8830        \u001b[32m0.4441\u001b[0m       0.8808        \u001b[31m0.4460\u001b[0m  0.0219\n",
            "     42       0.8830        \u001b[32m0.4440\u001b[0m       0.8808        \u001b[31m0.4458\u001b[0m  0.0189\n",
            "     43       0.8830        \u001b[32m0.4437\u001b[0m       0.8808        \u001b[31m0.4456\u001b[0m  0.0196\n",
            "     44       0.8830        \u001b[32m0.4435\u001b[0m       0.8808        \u001b[31m0.4454\u001b[0m  0.0200\n",
            "     45       0.8830        \u001b[32m0.4433\u001b[0m       0.8808        \u001b[31m0.4451\u001b[0m  0.0249\n",
            "     46       0.8830        \u001b[32m0.4431\u001b[0m       0.8808        \u001b[31m0.4449\u001b[0m  0.0333\n",
            "     47       0.8830        \u001b[32m0.4428\u001b[0m       0.8808        \u001b[31m0.4447\u001b[0m  0.0289\n",
            "     48       0.8830        \u001b[32m0.4426\u001b[0m       0.8808        \u001b[31m0.4445\u001b[0m  0.0274\n",
            "     49       0.8830        \u001b[32m0.4424\u001b[0m       0.8808        \u001b[31m0.4443\u001b[0m  0.0194\n",
            "     50       0.8830        \u001b[32m0.4423\u001b[0m       0.8808        \u001b[31m0.4442\u001b[0m  0.0207\n",
            "     51       0.8830        \u001b[32m0.4421\u001b[0m       0.8808        \u001b[31m0.4440\u001b[0m  0.0201\n",
            "     52       0.8830        \u001b[32m0.4419\u001b[0m       0.8808        \u001b[31m0.4438\u001b[0m  0.0189\n",
            "     53       0.8830        \u001b[32m0.4417\u001b[0m       0.8808        \u001b[31m0.4436\u001b[0m  0.0209\n",
            "     54       0.8830        \u001b[32m0.4415\u001b[0m       0.8808        \u001b[31m0.4435\u001b[0m  0.0203\n",
            "     55       0.8830        \u001b[32m0.4414\u001b[0m       0.8808        \u001b[31m0.4433\u001b[0m  0.0209\n",
            "     56       0.8830        \u001b[32m0.4412\u001b[0m       0.8808        \u001b[31m0.4432\u001b[0m  0.0214\n",
            "     57       0.8830        \u001b[32m0.4411\u001b[0m       0.8808        \u001b[31m0.4430\u001b[0m  0.0195\n",
            "     58       0.8830        \u001b[32m0.4409\u001b[0m       0.8808        \u001b[31m0.4429\u001b[0m  0.0192\n",
            "     59       0.8830        \u001b[32m0.4408\u001b[0m       0.8808        \u001b[31m0.4427\u001b[0m  0.0194\n",
            "     60       0.8830        \u001b[32m0.4406\u001b[0m       0.8808        \u001b[31m0.4426\u001b[0m  0.0195\n",
            "     61       0.8830        \u001b[32m0.4405\u001b[0m       0.8808        \u001b[31m0.4425\u001b[0m  0.0195\n",
            "     62       0.8830        \u001b[32m0.4404\u001b[0m       0.8808        \u001b[31m0.4423\u001b[0m  0.0216\n",
            "     63       0.8830        \u001b[32m0.4403\u001b[0m       0.8808        \u001b[31m0.4422\u001b[0m  0.0198\n",
            "     64       0.8830        \u001b[32m0.4401\u001b[0m       0.8808        \u001b[31m0.4421\u001b[0m  0.0192\n",
            "     65       0.8830        \u001b[32m0.4400\u001b[0m       0.8808        \u001b[31m0.4420\u001b[0m  0.0200\n",
            "     66       0.8830        \u001b[32m0.4399\u001b[0m       0.8808        \u001b[31m0.4419\u001b[0m  0.0207\n",
            "     67       0.8830        \u001b[32m0.4397\u001b[0m       0.8808        \u001b[31m0.4417\u001b[0m  0.0194\n",
            "     68       0.8830        \u001b[32m0.4396\u001b[0m       0.8808        \u001b[31m0.4416\u001b[0m  0.0212\n",
            "     69       0.8830        \u001b[32m0.4395\u001b[0m       0.8808        \u001b[31m0.4415\u001b[0m  0.0193\n",
            "     70       0.8830        \u001b[32m0.4394\u001b[0m       0.8808        \u001b[31m0.4414\u001b[0m  0.0237\n",
            "     71       0.8830        \u001b[32m0.4393\u001b[0m       0.8808        \u001b[31m0.4413\u001b[0m  0.0197\n",
            "     72       0.8830        \u001b[32m0.4392\u001b[0m       0.8808        \u001b[31m0.4412\u001b[0m  0.0304\n",
            "     73       0.8830        \u001b[32m0.4391\u001b[0m       0.8808        \u001b[31m0.4411\u001b[0m  0.0314\n",
            "     74       0.8830        \u001b[32m0.4390\u001b[0m       0.8808        \u001b[31m0.4410\u001b[0m  0.0198\n",
            "     75       0.8830        \u001b[32m0.4389\u001b[0m       0.8808        \u001b[31m0.4409\u001b[0m  0.0213\n",
            "     76       0.8830        \u001b[32m0.4388\u001b[0m       0.8808        \u001b[31m0.4408\u001b[0m  0.0205\n",
            "     77       0.8830        \u001b[32m0.4387\u001b[0m       0.8808        \u001b[31m0.4407\u001b[0m  0.0192\n",
            "     78       0.8830        \u001b[32m0.4386\u001b[0m       0.8808        \u001b[31m0.4406\u001b[0m  0.0194\n",
            "     79       0.8830        \u001b[32m0.4385\u001b[0m       0.8808        \u001b[31m0.4405\u001b[0m  0.0238\n",
            "     80       0.8830        \u001b[32m0.4384\u001b[0m       0.8808        \u001b[31m0.4405\u001b[0m  0.0301\n",
            "     81       0.8830        \u001b[32m0.4383\u001b[0m       0.8808        \u001b[31m0.4404\u001b[0m  0.0247\n",
            "     82       0.8830        \u001b[32m0.4382\u001b[0m       0.8808        \u001b[31m0.4403\u001b[0m  0.0204\n",
            "     83       0.8830        \u001b[32m0.4382\u001b[0m       0.8808        \u001b[31m0.4402\u001b[0m  0.0193\n",
            "     84       0.8830        \u001b[32m0.4381\u001b[0m       0.8808        \u001b[31m0.4401\u001b[0m  0.0188\n",
            "     85       0.8830        \u001b[32m0.4380\u001b[0m       0.8808        \u001b[31m0.4401\u001b[0m  0.0198\n",
            "     86       0.8830        \u001b[32m0.4380\u001b[0m       0.8808        \u001b[31m0.4400\u001b[0m  0.0211\n",
            "     87       0.8830        \u001b[32m0.4379\u001b[0m       0.8808        \u001b[31m0.4399\u001b[0m  0.0192\n",
            "     88       0.8830        \u001b[32m0.4378\u001b[0m       0.8808        \u001b[31m0.4398\u001b[0m  0.0298\n",
            "     89       0.8830        \u001b[32m0.4377\u001b[0m       0.8808        \u001b[31m0.4398\u001b[0m  0.0311\n",
            "     90       0.8830        \u001b[32m0.4376\u001b[0m       0.8808        \u001b[31m0.4397\u001b[0m  0.0258\n",
            "     91       0.8830        \u001b[32m0.4376\u001b[0m       0.8808        \u001b[31m0.4396\u001b[0m  0.0198\n",
            "     92       0.8830        \u001b[32m0.4375\u001b[0m       0.8808        \u001b[31m0.4396\u001b[0m  0.0193\n",
            "     93       0.8830        \u001b[32m0.4374\u001b[0m       0.8808        \u001b[31m0.4395\u001b[0m  0.0196\n",
            "     94       0.8830        \u001b[32m0.4374\u001b[0m       0.8808        \u001b[31m0.4394\u001b[0m  0.0197\n",
            "     95       0.8830        \u001b[32m0.4373\u001b[0m       0.8808        \u001b[31m0.4394\u001b[0m  0.0191\n",
            "     96       0.8830        \u001b[32m0.4372\u001b[0m       0.8808        \u001b[31m0.4393\u001b[0m  0.0197\n",
            "     97       0.8830        \u001b[32m0.4372\u001b[0m       0.8808        \u001b[31m0.4392\u001b[0m  0.0191\n",
            "     98       0.8830        \u001b[32m0.4371\u001b[0m       0.8808        \u001b[31m0.4392\u001b[0m  0.0206\n",
            "     99       0.8830        \u001b[32m0.4370\u001b[0m       0.8808        \u001b[31m0.4391\u001b[0m  0.0195\n",
            "    100       0.8830        \u001b[32m0.4370\u001b[0m       0.8808        \u001b[31m0.4390\u001b[0m  0.0192\n",
            "    101       0.8830        \u001b[32m0.4369\u001b[0m       0.8808        \u001b[31m0.4390\u001b[0m  0.0194\n",
            "    102       0.8830        \u001b[32m0.4369\u001b[0m       0.8808        \u001b[31m0.4389\u001b[0m  0.0357\n",
            "    103       0.8830        \u001b[32m0.4368\u001b[0m       0.8808        \u001b[31m0.4389\u001b[0m  0.0361\n",
            "    104       0.8830        \u001b[32m0.4367\u001b[0m       0.8808        \u001b[31m0.4388\u001b[0m  0.0675\n",
            "    105       0.8830        \u001b[32m0.4367\u001b[0m       0.8808        \u001b[31m0.4388\u001b[0m  0.0284\n",
            "    106       0.8830        \u001b[32m0.4367\u001b[0m       0.8808        \u001b[31m0.4387\u001b[0m  0.0193\n",
            "    107       0.8830        \u001b[32m0.4366\u001b[0m       0.8808        \u001b[31m0.4387\u001b[0m  0.0202\n",
            "    108       0.8830        \u001b[32m0.4365\u001b[0m       0.8808        \u001b[31m0.4386\u001b[0m  0.0245\n",
            "    109       0.8830        \u001b[32m0.4365\u001b[0m       0.8808        \u001b[31m0.4386\u001b[0m  0.0200\n",
            "    110       0.8830        \u001b[32m0.4365\u001b[0m       0.8808        \u001b[31m0.4385\u001b[0m  0.0196\n",
            "    111       0.8830        \u001b[32m0.4364\u001b[0m       0.8808        \u001b[31m0.4385\u001b[0m  0.0207\n",
            "    112       0.8830        \u001b[32m0.4363\u001b[0m       0.8808        \u001b[31m0.4384\u001b[0m  0.0193\n",
            "    113       0.8830        \u001b[32m0.4363\u001b[0m       0.8808        \u001b[31m0.4384\u001b[0m  0.0191\n",
            "    114       0.8830        \u001b[32m0.4363\u001b[0m       0.8808        \u001b[31m0.4383\u001b[0m  0.0191\n",
            "    115       0.8830        \u001b[32m0.4362\u001b[0m       0.8808        \u001b[31m0.4383\u001b[0m  0.0210\n",
            "    116       0.8830        \u001b[32m0.4362\u001b[0m       0.8808        \u001b[31m0.4382\u001b[0m  0.0194\n",
            "    117       0.8830        \u001b[32m0.4361\u001b[0m       0.8808        \u001b[31m0.4382\u001b[0m  0.0201\n",
            "    118       0.8830        \u001b[32m0.4361\u001b[0m       0.8808        \u001b[31m0.4381\u001b[0m  0.0209\n",
            "    119       0.8830        \u001b[32m0.4360\u001b[0m       0.8808        \u001b[31m0.4381\u001b[0m  0.0197\n",
            "    120       0.8830        \u001b[32m0.4360\u001b[0m       0.8808        \u001b[31m0.4381\u001b[0m  0.0202\n",
            "    121       0.8830        \u001b[32m0.4359\u001b[0m       0.8808        \u001b[31m0.4380\u001b[0m  0.0222\n",
            "    122       0.8830        0.4359       0.8808        \u001b[31m0.4380\u001b[0m  0.0195\n",
            "    123       0.8830        \u001b[32m0.4359\u001b[0m       0.8808        \u001b[31m0.4379\u001b[0m  0.0197\n",
            "    124       0.8830        \u001b[32m0.4358\u001b[0m       0.8808        \u001b[31m0.4379\u001b[0m  0.0195\n",
            "    125       0.8830        \u001b[32m0.4358\u001b[0m       0.8808        \u001b[31m0.4379\u001b[0m  0.0208\n",
            "    126       0.8830        \u001b[32m0.4357\u001b[0m       0.8808        \u001b[31m0.4378\u001b[0m  0.0198\n",
            "    127       0.8830        \u001b[32m0.4357\u001b[0m       0.8808        \u001b[31m0.4378\u001b[0m  0.0196\n",
            "    128       0.8830        \u001b[32m0.4357\u001b[0m       0.8808        \u001b[31m0.4377\u001b[0m  0.0234\n",
            "    129       0.8830        \u001b[32m0.4356\u001b[0m       0.8808        \u001b[31m0.4377\u001b[0m  0.0302\n",
            "    130       0.8830        \u001b[32m0.4356\u001b[0m       0.8808        \u001b[31m0.4377\u001b[0m  0.0299\n",
            "    131       0.8830        \u001b[32m0.4355\u001b[0m       0.8808        \u001b[31m0.4376\u001b[0m  0.0290\n",
            "    132       0.8830        \u001b[32m0.4355\u001b[0m       0.8808        \u001b[31m0.4376\u001b[0m  0.0196\n",
            "    133       0.8830        \u001b[32m0.4355\u001b[0m       0.8808        \u001b[31m0.4376\u001b[0m  0.0192\n",
            "    134       0.8830        \u001b[32m0.4354\u001b[0m       0.8808        \u001b[31m0.4375\u001b[0m  0.0192\n",
            "    135       0.8830        \u001b[32m0.4354\u001b[0m       0.8808        \u001b[31m0.4375\u001b[0m  0.0194\n",
            "    136       0.8830        \u001b[32m0.4354\u001b[0m       0.8808        \u001b[31m0.4374\u001b[0m  0.0206\n",
            "    137       0.8830        \u001b[32m0.4353\u001b[0m       0.8808        \u001b[31m0.4374\u001b[0m  0.0188\n",
            "    138       0.8830        \u001b[32m0.4353\u001b[0m       0.8808        \u001b[31m0.4374\u001b[0m  0.0236\n",
            "    139       0.8830        \u001b[32m0.4353\u001b[0m       0.8808        \u001b[31m0.4373\u001b[0m  0.0207\n",
            "    140       0.8830        \u001b[32m0.4352\u001b[0m       0.8808        \u001b[31m0.4373\u001b[0m  0.0196\n",
            "    141       0.8830        \u001b[32m0.4352\u001b[0m       0.8808        \u001b[31m0.4373\u001b[0m  0.0201\n",
            "    142       0.8830        \u001b[32m0.4351\u001b[0m       0.8808        \u001b[31m0.4373\u001b[0m  0.0202\n",
            "    143       0.8830        \u001b[32m0.4351\u001b[0m       0.8808        \u001b[31m0.4372\u001b[0m  0.0194\n",
            "    144       0.8830        0.4351       0.8808        \u001b[31m0.4372\u001b[0m  0.0196\n",
            "    145       0.8830        \u001b[32m0.4351\u001b[0m       0.8808        \u001b[31m0.4372\u001b[0m  0.0196\n",
            "    146       0.8830        \u001b[32m0.4350\u001b[0m       0.8808        \u001b[31m0.4371\u001b[0m  0.0202\n",
            "    147       0.8830        \u001b[32m0.4350\u001b[0m       0.8808        \u001b[31m0.4371\u001b[0m  0.0202\n",
            "    148       0.8830        \u001b[32m0.4350\u001b[0m       0.8808        \u001b[31m0.4371\u001b[0m  0.0200\n",
            "    149       0.8830        \u001b[32m0.4349\u001b[0m       0.8808        \u001b[31m0.4370\u001b[0m  0.0223\n",
            "    150       0.8830        0.4349       0.8808        \u001b[31m0.4370\u001b[0m  0.0239\n",
            "    151       0.8830        \u001b[32m0.4349\u001b[0m       0.8808        \u001b[31m0.4370\u001b[0m  0.0191\n",
            "    152       0.8830        \u001b[32m0.4349\u001b[0m       0.8808        \u001b[31m0.4370\u001b[0m  0.0207\n",
            "    153       0.8830        \u001b[32m0.4348\u001b[0m       0.8808        \u001b[31m0.4369\u001b[0m  0.0197\n",
            "    154       0.8830        \u001b[32m0.4348\u001b[0m       0.8808        \u001b[31m0.4369\u001b[0m  0.0192\n",
            "    155       0.8830        \u001b[32m0.4348\u001b[0m       0.8808        \u001b[31m0.4369\u001b[0m  0.0200\n",
            "    156       0.8830        \u001b[32m0.4347\u001b[0m       0.8808        \u001b[31m0.4368\u001b[0m  0.0207\n",
            "    157       0.8830        \u001b[32m0.4347\u001b[0m       0.8808        \u001b[31m0.4368\u001b[0m  0.0204\n",
            "    158       0.8830        \u001b[32m0.4347\u001b[0m       0.8808        \u001b[31m0.4368\u001b[0m  0.0195\n",
            "    159       0.8830        \u001b[32m0.4347\u001b[0m       0.8808        \u001b[31m0.4368\u001b[0m  0.0199\n",
            "    160       0.8830        \u001b[32m0.4347\u001b[0m       0.8808        \u001b[31m0.4367\u001b[0m  0.0194\n",
            "    161       0.8830        \u001b[32m0.4346\u001b[0m       0.8808        \u001b[31m0.4367\u001b[0m  0.0200\n",
            "    162       0.8830        \u001b[32m0.4346\u001b[0m       0.8808        \u001b[31m0.4367\u001b[0m  0.0196\n",
            "    163       0.8830        \u001b[32m0.4345\u001b[0m       0.8808        \u001b[31m0.4367\u001b[0m  0.0320\n",
            "    164       0.8830        0.4346       0.8808        \u001b[31m0.4366\u001b[0m  0.0301\n",
            "    165       0.8830        \u001b[32m0.4345\u001b[0m       0.8808        \u001b[31m0.4366\u001b[0m  0.0312\n",
            "    166       0.8830        \u001b[32m0.4345\u001b[0m       0.8808        \u001b[31m0.4366\u001b[0m  0.0314\n",
            "    167       0.8830        \u001b[32m0.4345\u001b[0m       0.8808        \u001b[31m0.4366\u001b[0m  0.0290\n",
            "    168       0.8830        \u001b[32m0.4344\u001b[0m       0.8808        \u001b[31m0.4365\u001b[0m  0.0365\n",
            "    169       0.8830        \u001b[32m0.4344\u001b[0m       0.8808        \u001b[31m0.4365\u001b[0m  0.0280\n",
            "    170       0.8830        \u001b[32m0.4344\u001b[0m       0.8808        \u001b[31m0.4365\u001b[0m  0.0198\n",
            "    171       0.8830        \u001b[32m0.4344\u001b[0m       0.8808        \u001b[31m0.4365\u001b[0m  0.0193\n",
            "    172       0.8830        \u001b[32m0.4344\u001b[0m       0.8808        \u001b[31m0.4365\u001b[0m  0.0198\n",
            "    173       0.8830        \u001b[32m0.4343\u001b[0m       0.8808        \u001b[31m0.4364\u001b[0m  0.0199\n",
            "    174       0.8830        \u001b[32m0.4343\u001b[0m       0.8808        \u001b[31m0.4364\u001b[0m  0.0189\n",
            "    175       0.8830        \u001b[32m0.4343\u001b[0m       0.8808        \u001b[31m0.4364\u001b[0m  0.0193\n",
            "    176       0.8830        \u001b[32m0.4342\u001b[0m       0.8808        \u001b[31m0.4364\u001b[0m  0.0200\n",
            "    177       0.8830        \u001b[32m0.4342\u001b[0m       0.8808        \u001b[31m0.4363\u001b[0m  0.0190\n",
            "    178       0.8830        \u001b[32m0.4342\u001b[0m       0.8808        \u001b[31m0.4363\u001b[0m  0.0205\n",
            "    179       0.8830        0.4342       0.8808        \u001b[31m0.4363\u001b[0m  0.0207\n",
            "    180       0.8830        \u001b[32m0.4342\u001b[0m       0.8808        \u001b[31m0.4363\u001b[0m  0.0194\n",
            "    181       0.8830        \u001b[32m0.4342\u001b[0m       0.8808        \u001b[31m0.4363\u001b[0m  0.0200\n",
            "    182       0.8830        \u001b[32m0.4341\u001b[0m       0.8808        \u001b[31m0.4362\u001b[0m  0.0206\n",
            "    183       0.8830        \u001b[32m0.4341\u001b[0m       0.8808        \u001b[31m0.4362\u001b[0m  0.0195\n",
            "    184       0.8830        \u001b[32m0.4341\u001b[0m       0.8808        \u001b[31m0.4362\u001b[0m  0.0204\n",
            "    185       0.8830        \u001b[32m0.4341\u001b[0m       0.8808        \u001b[31m0.4362\u001b[0m  0.0217\n",
            "    186       0.8830        \u001b[32m0.4340\u001b[0m       0.8808        \u001b[31m0.4362\u001b[0m  0.0273\n",
            "    187       0.8830        \u001b[32m0.4340\u001b[0m       0.8808        \u001b[31m0.4361\u001b[0m  0.0292\n",
            "    188       0.8830        \u001b[32m0.4340\u001b[0m       0.8808        \u001b[31m0.4361\u001b[0m  0.0483\n",
            "    189       0.8830        \u001b[32m0.4340\u001b[0m       0.8808        \u001b[31m0.4361\u001b[0m  0.0274\n",
            "    190       0.8830        \u001b[32m0.4340\u001b[0m       0.8808        \u001b[31m0.4361\u001b[0m  0.0251\n",
            "    191       0.8830        \u001b[32m0.4339\u001b[0m       0.8808        \u001b[31m0.4361\u001b[0m  0.0194\n",
            "    192       0.8830        \u001b[32m0.4339\u001b[0m       0.8808        \u001b[31m0.4361\u001b[0m  0.0197\n",
            "    193       0.8830        0.4339       0.8808        \u001b[31m0.4360\u001b[0m  0.0197\n",
            "    194       0.8830        0.4339       0.8808        \u001b[31m0.4360\u001b[0m  0.0193\n",
            "    195       0.8830        \u001b[32m0.4339\u001b[0m       0.8808        \u001b[31m0.4360\u001b[0m  0.0201\n",
            "    196       0.8830        \u001b[32m0.4339\u001b[0m       0.8808        \u001b[31m0.4360\u001b[0m  0.0193\n",
            "    197       0.8830        \u001b[32m0.4338\u001b[0m       0.8808        \u001b[31m0.4360\u001b[0m  0.0192\n",
            "    198       0.8830        0.4338       0.8808        \u001b[31m0.4359\u001b[0m  0.0197\n",
            "    199       0.8830        \u001b[32m0.4338\u001b[0m       0.8808        \u001b[31m0.4359\u001b[0m  0.0196\n",
            "    200       0.8830        \u001b[32m0.4338\u001b[0m       0.8808        \u001b[31m0.4359\u001b[0m  0.0191\n",
            "  epoch    train_acc    train_loss    valid_acc    valid_loss     dur\n",
            "-------  -----------  ------------  -----------  ------------  ------\n",
            "      1       \u001b[36m0.8500\u001b[0m        \u001b[32m0.5289\u001b[0m       \u001b[35m0.8667\u001b[0m        \u001b[31m0.5182\u001b[0m  0.0025\n",
            "      2       0.8500        \u001b[32m0.5278\u001b[0m       0.8667        \u001b[31m0.5175\u001b[0m  0.0032\n",
            "      3       0.8500        \u001b[32m0.5269\u001b[0m       0.8667        \u001b[31m0.5168\u001b[0m  0.0029\n",
            "      4       0.8500        0.5270       0.8667        \u001b[31m0.5160\u001b[0m  0.0031\n",
            "      5       0.8500        \u001b[32m0.5262\u001b[0m       0.8667        \u001b[31m0.5153\u001b[0m  0.0029\n",
            "      6       0.8500        \u001b[32m0.5257\u001b[0m       0.8667        \u001b[31m0.5146\u001b[0m  0.0030\n",
            "      7       0.8500        \u001b[32m0.5249\u001b[0m       0.8667        \u001b[31m0.5140\u001b[0m  0.0028\n",
            "      8       0.8500        \u001b[32m0.5239\u001b[0m       0.8667        \u001b[31m0.5133\u001b[0m  0.0027\n",
            "      9       0.8500        \u001b[32m0.5234\u001b[0m       0.8667        \u001b[31m0.5126\u001b[0m  0.0033\n",
            "     10       0.8500        \u001b[32m0.5232\u001b[0m       0.8667        \u001b[31m0.5120\u001b[0m  0.0028\n",
            "     11       0.8500        \u001b[32m0.5222\u001b[0m       0.8667        \u001b[31m0.5114\u001b[0m  0.0027\n",
            "     12       0.8500        \u001b[32m0.5215\u001b[0m       0.8667        \u001b[31m0.5108\u001b[0m  0.0030\n",
            "     13       0.8500        0.5216       0.8667        \u001b[31m0.5101\u001b[0m  0.0026\n",
            "     14       0.8500        \u001b[32m0.5207\u001b[0m       0.8667        \u001b[31m0.5095\u001b[0m  0.0023\n",
            "     15       0.8500        \u001b[32m0.5204\u001b[0m       0.8667        \u001b[31m0.5090\u001b[0m  0.0030\n",
            "     16       0.8500        \u001b[32m0.5195\u001b[0m       0.8667        \u001b[31m0.5084\u001b[0m  0.0024\n",
            "     17       0.8500        \u001b[32m0.5192\u001b[0m       0.8667        \u001b[31m0.5078\u001b[0m  0.0026\n",
            "     18       0.8500        \u001b[32m0.5184\u001b[0m       0.8667        \u001b[31m0.5073\u001b[0m  0.0030\n",
            "     19       0.8500        \u001b[32m0.5177\u001b[0m       0.8667        \u001b[31m0.5067\u001b[0m  0.0024\n",
            "     20       0.8500        0.5180       0.8667        \u001b[31m0.5062\u001b[0m  0.0028\n",
            "     21       0.8500        \u001b[32m0.5164\u001b[0m       0.8667        \u001b[31m0.5056\u001b[0m  0.0027\n",
            "     22       0.8500        0.5166       0.8667        \u001b[31m0.5051\u001b[0m  0.0027\n",
            "     23       0.8500        \u001b[32m0.5158\u001b[0m       0.8667        \u001b[31m0.5046\u001b[0m  0.0023\n",
            "     24       0.8500        \u001b[32m0.5158\u001b[0m       0.8667        \u001b[31m0.5041\u001b[0m  0.0029\n",
            "     25       0.8500        \u001b[32m0.5144\u001b[0m       0.8667        \u001b[31m0.5036\u001b[0m  0.0027\n",
            "     26       0.8500        0.5148       0.8667        \u001b[31m0.5031\u001b[0m  0.0026\n",
            "     27       0.8500        \u001b[32m0.5144\u001b[0m       0.8667        \u001b[31m0.5027\u001b[0m  0.0024\n",
            "     28       0.8500        \u001b[32m0.5135\u001b[0m       0.8667        \u001b[31m0.5022\u001b[0m  0.0023\n",
            "     29       0.8500        \u001b[32m0.5124\u001b[0m       0.8667        \u001b[31m0.5017\u001b[0m  0.0023\n",
            "     30       0.8500        0.5126       0.8667        \u001b[31m0.5013\u001b[0m  0.0029\n",
            "     31       0.8500        \u001b[32m0.5124\u001b[0m       0.8667        \u001b[31m0.5008\u001b[0m  0.0026\n",
            "     32       0.8500        \u001b[32m0.5120\u001b[0m       0.8667        \u001b[31m0.5004\u001b[0m  0.0026\n",
            "     33       0.8500        \u001b[32m0.5114\u001b[0m       0.8667        \u001b[31m0.5000\u001b[0m  0.0053\n",
            "     34       0.8500        \u001b[32m0.5101\u001b[0m       0.8667        \u001b[31m0.4995\u001b[0m  0.0031\n",
            "     35       0.8500        0.5111       0.8667        \u001b[31m0.4991\u001b[0m  0.0025\n",
            "     36       0.8500        \u001b[32m0.5099\u001b[0m       0.8667        \u001b[31m0.4987\u001b[0m  0.0028\n",
            "     37       0.8500        \u001b[32m0.5098\u001b[0m       0.8667        \u001b[31m0.4983\u001b[0m  0.0023\n",
            "     38       0.8500        \u001b[32m0.5097\u001b[0m       0.8667        \u001b[31m0.4979\u001b[0m  0.0031\n",
            "     39       0.8500        \u001b[32m0.5095\u001b[0m       0.8667        \u001b[31m0.4975\u001b[0m  0.0025\n",
            "     40       0.8500        \u001b[32m0.5081\u001b[0m       0.8667        \u001b[31m0.4971\u001b[0m  0.0033\n",
            "     41       0.8500        \u001b[32m0.5077\u001b[0m       0.8667        \u001b[31m0.4967\u001b[0m  0.0026\n",
            "     42       0.8500        0.5083       0.8667        \u001b[31m0.4964\u001b[0m  0.0027\n",
            "     43       0.8500        0.5082       0.8667        \u001b[31m0.4960\u001b[0m  0.0029\n",
            "     44       0.8500        0.5083       0.8667        \u001b[31m0.4956\u001b[0m  0.0027\n",
            "     45       0.8500        \u001b[32m0.5068\u001b[0m       0.8667        \u001b[31m0.4952\u001b[0m  0.0026\n",
            "     46       0.8500        \u001b[32m0.5063\u001b[0m       0.8667        \u001b[31m0.4949\u001b[0m  0.0029\n",
            "     47       0.8500        0.5063       0.8667        \u001b[31m0.4945\u001b[0m  0.0027\n",
            "     48       0.8500        \u001b[32m0.5058\u001b[0m       0.8667        \u001b[31m0.4942\u001b[0m  0.0030\n",
            "     49       0.8500        0.5066       0.8667        \u001b[31m0.4939\u001b[0m  0.0029\n",
            "     50       0.8500        \u001b[32m0.5049\u001b[0m       0.8667        \u001b[31m0.4935\u001b[0m  0.0027\n",
            "     51       0.8500        0.5054       0.8667        \u001b[31m0.4932\u001b[0m  0.0025\n",
            "     52       0.8500        0.5050       0.8667        \u001b[31m0.4929\u001b[0m  0.0033\n",
            "     53       0.8500        \u001b[32m0.5044\u001b[0m       0.8667        \u001b[31m0.4925\u001b[0m  0.0033\n",
            "     54       0.8500        \u001b[32m0.5043\u001b[0m       0.8667        \u001b[31m0.4922\u001b[0m  0.0033\n",
            "     55       0.8500        \u001b[32m0.5029\u001b[0m       0.8667        \u001b[31m0.4919\u001b[0m  0.0030\n",
            "     56       0.8500        0.5035       0.8667        \u001b[31m0.4916\u001b[0m  0.0030\n",
            "     57       0.8500        0.5035       0.8667        \u001b[31m0.4913\u001b[0m  0.0037\n",
            "     58       0.8500        0.5030       0.8667        \u001b[31m0.4910\u001b[0m  0.0035\n",
            "     59       0.8500        0.5031       0.8667        \u001b[31m0.4907\u001b[0m  0.0034\n",
            "     60       0.8500        \u001b[32m0.5028\u001b[0m       0.8667        \u001b[31m0.4904\u001b[0m  0.0038\n",
            "     61       0.8500        \u001b[32m0.5022\u001b[0m       0.8667        \u001b[31m0.4901\u001b[0m  0.0038\n",
            "     62       0.8500        \u001b[32m0.5017\u001b[0m       0.8667        \u001b[31m0.4898\u001b[0m  0.0039\n",
            "     63       0.8500        0.5020       0.8667        \u001b[31m0.4895\u001b[0m  0.0033\n",
            "     64       0.8500        \u001b[32m0.5016\u001b[0m       0.8667        \u001b[31m0.4893\u001b[0m  0.0032\n",
            "     65       0.8500        \u001b[32m0.5013\u001b[0m       0.8667        \u001b[31m0.4890\u001b[0m  0.0031\n",
            "     66       0.8500        0.5018       0.8667        \u001b[31m0.4887\u001b[0m  0.0030\n",
            "     67       0.8500        \u001b[32m0.5010\u001b[0m       0.8667        \u001b[31m0.4884\u001b[0m  0.0036\n",
            "     68       0.8500        \u001b[32m0.5006\u001b[0m       0.8667        \u001b[31m0.4882\u001b[0m  0.0028\n",
            "     69       0.8500        \u001b[32m0.5005\u001b[0m       0.8667        \u001b[31m0.4879\u001b[0m  0.0028\n",
            "     70       0.8500        \u001b[32m0.4994\u001b[0m       0.8667        \u001b[31m0.4877\u001b[0m  0.0027\n",
            "     71       0.8500        0.4998       0.8667        \u001b[31m0.4874\u001b[0m  0.0028\n",
            "     72       0.8500        0.4996       0.8667        \u001b[31m0.4872\u001b[0m  0.0041\n",
            "     73       0.8500        0.4996       0.8667        \u001b[31m0.4869\u001b[0m  0.0027\n",
            "     74       0.8500        0.4994       0.8667        \u001b[31m0.4867\u001b[0m  0.0028\n",
            "     75       0.8500        \u001b[32m0.4991\u001b[0m       0.8667        \u001b[31m0.4864\u001b[0m  0.0026\n",
            "     76       0.8500        \u001b[32m0.4988\u001b[0m       0.8667        \u001b[31m0.4862\u001b[0m  0.0028\n",
            "     77       0.8500        \u001b[32m0.4983\u001b[0m       0.8667        \u001b[31m0.4859\u001b[0m  0.0030\n",
            "     78       0.8500        \u001b[32m0.4981\u001b[0m       0.8667        \u001b[31m0.4857\u001b[0m  0.0027\n",
            "     79       0.8500        0.4986       0.8667        \u001b[31m0.4855\u001b[0m  0.0026\n",
            "     80       0.8500        \u001b[32m0.4980\u001b[0m       0.8667        \u001b[31m0.4852\u001b[0m  0.0031\n",
            "     81       0.8500        \u001b[32m0.4978\u001b[0m       0.8667        \u001b[31m0.4850\u001b[0m  0.0028\n",
            "     82       0.8500        \u001b[32m0.4978\u001b[0m       0.8667        \u001b[31m0.4848\u001b[0m  0.0035\n",
            "     83       0.8500        \u001b[32m0.4969\u001b[0m       0.8667        \u001b[31m0.4846\u001b[0m  0.0026\n",
            "     84       0.8500        \u001b[32m0.4968\u001b[0m       0.8667        \u001b[31m0.4844\u001b[0m  0.0026\n",
            "     85       0.8500        0.4970       0.8667        \u001b[31m0.4841\u001b[0m  0.0025\n",
            "     86       0.8500        \u001b[32m0.4961\u001b[0m       0.8667        \u001b[31m0.4839\u001b[0m  0.0038\n",
            "     87       0.8500        0.4970       0.8667        \u001b[31m0.4837\u001b[0m  0.0040\n",
            "     88       0.8500        0.4968       0.8667        \u001b[31m0.4835\u001b[0m  0.0028\n",
            "     89       0.8500        \u001b[32m0.4960\u001b[0m       0.8667        \u001b[31m0.4833\u001b[0m  0.0025\n",
            "     90       0.8500        0.4962       0.8667        \u001b[31m0.4831\u001b[0m  0.0026\n",
            "     91       0.8500        \u001b[32m0.4954\u001b[0m       0.8667        \u001b[31m0.4829\u001b[0m  0.0036\n",
            "     92       0.8500        0.4958       0.8667        \u001b[31m0.4827\u001b[0m  0.0031\n",
            "     93       0.8500        0.4955       0.8667        \u001b[31m0.4825\u001b[0m  0.0031\n",
            "     94       0.8500        0.4960       0.8667        \u001b[31m0.4823\u001b[0m  0.0032\n",
            "     95       0.8500        0.4955       0.8667        \u001b[31m0.4821\u001b[0m  0.0033\n",
            "     96       0.8500        \u001b[32m0.4943\u001b[0m       0.8667        \u001b[31m0.4819\u001b[0m  0.0031\n",
            "     97       0.8500        \u001b[32m0.4942\u001b[0m       0.8667        \u001b[31m0.4817\u001b[0m  0.0031\n",
            "     98       0.8500        0.4945       0.8667        \u001b[31m0.4815\u001b[0m  0.0027\n",
            "     99       0.8500        0.4945       0.8667        \u001b[31m0.4814\u001b[0m  0.0025\n",
            "    100       0.8500        \u001b[32m0.4940\u001b[0m       0.8667        \u001b[31m0.4812\u001b[0m  0.0027\n",
            "    101       0.8500        \u001b[32m0.4935\u001b[0m       0.8667        \u001b[31m0.4810\u001b[0m  0.0028\n",
            "    102       0.8500        0.4939       0.8667        \u001b[31m0.4808\u001b[0m  0.0028\n",
            "    103       0.8500        \u001b[32m0.4934\u001b[0m       0.8667        \u001b[31m0.4806\u001b[0m  0.0027\n",
            "    104       0.8500        \u001b[32m0.4929\u001b[0m       0.8667        \u001b[31m0.4805\u001b[0m  0.0028\n",
            "    105       0.8500        0.4931       0.8667        \u001b[31m0.4803\u001b[0m  0.0027\n",
            "    106       0.8500        0.4934       0.8667        \u001b[31m0.4801\u001b[0m  0.0028\n",
            "    107       0.8500        0.4931       0.8667        \u001b[31m0.4799\u001b[0m  0.0028\n",
            "    108       0.8500        \u001b[32m0.4926\u001b[0m       0.8667        \u001b[31m0.4798\u001b[0m  0.0033\n",
            "    109       0.8500        0.4928       0.8667        \u001b[31m0.4796\u001b[0m  0.0026\n",
            "    110       0.8500        0.4928       0.8667        \u001b[31m0.4794\u001b[0m  0.0026\n",
            "    111       0.8500        \u001b[32m0.4923\u001b[0m       0.8667        \u001b[31m0.4793\u001b[0m  0.0029\n",
            "    112       0.8500        \u001b[32m0.4922\u001b[0m       0.8667        \u001b[31m0.4791\u001b[0m  0.0036\n",
            "    113       0.8500        0.4926       0.8667        \u001b[31m0.4790\u001b[0m  0.0035\n",
            "    114       0.8500        0.4923       0.8667        \u001b[31m0.4788\u001b[0m  0.0283\n",
            "    115       0.8500        \u001b[32m0.4916\u001b[0m       0.8667        \u001b[31m0.4786\u001b[0m  0.0084\n",
            "    116       0.8500        0.4923       0.8667        \u001b[31m0.4785\u001b[0m  0.0084\n",
            "    117       0.8500        \u001b[32m0.4912\u001b[0m       0.8667        \u001b[31m0.4783\u001b[0m  0.0038\n",
            "    118       0.8500        0.4914       0.8667        \u001b[31m0.4782\u001b[0m  0.0039\n",
            "    119       0.8500        0.4915       0.8667        \u001b[31m0.4780\u001b[0m  0.0038\n",
            "    120       0.8500        \u001b[32m0.4907\u001b[0m       0.8667        \u001b[31m0.4779\u001b[0m  0.0048\n",
            "    121       0.8500        0.4914       0.8667        \u001b[31m0.4777\u001b[0m  0.0040\n",
            "    122       0.8500        0.4912       0.8667        \u001b[31m0.4776\u001b[0m  0.0093\n",
            "    123       0.8500        \u001b[32m0.4907\u001b[0m       0.8667        \u001b[31m0.4774\u001b[0m  0.0037\n",
            "    124       0.8500        \u001b[32m0.4902\u001b[0m       0.8667        \u001b[31m0.4773\u001b[0m  0.0036\n",
            "    125       0.8500        \u001b[32m0.4901\u001b[0m       0.8667        \u001b[31m0.4772\u001b[0m  0.0035\n",
            "    126       0.8500        0.4908       0.8667        \u001b[31m0.4770\u001b[0m  0.0178\n",
            "    127       0.8500        0.4902       0.8667        \u001b[31m0.4769\u001b[0m  0.0043\n",
            "    128       0.8500        \u001b[32m0.4900\u001b[0m       0.8667        \u001b[31m0.4767\u001b[0m  0.0040\n",
            "    129       0.8500        \u001b[32m0.4896\u001b[0m       0.8667        \u001b[31m0.4766\u001b[0m  0.0028\n",
            "    130       0.8500        0.4899       0.8667        \u001b[31m0.4765\u001b[0m  0.0028\n",
            "    131       0.8500        0.4901       0.8667        \u001b[31m0.4763\u001b[0m  0.0030\n",
            "    132       0.8500        \u001b[32m0.4895\u001b[0m       0.8667        \u001b[31m0.4762\u001b[0m  0.0026\n",
            "    133       0.8500        \u001b[32m0.4892\u001b[0m       0.8667        \u001b[31m0.4761\u001b[0m  0.0026\n",
            "    134       0.8500        \u001b[32m0.4889\u001b[0m       0.8667        \u001b[31m0.4759\u001b[0m  0.0029\n",
            "    135       0.8500        0.4897       0.8667        \u001b[31m0.4758\u001b[0m  0.0030\n",
            "    136       0.8500        0.4890       0.8667        \u001b[31m0.4757\u001b[0m  0.0026\n",
            "    137       0.8500        0.4890       0.8667        \u001b[31m0.4755\u001b[0m  0.0028\n",
            "    138       0.8500        \u001b[32m0.4889\u001b[0m       0.8667        \u001b[31m0.4754\u001b[0m  0.0027\n",
            "    139       0.8500        \u001b[32m0.4885\u001b[0m       0.8667        \u001b[31m0.4753\u001b[0m  0.0029\n",
            "    140       0.8500        0.4890       0.8667        \u001b[31m0.4752\u001b[0m  0.0026\n",
            "    141       0.8500        \u001b[32m0.4882\u001b[0m       0.8667        \u001b[31m0.4750\u001b[0m  0.0025\n",
            "    142       0.8500        0.4885       0.8667        \u001b[31m0.4749\u001b[0m  0.0028\n",
            "    143       0.8500        0.4884       0.8667        \u001b[31m0.4748\u001b[0m  0.0030\n",
            "    144       0.8500        \u001b[32m0.4878\u001b[0m       0.8667        \u001b[31m0.4747\u001b[0m  0.0026\n",
            "    145       0.8500        0.4883       0.8667        \u001b[31m0.4746\u001b[0m  0.0029\n",
            "    146       0.8500        0.4881       0.8667        \u001b[31m0.4744\u001b[0m  0.0026\n",
            "    147       0.8500        0.4880       0.8667        \u001b[31m0.4743\u001b[0m  0.0027\n",
            "    148       0.8500        \u001b[32m0.4871\u001b[0m       0.8667        \u001b[31m0.4742\u001b[0m  0.0030\n",
            "    149       0.8500        0.4880       0.8667        \u001b[31m0.4741\u001b[0m  0.0027\n",
            "    150       0.8500        0.4874       0.8667        \u001b[31m0.4740\u001b[0m  0.0032\n",
            "    151       0.8500        0.4876       0.8667        \u001b[31m0.4739\u001b[0m  0.0030\n",
            "    152       0.8500        \u001b[32m0.4870\u001b[0m       0.8667        \u001b[31m0.4737\u001b[0m  0.0031\n",
            "    153       0.8500        0.4876       0.8667        \u001b[31m0.4736\u001b[0m  0.0029\n",
            "    154       0.8500        0.4873       0.8667        \u001b[31m0.4735\u001b[0m  0.0027\n",
            "    155       0.8500        0.4872       0.8667        \u001b[31m0.4734\u001b[0m  0.0027\n",
            "    156       0.8500        \u001b[32m0.4867\u001b[0m       0.8667        \u001b[31m0.4733\u001b[0m  0.0031\n",
            "    157       0.8500        \u001b[32m0.4863\u001b[0m       0.8667        \u001b[31m0.4732\u001b[0m  0.0028\n",
            "    158       0.8500        0.4871       0.8667        \u001b[31m0.4731\u001b[0m  0.0028\n",
            "    159       0.8500        0.4863       0.8667        \u001b[31m0.4730\u001b[0m  0.0029\n",
            "    160       0.8500        0.4872       0.8667        \u001b[31m0.4729\u001b[0m  0.0026\n",
            "    161       0.8500        \u001b[32m0.4855\u001b[0m       0.8667        \u001b[31m0.4728\u001b[0m  0.0034\n",
            "    162       0.8500        0.4868       0.8667        \u001b[31m0.4727\u001b[0m  0.0024\n",
            "    163       0.8500        0.4860       0.8667        \u001b[31m0.4726\u001b[0m  0.0026\n",
            "    164       0.8500        0.4861       0.8667        \u001b[31m0.4725\u001b[0m  0.0027\n",
            "    165       0.8500        0.4864       0.8667        \u001b[31m0.4724\u001b[0m  0.0037\n",
            "    166       0.8500        0.4856       0.8667        \u001b[31m0.4723\u001b[0m  0.0025\n",
            "    167       0.8500        0.4860       0.8667        \u001b[31m0.4722\u001b[0m  0.0026\n",
            "    168       0.8500        0.4862       0.8667        \u001b[31m0.4721\u001b[0m  0.0024\n",
            "    169       0.8500        0.4856       0.8667        \u001b[31m0.4720\u001b[0m  0.0026\n",
            "    170       0.8500        \u001b[32m0.4851\u001b[0m       0.8667        \u001b[31m0.4719\u001b[0m  0.0027\n",
            "    171       0.8500        0.4858       0.8667        \u001b[31m0.4718\u001b[0m  0.0025\n",
            "    172       0.8500        0.4861       0.8667        \u001b[31m0.4717\u001b[0m  0.0028\n",
            "    173       0.8500        0.4854       0.8667        \u001b[31m0.4716\u001b[0m  0.0030\n",
            "    174       0.8500        0.4853       0.8667        \u001b[31m0.4715\u001b[0m  0.0027\n",
            "    175       0.8500        0.4852       0.8667        \u001b[31m0.4714\u001b[0m  0.0029\n",
            "    176       0.8500        0.4857       0.8667        \u001b[31m0.4713\u001b[0m  0.0028\n",
            "    177       0.8500        \u001b[32m0.4850\u001b[0m       0.8667        \u001b[31m0.4712\u001b[0m  0.0027\n",
            "    178       0.8500        0.4858       0.8667        \u001b[31m0.4711\u001b[0m  0.0029\n",
            "    179       0.8500        \u001b[32m0.4848\u001b[0m       0.8667        \u001b[31m0.4710\u001b[0m  0.0028\n",
            "    180       0.8500        0.4849       0.8667        \u001b[31m0.4709\u001b[0m  0.0039\n",
            "    181       0.8500        \u001b[32m0.4847\u001b[0m       0.8667        \u001b[31m0.4708\u001b[0m  0.0030\n",
            "    182       0.8500        \u001b[32m0.4847\u001b[0m       0.8667        \u001b[31m0.4708\u001b[0m  0.0029\n",
            "    183       0.8500        0.4848       0.8667        \u001b[31m0.4707\u001b[0m  0.0032\n",
            "    184       0.8500        \u001b[32m0.4843\u001b[0m       0.8667        \u001b[31m0.4706\u001b[0m  0.0027\n",
            "    185       0.8500        \u001b[32m0.4842\u001b[0m       0.8667        \u001b[31m0.4705\u001b[0m  0.0031\n",
            "    186       0.8500        0.4848       0.8667        \u001b[31m0.4704\u001b[0m  0.0026\n",
            "    187       0.8500        0.4848       0.8667        \u001b[31m0.4703\u001b[0m  0.0029\n",
            "    188       0.8500        \u001b[32m0.4838\u001b[0m       0.8667        \u001b[31m0.4702\u001b[0m  0.0028\n",
            "    189       0.8500        0.4844       0.8667        \u001b[31m0.4701\u001b[0m  0.0027\n",
            "    190       0.8500        \u001b[32m0.4837\u001b[0m       0.8667        \u001b[31m0.4701\u001b[0m  0.0030\n",
            "    191       0.8500        0.4838       0.8667        \u001b[31m0.4700\u001b[0m  0.0034\n",
            "    192       0.8500        0.4843       0.8667        \u001b[31m0.4699\u001b[0m  0.0047\n",
            "    193       0.8500        0.4839       0.8667        \u001b[31m0.4698\u001b[0m  0.0038\n",
            "    194       0.8500        \u001b[32m0.4832\u001b[0m       0.8667        \u001b[31m0.4697\u001b[0m  0.0037\n",
            "    195       0.8500        0.4836       0.8667        \u001b[31m0.4697\u001b[0m  0.0036\n",
            "    196       0.8500        0.4838       0.8667        \u001b[31m0.4696\u001b[0m  0.0028\n",
            "    197       0.8500        0.4832       0.8667        \u001b[31m0.4695\u001b[0m  0.0029\n",
            "    198       0.8500        0.4832       0.8667        \u001b[31m0.4694\u001b[0m  0.0033\n",
            "    199       0.8500        0.4832       0.8667        \u001b[31m0.4693\u001b[0m  0.0029\n",
            "    200       0.8500        0.4832       0.8667        \u001b[31m0.4693\u001b[0m  0.0031\n",
            "  epoch    train_acc    train_loss    valid_acc    valid_loss     dur\n",
            "-------  -----------  ------------  -----------  ------------  ------\n",
            "      1       \u001b[36m0.8824\u001b[0m        \u001b[32m0.5377\u001b[0m       \u001b[35m0.8878\u001b[0m        \u001b[31m0.5300\u001b[0m  0.0072\n",
            "      2       0.8824        \u001b[32m0.5313\u001b[0m       0.8878        \u001b[31m0.5240\u001b[0m  0.0076\n",
            "      3       0.8824        \u001b[32m0.5254\u001b[0m       0.8878        \u001b[31m0.5192\u001b[0m  0.0097\n",
            "      4       0.8824        \u001b[32m0.5212\u001b[0m       0.8878        \u001b[31m0.5149\u001b[0m  0.0073\n",
            "      5       0.8824        \u001b[32m0.5167\u001b[0m       0.8878        \u001b[31m0.5108\u001b[0m  0.0078\n",
            "      6       0.8824        \u001b[32m0.5136\u001b[0m       0.8878        \u001b[31m0.5066\u001b[0m  0.0084\n",
            "      7       0.8824        \u001b[32m0.5088\u001b[0m       0.8878        \u001b[31m0.5028\u001b[0m  0.0079\n",
            "      8       0.8824        \u001b[32m0.5061\u001b[0m       0.8878        \u001b[31m0.5001\u001b[0m  0.0076\n",
            "      9       0.8824        \u001b[32m0.5028\u001b[0m       0.8878        \u001b[31m0.4972\u001b[0m  0.0073\n",
            "     10       0.8824        \u001b[32m0.5002\u001b[0m       0.8878        \u001b[31m0.4946\u001b[0m  0.0077\n",
            "     11       0.8824        \u001b[32m0.4973\u001b[0m       0.8878        \u001b[31m0.4924\u001b[0m  0.0078\n",
            "     12       0.8824        \u001b[32m0.4955\u001b[0m       0.8878        \u001b[31m0.4897\u001b[0m  0.0078\n",
            "     13       0.8824        \u001b[32m0.4919\u001b[0m       0.8878        \u001b[31m0.4875\u001b[0m  0.0076\n",
            "     14       0.8824        \u001b[32m0.4917\u001b[0m       0.8878        \u001b[31m0.4851\u001b[0m  0.0084\n",
            "     15       0.8824        \u001b[32m0.4882\u001b[0m       0.8878        \u001b[31m0.4835\u001b[0m  0.0072\n",
            "     16       0.8824        \u001b[32m0.4868\u001b[0m       0.8878        \u001b[31m0.4817\u001b[0m  0.0076\n",
            "     17       0.8824        \u001b[32m0.4851\u001b[0m       0.8878        \u001b[31m0.4797\u001b[0m  0.0068\n",
            "     18       0.8824        \u001b[32m0.4837\u001b[0m       0.8878        \u001b[31m0.4778\u001b[0m  0.0090\n",
            "     19       0.8824        \u001b[32m0.4809\u001b[0m       0.8878        \u001b[31m0.4763\u001b[0m  0.0070\n",
            "     20       0.8824        \u001b[32m0.4802\u001b[0m       0.8878        \u001b[31m0.4747\u001b[0m  0.0073\n",
            "     21       0.8824        \u001b[32m0.4785\u001b[0m       0.8878        \u001b[31m0.4732\u001b[0m  0.0084\n",
            "     22       0.8824        \u001b[32m0.4771\u001b[0m       0.8878        \u001b[31m0.4717\u001b[0m  0.0074\n",
            "     23       0.8824        \u001b[32m0.4753\u001b[0m       0.8878        \u001b[31m0.4707\u001b[0m  0.0076\n",
            "     24       0.8824        \u001b[32m0.4741\u001b[0m       0.8878        \u001b[31m0.4696\u001b[0m  0.0076\n",
            "     25       0.8824        0.4745       0.8878        \u001b[31m0.4685\u001b[0m  0.0077\n",
            "     26       0.8824        \u001b[32m0.4721\u001b[0m       0.8878        \u001b[31m0.4675\u001b[0m  0.0105\n",
            "     27       0.8824        \u001b[32m0.4714\u001b[0m       0.8878        \u001b[31m0.4665\u001b[0m  0.0073\n",
            "     28       0.8824        \u001b[32m0.4703\u001b[0m       0.8878        \u001b[31m0.4654\u001b[0m  0.0077\n",
            "     29       0.8824        \u001b[32m0.4701\u001b[0m       0.8878        \u001b[31m0.4647\u001b[0m  0.0074\n",
            "     30       0.8824        \u001b[32m0.4687\u001b[0m       0.8878        \u001b[31m0.4638\u001b[0m  0.0076\n",
            "     31       0.8824        \u001b[32m0.4684\u001b[0m       0.8878        \u001b[31m0.4629\u001b[0m  0.0148\n",
            "     32       0.8824        \u001b[32m0.4675\u001b[0m       0.8878        \u001b[31m0.4622\u001b[0m  0.0106\n",
            "     33       0.8824        \u001b[32m0.4666\u001b[0m       0.8878        \u001b[31m0.4616\u001b[0m  0.0077\n",
            "     34       0.8824        \u001b[32m0.4653\u001b[0m       0.8878        \u001b[31m0.4607\u001b[0m  0.0075\n",
            "     35       0.8824        \u001b[32m0.4649\u001b[0m       0.8878        \u001b[31m0.4602\u001b[0m  0.0105\n",
            "     36       0.8824        \u001b[32m0.4643\u001b[0m       0.8878        \u001b[31m0.4595\u001b[0m  0.0138\n",
            "     37       0.8824        \u001b[32m0.4638\u001b[0m       0.8878        \u001b[31m0.4587\u001b[0m  0.0146\n",
            "     38       0.8824        \u001b[32m0.4632\u001b[0m       0.8878        \u001b[31m0.4580\u001b[0m  0.0122\n",
            "     39       0.8824        \u001b[32m0.4623\u001b[0m       0.8878        \u001b[31m0.4574\u001b[0m  0.0128\n",
            "     40       0.8824        \u001b[32m0.4615\u001b[0m       0.8878        \u001b[31m0.4568\u001b[0m  0.0096\n",
            "     41       0.8824        \u001b[32m0.4609\u001b[0m       0.8878        \u001b[31m0.4563\u001b[0m  0.0113\n",
            "     42       0.8824        \u001b[32m0.4601\u001b[0m       0.8878        \u001b[31m0.4556\u001b[0m  0.0082\n",
            "     43       0.8824        0.4603       0.8878        \u001b[31m0.4552\u001b[0m  0.0068\n",
            "     44       0.8824        \u001b[32m0.4600\u001b[0m       0.8878        \u001b[31m0.4547\u001b[0m  0.0072\n",
            "     45       0.8824        \u001b[32m0.4591\u001b[0m       0.8878        \u001b[31m0.4541\u001b[0m  0.0073\n",
            "     46       0.8824        \u001b[32m0.4588\u001b[0m       0.8878        \u001b[31m0.4536\u001b[0m  0.0077\n",
            "     47       0.8824        \u001b[32m0.4582\u001b[0m       0.8878        \u001b[31m0.4530\u001b[0m  0.0073\n",
            "     48       0.8824        \u001b[32m0.4578\u001b[0m       0.8878        \u001b[31m0.4525\u001b[0m  0.0073\n",
            "     49       0.8824        \u001b[32m0.4564\u001b[0m       0.8878        \u001b[31m0.4521\u001b[0m  0.0076\n",
            "     50       0.8824        0.4566       0.8878        \u001b[31m0.4516\u001b[0m  0.0073\n",
            "     51       0.8824        \u001b[32m0.4563\u001b[0m       0.8878        \u001b[31m0.4512\u001b[0m  0.0073\n",
            "     52       0.8824        0.4564       0.8878        \u001b[31m0.4509\u001b[0m  0.0070\n",
            "     53       0.8824        \u001b[32m0.4556\u001b[0m       0.8878        \u001b[31m0.4505\u001b[0m  0.0076\n",
            "     54       0.8824        \u001b[32m0.4547\u001b[0m       0.8878        \u001b[31m0.4502\u001b[0m  0.0069\n",
            "     55       0.8824        0.4547       0.8878        \u001b[31m0.4499\u001b[0m  0.0074\n",
            "     56       0.8824        \u001b[32m0.4540\u001b[0m       0.8878        \u001b[31m0.4495\u001b[0m  0.0070\n",
            "     57       0.8824        0.4543       0.8878        \u001b[31m0.4492\u001b[0m  0.0078\n",
            "     58       0.8824        \u001b[32m0.4537\u001b[0m       0.8878        \u001b[31m0.4489\u001b[0m  0.0102\n",
            "     59       0.8824        \u001b[32m0.4536\u001b[0m       0.8878        \u001b[31m0.4486\u001b[0m  0.0070\n",
            "     60       0.8824        \u001b[32m0.4525\u001b[0m       0.8878        \u001b[31m0.4482\u001b[0m  0.0076\n",
            "     61       0.8824        0.4528       0.8878        \u001b[31m0.4479\u001b[0m  0.0073\n",
            "     62       0.8824        0.4527       0.8878        \u001b[31m0.4476\u001b[0m  0.0136\n",
            "     63       0.8824        \u001b[32m0.4518\u001b[0m       0.8878        \u001b[31m0.4473\u001b[0m  0.0072\n",
            "     64       0.8824        0.4521       0.8878        \u001b[31m0.4470\u001b[0m  0.0082\n",
            "     65       0.8824        0.4522       0.8878        \u001b[31m0.4468\u001b[0m  0.0072\n",
            "     66       0.8824        0.4520       0.8878        \u001b[31m0.4464\u001b[0m  0.0098\n",
            "     67       0.8824        \u001b[32m0.4512\u001b[0m       0.8878        \u001b[31m0.4462\u001b[0m  0.0071\n",
            "     68       0.8824        \u001b[32m0.4507\u001b[0m       0.8878        \u001b[31m0.4460\u001b[0m  0.0074\n",
            "     69       0.8824        0.4510       0.8878        \u001b[31m0.4457\u001b[0m  0.0121\n",
            "     70       0.8824        \u001b[32m0.4505\u001b[0m       0.8878        \u001b[31m0.4456\u001b[0m  0.0094\n",
            "     71       0.8824        0.4506       0.8878        \u001b[31m0.4453\u001b[0m  0.0118\n",
            "     72       0.8824        0.4506       0.8878        \u001b[31m0.4451\u001b[0m  0.0105\n",
            "     73       0.8824        \u001b[32m0.4501\u001b[0m       0.8878        \u001b[31m0.4449\u001b[0m  0.0117\n",
            "     74       0.8824        \u001b[32m0.4499\u001b[0m       0.8878        \u001b[31m0.4447\u001b[0m  0.0138\n",
            "     75       0.8824        0.4499       0.8878        \u001b[31m0.4446\u001b[0m  0.0124\n",
            "     76       0.8824        \u001b[32m0.4496\u001b[0m       0.8878        \u001b[31m0.4443\u001b[0m  0.0076\n",
            "     77       0.8824        \u001b[32m0.4492\u001b[0m       0.8878        \u001b[31m0.4441\u001b[0m  0.0074\n",
            "     78       0.8824        \u001b[32m0.4491\u001b[0m       0.8878        \u001b[31m0.4439\u001b[0m  0.0072\n",
            "     79       0.8824        \u001b[32m0.4490\u001b[0m       0.8878        \u001b[31m0.4437\u001b[0m  0.0075\n",
            "     80       0.8824        \u001b[32m0.4483\u001b[0m       0.8878        \u001b[31m0.4435\u001b[0m  0.0072\n",
            "     81       0.8824        0.4485       0.8878        \u001b[31m0.4433\u001b[0m  0.0102\n",
            "     82       0.8824        0.4485       0.8878        \u001b[31m0.4431\u001b[0m  0.0072\n",
            "     83       0.8824        \u001b[32m0.4482\u001b[0m       0.8878        \u001b[31m0.4429\u001b[0m  0.0083\n",
            "     84       0.8824        \u001b[32m0.4476\u001b[0m       0.8878        \u001b[31m0.4427\u001b[0m  0.0070\n",
            "     85       0.8824        \u001b[32m0.4474\u001b[0m       0.8878        \u001b[31m0.4426\u001b[0m  0.0072\n",
            "     86       0.8824        \u001b[32m0.4474\u001b[0m       0.8878        \u001b[31m0.4424\u001b[0m  0.0071\n",
            "     87       0.8824        0.4478       0.8878        \u001b[31m0.4422\u001b[0m  0.0072\n",
            "     88       0.8824        \u001b[32m0.4471\u001b[0m       0.8878        \u001b[31m0.4420\u001b[0m  0.0069\n",
            "     89       0.8824        \u001b[32m0.4467\u001b[0m       0.8878        \u001b[31m0.4418\u001b[0m  0.0073\n",
            "     90       0.8824        \u001b[32m0.4461\u001b[0m       0.8878        \u001b[31m0.4417\u001b[0m  0.0133\n",
            "     91       0.8824        0.4467       0.8878        \u001b[31m0.4416\u001b[0m  0.0109\n",
            "     92       0.8824        0.4463       0.8878        \u001b[31m0.4414\u001b[0m  0.0113\n",
            "     93       0.8824        0.4461       0.8878        \u001b[31m0.4412\u001b[0m  0.0072\n",
            "     94       0.8824        0.4464       0.8878        \u001b[31m0.4411\u001b[0m  0.0076\n",
            "     95       0.8824        0.4461       0.8878        \u001b[31m0.4409\u001b[0m  0.0072\n",
            "     96       0.8824        0.4464       0.8878        \u001b[31m0.4408\u001b[0m  0.0096\n",
            "     97       0.8824        \u001b[32m0.4454\u001b[0m       0.8878        \u001b[31m0.4406\u001b[0m  0.0095\n",
            "     98       0.8824        0.4461       0.8878        \u001b[31m0.4405\u001b[0m  0.0106\n",
            "     99       0.8824        0.4462       0.8878        \u001b[31m0.4404\u001b[0m  0.0113\n",
            "    100       0.8824        \u001b[32m0.4449\u001b[0m       0.8878        \u001b[31m0.4402\u001b[0m  0.0140\n",
            "    101       0.8824        0.4453       0.8878        \u001b[31m0.4402\u001b[0m  0.0108\n",
            "    102       0.8824        0.4453       0.8878        \u001b[31m0.4401\u001b[0m  0.0095\n",
            "    103       0.8824        0.4453       0.8878        \u001b[31m0.4399\u001b[0m  0.0171\n",
            "    104       0.8824        \u001b[32m0.4449\u001b[0m       0.8878        \u001b[31m0.4398\u001b[0m  0.0110\n",
            "    105       0.8824        \u001b[32m0.4448\u001b[0m       0.8878        \u001b[31m0.4397\u001b[0m  0.0110\n",
            "    106       0.8824        0.4448       0.8878        \u001b[31m0.4396\u001b[0m  0.0110\n",
            "    107       0.8824        \u001b[32m0.4446\u001b[0m       0.8878        \u001b[31m0.4395\u001b[0m  0.0111\n",
            "    108       0.8824        0.4446       0.8878        \u001b[31m0.4393\u001b[0m  0.0110\n",
            "    109       0.8824        \u001b[32m0.4443\u001b[0m       0.8878        \u001b[31m0.4393\u001b[0m  0.0109\n",
            "    110       0.8824        \u001b[32m0.4442\u001b[0m       0.8878        \u001b[31m0.4392\u001b[0m  0.0114\n",
            "    111       0.8824        \u001b[32m0.4439\u001b[0m       0.8878        \u001b[31m0.4391\u001b[0m  0.0109\n",
            "    112       0.8824        0.4440       0.8878        \u001b[31m0.4390\u001b[0m  0.0109\n",
            "    113       0.8824        0.4442       0.8878        \u001b[31m0.4389\u001b[0m  0.0107\n",
            "    114       0.8824        0.4441       0.8878        \u001b[31m0.4388\u001b[0m  0.0110\n",
            "    115       0.8824        0.4440       0.8878        \u001b[31m0.4386\u001b[0m  0.0111\n",
            "    116       0.8824        0.4441       0.8878        \u001b[31m0.4385\u001b[0m  0.0108\n",
            "    117       0.8824        \u001b[32m0.4436\u001b[0m       0.8878        \u001b[31m0.4384\u001b[0m  0.0094\n",
            "    118       0.8824        0.4437       0.8878        \u001b[31m0.4383\u001b[0m  0.0072\n",
            "    119       0.8824        \u001b[32m0.4434\u001b[0m       0.8878        \u001b[31m0.4382\u001b[0m  0.0074\n",
            "    120       0.8824        0.4438       0.8878        \u001b[31m0.4381\u001b[0m  0.0071\n",
            "    121       0.8824        \u001b[32m0.4430\u001b[0m       0.8878        \u001b[31m0.4380\u001b[0m  0.0074\n",
            "    122       0.8824        0.4430       0.8878        \u001b[31m0.4379\u001b[0m  0.0073\n",
            "    123       0.8824        \u001b[32m0.4427\u001b[0m       0.8878        \u001b[31m0.4379\u001b[0m  0.0079\n",
            "    124       0.8824        0.4434       0.8878        \u001b[31m0.4377\u001b[0m  0.0079\n",
            "    125       0.8824        0.4427       0.8878        \u001b[31m0.4377\u001b[0m  0.0078\n",
            "    126       0.8824        0.4428       0.8878        \u001b[31m0.4375\u001b[0m  0.0075\n",
            "    127       0.8824        0.4428       0.8878        \u001b[31m0.4375\u001b[0m  0.0073\n",
            "    128       0.8824        \u001b[32m0.4424\u001b[0m       0.8878        \u001b[31m0.4374\u001b[0m  0.0073\n",
            "    129       0.8824        0.4427       0.8878        \u001b[31m0.4373\u001b[0m  0.0074\n",
            "    130       0.8824        0.4426       0.8878        \u001b[31m0.4372\u001b[0m  0.0104\n",
            "    131       0.8824        \u001b[32m0.4424\u001b[0m       0.8878        \u001b[31m0.4371\u001b[0m  0.0071\n",
            "    132       0.8824        \u001b[32m0.4417\u001b[0m       0.8878        \u001b[31m0.4370\u001b[0m  0.0072\n",
            "    133       0.8824        0.4422       0.8878        \u001b[31m0.4369\u001b[0m  0.0074\n",
            "    134       0.8824        0.4418       0.8878        \u001b[31m0.4368\u001b[0m  0.0078\n",
            "    135       0.8824        0.4422       0.8878        \u001b[31m0.4367\u001b[0m  0.0070\n",
            "    136       0.8824        0.4417       0.8878        \u001b[31m0.4367\u001b[0m  0.0083\n",
            "    137       0.8824        0.4417       0.8878        \u001b[31m0.4366\u001b[0m  0.0074\n",
            "    138       0.8824        0.4417       0.8878        \u001b[31m0.4365\u001b[0m  0.0077\n",
            "    139       0.8824        0.4419       0.8878        \u001b[31m0.4365\u001b[0m  0.0079\n",
            "    140       0.8824        \u001b[32m0.4413\u001b[0m       0.8878        \u001b[31m0.4364\u001b[0m  0.0114\n",
            "    141       0.8824        \u001b[32m0.4411\u001b[0m       0.8878        \u001b[31m0.4363\u001b[0m  0.0084\n",
            "    142       0.8824        0.4415       0.8878        \u001b[31m0.4362\u001b[0m  0.0076\n",
            "    143       0.8824        0.4412       0.8878        \u001b[31m0.4362\u001b[0m  0.0074\n",
            "    144       0.8824        0.4415       0.8878        \u001b[31m0.4361\u001b[0m  0.0077\n",
            "    145       0.8824        0.4412       0.8878        \u001b[31m0.4360\u001b[0m  0.0071\n",
            "    146       0.8824        0.4412       0.8878        \u001b[31m0.4360\u001b[0m  0.0075\n",
            "    147       0.8824        \u001b[32m0.4410\u001b[0m       0.8878        \u001b[31m0.4359\u001b[0m  0.0070\n",
            "    148       0.8824        0.4413       0.8878        \u001b[31m0.4358\u001b[0m  0.0090\n",
            "    149       0.8824        0.4410       0.8878        \u001b[31m0.4358\u001b[0m  0.0071\n",
            "    150       0.8824        \u001b[32m0.4406\u001b[0m       0.8878        \u001b[31m0.4357\u001b[0m  0.0077\n",
            "    151       0.8824        0.4408       0.8878        \u001b[31m0.4357\u001b[0m  0.0071\n",
            "    152       0.8824        0.4407       0.8878        \u001b[31m0.4356\u001b[0m  0.0073\n",
            "    153       0.8824        0.4407       0.8878        \u001b[31m0.4355\u001b[0m  0.0071\n",
            "    154       0.8824        0.4407       0.8878        \u001b[31m0.4354\u001b[0m  0.0074\n",
            "    155       0.8824        \u001b[32m0.4405\u001b[0m       0.8878        \u001b[31m0.4354\u001b[0m  0.0073\n",
            "    156       0.8824        0.4406       0.8878        \u001b[31m0.4353\u001b[0m  0.0074\n",
            "    157       0.8824        \u001b[32m0.4400\u001b[0m       0.8878        \u001b[31m0.4353\u001b[0m  0.0074\n",
            "    158       0.8824        0.4403       0.8878        \u001b[31m0.4352\u001b[0m  0.0076\n",
            "    159       0.8824        0.4406       0.8878        \u001b[31m0.4352\u001b[0m  0.0072\n",
            "    160       0.8824        0.4402       0.8878        \u001b[31m0.4351\u001b[0m  0.0073\n",
            "    161       0.8824        0.4402       0.8878        \u001b[31m0.4350\u001b[0m  0.0067\n",
            "    162       0.8824        0.4401       0.8878        \u001b[31m0.4350\u001b[0m  0.0073\n",
            "    163       0.8824        0.4402       0.8878        \u001b[31m0.4350\u001b[0m  0.0076\n",
            "    164       0.8824        0.4404       0.8878        \u001b[31m0.4349\u001b[0m  0.0073\n",
            "    165       0.8824        0.4403       0.8878        \u001b[31m0.4349\u001b[0m  0.0071\n",
            "    166       0.8824        0.4400       0.8878        \u001b[31m0.4348\u001b[0m  0.0075\n",
            "    167       0.8824        0.4400       0.8878        \u001b[31m0.4348\u001b[0m  0.0093\n",
            "    168       0.8824        \u001b[32m0.4399\u001b[0m       0.8878        \u001b[31m0.4347\u001b[0m  0.0068\n",
            "    169       0.8824        0.4399       0.8878        \u001b[31m0.4347\u001b[0m  0.0111\n",
            "    170       0.8824        \u001b[32m0.4398\u001b[0m       0.8878        \u001b[31m0.4346\u001b[0m  0.0070\n",
            "    171       0.8824        \u001b[32m0.4396\u001b[0m       0.8878        \u001b[31m0.4346\u001b[0m  0.0075\n",
            "    172       0.8824        0.4396       0.8878        \u001b[31m0.4345\u001b[0m  0.0123\n",
            "    173       0.8824        \u001b[32m0.4395\u001b[0m       0.8878        \u001b[31m0.4345\u001b[0m  0.0079\n",
            "    174       0.8824        0.4397       0.8878        \u001b[31m0.4345\u001b[0m  0.0077\n",
            "    175       0.8824        \u001b[32m0.4395\u001b[0m       0.8878        \u001b[31m0.4344\u001b[0m  0.0090\n",
            "    176       0.8824        0.4396       0.8878        \u001b[31m0.4344\u001b[0m  0.0073\n",
            "    177       0.8824        0.4396       0.8878        \u001b[31m0.4343\u001b[0m  0.0076\n",
            "    178       0.8824        \u001b[32m0.4394\u001b[0m       0.8878        \u001b[31m0.4343\u001b[0m  0.0074\n",
            "    179       0.8824        \u001b[32m0.4392\u001b[0m       0.8878        \u001b[31m0.4342\u001b[0m  0.0076\n",
            "    180       0.8824        0.4393       0.8878        \u001b[31m0.4342\u001b[0m  0.0079\n",
            "    181       0.8824        0.4393       0.8878        \u001b[31m0.4341\u001b[0m  0.0075\n",
            "    182       0.8824        0.4393       0.8878        \u001b[31m0.4341\u001b[0m  0.0074\n",
            "    183       0.8824        \u001b[32m0.4392\u001b[0m       0.8878        \u001b[31m0.4340\u001b[0m  0.0072\n",
            "    184       0.8824        0.4393       0.8878        \u001b[31m0.4340\u001b[0m  0.0074\n",
            "    185       0.8824        \u001b[32m0.4391\u001b[0m       0.8878        \u001b[31m0.4339\u001b[0m  0.0083\n",
            "    186       0.8824        0.4392       0.8878        \u001b[31m0.4339\u001b[0m  0.0069\n",
            "    187       0.8824        0.4393       0.8878        \u001b[31m0.4338\u001b[0m  0.0074\n",
            "    188       0.8824        0.4392       0.8878        \u001b[31m0.4338\u001b[0m  0.0086\n",
            "    189       0.8824        \u001b[32m0.4389\u001b[0m       0.8878        \u001b[31m0.4337\u001b[0m  0.0074\n",
            "    190       0.8824        \u001b[32m0.4388\u001b[0m       0.8878        \u001b[31m0.4337\u001b[0m  0.0094\n",
            "    191       0.8824        0.4391       0.8878        \u001b[31m0.4336\u001b[0m  0.0072\n",
            "    192       0.8824        \u001b[32m0.4387\u001b[0m       0.8878        \u001b[31m0.4336\u001b[0m  0.0077\n",
            "    193       0.8824        0.4390       0.8878        \u001b[31m0.4336\u001b[0m  0.0072\n",
            "    194       0.8824        \u001b[32m0.4387\u001b[0m       0.8878        \u001b[31m0.4335\u001b[0m  0.0081\n",
            "    195       0.8824        0.4389       0.8878        \u001b[31m0.4335\u001b[0m  0.0089\n",
            "    196       0.8824        \u001b[32m0.4386\u001b[0m       0.8878        \u001b[31m0.4334\u001b[0m  0.0073\n",
            "    197       0.8824        0.4389       0.8878        \u001b[31m0.4334\u001b[0m  0.0075\n",
            "    198       0.8824        0.4387       0.8878        \u001b[31m0.4334\u001b[0m  0.0078\n",
            "    199       0.8824        \u001b[32m0.4383\u001b[0m       0.8878        \u001b[31m0.4333\u001b[0m  0.0074\n",
            "    200       0.8824        0.4387       0.8878        \u001b[31m0.4333\u001b[0m  0.0073\n",
            "  epoch    train_acc    train_loss    valid_acc    valid_loss     dur\n",
            "-------  -----------  ------------  -----------  ------------  ------\n",
            "      1       \u001b[36m0.6239\u001b[0m        \u001b[32m0.6913\u001b[0m       \u001b[35m0.8855\u001b[0m        \u001b[31m0.6753\u001b[0m  0.0110\n",
            "      2       0.8882        \u001b[32m0.6670\u001b[0m       0.8855        \u001b[31m0.6535\u001b[0m  0.0111\n",
            "      3       0.8882        \u001b[32m0.6459\u001b[0m       0.8855        \u001b[31m0.6332\u001b[0m  0.0115\n",
            "      4       0.8882        \u001b[32m0.6263\u001b[0m       0.8855        \u001b[31m0.6157\u001b[0m  0.0116\n",
            "      5       0.8882        \u001b[32m0.6093\u001b[0m       0.8855        \u001b[31m0.5994\u001b[0m  0.0153\n",
            "      6       0.8882        \u001b[32m0.5935\u001b[0m       0.8855        \u001b[31m0.5855\u001b[0m  0.0114\n",
            "      7       0.8882        \u001b[32m0.5801\u001b[0m       0.8855        \u001b[31m0.5736\u001b[0m  0.0129\n",
            "      8       0.8882        \u001b[32m0.5686\u001b[0m       0.8855        \u001b[31m0.5623\u001b[0m  0.0125\n",
            "      9       0.8882        \u001b[32m0.5576\u001b[0m       0.8855        \u001b[31m0.5524\u001b[0m  0.0167\n",
            "     10       0.8882        \u001b[32m0.5483\u001b[0m       0.8855        \u001b[31m0.5437\u001b[0m  0.0325\n",
            "     11       0.8882        \u001b[32m0.5395\u001b[0m       0.8855        \u001b[31m0.5363\u001b[0m  0.0194\n",
            "     12       0.8882        \u001b[32m0.5326\u001b[0m       0.8855        \u001b[31m0.5297\u001b[0m  0.0233\n",
            "     13       0.8882        \u001b[32m0.5260\u001b[0m       0.8855        \u001b[31m0.5236\u001b[0m  0.0114\n",
            "     14       0.8882        \u001b[32m0.5201\u001b[0m       0.8855        \u001b[31m0.5186\u001b[0m  0.0111\n",
            "     15       0.8882        \u001b[32m0.5151\u001b[0m       0.8855        \u001b[31m0.5134\u001b[0m  0.0113\n",
            "     16       0.8882        \u001b[32m0.5103\u001b[0m       0.8855        \u001b[31m0.5088\u001b[0m  0.0112\n",
            "     17       0.8882        \u001b[32m0.5056\u001b[0m       0.8855        \u001b[31m0.5047\u001b[0m  0.0117\n",
            "     18       0.8882        \u001b[32m0.5016\u001b[0m       0.8855        \u001b[31m0.5008\u001b[0m  0.0117\n",
            "     19       0.8882        \u001b[32m0.4980\u001b[0m       0.8855        \u001b[31m0.4975\u001b[0m  0.0121\n",
            "     20       0.8882        \u001b[32m0.4945\u001b[0m       0.8855        \u001b[31m0.4943\u001b[0m  0.0114\n",
            "     21       0.8882        \u001b[32m0.4913\u001b[0m       0.8855        \u001b[31m0.4915\u001b[0m  0.0130\n",
            "     22       0.8882        \u001b[32m0.4884\u001b[0m       0.8855        \u001b[31m0.4886\u001b[0m  0.0117\n",
            "     23       0.8882        \u001b[32m0.4857\u001b[0m       0.8855        \u001b[31m0.4862\u001b[0m  0.0112\n",
            "     24       0.8882        \u001b[32m0.4834\u001b[0m       0.8855        \u001b[31m0.4838\u001b[0m  0.0124\n",
            "     25       0.8882        \u001b[32m0.4810\u001b[0m       0.8855        \u001b[31m0.4817\u001b[0m  0.0138\n",
            "     26       0.8882        \u001b[32m0.4790\u001b[0m       0.8855        \u001b[31m0.4798\u001b[0m  0.0112\n",
            "     27       0.8882        \u001b[32m0.4770\u001b[0m       0.8855        \u001b[31m0.4780\u001b[0m  0.0112\n",
            "     28       0.8882        \u001b[32m0.4754\u001b[0m       0.8855        \u001b[31m0.4762\u001b[0m  0.0117\n",
            "     29       0.8882        \u001b[32m0.4735\u001b[0m       0.8855        \u001b[31m0.4745\u001b[0m  0.0115\n",
            "     30       0.8882        \u001b[32m0.4720\u001b[0m       0.8855        \u001b[31m0.4730\u001b[0m  0.0115\n",
            "     31       0.8882        \u001b[32m0.4704\u001b[0m       0.8855        \u001b[31m0.4717\u001b[0m  0.0124\n",
            "     32       0.8882        \u001b[32m0.4690\u001b[0m       0.8855        \u001b[31m0.4704\u001b[0m  0.0115\n",
            "     33       0.8882        \u001b[32m0.4678\u001b[0m       0.8855        \u001b[31m0.4691\u001b[0m  0.0114\n",
            "     34       0.8882        \u001b[32m0.4665\u001b[0m       0.8855        \u001b[31m0.4680\u001b[0m  0.0157\n",
            "     35       0.8882        \u001b[32m0.4653\u001b[0m       0.8855        \u001b[31m0.4670\u001b[0m  0.0137\n",
            "     36       0.8882        \u001b[32m0.4644\u001b[0m       0.8855        \u001b[31m0.4659\u001b[0m  0.0163\n",
            "     37       0.8882        \u001b[32m0.4633\u001b[0m       0.8855        \u001b[31m0.4649\u001b[0m  0.0132\n",
            "     38       0.8882        \u001b[32m0.4624\u001b[0m       0.8855        \u001b[31m0.4640\u001b[0m  0.0112\n",
            "     39       0.8882        \u001b[32m0.4612\u001b[0m       0.8855        \u001b[31m0.4632\u001b[0m  0.0112\n",
            "     40       0.8882        \u001b[32m0.4606\u001b[0m       0.8855        \u001b[31m0.4624\u001b[0m  0.0109\n",
            "     41       0.8882        \u001b[32m0.4598\u001b[0m       0.8855        \u001b[31m0.4615\u001b[0m  0.0113\n",
            "     42       0.8882        \u001b[32m0.4592\u001b[0m       0.8855        \u001b[31m0.4608\u001b[0m  0.0113\n",
            "     43       0.8882        \u001b[32m0.4583\u001b[0m       0.8855        \u001b[31m0.4600\u001b[0m  0.0131\n",
            "     44       0.8882        \u001b[32m0.4572\u001b[0m       0.8855        \u001b[31m0.4594\u001b[0m  0.0111\n",
            "     45       0.8882        \u001b[32m0.4567\u001b[0m       0.8855        \u001b[31m0.4587\u001b[0m  0.0181\n",
            "     46       0.8882        \u001b[32m0.4562\u001b[0m       0.8855        \u001b[31m0.4580\u001b[0m  0.0116\n",
            "     47       0.8882        \u001b[32m0.4557\u001b[0m       0.8855        \u001b[31m0.4574\u001b[0m  0.0113\n",
            "     48       0.8882        \u001b[32m0.4549\u001b[0m       0.8855        \u001b[31m0.4568\u001b[0m  0.0157\n",
            "     49       0.8882        \u001b[32m0.4543\u001b[0m       0.8855        \u001b[31m0.4562\u001b[0m  0.0173\n",
            "     50       0.8882        \u001b[32m0.4537\u001b[0m       0.8855        \u001b[31m0.4556\u001b[0m  0.0123\n",
            "     51       0.8882        \u001b[32m0.4529\u001b[0m       0.8855        \u001b[31m0.4550\u001b[0m  0.0112\n",
            "     52       0.8882        \u001b[32m0.4525\u001b[0m       0.8855        \u001b[31m0.4545\u001b[0m  0.0112\n",
            "     53       0.8882        \u001b[32m0.4518\u001b[0m       0.8855        \u001b[31m0.4539\u001b[0m  0.0142\n",
            "     54       0.8882        \u001b[32m0.4517\u001b[0m       0.8855        \u001b[31m0.4534\u001b[0m  0.0115\n",
            "     55       0.8882        \u001b[32m0.4511\u001b[0m       0.8855        \u001b[31m0.4530\u001b[0m  0.0121\n",
            "     56       0.8882        \u001b[32m0.4504\u001b[0m       0.8855        \u001b[31m0.4525\u001b[0m  0.0117\n",
            "     57       0.8882        \u001b[32m0.4500\u001b[0m       0.8855        \u001b[31m0.4521\u001b[0m  0.0111\n",
            "     58       0.8882        \u001b[32m0.4495\u001b[0m       0.8855        \u001b[31m0.4517\u001b[0m  0.0136\n",
            "     59       0.8882        \u001b[32m0.4490\u001b[0m       0.8855        \u001b[31m0.4513\u001b[0m  0.0147\n",
            "     60       0.8882        \u001b[32m0.4487\u001b[0m       0.8855        \u001b[31m0.4509\u001b[0m  0.0181\n",
            "     61       0.8882        \u001b[32m0.4484\u001b[0m       0.8855        \u001b[31m0.4505\u001b[0m  0.0207\n",
            "     62       0.8882        \u001b[32m0.4479\u001b[0m       0.8855        \u001b[31m0.4502\u001b[0m  0.0153\n",
            "     63       0.8882        \u001b[32m0.4479\u001b[0m       0.8855        \u001b[31m0.4498\u001b[0m  0.0190\n",
            "     64       0.8882        \u001b[32m0.4471\u001b[0m       0.8855        \u001b[31m0.4495\u001b[0m  0.0114\n",
            "     65       0.8882        \u001b[32m0.4469\u001b[0m       0.8855        \u001b[31m0.4491\u001b[0m  0.0171\n",
            "     66       0.8882        \u001b[32m0.4466\u001b[0m       0.8855        \u001b[31m0.4488\u001b[0m  0.0184\n",
            "     67       0.8882        \u001b[32m0.4461\u001b[0m       0.8855        \u001b[31m0.4485\u001b[0m  0.0166\n",
            "     68       0.8882        \u001b[32m0.4460\u001b[0m       0.8855        \u001b[31m0.4482\u001b[0m  0.0169\n",
            "     69       0.8882        \u001b[32m0.4455\u001b[0m       0.8855        \u001b[31m0.4479\u001b[0m  0.0129\n",
            "     70       0.8882        \u001b[32m0.4453\u001b[0m       0.8855        \u001b[31m0.4476\u001b[0m  0.0117\n",
            "     71       0.8882        \u001b[32m0.4451\u001b[0m       0.8855        \u001b[31m0.4473\u001b[0m  0.0114\n",
            "     72       0.8882        \u001b[32m0.4449\u001b[0m       0.8855        \u001b[31m0.4470\u001b[0m  0.0157\n",
            "     73       0.8882        \u001b[32m0.4444\u001b[0m       0.8855        \u001b[31m0.4467\u001b[0m  0.0156\n",
            "     74       0.8882        \u001b[32m0.4442\u001b[0m       0.8855        \u001b[31m0.4465\u001b[0m  0.0128\n",
            "     75       0.8882        \u001b[32m0.4441\u001b[0m       0.8855        \u001b[31m0.4462\u001b[0m  0.0111\n",
            "     76       0.8882        \u001b[32m0.4437\u001b[0m       0.8855        \u001b[31m0.4460\u001b[0m  0.0189\n",
            "     77       0.8882        \u001b[32m0.4436\u001b[0m       0.8855        \u001b[31m0.4458\u001b[0m  0.0112\n",
            "     78       0.8882        \u001b[32m0.4431\u001b[0m       0.8855        \u001b[31m0.4455\u001b[0m  0.0113\n",
            "     79       0.8882        0.4433       0.8855        \u001b[31m0.4453\u001b[0m  0.0114\n",
            "     80       0.8882        \u001b[32m0.4428\u001b[0m       0.8855        \u001b[31m0.4451\u001b[0m  0.0112\n",
            "     81       0.8882        \u001b[32m0.4428\u001b[0m       0.8855        \u001b[31m0.4449\u001b[0m  0.0110\n",
            "     82       0.8882        \u001b[32m0.4423\u001b[0m       0.8855        \u001b[31m0.4447\u001b[0m  0.0184\n",
            "     83       0.8882        \u001b[32m0.4420\u001b[0m       0.8855        \u001b[31m0.4445\u001b[0m  0.0203\n",
            "     84       0.8882        \u001b[32m0.4419\u001b[0m       0.8855        \u001b[31m0.4443\u001b[0m  0.0112\n",
            "     85       0.8882        \u001b[32m0.4417\u001b[0m       0.8855        \u001b[31m0.4441\u001b[0m  0.0113\n",
            "     86       0.8882        \u001b[32m0.4415\u001b[0m       0.8855        \u001b[31m0.4439\u001b[0m  0.0111\n",
            "     87       0.8882        \u001b[32m0.4412\u001b[0m       0.8855        \u001b[31m0.4437\u001b[0m  0.0113\n",
            "     88       0.8882        \u001b[32m0.4411\u001b[0m       0.8855        \u001b[31m0.4435\u001b[0m  0.0110\n",
            "     89       0.8882        \u001b[32m0.4410\u001b[0m       0.8855        \u001b[31m0.4433\u001b[0m  0.0111\n",
            "     90       0.8882        \u001b[32m0.4408\u001b[0m       0.8855        \u001b[31m0.4431\u001b[0m  0.0121\n",
            "     91       0.8882        \u001b[32m0.4405\u001b[0m       0.8855        \u001b[31m0.4429\u001b[0m  0.0158\n",
            "     92       0.8882        \u001b[32m0.4404\u001b[0m       0.8855        \u001b[31m0.4428\u001b[0m  0.0113\n",
            "     93       0.8882        \u001b[32m0.4403\u001b[0m       0.8855        \u001b[31m0.4426\u001b[0m  0.0114\n",
            "     94       0.8882        \u001b[32m0.4402\u001b[0m       0.8855        \u001b[31m0.4425\u001b[0m  0.0113\n",
            "     95       0.8882        \u001b[32m0.4397\u001b[0m       0.8855        \u001b[31m0.4423\u001b[0m  0.0124\n",
            "     96       0.8882        0.4398       0.8855        \u001b[31m0.4422\u001b[0m  0.0114\n",
            "     97       0.8882        \u001b[32m0.4396\u001b[0m       0.8855        \u001b[31m0.4420\u001b[0m  0.0114\n",
            "     98       0.8882        \u001b[32m0.4392\u001b[0m       0.8855        \u001b[31m0.4419\u001b[0m  0.0115\n",
            "     99       0.8882        0.4393       0.8855        \u001b[31m0.4418\u001b[0m  0.0114\n",
            "    100       0.8882        0.4393       0.8855        \u001b[31m0.4416\u001b[0m  0.0109\n",
            "    101       0.8882        \u001b[32m0.4390\u001b[0m       0.8855        \u001b[31m0.4415\u001b[0m  0.0112\n",
            "    102       0.8882        \u001b[32m0.4388\u001b[0m       0.8855        \u001b[31m0.4413\u001b[0m  0.0110\n",
            "    103       0.8882        \u001b[32m0.4388\u001b[0m       0.8855        \u001b[31m0.4412\u001b[0m  0.0114\n",
            "    104       0.8882        0.4389       0.8855        \u001b[31m0.4411\u001b[0m  0.0145\n",
            "    105       0.8882        \u001b[32m0.4384\u001b[0m       0.8855        \u001b[31m0.4410\u001b[0m  0.0111\n",
            "    106       0.8882        0.4385       0.8855        \u001b[31m0.4408\u001b[0m  0.0115\n",
            "    107       0.8882        \u001b[32m0.4382\u001b[0m       0.8855        \u001b[31m0.4407\u001b[0m  0.0204\n",
            "    108       0.8882        \u001b[32m0.4381\u001b[0m       0.8855        \u001b[31m0.4406\u001b[0m  0.0123\n",
            "    109       0.8882        \u001b[32m0.4379\u001b[0m       0.8855        \u001b[31m0.4405\u001b[0m  0.0114\n",
            "    110       0.8882        0.4380       0.8855        \u001b[31m0.4403\u001b[0m  0.0127\n",
            "    111       0.8882        \u001b[32m0.4378\u001b[0m       0.8855        \u001b[31m0.4402\u001b[0m  0.0109\n",
            "    112       0.8882        \u001b[32m0.4378\u001b[0m       0.8855        \u001b[31m0.4401\u001b[0m  0.0127\n",
            "    113       0.8882        \u001b[32m0.4376\u001b[0m       0.8855        \u001b[31m0.4400\u001b[0m  0.0122\n",
            "    114       0.8882        \u001b[32m0.4375\u001b[0m       0.8855        \u001b[31m0.4399\u001b[0m  0.0112\n",
            "    115       0.8882        \u001b[32m0.4374\u001b[0m       0.8855        \u001b[31m0.4398\u001b[0m  0.0125\n",
            "    116       0.8882        \u001b[32m0.4370\u001b[0m       0.8855        \u001b[31m0.4397\u001b[0m  0.0116\n",
            "    117       0.8882        \u001b[32m0.4369\u001b[0m       0.8855        \u001b[31m0.4396\u001b[0m  0.0124\n",
            "    118       0.8882        0.4370       0.8855        \u001b[31m0.4395\u001b[0m  0.0113\n",
            "    119       0.8882        \u001b[32m0.4368\u001b[0m       0.8855        \u001b[31m0.4394\u001b[0m  0.0127\n",
            "    120       0.8882        \u001b[32m0.4368\u001b[0m       0.8855        \u001b[31m0.4393\u001b[0m  0.0111\n",
            "    121       0.8882        0.4369       0.8855        \u001b[31m0.4392\u001b[0m  0.0112\n",
            "    122       0.8882        \u001b[32m0.4367\u001b[0m       0.8855        \u001b[31m0.4391\u001b[0m  0.0115\n",
            "    123       0.8882        \u001b[32m0.4365\u001b[0m       0.8855        \u001b[31m0.4390\u001b[0m  0.0113\n",
            "    124       0.8882        \u001b[32m0.4362\u001b[0m       0.8855        \u001b[31m0.4389\u001b[0m  0.0110\n",
            "    125       0.8882        0.4366       0.8855        \u001b[31m0.4388\u001b[0m  0.0114\n",
            "    126       0.8882        \u001b[32m0.4362\u001b[0m       0.8855        \u001b[31m0.4387\u001b[0m  0.0114\n",
            "    127       0.8882        0.4362       0.8855        \u001b[31m0.4386\u001b[0m  0.0111\n",
            "    128       0.8882        \u001b[32m0.4361\u001b[0m       0.8855        \u001b[31m0.4385\u001b[0m  0.0109\n",
            "    129       0.8882        \u001b[32m0.4361\u001b[0m       0.8855        \u001b[31m0.4384\u001b[0m  0.0128\n",
            "    130       0.8882        \u001b[32m0.4358\u001b[0m       0.8855        \u001b[31m0.4384\u001b[0m  0.0141\n",
            "    131       0.8882        0.4359       0.8855        \u001b[31m0.4383\u001b[0m  0.0142\n",
            "    132       0.8882        0.4359       0.8855        \u001b[31m0.4382\u001b[0m  0.0113\n",
            "    133       0.8882        \u001b[32m0.4355\u001b[0m       0.8855        \u001b[31m0.4381\u001b[0m  0.0125\n",
            "    134       0.8882        0.4355       0.8855        \u001b[31m0.4380\u001b[0m  0.0109\n",
            "    135       0.8882        0.4356       0.8855        \u001b[31m0.4380\u001b[0m  0.0121\n",
            "    136       0.8882        \u001b[32m0.4354\u001b[0m       0.8855        \u001b[31m0.4379\u001b[0m  0.0114\n",
            "    137       0.8882        \u001b[32m0.4353\u001b[0m       0.8855        \u001b[31m0.4378\u001b[0m  0.0110\n",
            "    138       0.8882        0.4354       0.8855        \u001b[31m0.4377\u001b[0m  0.0125\n",
            "    139       0.8882        \u001b[32m0.4352\u001b[0m       0.8855        \u001b[31m0.4377\u001b[0m  0.0168\n",
            "    140       0.8882        \u001b[32m0.4352\u001b[0m       0.8855        \u001b[31m0.4376\u001b[0m  0.0217\n",
            "    141       0.8882        \u001b[32m0.4350\u001b[0m       0.8855        \u001b[31m0.4375\u001b[0m  0.0199\n",
            "    142       0.8882        \u001b[32m0.4349\u001b[0m       0.8855        \u001b[31m0.4375\u001b[0m  0.0156\n",
            "    143       0.8882        \u001b[32m0.4349\u001b[0m       0.8855        \u001b[31m0.4374\u001b[0m  0.0133\n",
            "    144       0.8882        \u001b[32m0.4346\u001b[0m       0.8855        \u001b[31m0.4373\u001b[0m  0.0114\n",
            "    145       0.8882        0.4347       0.8855        \u001b[31m0.4373\u001b[0m  0.0112\n",
            "    146       0.8882        0.4348       0.8855        \u001b[31m0.4372\u001b[0m  0.0118\n",
            "    147       0.8882        \u001b[32m0.4345\u001b[0m       0.8855        \u001b[31m0.4371\u001b[0m  0.0115\n",
            "    148       0.8882        0.4346       0.8855        \u001b[31m0.4371\u001b[0m  0.0109\n",
            "    149       0.8882        \u001b[32m0.4345\u001b[0m       0.8855        \u001b[31m0.4370\u001b[0m  0.0109\n",
            "    150       0.8882        0.4346       0.8855        \u001b[31m0.4369\u001b[0m  0.0114\n",
            "    151       0.8882        \u001b[32m0.4344\u001b[0m       0.8855        \u001b[31m0.4369\u001b[0m  0.0115\n",
            "    152       0.8882        0.4344       0.8855        \u001b[31m0.4368\u001b[0m  0.0108\n",
            "    153       0.8882        \u001b[32m0.4342\u001b[0m       0.8855        \u001b[31m0.4368\u001b[0m  0.0113\n",
            "    154       0.8882        \u001b[32m0.4341\u001b[0m       0.8855        \u001b[31m0.4367\u001b[0m  0.0114\n",
            "    155       0.8882        0.4342       0.8855        \u001b[31m0.4366\u001b[0m  0.0117\n",
            "    156       0.8882        \u001b[32m0.4340\u001b[0m       0.8855        \u001b[31m0.4366\u001b[0m  0.0113\n",
            "    157       0.8882        \u001b[32m0.4339\u001b[0m       0.8855        \u001b[31m0.4365\u001b[0m  0.0114\n",
            "    158       0.8882        \u001b[32m0.4339\u001b[0m       0.8855        \u001b[31m0.4365\u001b[0m  0.0114\n",
            "    159       0.8882        0.4339       0.8855        \u001b[31m0.4364\u001b[0m  0.0108\n",
            "    160       0.8882        \u001b[32m0.4338\u001b[0m       0.8855        \u001b[31m0.4364\u001b[0m  0.0134\n",
            "    161       0.8882        0.4338       0.8855        \u001b[31m0.4363\u001b[0m  0.0115\n",
            "    162       0.8882        \u001b[32m0.4338\u001b[0m       0.8855        \u001b[31m0.4363\u001b[0m  0.0137\n",
            "    163       0.8882        0.4338       0.8855        \u001b[31m0.4362\u001b[0m  0.0112\n",
            "    164       0.8882        0.4339       0.8855        \u001b[31m0.4361\u001b[0m  0.0113\n",
            "    165       0.8882        \u001b[32m0.4337\u001b[0m       0.8855        \u001b[31m0.4361\u001b[0m  0.0121\n",
            "    166       0.8882        \u001b[32m0.4334\u001b[0m       0.8855        \u001b[31m0.4360\u001b[0m  0.0110\n",
            "    167       0.8882        0.4335       0.8855        \u001b[31m0.4360\u001b[0m  0.0112\n",
            "    168       0.8882        \u001b[32m0.4333\u001b[0m       0.8855        \u001b[31m0.4359\u001b[0m  0.0123\n",
            "    169       0.8882        \u001b[32m0.4332\u001b[0m       0.8855        \u001b[31m0.4359\u001b[0m  0.0117\n",
            "    170       0.8882        0.4333       0.8855        \u001b[31m0.4358\u001b[0m  0.0115\n",
            "    171       0.8882        \u001b[32m0.4332\u001b[0m       0.8855        \u001b[31m0.4358\u001b[0m  0.0114\n",
            "    172       0.8882        0.4334       0.8855        \u001b[31m0.4357\u001b[0m  0.0115\n",
            "    173       0.8882        \u001b[32m0.4331\u001b[0m       0.8855        \u001b[31m0.4357\u001b[0m  0.0131\n",
            "    174       0.8882        0.4331       0.8855        \u001b[31m0.4356\u001b[0m  0.0118\n",
            "    175       0.8882        \u001b[32m0.4330\u001b[0m       0.8855        \u001b[31m0.4356\u001b[0m  0.0114\n",
            "    176       0.8882        0.4331       0.8855        \u001b[31m0.4356\u001b[0m  0.0112\n",
            "    177       0.8882        \u001b[32m0.4330\u001b[0m       0.8855        \u001b[31m0.4355\u001b[0m  0.0114\n",
            "    178       0.8882        \u001b[32m0.4329\u001b[0m       0.8855        \u001b[31m0.4355\u001b[0m  0.0113\n",
            "    179       0.8882        0.4330       0.8855        \u001b[31m0.4354\u001b[0m  0.0136\n",
            "    180       0.8882        \u001b[32m0.4328\u001b[0m       0.8855        \u001b[31m0.4354\u001b[0m  0.0118\n",
            "    181       0.8882        \u001b[32m0.4328\u001b[0m       0.8855        \u001b[31m0.4353\u001b[0m  0.0123\n",
            "    182       0.8882        \u001b[32m0.4327\u001b[0m       0.8855        \u001b[31m0.4353\u001b[0m  0.0153\n",
            "    183       0.8882        0.4328       0.8855        \u001b[31m0.4352\u001b[0m  0.0114\n",
            "    184       0.8882        0.4327       0.8855        \u001b[31m0.4352\u001b[0m  0.0113\n",
            "    185       0.8882        \u001b[32m0.4326\u001b[0m       0.8855        \u001b[31m0.4352\u001b[0m  0.0120\n",
            "    186       0.8882        \u001b[32m0.4326\u001b[0m       0.8855        \u001b[31m0.4351\u001b[0m  0.0128\n",
            "    187       0.8882        0.4327       0.8855        \u001b[31m0.4351\u001b[0m  0.0113\n",
            "    188       0.8882        \u001b[32m0.4326\u001b[0m       0.8855        \u001b[31m0.4350\u001b[0m  0.0131\n",
            "    189       0.8882        \u001b[32m0.4325\u001b[0m       0.8855        \u001b[31m0.4350\u001b[0m  0.0113\n",
            "    190       0.8882        \u001b[32m0.4324\u001b[0m       0.8855        \u001b[31m0.4350\u001b[0m  0.0113\n",
            "    191       0.8882        \u001b[32m0.4324\u001b[0m       0.8855        \u001b[31m0.4349\u001b[0m  0.0113\n",
            "    192       0.8882        \u001b[32m0.4323\u001b[0m       0.8855        \u001b[31m0.4349\u001b[0m  0.0112\n",
            "    193       0.8882        0.4324       0.8855        \u001b[31m0.4349\u001b[0m  0.0113\n",
            "    194       0.8882        \u001b[32m0.4322\u001b[0m       0.8855        \u001b[31m0.4348\u001b[0m  0.0118\n",
            "    195       0.8882        \u001b[32m0.4322\u001b[0m       0.8855        \u001b[31m0.4348\u001b[0m  0.0161\n",
            "    196       0.8882        \u001b[32m0.4321\u001b[0m       0.8855        \u001b[31m0.4347\u001b[0m  0.0249\n",
            "    197       0.8882        0.4322       0.8855        \u001b[31m0.4347\u001b[0m  0.0196\n",
            "    198       0.8882        0.4322       0.8855        \u001b[31m0.4347\u001b[0m  0.0160\n",
            "    199       0.8882        \u001b[32m0.4321\u001b[0m       0.8855        \u001b[31m0.4346\u001b[0m  0.0157\n",
            "    200       0.8882        0.4321       0.8855        \u001b[31m0.4346\u001b[0m  0.0118\n",
            "  epoch    train_acc    train_loss    valid_acc    valid_loss     dur\n",
            "-------  -----------  ------------  -----------  ------------  ------\n",
            "      1       \u001b[36m0.8832\u001b[0m        \u001b[32m0.5901\u001b[0m       \u001b[35m0.8803\u001b[0m        \u001b[31m0.5812\u001b[0m  0.0191\n",
            "      2       0.8832        \u001b[32m0.5736\u001b[0m       0.8803        \u001b[31m0.5656\u001b[0m  0.0162\n",
            "      3       0.8832        \u001b[32m0.5581\u001b[0m       0.8803        \u001b[31m0.5525\u001b[0m  0.0169\n",
            "      4       0.8832        \u001b[32m0.5458\u001b[0m       0.8803        \u001b[31m0.5416\u001b[0m  0.0155\n",
            "      5       0.8832        \u001b[32m0.5359\u001b[0m       0.8803        \u001b[31m0.5320\u001b[0m  0.0150\n",
            "      6       0.8832        \u001b[32m0.5260\u001b[0m       0.8803        \u001b[31m0.5240\u001b[0m  0.0173\n",
            "      7       0.8832        \u001b[32m0.5193\u001b[0m       0.8803        \u001b[31m0.5169\u001b[0m  0.0157\n",
            "      8       0.8832        \u001b[32m0.5118\u001b[0m       0.8803        \u001b[31m0.5110\u001b[0m  0.0159\n",
            "      9       0.8832        \u001b[32m0.5066\u001b[0m       0.8803        \u001b[31m0.5056\u001b[0m  0.0151\n",
            "     10       0.8832        \u001b[32m0.5016\u001b[0m       0.8803        \u001b[31m0.5009\u001b[0m  0.0150\n",
            "     11       0.8832        \u001b[32m0.4973\u001b[0m       0.8803        \u001b[31m0.4966\u001b[0m  0.0153\n",
            "     12       0.8832        \u001b[32m0.4923\u001b[0m       0.8803        \u001b[31m0.4930\u001b[0m  0.0152\n",
            "     13       0.8832        \u001b[32m0.4891\u001b[0m       0.8803        \u001b[31m0.4897\u001b[0m  0.0160\n",
            "     14       0.8832        \u001b[32m0.4863\u001b[0m       0.8803        \u001b[31m0.4867\u001b[0m  0.0153\n",
            "     15       0.8832        \u001b[32m0.4833\u001b[0m       0.8803        \u001b[31m0.4840\u001b[0m  0.0166\n",
            "     16       0.8832        \u001b[32m0.4807\u001b[0m       0.8803        \u001b[31m0.4815\u001b[0m  0.0153\n",
            "     17       0.8832        \u001b[32m0.4779\u001b[0m       0.8803        \u001b[31m0.4793\u001b[0m  0.0163\n",
            "     18       0.8832        \u001b[32m0.4760\u001b[0m       0.8803        \u001b[31m0.4773\u001b[0m  0.0154\n",
            "     19       0.8832        \u001b[32m0.4736\u001b[0m       0.8803        \u001b[31m0.4754\u001b[0m  0.0150\n",
            "     20       0.8832        \u001b[32m0.4723\u001b[0m       0.8803        \u001b[31m0.4736\u001b[0m  0.0152\n",
            "     21       0.8832        \u001b[32m0.4704\u001b[0m       0.8803        \u001b[31m0.4721\u001b[0m  0.0150\n",
            "     22       0.8832        \u001b[32m0.4684\u001b[0m       0.8803        \u001b[31m0.4705\u001b[0m  0.0155\n",
            "     23       0.8832        \u001b[32m0.4677\u001b[0m       0.8803        \u001b[31m0.4691\u001b[0m  0.0152\n",
            "     24       0.8832        \u001b[32m0.4656\u001b[0m       0.8803        \u001b[31m0.4678\u001b[0m  0.0157\n",
            "     25       0.8832        \u001b[32m0.4646\u001b[0m       0.8803        \u001b[31m0.4666\u001b[0m  0.0159\n",
            "     26       0.8832        \u001b[32m0.4638\u001b[0m       0.8803        \u001b[31m0.4655\u001b[0m  0.0154\n",
            "     27       0.8832        \u001b[32m0.4624\u001b[0m       0.8803        \u001b[31m0.4644\u001b[0m  0.0152\n",
            "     28       0.8832        \u001b[32m0.4616\u001b[0m       0.8803        \u001b[31m0.4634\u001b[0m  0.0157\n",
            "     29       0.8832        \u001b[32m0.4603\u001b[0m       0.8803        \u001b[31m0.4625\u001b[0m  0.0158\n",
            "     30       0.8832        \u001b[32m0.4595\u001b[0m       0.8803        \u001b[31m0.4616\u001b[0m  0.0155\n",
            "     31       0.8832        \u001b[32m0.4588\u001b[0m       0.8803        \u001b[31m0.4607\u001b[0m  0.0147\n",
            "     32       0.8832        \u001b[32m0.4582\u001b[0m       0.8803        \u001b[31m0.4599\u001b[0m  0.0152\n",
            "     33       0.8832        \u001b[32m0.4571\u001b[0m       0.8803        \u001b[31m0.4591\u001b[0m  0.0153\n",
            "     34       0.8832        \u001b[32m0.4563\u001b[0m       0.8803        \u001b[31m0.4584\u001b[0m  0.0157\n",
            "     35       0.8832        \u001b[32m0.4559\u001b[0m       0.8803        \u001b[31m0.4578\u001b[0m  0.0158\n",
            "     36       0.8832        \u001b[32m0.4549\u001b[0m       0.8803        \u001b[31m0.4571\u001b[0m  0.0155\n",
            "     37       0.8832        \u001b[32m0.4543\u001b[0m       0.8803        \u001b[31m0.4565\u001b[0m  0.0162\n",
            "     38       0.8832        \u001b[32m0.4538\u001b[0m       0.8803        \u001b[31m0.4559\u001b[0m  0.0159\n",
            "     39       0.8832        \u001b[32m0.4530\u001b[0m       0.8803        \u001b[31m0.4554\u001b[0m  0.0160\n",
            "     40       0.8832        \u001b[32m0.4524\u001b[0m       0.8803        \u001b[31m0.4548\u001b[0m  0.0236\n",
            "     41       0.8832        \u001b[32m0.4519\u001b[0m       0.8803        \u001b[31m0.4543\u001b[0m  0.0241\n",
            "     42       0.8832        \u001b[32m0.4517\u001b[0m       0.8803        \u001b[31m0.4539\u001b[0m  0.0239\n",
            "     43       0.8832        \u001b[32m0.4512\u001b[0m       0.8803        \u001b[31m0.4534\u001b[0m  0.0254\n",
            "     44       0.8832        \u001b[32m0.4504\u001b[0m       0.8803        \u001b[31m0.4530\u001b[0m  0.0248\n",
            "     45       0.8832        \u001b[32m0.4499\u001b[0m       0.8803        \u001b[31m0.4526\u001b[0m  0.0221\n",
            "     46       0.8832        \u001b[32m0.4496\u001b[0m       0.8803        \u001b[31m0.4521\u001b[0m  0.0266\n",
            "     47       0.8832        \u001b[32m0.4494\u001b[0m       0.8803        \u001b[31m0.4517\u001b[0m  0.0154\n",
            "     48       0.8832        \u001b[32m0.4491\u001b[0m       0.8803        \u001b[31m0.4514\u001b[0m  0.0156\n",
            "     49       0.8832        \u001b[32m0.4486\u001b[0m       0.8803        \u001b[31m0.4510\u001b[0m  0.0156\n",
            "     50       0.8832        \u001b[32m0.4484\u001b[0m       0.8803        \u001b[31m0.4506\u001b[0m  0.0152\n",
            "     51       0.8832        \u001b[32m0.4480\u001b[0m       0.8803        \u001b[31m0.4503\u001b[0m  0.0153\n",
            "     52       0.8832        \u001b[32m0.4477\u001b[0m       0.8803        \u001b[31m0.4500\u001b[0m  0.0157\n",
            "     53       0.8832        \u001b[32m0.4473\u001b[0m       0.8803        \u001b[31m0.4497\u001b[0m  0.0152\n",
            "     54       0.8832        \u001b[32m0.4468\u001b[0m       0.8803        \u001b[31m0.4494\u001b[0m  0.0207\n",
            "     55       0.8832        \u001b[32m0.4466\u001b[0m       0.8803        \u001b[31m0.4491\u001b[0m  0.0527\n",
            "     56       0.8832        \u001b[32m0.4463\u001b[0m       0.8803        \u001b[31m0.4488\u001b[0m  0.0220\n",
            "     57       0.8832        \u001b[32m0.4462\u001b[0m       0.8803        \u001b[31m0.4485\u001b[0m  0.0156\n",
            "     58       0.8832        \u001b[32m0.4458\u001b[0m       0.8803        \u001b[31m0.4483\u001b[0m  0.0162\n",
            "     59       0.8832        \u001b[32m0.4454\u001b[0m       0.8803        \u001b[31m0.4480\u001b[0m  0.0162\n",
            "     60       0.8832        0.4454       0.8803        \u001b[31m0.4478\u001b[0m  0.0152\n",
            "     61       0.8832        \u001b[32m0.4448\u001b[0m       0.8803        \u001b[31m0.4475\u001b[0m  0.0151\n",
            "     62       0.8832        \u001b[32m0.4447\u001b[0m       0.8803        \u001b[31m0.4473\u001b[0m  0.0190\n",
            "     63       0.8832        \u001b[32m0.4446\u001b[0m       0.8803        \u001b[31m0.4471\u001b[0m  0.0252\n",
            "     64       0.8832        \u001b[32m0.4442\u001b[0m       0.8803        \u001b[31m0.4468\u001b[0m  0.0237\n",
            "     65       0.8832        \u001b[32m0.4440\u001b[0m       0.8803        \u001b[31m0.4466\u001b[0m  0.0234\n",
            "     66       0.8832        \u001b[32m0.4438\u001b[0m       0.8803        \u001b[31m0.4464\u001b[0m  0.0150\n",
            "     67       0.8832        \u001b[32m0.4435\u001b[0m       0.8803        \u001b[31m0.4462\u001b[0m  0.0152\n",
            "     68       0.8832        \u001b[32m0.4435\u001b[0m       0.8803        \u001b[31m0.4460\u001b[0m  0.0155\n",
            "     69       0.8832        \u001b[32m0.4433\u001b[0m       0.8803        \u001b[31m0.4458\u001b[0m  0.0153\n",
            "     70       0.8832        \u001b[32m0.4430\u001b[0m       0.8803        \u001b[31m0.4457\u001b[0m  0.0155\n",
            "     71       0.8832        \u001b[32m0.4428\u001b[0m       0.8803        \u001b[31m0.4455\u001b[0m  0.0147\n",
            "     72       0.8832        \u001b[32m0.4427\u001b[0m       0.8803        \u001b[31m0.4453\u001b[0m  0.0241\n",
            "     73       0.8832        \u001b[32m0.4426\u001b[0m       0.8803        \u001b[31m0.4451\u001b[0m  0.0153\n",
            "     74       0.8832        \u001b[32m0.4423\u001b[0m       0.8803        \u001b[31m0.4450\u001b[0m  0.0152\n",
            "     75       0.8832        \u001b[32m0.4422\u001b[0m       0.8803        \u001b[31m0.4448\u001b[0m  0.0148\n",
            "     76       0.8832        \u001b[32m0.4421\u001b[0m       0.8803        \u001b[31m0.4447\u001b[0m  0.0226\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "# Plot the learning curve\n",
        "train_sizes, train_scores, test_scores = learning_curve(\n",
        "    net, X1.values.astype(np.float32), Y1.values.ravel().astype(np.int64), cv=3\n",
        ")\n",
        "\n",
        "train_scores_mean = train_scores.mean(axis=1)\n",
        "train_scores_std = train_scores.std(axis=1)\n",
        "test_scores_mean = test_scores.mean(axis=1)\n",
        "test_scores_std = test_scores.std(axis=1)\n",
        "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                     color='cyan')\n",
        "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color='darkorchid')\n",
        "plt.plot(train_sizes, train_scores_mean, label=\"Training score\", color='cyan')\n",
        "plt.plot(train_sizes, test_scores_mean, label=\"Test score\", color='darkorchid')\n",
        "plt.title(\"Learning Curve\")\n",
        "plt.xlabel(\"Training size\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.grid(visible=True)\n",
        "plt.legend(frameon=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c40af005-0621-4017-a880-56ba295f91b6",
      "metadata": {
        "id": "c40af005-0621-4017-a880-56ba295f91b6"
      },
      "source": [
        "## Using sklearn pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1301a1b-8a82-4c68-9172-aebbfd6cec3d",
      "metadata": {
        "id": "f1301a1b-8a82-4c68-9172-aebbfd6cec3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Re-initializing module because the following parameters were re-set: activation, dropout_percent, hidden_layers, hidden_units, input_dim, output_dim.\n",
            "Re-initializing criterion because the following parameters were re-set: weight.\n",
            "Re-initializing optimizer.\n",
            "  epoch    train_acc    train_loss    valid_acc    valid_loss     dur\n",
            "-------  -----------  ------------  -----------  ------------  ------\n",
            "      1       \u001b[36m0.2568\u001b[0m        \u001b[32m0.6914\u001b[0m       \u001b[35m0.2641\u001b[0m        \u001b[31m0.6909\u001b[0m  0.0527\n",
            "      2       0.3102        \u001b[32m0.6906\u001b[0m       \u001b[35m0.3061\u001b[0m        \u001b[31m0.6902\u001b[0m  0.0524\n",
            "      3       0.3625        \u001b[32m0.6898\u001b[0m       \u001b[35m0.3304\u001b[0m        \u001b[31m0.6895\u001b[0m  0.0518\n",
            "      4       0.3743        \u001b[32m0.6891\u001b[0m       \u001b[35m0.3624\u001b[0m        \u001b[31m0.6887\u001b[0m  0.0527\n",
            "      5       0.4086        \u001b[32m0.6880\u001b[0m       \u001b[35m0.3757\u001b[0m        \u001b[31m0.6879\u001b[0m  0.0511\n",
            "      6       0.4233        \u001b[32m0.6870\u001b[0m       \u001b[35m0.4265\u001b[0m        \u001b[31m0.6867\u001b[0m  0.0524\n",
            "      7       0.4612        \u001b[32m0.6857\u001b[0m       \u001b[35m0.4376\u001b[0m        \u001b[31m0.6856\u001b[0m  0.0569\n",
            "      8       0.4811        \u001b[32m0.6842\u001b[0m       \u001b[35m0.4475\u001b[0m        \u001b[31m0.6842\u001b[0m  0.0514\n",
            "      9       0.4871        \u001b[32m0.6824\u001b[0m       \u001b[35m0.4740\u001b[0m        \u001b[31m0.6824\u001b[0m  0.0568\n",
            "     10       0.5297        \u001b[32m0.6802\u001b[0m       0.4740        \u001b[31m0.6803\u001b[0m  0.0608\n",
            "     11       0.5198        \u001b[32m0.6771\u001b[0m       \u001b[35m0.4906\u001b[0m        \u001b[31m0.6777\u001b[0m  0.0707\n",
            "     12       0.5358        \u001b[32m0.6738\u001b[0m       \u001b[35m0.5116\u001b[0m        \u001b[31m0.6745\u001b[0m  0.2369\n",
            "     13       0.5552        \u001b[32m0.6691\u001b[0m       \u001b[35m0.5304\u001b[0m        \u001b[31m0.6707\u001b[0m  0.1139\n",
            "     14       0.5693        \u001b[32m0.6654\u001b[0m       \u001b[35m0.5381\u001b[0m        \u001b[31m0.6665\u001b[0m  0.0616\n",
            "     15       0.5778        \u001b[32m0.6601\u001b[0m       \u001b[35m0.5536\u001b[0m        \u001b[31m0.6617\u001b[0m  0.0591\n",
            "     16       0.5914        \u001b[32m0.6541\u001b[0m       \u001b[35m0.5680\u001b[0m        \u001b[31m0.6566\u001b[0m  0.0512\n",
            "     17       0.6011        \u001b[32m0.6479\u001b[0m       \u001b[35m0.5801\u001b[0m        \u001b[31m0.6511\u001b[0m  0.0716\n",
            "     18       0.6163        \u001b[32m0.6416\u001b[0m       \u001b[35m0.5878\u001b[0m        \u001b[31m0.6453\u001b[0m  0.1321\n",
            "     19       0.6254        \u001b[32m0.6346\u001b[0m       \u001b[35m0.6022\u001b[0m        \u001b[31m0.6397\u001b[0m  0.1746\n",
            "     20       0.6387        \u001b[32m0.6286\u001b[0m       \u001b[35m0.6033\u001b[0m        \u001b[31m0.6335\u001b[0m  0.0902\n",
            "     21       0.6458        \u001b[32m0.6220\u001b[0m       \u001b[35m0.6122\u001b[0m        \u001b[31m0.6276\u001b[0m  0.0615\n",
            "     22       0.6525        \u001b[32m0.6150\u001b[0m       \u001b[35m0.6210\u001b[0m        \u001b[31m0.6219\u001b[0m  0.0594\n",
            "     23       0.6699        \u001b[32m0.6080\u001b[0m       \u001b[35m0.6365\u001b[0m        \u001b[31m0.6156\u001b[0m  0.0560\n",
            "     24       0.6815        \u001b[32m0.6008\u001b[0m       \u001b[35m0.6464\u001b[0m        \u001b[31m0.6096\u001b[0m  0.0542\n",
            "     25       0.6873        \u001b[32m0.5953\u001b[0m       \u001b[35m0.6608\u001b[0m        \u001b[31m0.6033\u001b[0m  0.0698\n",
            "     26       0.6984        \u001b[32m0.5890\u001b[0m       \u001b[35m0.6674\u001b[0m        \u001b[31m0.5968\u001b[0m  0.0528\n",
            "     27       0.7086        \u001b[32m0.5826\u001b[0m       \u001b[35m0.6807\u001b[0m        \u001b[31m0.5904\u001b[0m  0.0531\n",
            "     28       0.7186        \u001b[32m0.5760\u001b[0m       \u001b[35m0.6983\u001b[0m        \u001b[31m0.5842\u001b[0m  0.0517\n",
            "     29       0.7340        \u001b[32m0.5713\u001b[0m       \u001b[35m0.7094\u001b[0m        \u001b[31m0.5788\u001b[0m  0.0534\n",
            "     30       0.7429        \u001b[32m0.5657\u001b[0m       \u001b[35m0.7160\u001b[0m        \u001b[31m0.5745\u001b[0m  0.0525\n",
            "     31       0.7487        \u001b[32m0.5621\u001b[0m       \u001b[35m0.7215\u001b[0m        \u001b[31m0.5710\u001b[0m  0.0520\n",
            "     32       0.7562        \u001b[32m0.5594\u001b[0m       \u001b[35m0.7227\u001b[0m        \u001b[31m0.5687\u001b[0m  0.0527\n",
            "     33       0.7581        \u001b[32m0.5555\u001b[0m       \u001b[35m0.7282\u001b[0m        \u001b[31m0.5667\u001b[0m  0.0519\n",
            "     34       0.7631        \u001b[32m0.5544\u001b[0m       0.7260        \u001b[31m0.5656\u001b[0m  0.0541\n",
            "     35       0.7625        \u001b[32m0.5525\u001b[0m       0.7282        \u001b[31m0.5646\u001b[0m  0.0550\n",
            "     36       0.7586        \u001b[32m0.5513\u001b[0m       0.7282        \u001b[31m0.5633\u001b[0m  0.0520\n",
            "     37       0.7589        \u001b[32m0.5488\u001b[0m       0.7282        \u001b[31m0.5613\u001b[0m  0.0515\n",
            "     38       0.7559        \u001b[32m0.5477\u001b[0m       \u001b[35m0.7304\u001b[0m        \u001b[31m0.5601\u001b[0m  0.0533\n",
            "     39       0.7575        \u001b[32m0.5461\u001b[0m       0.7282        \u001b[31m0.5590\u001b[0m  0.0515\n",
            "     40       0.7564        \u001b[32m0.5453\u001b[0m       0.7282        \u001b[31m0.5580\u001b[0m  0.0560\n",
            "     41       0.7553        \u001b[32m0.5446\u001b[0m       0.7304        \u001b[31m0.5572\u001b[0m  0.0531\n",
            "     42       0.7550        \u001b[32m0.5436\u001b[0m       \u001b[35m0.7326\u001b[0m        \u001b[31m0.5566\u001b[0m  0.0527\n",
            "     43       0.7567        \u001b[32m0.5415\u001b[0m       0.7293        0.5571  0.0522\n",
            "     44       0.7545        0.5425       0.7326        \u001b[31m0.5557\u001b[0m  0.0514\n",
            "     45       0.7564        \u001b[32m0.5407\u001b[0m       \u001b[35m0.7348\u001b[0m        \u001b[31m0.5543\u001b[0m  0.0524\n",
            "     46       0.7559        0.5412       0.7326        \u001b[31m0.5543\u001b[0m  0.0517\n",
            "     47       0.7567        \u001b[32m0.5405\u001b[0m       0.7315        0.5545  0.0528\n",
            "     48       0.7542        \u001b[32m0.5396\u001b[0m       0.7304        \u001b[31m0.5539\u001b[0m  0.0529\n",
            "     49       0.7526        \u001b[32m0.5391\u001b[0m       \u001b[35m0.7359\u001b[0m        \u001b[31m0.5528\u001b[0m  0.0523\n",
            "     50       0.7581        \u001b[32m0.5389\u001b[0m       0.7359        \u001b[31m0.5522\u001b[0m  0.0530\n",
            "     51       0.7578        \u001b[32m0.5384\u001b[0m       0.7348        0.5522  0.0798\n",
            "     52       0.7603        \u001b[32m0.5374\u001b[0m       0.7315        \u001b[31m0.5521\u001b[0m  0.1173\n",
            "     53       0.7520        \u001b[32m0.5366\u001b[0m       0.7326        0.5522  0.1334\n",
            "     54       0.7553        \u001b[32m0.5363\u001b[0m       0.7337        \u001b[31m0.5520\u001b[0m  0.1366\n",
            "     55       0.7553        0.5376       0.7348        \u001b[31m0.5509\u001b[0m  0.1481\n",
            "     56       0.7550        0.5368       0.7337        0.5511  0.0834\n",
            "     57       0.7581        0.5364       0.7315        0.5511  0.0645\n",
            "     58       0.7537        0.5376       0.7304        0.5520  0.0531\n",
            "     59       0.7567        \u001b[32m0.5361\u001b[0m       0.7326        0.5511  0.0615\n",
            "     60       0.7570        \u001b[32m0.5355\u001b[0m       0.7326        0.5509  0.1012\n",
            "     61       0.7581        0.5365       0.7304        0.5509  0.0580\n",
            "     62       0.7545        \u001b[32m0.5348\u001b[0m       0.7282        0.5518  0.0791\n",
            "     63       0.7562        0.5354       0.7304        0.5511  0.0738\n",
            "     64       0.7581        0.5363       0.7326        \u001b[31m0.5506\u001b[0m  0.0530\n",
            "     65       0.7603        0.5355       0.7315        \u001b[31m0.5503\u001b[0m  0.0517\n",
            "     66       0.7592        \u001b[32m0.5339\u001b[0m       0.7315        \u001b[31m0.5500\u001b[0m  0.1016\n",
            "     67       0.7531        0.5351       0.7315        0.5501  0.0537\n",
            "     68       0.7556        0.5347       0.7359        \u001b[31m0.5489\u001b[0m  0.0564\n",
            "     69       0.7548        0.5346       0.7282        0.5506  0.0531\n",
            "     70       0.7515        0.5343       0.7315        0.5495  0.0543\n",
            "     71       0.7523        \u001b[32m0.5326\u001b[0m       0.7326        0.5492  0.0537\n",
            "     72       0.7512        0.5326       0.7326        \u001b[31m0.5489\u001b[0m  0.0525\n",
            "     73       0.7553        0.5344       0.7337        0.5490  0.0521\n",
            "     74       0.7553        0.5333       0.7304        0.5493  0.0531\n",
            "     75       0.7562        0.5331       0.7304        0.5500  0.0516\n",
            "     76       0.7539        0.5333       0.7326        0.5495  0.0603\n",
            "     77       0.7586        0.5337       0.7315        0.5495  0.0907\n",
            "     78       0.7567        0.5330       0.7337        0.5496  0.0834\n",
            "     79       0.7597        0.5345       0.7271        0.5502  0.0520\n",
            "     80       0.7520        \u001b[32m0.5322\u001b[0m       0.7326        \u001b[31m0.5485\u001b[0m  0.0725\n",
            "     81       0.7559        0.5329       0.7315        \u001b[31m0.5483\u001b[0m  0.0573\n",
            "     82       0.7584        0.5332       0.7293        \u001b[31m0.5480\u001b[0m  0.0560\n",
            "     83       0.7512        \u001b[32m0.5320\u001b[0m       0.7304        0.5488  0.0801\n",
            "     84       0.7539        0.5335       0.7326        \u001b[31m0.5478\u001b[0m  0.0779\n",
            "     85       0.7578        0.5323       0.7315        0.5481  0.0537\n",
            "     86       0.7595        \u001b[32m0.5307\u001b[0m       0.7326        \u001b[31m0.5472\u001b[0m  0.0525\n",
            "     87       0.7564        0.5334       0.7337        0.5479  0.0536\n",
            "     88       0.7559        0.5322       0.7304        0.5485  0.0544\n",
            "     89       0.7562        0.5327       0.7348        0.5475  0.0562\n",
            "     90       0.7562        0.5318       0.7315        0.5482  0.0536\n",
            "     91       0.7556        0.5323       0.7260        0.5495  0.0570\n",
            "     92       0.7509        0.5320       0.7282        0.5486  0.0531\n",
            "     93       0.7545        0.5317       0.7326        0.5479  0.0533\n",
            "     94       0.7545        0.5324       0.7348        0.5473  0.0535\n",
            "     95       0.7581        0.5329       0.7337        0.5477  0.0523\n",
            "     96       0.7556        0.5316       0.7337        0.5478  0.0520\n",
            "     97       0.7570        0.5319       0.7304        0.5483  0.0564\n",
            "     98       0.7548        0.5317       0.7271        0.5490  0.0573\n",
            "     99       0.7584        0.5313       0.7260        0.5488  0.0532\n",
            "    100       0.7539        0.5323       0.7260        0.5484  0.0521\n",
            "    101       0.7562        0.5311       0.7271        0.5482  0.0530\n",
            "    102       0.7526        0.5323       0.7304        0.5473  0.0538\n",
            "    103       0.7575        \u001b[32m0.5305\u001b[0m       0.7260        0.5484  0.0520\n",
            "    104       0.7531        \u001b[32m0.5305\u001b[0m       0.7271        0.5479  0.0520\n",
            "    105       0.7534        0.5320       0.7304        0.5476  0.0537\n",
            "    106       0.7562        0.5322       0.7293        0.5474  0.0528\n",
            "    107       0.7567        0.5306       0.7304        0.5479  0.0520\n",
            "    108       0.7528        0.5311       0.7315        0.5479  0.0526\n",
            "    109       0.7556        0.5309       0.7315        0.5479  0.0525\n",
            "    110       0.7578        0.5312       0.7282        0.5478  0.0526\n",
            "    111       0.7553        0.5306       0.7304        0.5480  0.0527\n",
            "    112       0.7573        \u001b[32m0.5302\u001b[0m       \u001b[35m0.7381\u001b[0m        \u001b[31m0.5471\u001b[0m  0.0518\n",
            "    113       0.7586        0.5311       0.7337        \u001b[31m0.5470\u001b[0m  0.0601\n",
            "    114       0.7559        \u001b[32m0.5302\u001b[0m       0.7337        \u001b[31m0.5461\u001b[0m  0.0515\n",
            "    115       0.7545        0.5303       0.7315        0.5462  0.0569\n",
            "    116       0.7520        \u001b[32m0.5298\u001b[0m       0.7315        0.5463  0.0526\n",
            "    117       0.7545        0.5302       0.7282        0.5475  0.0536\n",
            "    118       0.7562        0.5312       0.7249        0.5476  0.0527\n",
            "    119       0.7531        0.5305       0.7304        0.5470  0.0748\n",
            "    120       0.7542        \u001b[32m0.5296\u001b[0m       0.7304        0.5470  0.0653\n",
            "    121       0.7562        0.5327       0.7282        0.5473  0.0517\n",
            "    122       0.7520        \u001b[32m0.5293\u001b[0m       0.7315        0.5470  0.0517\n",
            "    123       0.7564        0.5309       0.7304        0.5475  0.0554\n",
            "    124       0.7564        0.5297       0.7315        0.5466  0.0522\n",
            "    125       0.7553        0.5306       0.7326        0.5464  0.0543\n",
            "    126       0.7559        0.5309       0.7271        0.5490  0.0533\n",
            "    127       0.7539        0.5316       0.7293        0.5477  0.0822\n",
            "    128       0.7539        0.5301       0.7282        0.5476  0.0664\n",
            "    129       0.7539        0.5312       0.7293        0.5475  0.0525\n",
            "    130       0.7537        0.5300       0.7337        0.5470  0.0520\n",
            "    131       0.7578        0.5301       0.7304        0.5470  0.0524\n",
            "    132       0.7553        0.5304       0.7293        0.5465  0.0575\n",
            "    133       0.7559        0.5325       0.7315        \u001b[31m0.5460\u001b[0m  0.0537\n",
            "    134       0.7539        0.5300       0.7326        \u001b[31m0.5459\u001b[0m  0.0519\n",
            "    135       0.7575        0.5303       0.7260        0.5460  0.0524\n",
            "    136       0.7515        0.5308       0.7304        0.5459  0.0524\n",
            "    137       0.7539        0.5305       0.7260        0.5462  0.0572\n",
            "    138       0.7531        0.5308       0.7193        0.5467  0.0541\n",
            "    139       0.7509        0.5296       0.7282        \u001b[31m0.5458\u001b[0m  0.0535\n",
            "    140       0.7542        0.5300       0.7260        0.5467  0.0527\n",
            "    141       0.7526        0.5296       0.7260        0.5469  0.0538\n",
            "    142       0.7537        0.5318       0.7249        0.5475  0.0509\n",
            "    143       0.7517        0.5299       0.7271        0.5470  0.0526\n",
            "    144       0.7545        0.5296       0.7271        0.5466  0.0521\n",
            "    145       0.7550        0.5298       0.7271        0.5466  0.0529\n",
            "    146       0.7515        0.5320       0.7282        0.5464  0.0537\n",
            "    147       0.7559        0.5301       0.7304        0.5460  0.0507\n",
            "    148       0.7515        0.5300       0.7315        0.5463  0.0546\n",
            "    149       0.7548        \u001b[32m0.5291\u001b[0m       0.7315        0.5462  0.0591\n",
            "    150       0.7539        0.5295       0.7315        0.5468  0.0746\n",
            "    151       0.7567        0.5308       0.7304        \u001b[31m0.5456\u001b[0m  0.0530\n",
            "    152       0.7539        \u001b[32m0.5291\u001b[0m       0.7227        0.5476  0.0530\n",
            "    153       0.7492        \u001b[32m0.5283\u001b[0m       0.7304        0.5473  0.0693\n",
            "    154       0.7584        0.5307       0.7304        0.5463  0.0802\n",
            "    155       0.7556        0.5304       0.7304        0.5458  0.0665\n",
            "    156       0.7573        0.5293       0.7293        0.5470  0.0764\n",
            "    157       0.7537        0.5304       0.7271        0.5474  0.0544\n",
            "    158       0.7559        0.5309       0.7282        0.5475  0.0521\n",
            "    159       0.7545        0.5309       0.7282        0.5471  0.0527\n",
            "    160       0.7550        0.5304       0.7315        0.5462  0.0530\n",
            "    161       0.7592        0.5295       0.7315        0.5466  0.0519\n",
            "    162       0.7559        0.5288       0.7315        0.5466  0.0521\n",
            "    163       0.7542        0.5321       0.7293        0.5474  0.0645\n",
            "    164       0.7562        0.5295       0.7293        0.5474  0.0640\n",
            "    165       0.7517        0.5300       0.7238        0.5476  0.0617\n",
            "    166       0.7542        0.5299       0.7282        0.5467  0.0522\n",
            "    167       0.7539        0.5306       0.7293        0.5472  0.0530\n",
            "    168       0.7556        0.5300       0.7238        0.5493  0.0523\n",
            "    169       0.7545        0.5294       0.7204        0.5499  0.0512\n",
            "    170       0.7487        0.5314       0.7293        0.5475  0.0523\n",
            "    171       0.7548        0.5294       0.7326        0.5467  0.0520\n",
            "    172       0.7564        0.5298       0.7326        0.5458  0.0534\n",
            "    173       0.7570        \u001b[32m0.5279\u001b[0m       0.7260        0.5473  0.0517\n",
            "    174       0.7548        0.5284       0.7293        0.5465  0.0536\n",
            "    175       0.7537        0.5288       0.7315        0.5466  0.0543\n",
            "    176       0.7559        0.5292       0.7348        0.5458  0.0519\n",
            "    177       0.7564        0.5288       0.7359        \u001b[31m0.5452\u001b[0m  0.0536\n",
            "    178       0.7548        0.5306       0.7293        0.5467  0.0529\n",
            "    179       0.7539        0.5301       0.7326        0.5454  0.0519\n",
            "    180       0.7506        0.5296       0.7315        0.5461  0.0529\n",
            "    181       0.7562        0.5300       0.7293        0.5460  0.0530\n",
            "    182       0.7515        \u001b[32m0.5277\u001b[0m       0.7282        0.5470  0.0590\n",
            "    183       0.7542        0.5303       0.7238        0.5483  0.0524\n",
            "    184       0.7517        0.5298       0.7315        0.5473  0.0603\n",
            "    185       0.7562        0.5300       0.7293        0.5459  0.0526\n",
            "    186       0.7545        0.5293       0.7293        0.5455  0.0535\n",
            "    187       0.7520        0.5291       0.7304        0.5453  0.0520\n",
            "    188       0.7537        0.5289       0.7282        0.5472  0.0528\n",
            "    189       0.7520        0.5309       0.7293        0.5466  0.0800\n",
            "    190       0.7539        0.5280       0.7359        0.5462  0.0579\n",
            "    191       0.7562        0.5301       0.7326        0.5458  0.0527\n",
            "    192       0.7562        0.5285       0.7326        0.5458  0.0519\n",
            "    193       0.7542        0.5288       0.7348        0.5459  0.0526\n",
            "    194       0.7581        0.5301       0.7348        0.5460  0.0534\n",
            "    195       0.7550        0.5290       0.7348        0.5464  0.0543\n",
            "    196       0.7534        0.5302       0.7337        0.5457  0.0524\n",
            "    197       0.7523        0.5289       0.7337        0.5457  0.0518\n",
            "    198       0.7550        0.5304       0.7282        0.5455  0.0572\n",
            "    199       0.7503        0.5285       0.7326        0.5456  0.0585\n",
            "    200       0.7548        0.5287       0.7304        0.5458  0.0590\n"
          ]
        }
      ],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('scale', StandardScaler()),\n",
        "    ('net', net),\n",
        "])\n",
        "\n",
        "pipe.fit(X1.values.astype(np.float32), Y1.values.ravel().astype(np.int64))\n",
        "y_proba = pipe.predict_proba(X1.values.astype(np.float32))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca651d54-9883-49e4-9e93-82c1ec55f865",
      "metadata": {
        "id": "ca651d54-9883-49e4-9e93-82c1ec55f865"
      },
      "source": [
        "## Using sklearn grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88d0a8dc-4b2d-4d08-9cf6-3c02c69742a4",
      "metadata": {
        "id": "88d0a8dc-4b2d-4d08-9cf6-3c02c69742a4",
        "outputId": "3048c8ca-de72-476b-d694-08346088ab91"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'X' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[107], line 20\u001b[0m\n\u001b[1;32m     12\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.02\u001b[39m],\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m],\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodule__hidden_units\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m],\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdefault_params,\n\u001b[1;32m     17\u001b[0m }\n\u001b[1;32m     18\u001b[0m gs \u001b[38;5;241m=\u001b[39m GridSearchCV(net, params, refit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m gs\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX\u001b[49m, y)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest score: \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m, best params: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(gs\u001b[38;5;241m.\u001b[39mbest_score_, gs\u001b[38;5;241m.\u001b[39mbest_params_))\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# deactivate skorch-internal train-valid split and verbose logging\n",
        "net.set_params(train_split=False, verbose=0)\n",
        "\n",
        "default_params = {\n",
        "    'module__input_dim': [12],\n",
        "    'module__output_dim': [2],\n",
        "}\n",
        "\n",
        "# module specific parameters need to begin with 'module__'\n",
        "params = {\n",
        "    'lr': [0.01, 0.02],\n",
        "    'max_epochs': [10, 20],\n",
        "    'module__hidden_units': [10, 20],\n",
        "    **default_params,\n",
        "}\n",
        "gs = GridSearchCV(net, params, refit=False, cv=3, scoring='accuracy', verbose=2)\n",
        "\n",
        "gs.fit(X, y)\n",
        "print(\"best score: {:.3f}, best params: {}\".format(gs.best_score_, gs.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "78e7f350",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hidden units: 1, Train Acc: 0.8830522532485485,Valid Acc: 0.8827865831183193 \n",
            "Hidden units: 5, Train Acc: 0.8830522532485485,Valid Acc: 0.8827865831183193 \n",
            "Hidden units: 10, Train Acc: 0.8830522532485485,Valid Acc: 0.8827865831183193 \n",
            "Hidden units: 15, Train Acc: 0.8830522532485485,Valid Acc: 0.8827865831183193 \n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X1, Y1, test_size=0.1, random_state=21)\n",
        "\n",
        "hidden_units_list = [1,5,10, 15]  # Different widths to test\n",
        "train_accuracies = []\n",
        "valid_accuracies = []\n",
        "valid_aucs = []\n",
        "\n",
        "for hidden_units in hidden_units_list:\n",
        "    net = NeuralNetClassifier(\n",
        "        module=BackpropModule,\n",
        "        module__input_dim=X1.shape[1],\n",
        "        module__output_dim=2,  # Assuming target is categorical\n",
        "        module__hidden_units=hidden_units,\n",
        "        module__hidden_layers=1,  # Fixed number of hidden layers\n",
        "        max_epochs=20,  # Number of epochs for training\n",
        "        verbose=0,  # Suppress verbose output for clarity\n",
        "        callbacks=[\n",
        "            EpochScoring(scoring='accuracy', name='train_acc', on_train=True),\n",
        "            #EpochScoring(scoring='accuracy', name='valid_acc', lower_is_better=False),\n",
        "            #EpochScoring(scoring='roc_auc', name='valid_auc', lower_is_better=False),\n",
        "        ],\n",
        "        criterion=nn.CrossEntropyLoss,\n",
        "        optimizer=torch.optim.SGD,\n",
        "        lr=0.5,\n",
        "        iterator_train__shuffle=True,\n",
        "    )\n",
        "\n",
        "    net.fit(X1.values.astype(np.float32), Y1.values.ravel().astype(np.int64))\n",
        "\n",
        "    train_acc = net.history[-1, 'train_acc']\n",
        "    valid_acc = net.history[-1, 'valid_acc']\n",
        "    train_accuracies.append(train_acc)\n",
        "    valid_accuracies.append(valid_acc)\n",
        "    print(f'Hidden units: {hidden_units}, Train Acc: {train_acc},Valid Acc: {valid_acc} ')\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ba43b70",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'batches': [{'train_loss': 0.9882130026817322, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.49303191900253296, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.39758503437042236, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.41578659415245056, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.44001591205596924, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4707050919532776, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4660722613334656, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4699136018753052, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.42321330308914185, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4577793478965759, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.42786705493927, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.43149587512016296, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4028521776199341, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.42452019453048706, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.5006234049797058, 'train_batch_size': 16},\n",
              "   {'valid_loss': 0.47411057353019714, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.37435707449913025, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.44868794083595276, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.44743505120277405, 'valid_batch_size': 69}],\n",
              "  'epoch': 1,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.0498957633972168,\n",
              "  'train_loss': 0.47940738380482767,\n",
              "  'train_loss_best': True,\n",
              "  'valid_loss': 0.43467755829524785,\n",
              "  'valid_loss_best': True,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': True,\n",
              "  'train_acc': 0.8329646017699115,\n",
              "  'train_acc_best': True},\n",
              " {'batches': [{'train_loss': 0.44111645221710205, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.3993428945541382, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.3937452435493469, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.459674209356308, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.45466098189353943, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.45020148158073425, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4085928499698639, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4191887676715851, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.48696017265319824, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4169132709503174, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4031357765197754, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4477466642856598, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.3781708776950836, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4950644373893738, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.5639712810516357, 'train_batch_size': 16},\n",
              "   {'valid_loss': 0.47345802187919617, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.3736668527126312, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.4482579529285431, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.4468623399734497, 'valid_batch_size': 69}],\n",
              "  'epoch': 2,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.06627511978149414,\n",
              "  'train_loss': 0.4336290612684942,\n",
              "  'train_loss_best': True,\n",
              "  'valid_loss': 0.4340894114365904,\n",
              "  'valid_loss_best': True,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False},\n",
              " {'batches': [{'train_loss': 0.36907145380973816, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.47310298681259155, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.42564478516578674, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4083895981311798, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.425569623708725, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4254935383796692, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4341152012348175, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.43965184688568115, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.44166672229766846, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.392433226108551, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.5324549078941345, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4087561070919037, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.43854811787605286, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4405857026576996, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4448876976966858, 'train_batch_size': 16},\n",
              "   {'valid_loss': 0.4730719029903412, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.37325724959373474, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.4480223059654236, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.44650864601135254, 'valid_batch_size': 69}],\n",
              "  'epoch': 3,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.04321789741516113,\n",
              "  'train_loss': 0.4326438782489405,\n",
              "  'train_loss_best': True,\n",
              "  'valid_loss': 0.43374411317686373,\n",
              "  'valid_loss_best': True,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False},\n",
              " {'batches': [{'train_loss': 0.43144744634628296, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.43862754106521606, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4242510199546814, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.43260881304740906, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.3965916633605957, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4024549722671509, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.3850560188293457, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.42305150628089905, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4401056170463562, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4236293137073517, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4307463765144348, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.45501798391342163, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.5109341144561768, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4552029073238373, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4390576183795929, 'train_batch_size': 16},\n",
              "   {'valid_loss': 0.4727713465690613, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.37293732166290283, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.4478064477443695, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.44623544812202454, 'valid_batch_size': 69}],\n",
              "  'epoch': 4,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.05094170570373535,\n",
              "  'train_loss': 0.4321846015157953,\n",
              "  'train_loss_best': True,\n",
              "  'valid_loss': 0.43346618270505605,\n",
              "  'valid_loss_best': True,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False},\n",
              " {'batches': [{'train_loss': 0.44759601354599, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.40103065967559814, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.3918555676937103, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.43217501044273376, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.40010878443717957, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4705568850040436, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.47972923517227173, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.39232558012008667, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.44241538643836975, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4307084083557129, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4627555012702942, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4464016258716583, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.42423760890960693, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4230138659477234, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4385792911052704, 'train_batch_size': 16},\n",
              "   {'valid_loss': 0.47262343764305115, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.3727736473083496, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.4477241337299347, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.44608747959136963, 'valid_batch_size': 69}],\n",
              "  'epoch': 5,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.04198575019836426,\n",
              "  'train_loss': 0.4318394721609301,\n",
              "  'train_loss_best': True,\n",
              "  'valid_loss': 0.43333234455411795,\n",
              "  'valid_loss_best': True,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False},\n",
              " {'batches': [{'train_loss': 0.40803760290145874, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.41554978489875793, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.45426395535469055, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.45478036999702454, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.41522374749183655, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.3918490409851074, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4400085210800171, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4243999421596527, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.487155020236969, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.41579097509384155, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.41626909375190735, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.44718268513679504, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.43135470151901245, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4262450933456421, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.5632649064064026, 'train_batch_size': 16},\n",
              "   {'valid_loss': 0.4725007116794586, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.37263810634613037, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.4476556181907654, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.44596385955810547, 'valid_batch_size': 69}],\n",
              "  'epoch': 6,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.0668644905090332,\n",
              "  'train_loss': 0.4317535325489213,\n",
              "  'train_loss_best': True,\n",
              "  'valid_loss': 0.4332211791284826,\n",
              "  'valid_loss_best': True,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False},\n",
              " {'batches': [{'train_loss': 0.401211142539978, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4308999180793762, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4160522520542145, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.47809359431266785, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.45443567633628845, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4623699486255646, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.3854788541793823, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4940875172615051, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.42389628291130066, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.392255961894989, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.433230459690094, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.45407113432884216, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4502808451652527, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.37622684240341187, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.38021591305732727, 'train_batch_size': 16},\n",
              "   {'valid_loss': 0.4723447859287262, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.3724651038646698, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.44755929708480835, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.44580498337745667, 'valid_batch_size': 69}],\n",
              "  'epoch': 7,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.04214000701904297,\n",
              "  'train_loss': 0.4318667199231882,\n",
              "  'train_loss_best': False,\n",
              "  'valid_loss': 0.4330768206919529,\n",
              "  'valid_loss_best': True,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False},\n",
              " {'batches': [{'train_loss': 0.47770529985427856, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4546646177768707, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.44093871116638184, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.385089635848999, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.41755416989326477, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.42324167490005493, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4465350806713104, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.42353731393814087, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4469868540763855, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.3923289179801941, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4540696442127228, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4555472135543823, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.392788827419281, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4394252300262451, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.3759017884731293, 'train_batch_size': 16},\n",
              "   {'valid_loss': 0.4722542464733124, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.3723623752593994, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.4475100338459015, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.44571080803871155, 'valid_batch_size': 69}],\n",
              "  'epoch': 8,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.043005943298339844,\n",
              "  'train_loss': 0.4316744010532852,\n",
              "  'train_loss_best': True,\n",
              "  'valid_loss': 0.4329939462885951,\n",
              "  'valid_loss_best': True,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False},\n",
              " {'batches': [{'train_loss': 0.4792855381965637, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4405427575111389, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.41518300771713257, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.3920884430408478, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4466035068035126, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4857693314552307, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.42370158433914185, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.37590089440345764, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.439669132232666, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.40783774852752686, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.39277517795562744, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.43151620030403137, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4700959324836731, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.43857574462890625, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.5007644891738892, 'train_batch_size': 16},\n",
              "   {'valid_loss': 0.47218045592308044, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.37227872014045715, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.44747257232666016, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.44563257694244385, 'valid_batch_size': 69}],\n",
              "  'epoch': 9,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.043828725814819336,\n",
              "  'train_loss': 0.4320099512032703,\n",
              "  'train_loss_best': False,\n",
              "  'valid_loss': 0.4329269571809579,\n",
              "  'valid_loss_best': True,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8821902654867256,\n",
              "  'train_acc_best': False},\n",
              " {'batches': [{'train_loss': 0.4084760248661041, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4484121799468994, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.43062305450439453, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.43926316499710083, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.40775829553604126, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4313983917236328, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.44908779859542847, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4774307608604431, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4073987305164337, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.44716915488243103, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4623378813266754, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4466325044631958, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.3708759844303131, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.415702760219574, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4386166036128998, 'train_batch_size': 16},\n",
              "   {'valid_loss': 0.47210901975631714, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.37219560146331787, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.4474351704120636, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.4455563724040985, 'valid_batch_size': 69}],\n",
              "  'epoch': 10,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.04238438606262207,\n",
              "  'train_loss': 0.43167389467754197,\n",
              "  'train_loss_best': True,\n",
              "  'valid_loss': 0.4328611104298901,\n",
              "  'valid_loss_best': True,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False},\n",
              " {'batches': [{'train_loss': 0.4933044910430908, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4027075171470642, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.43163567781448364, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.44636672735214233, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4707813560962677, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.42340385913848877, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4084189534187317, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.3769720494747162, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.438318133354187, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.45453956723213196, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.42397961020469666, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4231688976287842, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4230738580226898, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.43118372559547424, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.375777006149292, 'train_batch_size': 16},\n",
              "   {'valid_loss': 0.47205039858818054, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.372128427028656, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.44740599393844604, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.4454934597015381, 'valid_batch_size': 69}],\n",
              "  'epoch': 11,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.048410654067993164,\n",
              "  'train_loss': 0.4314921450825919,\n",
              "  'train_loss_best': True,\n",
              "  'valid_loss': 0.4328077386809764,\n",
              "  'valid_loss_best': True,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False},\n",
              " {'batches': [{'train_loss': 0.44647881388664246, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.45433998107910156, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4009913206100464, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4231126606464386, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4777366816997528, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4080127477645874, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4306076169013977, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4167098104953766, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.41629546880722046, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.48656049370765686, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4230388104915619, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4228323698043823, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4154224097728729, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4163653552532196, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4382624328136444, 'train_batch_size': 16},\n",
              "   {'valid_loss': 0.47199609875679016, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.37206608057022095, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.4473794102668762, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.4454348385334015, 'valid_batch_size': 69}],\n",
              "  'epoch': 12,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.06561946868896484,\n",
              "  'train_loss': 0.43138317486881156,\n",
              "  'train_loss_best': True,\n",
              "  'valid_loss': 0.4327583384698064,\n",
              "  'valid_loss_best': True,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False},\n",
              " {'batches': [{'train_loss': 0.43095147609710693, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4074130356311798, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4385804533958435, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4234217703342438, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4003756642341614, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.47134798765182495, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.46180570125579834, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.47853171825408936, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.47769424319267273, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4385421872138977, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.41602420806884766, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4331265389919281, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.37664738297462463, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.3998699486255646, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.313273549079895, 'train_batch_size': 16},\n",
              "   {'valid_loss': 0.4719473421573639, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.3720110356807709, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.44735535979270935, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.4453820288181305, 'valid_batch_size': 69}],\n",
              "  'epoch': 13,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.07298994064331055,\n",
              "  'train_loss': 0.4313976289951696,\n",
              "  'train_loss_best': False,\n",
              "  'valid_loss': 0.4327141686648986,\n",
              "  'valid_loss_best': True,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False},\n",
              " {'batches': [{'train_loss': 0.39186882972717285, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.455398291349411, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4389360547065735, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.44809120893478394, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4320083558559418, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.43120741844177246, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4235292375087738, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.36885711550712585, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4404570758342743, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4633393883705139, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.46180543303489685, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4777936339378357, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.39211463928222656, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.43163082003593445, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.3145117461681366, 'train_batch_size': 16},\n",
              "   {'valid_loss': 0.47188475728034973, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.3719395697116852, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.44732341170310974, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.44531458616256714, 'valid_batch_size': 69}],\n",
              "  'epoch': 14,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.06467485427856445,\n",
              "  'train_loss': 0.43160010412731004,\n",
              "  'train_loss_best': False,\n",
              "  'valid_loss': 0.4326569911659948,\n",
              "  'valid_loss_best': True,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False},\n",
              " {'batches': [{'train_loss': 0.39987805485725403, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4094867408275604, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.43846365809440613, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4404083490371704, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4230191707611084, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4316374957561493, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.40711864829063416, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4463798701763153, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4465859532356262, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4388280510902405, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4542537331581116, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4774104356765747, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4023493826389313, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.43877649307250977, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.31331151723861694, 'train_batch_size': 16},\n",
              "   {'valid_loss': 0.4718448519706726, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.371893972158432, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.44730353355407715, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.4452725350856781, 'valid_batch_size': 69}],\n",
              "  'epoch': 15,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.06488513946533203,\n",
              "  'train_loss': 0.43141663549220666,\n",
              "  'train_loss_best': False,\n",
              "  'valid_loss': 0.43262080950189635,\n",
              "  'valid_loss_best': True,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False},\n",
              " {'batches': [{'train_loss': 0.43203091621398926, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.47746989130973816, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4393041729927063, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.41650688648223877, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.422955721616745, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4931766390800476, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4315081536769867, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.40003710985183716, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4707605242729187, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4073317348957062, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.42392563819885254, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.39152708649635315, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.42360740900039673, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.39264848828315735, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.5634827017784119, 'train_batch_size': 16},\n",
              "   {'valid_loss': 0.4718047082424164, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.3718474209308624, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.44728317856788635, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.44522905349731445, 'valid_batch_size': 69}],\n",
              "  'epoch': 16,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.04272150993347168,\n",
              "  'train_loss': 0.43137881133408673,\n",
              "  'train_loss_best': True,\n",
              "  'valid_loss': 0.43258393837126674,\n",
              "  'valid_loss_best': True,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False},\n",
              " {'batches': [{'train_loss': 0.44654974341392517, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.493003249168396, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.42321687936782837, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.469573974609375, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.43275701999664307, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4308914542198181, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4548141658306122, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.40900248289108276, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4072980582714081, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4079166352748871, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.39205241203308105, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.3918536901473999, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4237433075904846, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.44617143273353577, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.5007616281509399, 'train_batch_size': 16},\n",
              "   {'valid_loss': 0.4717683494091034, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.37180525064468384, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.4472654461860657, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.44519007205963135, 'valid_batch_size': 69}],\n",
              "  'epoch': 17,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.0428011417388916,\n",
              "  'train_loss': 0.43125236878352885,\n",
              "  'train_loss_best': True,\n",
              "  'valid_loss': 0.43255080108347843,\n",
              "  'valid_loss_best': True,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False},\n",
              " {'batches': [{'train_loss': 0.45441192388534546, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.44685274362564087, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4477755129337311, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4078388810157776, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4078558683395386, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.3998253345489502, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4384021759033203, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.3999055027961731, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4149750769138336, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.43950167298316956, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4315149784088135, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.48641565442085266, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4310857951641083, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.44621098041534424, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.31465980410575867, 'train_batch_size': 16},\n",
              "   {'valid_loss': 0.4717223644256592, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.3717520534992218, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.447243869304657, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.4451402425765991, 'valid_batch_size': 69}],\n",
              "  'epoch': 18,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.04938864707946777,\n",
              "  'train_loss': 0.43128527977825265,\n",
              "  'train_loss_best': False,\n",
              "  'valid_loss': 0.4325090894109629,\n",
              "  'valid_loss_best': True,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False},\n",
              " {'batches': [{'train_loss': 0.3999824523925781, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.48551082611083984, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4464515447616577, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.43835192918777466, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4013258218765259, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.39984309673309326, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4308607280254364, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4309593141078949, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.43133682012557983, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4550302028656006, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4082745313644409, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.40710166096687317, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4707964062690735, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.43058741092681885, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4382767379283905, 'train_batch_size': 16},\n",
              "   {'valid_loss': 0.4716891944408417, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.37171411514282227, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.4472271203994751, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.44510483741760254, 'valid_batch_size': 69}],\n",
              "  'epoch': 19,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.13360953330993652,\n",
              "  'train_loss': 0.4312352097667424,\n",
              "  'train_loss_best': True,\n",
              "  'valid_loss': 0.43247887156657033,\n",
              "  'valid_loss_best': True,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False},\n",
              " {'batches': [{'train_loss': 0.4560995399951935, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4386051893234253, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4153251051902771, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.41663995385169983, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4478759467601776, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.47839757800102234, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.3836475610733032, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4620654284954071, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4149796664714813, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.39975547790527344, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4161772131919861, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.42279326915740967, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4621453285217285, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.42314469814300537, 'train_batch_size': 128},\n",
              "   {'train_loss': 0.4385654330253601, 'train_batch_size': 16},\n",
              "   {'valid_loss': 0.47165629267692566, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.37167632579803467, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.44721052050590515, 'valid_batch_size': 128},\n",
              "   {'valid_loss': 0.4450698494911194, 'valid_batch_size': 69}],\n",
              "  'epoch': 20,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.1535627841949463,\n",
              "  'train_loss': 0.43132549629802197,\n",
              "  'train_loss_best': False,\n",
              "  'valid_loss': 0.4324488772724901,\n",
              "  'valid_loss_best': True,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False}]"
            ]
          },
          "execution_count": 157,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net.history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "380087d6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'batches': [{'train_loss': 0.8532590866088867, 'train_batch_size': 128},\n",
              "   {'train_loss': 276.5985412597656, 'train_batch_size': 128},\n",
              "   {'train_loss': 171.8120880126953, 'train_batch_size': 128},\n",
              "   {'train_loss': 267.4521789550781, 'train_batch_size': 128},\n",
              "   {'train_loss': 1190.329833984375, 'train_batch_size': 128},\n",
              "   {'train_loss': 826.4951171875, 'train_batch_size': 128},\n",
              "   {'train_loss': 520.3899536132812, 'train_batch_size': 128},\n",
              "   {'train_loss': 495203.8125, 'train_batch_size': 128},\n",
              "   {'train_loss': 4300234.0, 'train_batch_size': 128},\n",
              "   {'train_loss': 1.3066996690190336e+16, 'train_batch_size': 128},\n",
              "   {'train_loss': 2897407639552.0, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 16},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 69}],\n",
              "  'epoch': 1,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.0329740047454834,\n",
              "  'train_loss': nan,\n",
              "  'train_loss_best': False,\n",
              "  'valid_loss': nan,\n",
              "  'valid_loss_best': False,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': True,\n",
              "  'train_acc': 0.6095132743362832,\n",
              "  'train_acc_best': True},\n",
              " {'batches': [{'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 16},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 69}],\n",
              "  'epoch': 2,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.048258304595947266,\n",
              "  'train_loss': nan,\n",
              "  'train_loss_best': False,\n",
              "  'valid_loss': nan,\n",
              "  'valid_loss_best': False,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False},\n",
              " {'batches': [{'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 16},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 69}],\n",
              "  'epoch': 3,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.033440351486206055,\n",
              "  'train_loss': nan,\n",
              "  'train_loss_best': False,\n",
              "  'valid_loss': nan,\n",
              "  'valid_loss_best': False,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False},\n",
              " {'batches': [{'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 16},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 69}],\n",
              "  'epoch': 4,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.03937268257141113,\n",
              "  'train_loss': nan,\n",
              "  'train_loss_best': False,\n",
              "  'valid_loss': nan,\n",
              "  'valid_loss_best': False,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False},\n",
              " {'batches': [{'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 16},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 69}],\n",
              "  'epoch': 5,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.03308820724487305,\n",
              "  'train_loss': nan,\n",
              "  'train_loss_best': False,\n",
              "  'valid_loss': nan,\n",
              "  'valid_loss_best': False,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False},\n",
              " {'batches': [{'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 16},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 69}],\n",
              "  'epoch': 6,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.031857967376708984,\n",
              "  'train_loss': nan,\n",
              "  'train_loss_best': False,\n",
              "  'valid_loss': nan,\n",
              "  'valid_loss_best': False,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False},\n",
              " {'batches': [{'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 16},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 69}],\n",
              "  'epoch': 7,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.07709765434265137,\n",
              "  'train_loss': nan,\n",
              "  'train_loss_best': False,\n",
              "  'valid_loss': nan,\n",
              "  'valid_loss_best': False,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False},\n",
              " {'batches': [{'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 16},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 69}],\n",
              "  'epoch': 8,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.05494379997253418,\n",
              "  'train_loss': nan,\n",
              "  'train_loss_best': False,\n",
              "  'valid_loss': nan,\n",
              "  'valid_loss_best': False,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False},\n",
              " {'batches': [{'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 16},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 69}],\n",
              "  'epoch': 9,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.04038715362548828,\n",
              "  'train_loss': nan,\n",
              "  'train_loss_best': False,\n",
              "  'valid_loss': nan,\n",
              "  'valid_loss_best': False,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False},\n",
              " {'batches': [{'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 16},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 69}],\n",
              "  'epoch': 10,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.03358173370361328,\n",
              "  'train_loss': nan,\n",
              "  'train_loss_best': False,\n",
              "  'valid_loss': nan,\n",
              "  'valid_loss_best': False,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False},\n",
              " {'batches': [{'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 16},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 69}],\n",
              "  'epoch': 11,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.03350090980529785,\n",
              "  'train_loss': nan,\n",
              "  'train_loss_best': False,\n",
              "  'valid_loss': nan,\n",
              "  'valid_loss_best': False,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False},\n",
              " {'batches': [{'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 16},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 69}],\n",
              "  'epoch': 12,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.03242850303649902,\n",
              "  'train_loss': nan,\n",
              "  'train_loss_best': False,\n",
              "  'valid_loss': nan,\n",
              "  'valid_loss_best': False,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False},\n",
              " {'batches': [{'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 16},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 69}],\n",
              "  'epoch': 13,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.03442120552062988,\n",
              "  'train_loss': nan,\n",
              "  'train_loss_best': False,\n",
              "  'valid_loss': nan,\n",
              "  'valid_loss_best': False,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False},\n",
              " {'batches': [{'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 16},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 69}],\n",
              "  'epoch': 14,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.032315969467163086,\n",
              "  'train_loss': nan,\n",
              "  'train_loss_best': False,\n",
              "  'valid_loss': nan,\n",
              "  'valid_loss_best': False,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False},\n",
              " {'batches': [{'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 16},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 69}],\n",
              "  'epoch': 15,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.03167128562927246,\n",
              "  'train_loss': nan,\n",
              "  'train_loss_best': False,\n",
              "  'valid_loss': nan,\n",
              "  'valid_loss_best': False,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False},\n",
              " {'batches': [{'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 16},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 69}],\n",
              "  'epoch': 16,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.032965660095214844,\n",
              "  'train_loss': nan,\n",
              "  'train_loss_best': False,\n",
              "  'valid_loss': nan,\n",
              "  'valid_loss_best': False,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False},\n",
              " {'batches': [{'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 16},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 69}],\n",
              "  'epoch': 17,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.03312563896179199,\n",
              "  'train_loss': nan,\n",
              "  'train_loss_best': False,\n",
              "  'valid_loss': nan,\n",
              "  'valid_loss_best': False,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False},\n",
              " {'batches': [{'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 16},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 69}],\n",
              "  'epoch': 18,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.031740427017211914,\n",
              "  'train_loss': nan,\n",
              "  'train_loss_best': False,\n",
              "  'valid_loss': nan,\n",
              "  'valid_loss_best': False,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False},\n",
              " {'batches': [{'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 16},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 69}],\n",
              "  'epoch': 19,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.035584449768066406,\n",
              "  'train_loss': nan,\n",
              "  'train_loss_best': False,\n",
              "  'valid_loss': nan,\n",
              "  'valid_loss_best': False,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False},\n",
              " {'batches': [{'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 128},\n",
              "   {'train_loss': nan, 'train_batch_size': 16},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 128},\n",
              "   {'valid_loss': nan, 'valid_batch_size': 69}],\n",
              "  'epoch': 20,\n",
              "  'train_batch_count': 15,\n",
              "  'valid_batch_count': 4,\n",
              "  'dur': 0.03283548355102539,\n",
              "  'train_loss': nan,\n",
              "  'train_loss_best': False,\n",
              "  'valid_loss': nan,\n",
              "  'valid_loss_best': False,\n",
              "  'valid_acc': 0.8830022075055187,\n",
              "  'valid_acc_best': False,\n",
              "  'train_acc': 0.8827433628318584,\n",
              "  'train_acc_best': False}]"
            ]
          },
          "execution_count": 143,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_acc = net.history[-1, 'train_acc']\n",
        "    valid_acc = net.history[-1, 'valid_acc']\n",
        "    valid_auc = net.history[-1, 'valid_auc']\n",
        "    \n",
        "    train_accuracies.append(train_acc)\n",
        "    valid_accuracies.append(valid_acc)\n",
        "    valid_aucs.append(valid_auc)\n",
        "\n",
        "    print(f'Hidden units: {hidden_units}, Train Acc: {train_acc}, Valid Acc: {valid_acc}, Valid AUC: {valid_auc}')\n",
        "\n",
        "net.history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a1887b4",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_acc = net.history[-1, 'train_acc']\n",
        "    valid_acc = net.history[-1, 'valid_acc']\n",
        "    #valid_auc = net.history[-1, 'valid_auc']\n",
        "    \n",
        "    train_accuracies.append(train_acc)\n",
        "    #valid_accuracies.append(valid_acc)\n",
        "    #valid_aucs.append(valid_auc)\n",
        "\n",
        "    #print(f'Hidden units: {hidden_units}, Train Acc: {train_acc}, Valid Acc: {valid_acc}, Valid AUC: {valid_auc}')\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(hidden_units_list, train_accuracies, label='Train Accuracy')\n",
        "#plt.plot(hidden_units_list, valid_accuracies, label='Validation Accuracy')\n",
        "#plt.plot(hidden_units_list, valid_aucs, label='Validation AUC')\n",
        "plt.xlabel('Number of Hidden Units')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Validation Curve for Different Neural Net Widths')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
